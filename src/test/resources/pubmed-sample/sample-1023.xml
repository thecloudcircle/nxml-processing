
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Photoacoustics</journal-id><journal-id journal-id-type="iso-abbrev">Photoacoustics</journal-id><journal-title-group><journal-title>Photoacoustics</journal-title></journal-title-group><issn pub-type="epub">2213-5979</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">33194545</article-id><article-id pub-id-type="pmc">7644749</article-id><article-id pub-id-type="pii">S2213-5979(20)30043-4</article-id><article-id pub-id-type="doi">10.1016/j.pacs.2020.100203</article-id><article-id pub-id-type="publisher-id">100203</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A sparse deep learning approach for automatic segmentation of human vasculature in multispectral optoacoustic tomography</article-title></title-group><contrib-group><contrib contrib-type="author" id="aut0005"><name><surname>Chlis</surname><given-names>Nikolaos-Kosmas</given-names></name><xref rid="aff0005" ref-type="aff">a</xref><xref rid="aff0010" ref-type="aff">b</xref><xref rid="aff0035" ref-type="aff">g</xref><xref rid="fn0005" ref-type="fn">1</xref></contrib><contrib contrib-type="author" id="aut0010"><name><surname>Karlas</surname><given-names>Angelos</given-names></name><xref rid="aff0010" ref-type="aff">b</xref><xref rid="aff0015" ref-type="aff">c</xref><xref rid="aff0020" ref-type="aff">d</xref><xref rid="aff0025" ref-type="aff">e</xref><xref rid="fn0005" ref-type="fn">1</xref></contrib><contrib contrib-type="author" id="aut0015"><name><surname>Fasoula</surname><given-names>Nikolina-Alexia</given-names></name><xref rid="aff0010" ref-type="aff">b</xref><xref rid="aff0015" ref-type="aff">c</xref></contrib><contrib contrib-type="author" id="aut0020"><name><surname>Kallmayer</surname><given-names>Michael</given-names></name><xref rid="aff0025" ref-type="aff">e</xref></contrib><contrib contrib-type="author" id="aut0025"><name><surname>Eckstein</surname><given-names>Hans-Henning</given-names></name><xref rid="aff0025" ref-type="aff">e</xref></contrib><contrib contrib-type="author" id="aut0030"><name><surname>Theis</surname><given-names>Fabian J.</given-names></name><xref rid="aff0005" ref-type="aff">a</xref><xref rid="aff0030" ref-type="aff">f</xref></contrib><contrib contrib-type="author" id="aut0035"><name><surname>Ntziachristos</surname><given-names>Vasilis</given-names></name><xref rid="aff0010" ref-type="aff">b</xref><xref rid="aff0015" ref-type="aff">c</xref><xref rid="aff0020" ref-type="aff">d</xref><xref rid="fn0005" ref-type="fn">1</xref></contrib><contrib contrib-type="author" id="aut0040"><name><surname>Marr</surname><given-names>Carsten</given-names></name><email>carsten.marr@helmholtz-muenchen.de</email><xref rid="aff0005" ref-type="aff">a</xref><xref rid="cor0005" ref-type="corresp">*</xref><xref rid="fn0005" ref-type="fn">1</xref></contrib><aff id="aff0005"><label>a</label>Institute of Computational Biology, Helmholtz Center Munich, Neuherberg, Germany</aff><aff id="aff0010"><label>b</label>Institute of Biological and Medical Imaging, Helmholtz Center Munich, Neuherberg, Germany</aff><aff id="aff0015"><label>c</label>Chair of Biological Imaging and Center for Translational Cancer Research (TranslaTUM), Munich, Germany</aff><aff id="aff0020"><label>d</label>DZHK (German Centre for Cardiovascular Research), Partner Site Munich Heart Alliance, Munich, Germany</aff><aff id="aff0025"><label>e</label>Clinic for Vascular and Endovascular Surgery, Rechts Der Isar Hospital, Munich, Germany</aff><aff id="aff0030"><label>f</label>Department of Mathematics, Technical University of Munich, Munich, Germany</aff><aff id="aff0035"><label>g</label>Roche Pharma Research and Early Development, Large Molecule Research, Roche Innovation Center Munich, Penzberg 82377, Germany</aff></contrib-group><author-notes><corresp id="cor0005"><label>&#x0204e;</label>Corresponding author. <email>carsten.marr@helmholtz-muenchen.de</email></corresp><fn id="fn0005"><label>1</label><p id="npar0005">Equal contribution.</p></fn></author-notes><pub-date pub-type="pmc-release"><day>10</day><month>9</month><year>2020</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><month>12</month><year>2020</year></pub-date><pub-date pub-type="epub"><day>10</day><month>9</month><year>2020</year></pub-date><volume>20</volume><elocation-id>100203</elocation-id><history><date date-type="received"><day>14</day><month>2</month><year>2020</year></date><date date-type="rev-recd"><day>20</day><month>7</month><year>2020</year></date><date date-type="accepted"><day>26</day><month>7</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; 2020 The Authors</copyright-statement><copyright-year>2020</copyright-year><license license-type="CC BY" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><abstract id="abs0005"><p>Multispectral Optoacoustic Tomography (MSOT) resolves oxy- (HbO<sub>2</sub>) and deoxy-hemoglobin (Hb) to perform vascular imaging. MSOT suffers from gradual signal attenuation with depth due to light-tissue interactions: an effect that hinders the precise manual segmentation of vessels. Furthermore, vascular assessment requires functional tests, which last several minutes and result in recording thousands of images. Here, we introduce a deep learning approach with a sparse-UNET (S-UNET) for automatic vascular segmentation in MSOT images to avoid the rigorous and time-consuming manual segmentation. We evaluated the S-UNET on a test-set of 33 images, achieving a median DICE score of 0.88. Apart from high segmentation performance, our method based its decision on two wavelengths with physical meaning for the task-at-hand: 850&#x02009;nm (peak absorption of oxy-hemoglobin) and 810&#x02009;nm (isosbestic point of oxy-and deoxy-hemoglobin). Thus, our approach achieves precise data-driven vascular segmentation for automated vascular assessment and may boost MSOT further towards its clinical translation.</p></abstract><kwd-group id="kwd0005"><title>Keywords</title><kwd>Segmentation</kwd><kwd>Deep learning</kwd><kwd>Machine learning</kwd><kwd>Artificial intelligence</kwd><kwd>Clinical</kwd><kwd>Translational</kwd><kwd>Multispectral optoacoustic tomography</kwd></kwd-group></article-meta></front><body><sec id="sec0005"><label>1</label><title>Introduction</title><p id="par0005">The abundant presence of hemoglobin in the blood renders multispectral optoacoustic tomography (MSOT) an ideal technique for imaging vasculature [<xref rid="bib0005" ref-type="bibr">[1]</xref>, <xref rid="bib0010" ref-type="bibr">[2]</xref>, <xref rid="bib0015" ref-type="bibr">[3]</xref>]. By illuminating tissue at multiple different light wavelengths at the near infrared range (&#x0223c;680&#x02212;980&#x02009;nm), MSOT is capable of resolving several tissue chromophores, in particular oxy- (HbO<sub>2</sub>) and deoxy-hemoglobin (Hb), with a wide range of clinical applications, such as Crohn&#x02019;s disease, systemic sclerosis, breast cancer, brown adipose tissue imaging and thyroid disease [<xref rid="bib0020" ref-type="bibr">[4]</xref>, <xref rid="bib0025" ref-type="bibr">[5]</xref>, <xref rid="bib0030" ref-type="bibr">[6]</xref>, <xref rid="bib0035" ref-type="bibr">[7]</xref>, <xref rid="bib0040" ref-type="bibr">[8]</xref>]. MSOT can provide precise structural visualizations of arteries and veins by recording multispectral data and resolving the different oxygenation states of human hemoglobin molecule. Moreover, the dynamic nature of the vascular system requires the acquisition not only of structural but also of functional data over multiple seconds or minutes to observe, for example, the vascular wall kinetics during the cardiac cycle or the arterial responses to stimuli such as the transient arterial occlusion or hyperthermia, which are valid descriptors of cardiovascular risk [<xref rid="bib0045" ref-type="bibr">9</xref>,<xref rid="bib0050" ref-type="bibr">10</xref>]. The need to record multispectral data in order to extract molecular information and to perform longitudinal measurements over several minutes radically increases the number of recorded images and the data volume.</p><p id="par0010">Both structural and functional vascular imaging require the precise segmentation of the vascular lumen in several applications, such as the quantification of an atheromatous arterial stenosis, the detection of a venous thrombosis or the tracking of the arterial diameter over a 5-minute arterial occlusion challenge to quantify the degree of endothelial dysfunction. The segmentation of the vascular lumen is usually performed by expert physicians who manually draw the regions of interest (ROIs) on the recorded MSOT images. However, manual segmentation is a time-consuming process, in particular in the case of longitudinal recordings of several minutes and thus of hundreds or thousands of frames. Furthermore, because of the gradual light attenuation due to scattering and absorption when propagating in living tissue, the vascular lumen shows an inhomogeneous and fainting intensity profile with increasing depth, making its manual delineation a challenging process. But even in routine imaging diagnostics, a reliable automated segmentation method can be beneficial by aiding the clinician in performing the same task much faster. Deep learning has been recently shown to be very effective in computer vision tasks [<xref rid="bib0055" ref-type="bibr">11</xref>,<xref rid="bib0060" ref-type="bibr">12</xref>] and segmentation in particular [<xref rid="bib0065" ref-type="bibr">[13]</xref>, <xref rid="bib0070" ref-type="bibr">[14]</xref>, <xref rid="bib0075" ref-type="bibr">[15]</xref>]. As such, deep learning has been successfully applied to clinical diagnostics [<xref rid="bib0080" ref-type="bibr">[16]</xref>, <xref rid="bib0085" ref-type="bibr">[17]</xref>, <xref rid="bib0090" ref-type="bibr">[18]</xref>], with medical image segmentation applications including prostate [<xref rid="bib0095" ref-type="bibr">19</xref>], retinal disease [<xref rid="bib0100" ref-type="bibr">20</xref>], brain [<xref rid="bib0105" ref-type="bibr">21</xref>,<xref rid="bib0110" ref-type="bibr">22</xref>] and cervical cell segmentation [<xref rid="bib0115" ref-type="bibr">23</xref>]. Surveys of deep learning applications for medical imaging can be found in [<xref rid="bib0120" ref-type="bibr">24</xref>,<xref rid="bib0125" ref-type="bibr">25</xref>].</p><p id="par0015">We present herein a pilot study to achieve automated vascular segmentation in clinical raw MSOT images via a deep learning approach, based on an extension of the UNET architecture originally introduced in [<xref rid="bib0075" ref-type="bibr">15</xref>] that is specifically tailored for multispectral optoacoustic data. The proposed Sparse UNET (S-UNET) allows for automated segmentation of vascular ROIs in clinical MSOT images, while simultaneously identifying which of the employed illumination wavelengths are relevant to the specific task. This way we aim at radically reducing the time needed for vascular segmentation in longitudinal scans as well as the number of illumination wavelengths for future task-specific scans, facilitating this way the data analysis, increasing the time resolution and reducing the data volume.</p></sec><sec id="sec0010"><label>2</label><title>Methods</title><sec id="sec0015"><label>2.1</label><title>Network architecture</title><p id="par0020">The proposed Sparse-UNET (S-UNET) is based on the fully convolutional architecture of the original UNET [<xref rid="bib0075" ref-type="bibr">15</xref>], with the added capability of sparse wavelength selection. The goal of S-UNET is to transform each input image with dimensions 400&#x02009;&#x000d7;&#x02009;400&#x02009;&#x000d7;&#x02009;28 (Height x Width x Wavelengths) into a 400&#x02009;&#x000d7;&#x02009;400 probability map <bold><italic>p</italic></bold> that corresponds to a ground truth segmentation mask, while simultaneously assigning a weight <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:msub><mml:mi>w</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> (wavelength importance) to each of the 28 illumination wavelengths (from 700 to 970&#x02009;nm at steps of 10&#x02009;nm), which correspond to the 28 channels of the input image. The ground truth segmentation mask <bold><italic>y</italic></bold> is a binary image (each pixel is either 0 or 1), extracted from the recorded MSOT image in consensus between two clinical MSOT experts. To arrive at a predicted segmentation mask, the resulting S-UNET probability map <bold><italic>p</italic></bold> is discretized by thresholding at 0.5: pixels with probabilities less than 0.5 are set to 0, while the rest are set to 1.</p><p id="par0025">In order to perform wavelength selection, the first layer of the S-UNET corresponds to a 1&#x02009;&#x000d7;&#x02009;1 2D convolution of a single filter and no bias. Given each 400&#x02009;&#x000d7;&#x02009;400&#x02009;&#x000d7;&#x02009;28 input image stack, the first layer essentially performs a linear combination of the 28 wavelengths, resulting in a 400&#x02009;&#x000d7;&#x02009;400&#x02009;&#x000d7;&#x02009;1 image that is forward-propagated to the rest of the network. In this manner, each wavelength is assigned a unique scalar weight. Moreover, to ensure sparsity of wavelength selection we add L1 regularization [<xref rid="bib0130" ref-type="bibr">26</xref>] on the wavelength weights. L1 regularization refers to adding a term &#x003bb;|<bold>&#x003b2;</bold>|, where &#x003bb; is a scalar hyperparameter and <bold>&#x003b2;</bold> refers to the first convolutional layer&#x02019;s trainable parameters, in this case, the parameters of the first 1&#x02009;&#x000d7;&#x02009;1 convolutional layer. Here, we employed a regularization parameter &#x003bb;&#x02009;=&#x02009;0.01. Regularization does not necessarily result in an interpretable model. To ensure interpretability of wavelength selection we force the weights of the first layer to be non-negative. As such, there is no possibility to have irrelevant wavelengths of similar wavelengths cancelling each other out with weights of similar, potentially high, magnitude and opposite signs. Taken together, the two constraints of L1 regularization and non-negative weights ensure that only few relevant wavelengths will be assigned with positive weights, while all other non-relevant wavelengths will be set to zero and will effectively be excluded from the model. After wavelength selection, we add a batch normalization [<xref rid="bib0135" ref-type="bibr">27</xref>] layer between every convolution layer and its respective activation function. The S-UNET architecture employed is visualized in <xref rid="fig0005" ref-type="fig">Fig. 1</xref>.<fig id="fig0005"><label>Fig. 1</label><caption><p>The S-UNET identifies important illumination wavelengths in MSOT images while learning to predict segmentation masks of human blood vessels. Each wavelength is weighted by a corresponding non-negative weight and all weighted wavelengths are combined before being inserted as input into a UNET architecture. Sparsity of wavelength selection is enforced by L1 regularization on the non-negative wavelength weights and the weights themselves are learned through standard back-propagation, along with the rest of the UNET parameters.</p></caption><alt-text id="at0005">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p></sec><sec id="sec0020"><label>2.2</label><title>Training and data augmentation</title><p id="par0030">The original dataset of 164 raw MSOT images was randomly split into training, validation and test sets of 98, 33 and 33 images, respectively.</p><p id="par0035">Each raw MSOT image corresponds to spatial dimensions of 400&#x02009;&#x000d7;&#x02009;400 pixels (which corresponds to 4&#x02009;&#x000d7;&#x02009;4&#x02009;cm) and 28 wavelengths. Each wavelength is normalized to values between 0 and 1 separately, as part of pre-processing. Normalization of the input image is a common practice in deep learning applications [<xref rid="bib0140" ref-type="bibr">28</xref>]. We train the model on a training subset of the data using Adam [<xref rid="bib0145" ref-type="bibr">29</xref>] while evaluating model performance on a validation set. The model is trained for a maximum number of 200 epochs, or until model performance on the validation set has not improved for 20 consecutive epochs (early stopping). The instance of the model that achieved the best performance on the validation set is saved as the final model. We keep a separate test set that is hidden from the model during training.</p><p id="par0040">The model is trained using a batch size of 4 images and data augmentation is performed on-the-fly on each image in every batch to increase model performance. Data augmentation includes flipping the x axis and rotating the image in a random angle from 5 to 15 degrees. Each of the two augmentation schemes has a 50 % probability of being performed on any given image. According to our experiments more aggressive augmentation hinders model performance on the given task. The last layer of the model corresponds to a pixel-wise binary classification problem of computing a probability map of the predicted segmentation mask. The model&#x02019;s loss corresponds to the loss of the 400&#x02009;&#x000d7;&#x02009;400 binary classification tasks. Thus, the total binary cross entropy loss function <italic>L,</italic> is used to train the model:<disp-formula id="eq0005"><mml:math id="M2" altimg="si2.svg"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mrow/></mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:mrow/></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#x022c5;</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mo>.</mml:mo></mml:math></disp-formula>Here, <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mi>H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mi>W</mml:mi></mml:math></inline-formula> correspond to the image height and width in pixels (each being 400), <inline-formula><mml:math id="M5" altimg="si5.svg"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0220a;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:math></inline-formula> corresponds to the ground truth segmentation class, <inline-formula><mml:math id="M6" altimg="si6.svg"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0220a;</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mo>[</mml:mo><mml:mn>0,1</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula> corresponds to the predicted class probability for the corresponding pixel in position (<italic>h</italic>, <italic>w)</italic> and <inline-formula><mml:math id="M7" altimg="si7.svg"><mml:mi>l</mml:mi><mml:mi>n</mml:mi></mml:math></inline-formula> is the natural logarithm.</p></sec></sec><sec id="sec0025"><label>3</label><title>Experiments</title><sec id="sec0030"><label>3.1</label><title>Data acquisition</title><p id="par0045">In this pilot study we scanned six (n&#x02009;=&#x02009;6) healthy volunteers (3 men, 3 women, age 30&#x02009;&#x000b1;&#x02009;5.44 years). All healthy volunteers consented to participate in this study in full accordance with the work safety regulations of the Helmholtz Center Munich (Neuherberg, Germany). The radial artery, the brachial artery, the dorsal artery of the foot, as well as the cephalic vein, the radial veins and the dorsal vein of the foot were scanned by means of a clinical hand-held MSOT/Ultrasound system (iThera Medical GmbH, Munich, Germany). All subjects were asked to consume no food or caffeine for 8&#x02009;h before the examination, which was conducted in a quiet dark room with normal temperature of 25&#x02009;&#x000b0;C. Each scan lasted for 5&#x02013;10 seconds. The system used was equipped with a near-infrared laser for achieving optimal penetration depth in tissue (3&#x02212;4&#x02009;cm) even with low illumination energy (&#x0223c;15&#x02009;mJ per pulse). For multispectral data recording we used 28 wavelengths (700:10:980&#x02009;nm). Tissue was illuminated by short light pulses (&#x0223c;10&#x02009;ns) at a frame rate of 25fps. The ultrasound detection was performed by 256 ultrasound sensors with a central frequency of 4&#x02009;MHz which covered an angle of 145&#x000b0; and was mounted on the hand-held scanning probe. Acquired ultrasound signals for each illumination pulse were reconstructed into a tomographic image using a model-based reconstruction algorithm [<xref rid="bib0150" ref-type="bibr">30</xref>]. For each MSOT image a co-registered ultrasound image was recorded. The segmentation of the scanned arteries and veins was manually performed on the appropriate MSOT frame by simultaneous view of the co-registered ultrasound image. We decided to segment the blood vessels directly on the MSOT frames because of better contrast, compared to ultrasound, provided by the high light absorption of hemoglobin at the near-infrared illumination range. The appropriate frame for vein segmentation was the frame corresponding to the 750&#x02009;nm illumination wavelength were the absorption of Hb is clearly higher than that of HbO<sub>2</sub>. The appropriate frame for artery segmentation was the frame corresponding to the 850&#x02009;nm illumination wavelength were the absorption of HbO<sub>2</sub> is clearly higher than that of Hb. Manual segmentation was conducted in consensus of two clinicians with experience in MSOT and clinical ultrasound imaging.</p></sec><sec id="sec0035"><label>3.2</label><title>Model comparison</title><p id="par0050">We compared the performance of four segmentation methods on the recorded MSOT dataset: the proposed S-UNET, UNET++ [<xref rid="bib0155" ref-type="bibr">31</xref>], and two differently-sized variants of a standard UNET on the segmentation task. The S-UNET architecture is described in section <xref rid="sec0015" ref-type="sec">2.1</xref> above. The wavelength selection layer is followed by a downsized UNET where every convolutional layer corresponds to 1/8 of filters compared to the architecture in [<xref rid="bib0075" ref-type="bibr">15</xref>]. Two variants of the UNET were applied: one with the same number of filters as in [<xref rid="bib0075" ref-type="bibr">15</xref>] (&#x02018;original&#x02019;) and a variant with 1/8 of filters (&#x02018;downsized&#x02019;). Additionally, a batch normalization layer was inserted between every convolutional layer and its corresponding activation function in both UNET variants. Training was performed as described in the previous section. The results of all segmentation methods were compared to the binary ground truth segmentation mask, which was manually generated from expert clinicians on the recorded MSOT images under co-registered ultrasound guidance (see Methods). Model comparison is based on the Dice coefficient [<xref rid="bib0110" ref-type="bibr">22</xref>] defined as:<disp-formula id="eq0010"><mml:math id="M8" altimg="si8.svg"><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula>where TP, FP and FN correspond to true positive, false positive and false negative classified pixels: A TP pixel is a correctly classified foreground pixel, a FP pixel is a background pixel falsely classified as foreground, and a FN pixel corresponds to a foreground pixel that was incorrectly classified as background by the model. The Dice coefficient is well-suited to tackle the class imbalance inherent to the segmentation task [<xref rid="bib0160" ref-type="bibr">32</xref>], where more than 99 % of the pixels in our dataset are background pixels. As such, it is preferred for model assessment compared to the standard cross entropy used to train the model. The Dice coefficient lies between 0 and 1, with higher values being better since they correspond to larger overlap between the ground truth and predicted segmentation masks.</p></sec><sec id="sec0040"><label>3.3</label><title>Wavelength selection</title><p id="par0055">Wavelength selection was performed by the first layer of the S-UNET (see Methods). However, since feature selection is an inherently noisy process [<xref rid="bib0165" ref-type="bibr">33</xref>,<xref rid="bib0170" ref-type="bibr">34</xref>] it is good practice to average a number of models [<xref rid="bib0175" ref-type="bibr">35</xref>] in order to obtain a smoothed version of wavelength importance. We thus train 100 different instances of the S-UNET and aggregate their results for the tasks of segmentation, as well as for wavelength selection. In the case of segmentation, we average the probability maps of all models before discretizing in order to obtain the binary segmentation mask.</p></sec><sec id="sec0045"><label>3.4</label><title>Results</title><p id="par0060">The performance of all four segmentation models (Dice coefficient) is reported in <xref rid="tbl0005" ref-type="table">Table 1</xref>. The original UNET with over 30 million parameters is potentially slightly overfitting the training dataset while the downsized UNET, as well as the S-UNET achieve very similar segmentation results with roughly half a million parameters. The downsized UNET achieves slightly higher Dice scores on average (0.90&#x02009;&#x000b1;&#x02009;0.08) than the S-UNET ensemble (0.86&#x02009;&#x000b1;&#x02009;0.11), but the difference is not statistically significant given the test set size of 33 images (p-value&#x02009;=&#x02009;0.37, two-sample Wilcoxon rank-sum test). This similarity in performance is to be expected since both methods correspond to a similar number of parameters. However, this also suggests that the added sparsity of wavelength selection does not affect, at least not significantly, the quality of the generated segmentation masks in the case of S-UNET. Finally, UNET++ performs worse than all other deep learning methods, achieving a Dice coefficient of 0.61&#x02009;&#x000b1;&#x02009;0.26.<table-wrap position="float" id="tbl0005"><label>Table 1</label><caption><p>Model Performance. Dice results correspond to mean&#x02009;&#x000b1;&#x02009;std.</p></caption><alt-text id="at0020">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Test Set Dice</th><th align="left">Parameters</th><th align="left">Wavelength Selection</th></tr></thead><tbody><tr><td align="left">UNET (original)</td><td align="left">0.75&#x02009;&#x000b1;&#x02009;0.28</td><td align="left">31,416,897</td><td align="left">No</td></tr><tr><td align="left">UNET (downsized)</td><td align="left">0.90&#x02009;&#x000b1;&#x02009;0.08</td><td align="left">495,881</td><td align="left">No</td></tr><tr><td align="left">UNET++</td><td align="left">0.61&#x02009;&#x000b1;&#x02009;0.26</td><td align="left">9,049,377</td><td align="left">No</td></tr><tr><td align="left">S-UNET</td><td align="left">0.86&#x02009;&#x000b1;&#x02009;0.11</td><td align="left">493,965</td><td align="left">Yes</td></tr></tbody></table></table-wrap></p><p id="par0065">The advantage of the proposed S-UNET approach over the other UNET approaches is clearly its interpretability of results due to the embedded wavelength selection. As visualized in <xref rid="fig0010" ref-type="fig">Fig. 2</xref>, out of the 28 input wavelengths the model has identified two as being the most important in a purely data-driven manner. These wavelengths correspond to the maximum of the absorption spectra of total blood volume (HbO<sub>2</sub> and Hb) at 810&#x02009;nm and HbO<sub>2</sub> at 850&#x02009;nm. Both of these identified wavelengths are thus meaningful since they mark the presence of blood in the detected image regions. To further highlight the importance of wavelength selection we re-evaluated the S-UNET performance using only the two most important wavelengths (810 and 850&#x02009;nm). The 2-wavelength S-UNET achieved a mean Dice score of 0.73 on the test set. This corresponds to a 13 % decrease in performance compared to the 28-wavelength S-UNET (0.73 Dice instead of 0.86), while it simultaneously reduces the required data volume by 93 % (2 wavelengths instead of 28). Additionally, when we train the same model using only two alternative wavelengths (750 and 870&#x02009;nm), it achieves a lower mean Dice score of 0.70. This suggests that while segmentation is possible using other wavelengths, the two wavelengths identified by S-UNET have a higher predictive value. The segmentation results of S-UNET on an exemplary set of images are visualized in <xref rid="fig0015" ref-type="fig">Fig. 3</xref>. Interestingly, our approach is able to discriminate blood vessels from similar objects probably by exploiting the wavelength information.<fig id="fig0010"><label>Fig. 2</label><caption><p>The S-UNET identifies wavelengths relevant to the segmentation task. Each boxplot (the box&#x02019;s edges correspond to quartiles 1 and 3 while whiskers extend to &#x000b1;1.5 times the interquartile range) corresponds to the weights assigned by the ensemble of 100 S-UNET instances to each wavelength. Averaging results is necessary since feature selection is an inherently noisy process. For every S-UNET instance, each of the 28 wavelengths of the input image is multiplied by its corresponding weight and all 28 weighted single-wavelength images are added in a pixel-wise manner. This step results in a single-channel image being passed on to the following layers of the network. According to the median weight of each wavelength, the two most important wavelengths are 850&#x02009;nm and 810&#x02009;nm, corresponding to the maximum absorption of HbO<sub>2</sub> and total hemoglobin (THb), respectively.</p></caption><alt-text id="at0010">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig><fig id="fig0015"><label>Fig. 3</label><caption><p>The S-UNET successfully segments human vasculature from MSOT images. Each row corresponds to a different image of the test set. The first column (images a, e, i, m) shows the 850&#x02009;nm channel of the MSOT image. The second (images b, f, j, n) and third columns (c, g, k, o) show the ground truth (true mask, blue) and predicted segmentation masks (red), respectively, visualized on top of the input image. The true segmentation mask is identified by expert physicians, while the S-UNET predicted segmentation mask corresponds to the output of the S-UNET ensemble. The fourth column (images d, h, l, p) corresponds to the absolute difference between the true and predicted binary segmentation masks and is equivalent to the logical operation of XOR (exclusive or). The predicted masks almost completely overlap with the ground truth segmentation. The S-UNET is successful even in the last two cases (rows) where the mask is relatively small and located in an area where similar bright spots are present. The white dashed line represents the skin surface. The white arrows point to the blood vessel of interest. The scale bar is 5&#x02009;mm. The gray color bar ranges from 0 to 1 and corresponds to the normalized intensity of each image (columns 1-3) or the difference of the true and predicted segmentation masks (column 4).</p></caption><alt-text id="at0015">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p><p id="par0070">Blood vessel segmentation via deep learning is more accurate than classical thresholding methods, such as Sauvola&#x02019;s adaptive [<xref rid="bib0180" ref-type="bibr">36</xref>] and Otsu&#x02019;s global thresholding [<xref rid="bib0185" ref-type="bibr">37</xref>]. Both methods are available in scikit-image, a Python package for image processing [<xref rid="bib0190" ref-type="bibr">38</xref>] and require a single grayscale image as input. For this reason, a grayscale image was produced by calculating the average value of each pixel across the 28 image channels. On the one hand, local thresholding (Sauvola&#x02019;s method) achieved poor results (mean Dice of 0.02&#x02009;+&#x02009;0.01) since the region of interest (vascular lumen) corresponds to a single region of maximum intensity values. On the other hand, Otsu&#x02019;s global thresholding performs better than local thresholding (mean Dice of 0.24&#x02009;+&#x02009;0.23), but considerably worse than deep learning approaches. When using only the two most important wavelengths (instead of all 28) to compute the grayscale input image, the performance of Sauvola thresholding remains practically identical while Otsu&#x02019;s method achieves better results on average (mean Dice of 0.41&#x02009;+&#x02009;0.33).</p></sec></sec><sec id="sec0050"><label>4</label><title>Discussion</title><p id="par0075">In this work we applied a deep learning approach based on an adapted S-UNET to perform automated vascular segmentation in clinical MSOT images. Our model successfully segments blood vessels (arteries and veins) and its performance is comparable to a standard UNET of similar model size. Furthermore, our model is capable of selecting the illumination wavelengths that are most important for the segmentation task at hand in a purely data driven manner. Our results show that among the 28 illumination wavelengths used for data acquisition, two wavelengths are associated with the light absorption of hemoglobin at the near-infrared range of illumination (700&#x02013;970&#x02009;nm). These correspond to 810&#x02009;nm, which is the isosbestic point of HbO<sub>2</sub> and Hb and reflects the absorption of total hemoglobin or else the total blood within the vasculature and 850&#x02009;nm, which is the point where HbO<sub>2</sub> absorbs significantly more than Hb and reflects the arterial blood.</p><p id="par0080">Our approach achieves accurate automated segmentation of both arteries and veins on raw clinical MSOT data. Apart from facilitating the segmentation process, which is time-costly for longitudinal scans of several minutes during functional vascular testing, it may help tackling a significant limitation of optical and optoacoustic imaging: the attenuation of light due to scattering and absorption when propagating in living tissue. This effect causes a gradual attenuation of the signal intensity in the vascular lumen with increasing depth. Thus, the accurate visualization and segmentation of the lumen constitute real challenges even for clinicians with extensive MSOT experience. In the current study, we scanned blood vessels where this effect was apparent (e.g. <xref rid="fig0015" ref-type="fig">Fig. 3</xref>a) but not to an extent that would jeopardize the accurate manual segmentation of the vascular lumen directly on the MSOT images under ultrasound guidance. Thus, future studies are required to further investigate the efficacy of deep learning approaches in automatically detecting and segmenting vessels with clinical interest (e.g. the carotid artery) deep in tissue in clinical MSOT data.</p><p id="par0085">In this study, we preferred to work on raw MSOT data. However, the discrete spectral difference of HbO2 and Hb at the near-infrared range as well as the strong presence of HbO2 in arteries and Hb in veins would allow for the direct spectral unmixing of HbO2 and Hb in the MSOT data and thus for direct vascular segmentation. Nevertheless, spectrally unmixed data suffer from errors related to imaging depth and motion, either exogenous (e.g. operator&#x02019;s hand and random patient movement) or endogenous (e.g. arterial pulsation or breathing).</p><p id="par0090">Regarding motion-related errors, the dynamic character of the vascular system introduces significant inaccuracy when it comes to spectral unmixing results, especially when illuminating at multiple different wavelengths (e.g. 28) to achieve high spectral quality. For example, the recording of a multispectral stack of 28 wavelengths at a frame rate of 25&#x02009;Hz takes more than one second. Considering that the cardiac cycle of a normal individual with a heart rate of 70&#x02212;75&#x02009;Hz is approximately 0.8&#x02009;s, the use of 28 wavelengths renders the spectrally unmixed data vulnerable to errors due to arterial wall motion, especially in the periphery of the vascular lumen, potentially degrading the precision of vascular segmentation when performed by means of direct spectral unmixing.</p><p id="par0095">Moreover, multispectral optoacoustic imaging at increased tissue depths (&#x0003e; 1&#x02009;cm), where normally the blood vessels lie, renders the spectral unmixing output vulnerable to the spectral coloring effect: the random absorption and scattering of each illumination wavelength before reaching the HbO2 or the Hb of the vascular lumen according to the optical properties of the set of tissues covering them (e.g. skin, subcutaneous fat, muscle). Thus, usual linear spectral unmixing methods fail to unmix the absorbers of interest (e.g. HbO2 and Hb) at increasing depths since the measured spectra have been colored and thus deflected from the known absorption spectra, as measured in the lab. For the above mentioned reasons, we decided to work on the recorded raw MSOT data.</p><p id="par0100">Our model showed that the decision for segmenting the vasculature was mainly based on two near-infrared wavelengths: the 810&#x02009;nm where HbO<sub>2</sub> and Hb absorb light to the same extent and the 850&#x02009;nm where the light absorption of HbO<sub>2</sub> is significantly higher than that of Hb. Our results provide evidence for effective and task-specific wavelength selection via the suggested deep learning model for accurate segmentation of blood vessels in clinical MSOT data. Apart from increasing the time resolution by skipping a number of unnecessary illumination wavelengths and decreasing the data volume, the effective wavelength selection may be used for indirect spectral characterization of more complex tissues or even homogeneous tissues at high depths by identifying the wavelengths critical for achieving their segmentation. This approach may help overcoming the limitations introduced by the spectral coloring effect and thus providing a blind or data-driven spectral unmixing with great implications for clinical MSOT imaging. Our method may be used for segmenting and characterizing tissues with clinical relevance (e.g. the subcutaneous fat or the atherosclerotic plaques which contain lipids, the skeletal muscle which contains water) or even the detection and distribution mapping of injected contrast agents targeting specific molecules involved in the pathophysiology of a disease.</p><p id="par0105">To the best of our knowledge, while deep learning has been used before in the context of optoacoustic imaging data [<xref rid="bib0195" ref-type="bibr">39</xref>,<xref rid="bib0200" ref-type="bibr">40</xref>], this is the first time where a deep learning method is applied to clinical MSOT data. Our approach has significant implications for future MSOT applications with clinical relevance, such as the automated segmentation of more complex soft tissues (e.g. muscle, fat, atherosclerotic plaques) and foreseeable for more accurate diagnosis of vascular disease.</p></sec><sec id="sec0055"><title>Code availability</title><p id="par0110">The keras implementation of the S-UNET is freely available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/nchlis/sunet" id="intr0005">https://github.com/nchlis/sunet</ext-link>.</p></sec><sec sec-type="COI-statement"><title>Declaration of Competing Interest</title><p id="par0115">The authors declare that there are no conflicts of interest.</p></sec></body><back><ref-list id="bibl0005"><title>References</title><ref id="bib0005"><label>1</label><element-citation publication-type="journal" id="sbref0005"><person-group person-group-type="author"><name><surname>Karlas</surname><given-names>A.</given-names></name><name><surname>Fasoula</surname><given-names>N.-A.</given-names></name><name><surname>Paul-Yuan</surname><given-names>K.</given-names></name><name><surname>Reber</surname><given-names>J.</given-names></name><name><surname>Kallmayer</surname><given-names>M.</given-names></name><name><surname>Bozhko</surname><given-names>D.</given-names></name><name><surname>Seeger</surname><given-names>M.</given-names></name><name><surname>Eckstein</surname><given-names>H.-H.</given-names></name><name><surname>Wildgruber</surname><given-names>M.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name></person-group><article-title>Cardiovascular optoacoustics: from mice to men &#x02013; a review</article-title><source>Photoacoustics</source><volume>14</volume><year>2019</year><fpage>19</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.pacs.2019.03.001</pub-id><pub-id pub-id-type="pmid">31024796</pub-id></element-citation></ref><ref id="bib0010"><label>2</label><element-citation publication-type="journal" id="sbref0010"><person-group person-group-type="author"><name><surname>Karlas</surname><given-names>A.</given-names></name><name><surname>Reber</surname><given-names>J.</given-names></name><name><surname>Diot</surname><given-names>G.</given-names></name><name><surname>Bozhko</surname><given-names>D.</given-names></name><name><surname>Anastasopoulou</surname><given-names>M.</given-names></name><name><surname>Ibrahim</surname><given-names>T.</given-names></name><name><surname>Schwaiger</surname><given-names>M.</given-names></name><name><surname>Hyafil</surname><given-names>F.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name></person-group><article-title>Flow-mediated dilatation test using optoacoustic imaging: a proof-of-concept</article-title><source>Biomed. Opt. Express, BOE</source><volume>8</volume><year>2017</year><fpage>3395</fpage><lpage>3403</lpage><pub-id pub-id-type="doi">10.1364/BOE.8.003395</pub-id><pub-id pub-id-type="pmid">28717575</pub-id></element-citation></ref><ref id="bib0015"><label>3</label><element-citation publication-type="journal" id="sbref0015"><person-group person-group-type="author"><name><surname>Masthoff</surname><given-names>M.</given-names></name><name><surname>Helfen</surname><given-names>A.</given-names></name><name><surname>Claussen</surname><given-names>J.</given-names></name><name><surname>Karlas</surname><given-names>A.</given-names></name><name><surname>Markwardt</surname><given-names>N.A.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name><name><surname>Eisenbl&#x000e4;tter</surname><given-names>M.</given-names></name><name><surname>Wildgruber</surname><given-names>M.</given-names></name></person-group><article-title>Use of multispectral optoacoustic tomography to diagnose vascular malformations</article-title><source>JAMA Dermatol.</source><volume>154</volume><year>2018</year><fpage>1457</fpage><lpage>1462</lpage><pub-id pub-id-type="doi">10.1001/jamadermatol.2018.3269</pub-id><pub-id pub-id-type="pmid">30267083</pub-id></element-citation></ref><ref id="bib0020"><label>4</label><element-citation publication-type="journal" id="sbref0020"><person-group person-group-type="author"><name><surname>Roll</surname><given-names>W.</given-names></name><name><surname>Markwardt</surname><given-names>N.A.</given-names></name><name><surname>Masthoff</surname><given-names>M.</given-names></name><name><surname>Helfen</surname><given-names>A.</given-names></name><name><surname>Claussen</surname><given-names>J.</given-names></name><name><surname>Eisenbl&#x000e4;tter</surname><given-names>M.</given-names></name><name><surname>Hasenbach</surname><given-names>A.</given-names></name><name><surname>Hermann</surname><given-names>S.</given-names></name><name><surname>Karlas</surname><given-names>A.</given-names></name><name><surname>Wildgruber</surname><given-names>M.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name><name><surname>Sch&#x000e4;fers</surname><given-names>M.</given-names></name></person-group><article-title>Multispectral optoacoustic tomography of benign and malignant thyroid disorders &#x02013; a pilot study</article-title><source>J. Nucl. Med.</source><year>2019</year><pub-id pub-id-type="doi">10.2967/jnumed.118.222174</pub-id></element-citation></ref><ref id="bib0025"><label>5</label><element-citation publication-type="journal" id="sbref0025"><person-group person-group-type="author"><name><surname>Reber</surname><given-names>J.</given-names></name><name><surname>Willersh&#x000e4;user</surname><given-names>M.</given-names></name><name><surname>Karlas</surname><given-names>A.</given-names></name><name><surname>Paul-Yuan</surname><given-names>K.</given-names></name><name><surname>Diot</surname><given-names>G.</given-names></name><name><surname>Franz</surname><given-names>D.</given-names></name><name><surname>Fromme</surname><given-names>T.</given-names></name><name><surname>Ovsepian</surname><given-names>S.V.</given-names></name><name><surname>B&#x000e9;zi&#x000e8;re</surname><given-names>N.</given-names></name><name><surname>Dubikovskaya</surname><given-names>E.</given-names></name><name><surname>Karampinos</surname><given-names>D.C.</given-names></name><name><surname>Holzapfel</surname><given-names>C.</given-names></name><name><surname>Hauner</surname><given-names>H.</given-names></name><name><surname>Klingenspor</surname><given-names>M.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name></person-group><article-title>Non-invasive measurement of brown fat metabolism based on optoacoustic imaging of hemoglobin gradients</article-title><source>Cell Metab.</source><volume>27</volume><year>2018</year><fpage>689</fpage><lpage>701</lpage><pub-id pub-id-type="doi">10.1016/j.cmet.2018.02.002</pub-id><comment>e4</comment><pub-id pub-id-type="pmid">29514074</pub-id></element-citation></ref><ref id="bib0030"><label>6</label><element-citation publication-type="journal" id="sbref0030"><person-group person-group-type="author"><name><surname>Masthoff</surname><given-names>M.</given-names></name><name><surname>Helfen</surname><given-names>A.</given-names></name><name><surname>Claussen</surname><given-names>J.</given-names></name><name><surname>Roll</surname><given-names>W.</given-names></name><name><surname>Karlas</surname><given-names>A.</given-names></name><name><surname>Becker</surname><given-names>H.</given-names></name><name><surname>Gabri&#x000eb;ls</surname><given-names>G.</given-names></name><name><surname>Riess</surname><given-names>J.</given-names></name><name><surname>Heindel</surname><given-names>W.</given-names></name><name><surname>Sch&#x000e4;fers</surname><given-names>M.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name><name><surname>Eisenbl&#x000e4;tter</surname><given-names>M.</given-names></name><name><surname>Gerth</surname><given-names>U.</given-names></name><name><surname>Wildgruber</surname><given-names>M.</given-names></name></person-group><article-title>Multispectral optoacoustic tomography of systemic sclerosis</article-title><source>J. Biophotonics</source><volume>11</volume><year>2018</year><fpage>e201800155</fpage><pub-id pub-id-type="doi">10.1002/jbio.201800155</pub-id><pub-id pub-id-type="pmid">29974645</pub-id></element-citation></ref><ref id="bib0035"><label>7</label><element-citation publication-type="journal" id="sbref0035"><person-group person-group-type="author"><name><surname>Diot</surname><given-names>G.</given-names></name><name><surname>Metz</surname><given-names>S.</given-names></name><name><surname>Noske</surname><given-names>A.</given-names></name><name><surname>Liapis</surname><given-names>E.</given-names></name><name><surname>Schroeder</surname><given-names>B.</given-names></name><name><surname>Ovsepian</surname><given-names>S.V.</given-names></name><name><surname>Meier</surname><given-names>R.</given-names></name><name><surname>Rummeny</surname><given-names>E.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name></person-group><article-title>Multispectral optoacoustic tomography (MSOT) of human breast cancer</article-title><source>Clin. Cancer Res.</source><volume>23</volume><year>2017</year><fpage>6912</fpage><lpage>6922</lpage><pub-id pub-id-type="doi">10.1158/1078-0432.CCR-16-3200</pub-id><pub-id pub-id-type="pmid">28899968</pub-id></element-citation></ref><ref id="bib0040"><label>8</label><element-citation publication-type="journal" id="sbref0040"><person-group person-group-type="author"><name><surname>Knieling</surname><given-names>F.</given-names></name><name><surname>Neufert</surname><given-names>C.</given-names></name><name><surname>Hartmann</surname><given-names>A.</given-names></name><name><surname>Claussen</surname><given-names>J.</given-names></name><name><surname>Urich</surname><given-names>A.</given-names></name><name><surname>Egger</surname><given-names>C.</given-names></name><name><surname>Vetter</surname><given-names>M.</given-names></name><name><surname>Fischer</surname><given-names>S.</given-names></name><name><surname>Pfeifer</surname><given-names>L.</given-names></name><name><surname>Hagel</surname><given-names>A.</given-names></name><name><surname>Kielisch</surname><given-names>C.</given-names></name><name><surname>G&#x000f6;rtz</surname><given-names>R.S.</given-names></name><name><surname>Wildner</surname><given-names>D.</given-names></name><name><surname>Engel</surname><given-names>M.</given-names></name><name><surname>R&#x000f6;ther</surname><given-names>J.</given-names></name><name><surname>Uter</surname><given-names>W.</given-names></name><name><surname>Siebler</surname><given-names>J.</given-names></name><name><surname>Atreya</surname><given-names>R.</given-names></name><name><surname>Rascher</surname><given-names>W.</given-names></name><name><surname>Strobel</surname><given-names>D.</given-names></name><name><surname>Neurath</surname><given-names>M.F.</given-names></name><name><surname>Waldner</surname><given-names>M.J.</given-names></name></person-group><article-title>Multispectral optoacoustic tomography for assessment of Crohn&#x02019;s disease activity</article-title><source>N. Engl. J. Med.</source><volume>376</volume><year>2017</year><fpage>1292</fpage><lpage>1294</lpage><pub-id pub-id-type="doi">10.1056/NEJMc1612455</pub-id><pub-id pub-id-type="pmid">28355498</pub-id></element-citation></ref><ref id="bib0045"><label>9</label><element-citation publication-type="journal" id="sbref0045"><person-group person-group-type="author"><name><surname>Green Daniel</surname><given-names>J.</given-names></name><name><surname>Helen</surname><given-names>Jones</given-names></name><name><surname>Dick</surname><given-names>Thijssen</given-names></name><name><surname>Cable</surname><given-names>N.T.</given-names></name><name><surname>Greg</surname><given-names>Atkinson</given-names></name></person-group><article-title>Flow-mediated dilation and cardiovascular event prediction</article-title><source>Hypertension</source><volume>57</volume><year>2011</year><fpage>363</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1161/HYPERTENSIONAHA.110.167015</pub-id><pub-id pub-id-type="pmid">21263128</pub-id></element-citation></ref><ref id="bib0050"><label>10</label><element-citation publication-type="journal" id="sbref0050"><person-group person-group-type="author"><name><surname>Agarwal</surname><given-names>S.C.</given-names></name><name><surname>Allen</surname><given-names>J.</given-names></name><name><surname>Murray</surname><given-names>A.</given-names></name><name><surname>Purcell</surname><given-names>I.F.</given-names></name></person-group><article-title>Comparative reproducibility of dermal microvascular blood flow changes in response to acetylcholine iontophoresis, hyperthermia and reactive hyperaemia</article-title><source>Physiol. Meas.</source><volume>31</volume><year>2009</year><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1088/0967-3334/31/1/001</pub-id><pub-id pub-id-type="pmid">19940349</pub-id></element-citation></ref><ref id="bib0055"><label>11</label><element-citation publication-type="book" id="sbref0055"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><chapter-title>ImageNet classification with deep convolutional neural networks</chapter-title><person-group person-group-type="editor"><name><surname>Pereira</surname><given-names>F.</given-names></name><name><surname>Burges</surname><given-names>C.J.C.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Weinberger</surname><given-names>K.Q.</given-names></name></person-group><source>Advances in Neural Information Processing Systems 25</source><year>2012</year><publisher-name>Curran Associates, Inc.</publisher-name><fpage>1097</fpage><lpage>1105</lpage><comment>(Accessed 17 December 2018)</comment><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" id="intr0010">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link></element-citation></ref><ref id="bib0060"><label>12</label><element-citation publication-type="journal" id="sbref0060"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2016</year><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib0065"><label>13</label><element-citation publication-type="journal" id="sbref0065"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name><surname>Kendall</surname><given-names>A.</given-names></name><name><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>39</volume><year>2017</year><fpage>2481</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type="pmid">28060704</pub-id></element-citation></ref><ref id="bib0070"><label>14</label><element-citation publication-type="journal" id="sbref0070"><person-group person-group-type="author"><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Shelhamer</surname><given-names>E.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2015</year><fpage>3431</fpage><lpage>3440</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298965</pub-id></element-citation></ref><ref id="bib0075"><label>15</label><element-citation publication-type="book" id="sbref0075"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group><chapter-title>U-net: convolutional networks for biomedical image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Navab</surname><given-names>N.</given-names></name><name><surname>Hornegger</surname><given-names>J.</given-names></name><name><surname>Wells</surname><given-names>W.M.</given-names></name><name><surname>Frangi</surname><given-names>A.F.</given-names></name></person-group><source>Medical Image Computing and Computer-Assisted Intervention &#x02013; MICCAI</source><year>2015</year><publisher-name>Springer International Publishing</publisher-name><fpage>234</fpage><lpage>241</lpage><ext-link ext-link-type="uri" xlink:href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28" id="intr0015">https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28</ext-link></element-citation></ref><ref id="bib0080"><label>16</label><element-citation publication-type="journal" id="sbref0080"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>U.R.</given-names></name><name><surname>Oh</surname><given-names>S.L.</given-names></name><name><surname>Hagiwara</surname><given-names>Y.</given-names></name><name><surname>Tan</surname><given-names>J.H.</given-names></name><name><surname>Adam</surname><given-names>M.</given-names></name><name><surname>Gertych</surname><given-names>A.</given-names></name><name><surname>Tan</surname><given-names>R.S.</given-names></name></person-group><article-title>A deep convolutional neural network model to classify heartbeats</article-title><source>Comput. Biol. Med.</source><volume>89</volume><year>2017</year><fpage>389</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.08.022</pub-id><pub-id pub-id-type="pmid">28869899</pub-id></element-citation></ref><ref id="bib0085"><label>17</label><element-citation publication-type="journal" id="sbref0085"><person-group person-group-type="author"><name><surname>Esteva</surname><given-names>A.</given-names></name><name><surname>Kuprel</surname><given-names>B.</given-names></name><name><surname>Novoa</surname><given-names>R.A.</given-names></name><name><surname>Ko</surname><given-names>J.</given-names></name><name><surname>Swetter</surname><given-names>S.M.</given-names></name><name><surname>Blau</surname><given-names>H.M.</given-names></name><name><surname>Thrun</surname><given-names>S.</given-names></name></person-group><article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title><source>Nature</source><volume>542</volume><year>2017</year><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1038/nature21056</pub-id><pub-id pub-id-type="pmid">28117445</pub-id></element-citation></ref><ref id="bib0090"><label>18</label><element-citation publication-type="journal" id="sbref0090"><person-group person-group-type="author"><name><surname>Rajpurkar</surname><given-names>P.</given-names></name><name><surname>Irvin</surname><given-names>J.</given-names></name><name><surname>Ball</surname><given-names>R.L.</given-names></name><name><surname>Zhu</surname><given-names>K.</given-names></name><name><surname>Yang</surname><given-names>B.</given-names></name><name><surname>Mehta</surname><given-names>H.</given-names></name><name><surname>Duan</surname><given-names>T.</given-names></name><name><surname>Ding</surname><given-names>D.</given-names></name><name><surname>Bagul</surname><given-names>A.</given-names></name><name><surname>Langlotz</surname><given-names>C.P.</given-names></name><name><surname>Patel</surname><given-names>B.N.</given-names></name><name><surname>Yeom</surname><given-names>K.W.</given-names></name><name><surname>Shpanskaya</surname><given-names>K.</given-names></name><name><surname>Blankenberg</surname><given-names>F.G.</given-names></name><name><surname>Seekins</surname><given-names>J.</given-names></name><name><surname>Amrhein</surname><given-names>T.J.</given-names></name><name><surname>Mong</surname><given-names>D.A.</given-names></name><name><surname>Halabi</surname><given-names>S.S.</given-names></name><name><surname>Zucker</surname><given-names>E.J.</given-names></name><name><surname>Ng</surname><given-names>A.Y.</given-names></name><name><surname>Lungren</surname><given-names>M.P.</given-names></name></person-group><article-title>Deep learning for chest radiograph diagnosis: a retrospective comparison of the CheXNeXt algorithm to practicing radiologists</article-title><source>PLoS Med.</source><volume>15</volume><year>2018</year><fpage>e1002686</fpage><pub-id pub-id-type="doi">10.1371/journal.pmed.1002686</pub-id><pub-id pub-id-type="pmid">30457988</pub-id></element-citation></ref><ref id="bib0095"><label>19</label><element-citation publication-type="book" id="sbref0095"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>T.</given-names></name><name><surname>Peters</surname><given-names>J.</given-names></name><name><surname>Groth</surname><given-names>A.</given-names></name><name><surname>Stehle</surname><given-names>T.</given-names></name><name><surname>Weese</surname><given-names>J.</given-names></name></person-group><chapter-title>Deep learning-based boundary detection for model-based segmentation with application to MR prostate segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Frangi</surname><given-names>A.F.</given-names></name><name><surname>Schnabel</surname><given-names>J.A.</given-names></name><name><surname>Davatzikos</surname><given-names>C.</given-names></name><name><surname>Alberola-L&#x000f3;pez</surname><given-names>C.</given-names></name><name><surname>Fichtinger</surname><given-names>G.</given-names></name></person-group><source>Medical Image Computing and Computer Assisted Intervention &#x02013; MICCAI 2018</source><year>2018</year><publisher-name>Springer International Publishing</publisher-name><fpage>515</fpage><lpage>522</lpage></element-citation></ref><ref id="bib0100"><label>20</label><element-citation publication-type="journal" id="sbref0100"><person-group person-group-type="author"><name><surname>Fauw</surname><given-names>J.D.</given-names></name><name><surname>Ledsam</surname><given-names>J.R.</given-names></name><name><surname>Romera-Paredes</surname><given-names>B.</given-names></name><name><surname>Nikolov</surname><given-names>S.</given-names></name><name><surname>Tomasev</surname><given-names>N.</given-names></name><name><surname>Blackwell</surname><given-names>S.</given-names></name><name><surname>Askham</surname><given-names>H.</given-names></name><name><surname>Glorot</surname><given-names>X.</given-names></name><name><surname>O&#x02019;Donoghue</surname><given-names>B.</given-names></name><name><surname>Visentin</surname><given-names>D.</given-names></name><name><surname>van den Driessche</surname><given-names>G.</given-names></name><name><surname>Lakshminarayanan</surname><given-names>B.</given-names></name><name><surname>Meyer</surname><given-names>C.</given-names></name><name><surname>Mackinder</surname><given-names>F.</given-names></name><name><surname>Bouton</surname><given-names>S.</given-names></name><name><surname>Ayoub</surname><given-names>K.</given-names></name><name><surname>Chopra</surname><given-names>R.</given-names></name><name><surname>King</surname><given-names>D.</given-names></name><name><surname>Karthikesalingam</surname><given-names>A.</given-names></name><name><surname>Hughes</surname><given-names>C.O.</given-names></name><name><surname>Raine</surname><given-names>R.</given-names></name><name><surname>Hughes</surname><given-names>J.</given-names></name><name><surname>Sim</surname><given-names>D.A.</given-names></name><name><surname>Egan</surname><given-names>C.</given-names></name><name><surname>Tufail</surname><given-names>A.</given-names></name><name><surname>Montgomery</surname><given-names>H.</given-names></name><name><surname>Hassabis</surname><given-names>D.</given-names></name><name><surname>Rees</surname><given-names>G.</given-names></name><name><surname>Back</surname><given-names>T.</given-names></name><name><surname>Khaw</surname><given-names>P.T.</given-names></name><name><surname>Suleyman</surname><given-names>M.</given-names></name><name><surname>Cornebise</surname><given-names>J.</given-names></name><name><surname>Keane</surname><given-names>P.A.</given-names></name><name><surname>Ronneberger</surname><given-names>O.</given-names></name></person-group><article-title>Clinically applicable deep learning for diagnosis and referral in retinal disease</article-title><source>Nat. Med.</source><volume>24</volume><year>2018</year><fpage>1342</fpage><pub-id pub-id-type="doi">10.1038/s41591-018-0107-6</pub-id><pub-id pub-id-type="pmid">30104768</pub-id></element-citation></ref><ref id="bib0105"><label>21</label><element-citation publication-type="journal" id="sbref0105"><person-group person-group-type="author"><name><surname>Kamnitsas</surname><given-names>K.</given-names></name><name><surname>Ledig</surname><given-names>C.</given-names></name><name><surname>Newcombe</surname><given-names>V.F.J.</given-names></name><name><surname>Simpson</surname><given-names>J.P.</given-names></name><name><surname>Kane</surname><given-names>A.D.</given-names></name><name><surname>Menon</surname><given-names>D.K.</given-names></name><name><surname>Rueckert</surname><given-names>D.</given-names></name><name><surname>Glocker</surname><given-names>B.</given-names></name></person-group><article-title>Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</article-title><source>Med. Image Anal.</source><volume>36</volume><year>2017</year><fpage>61</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.media.2016.10.004</pub-id><pub-id pub-id-type="pmid">27865153</pub-id></element-citation></ref><ref id="bib0110"><label>22</label><element-citation publication-type="journal" id="sbref0110"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>S.</given-names></name><name><surname>Pinto</surname><given-names>A.</given-names></name><name><surname>Alves</surname><given-names>V.</given-names></name><name><surname>Silva</surname><given-names>C.A.</given-names></name></person-group><article-title>Brain tumor segmentation using convolutional neural networks in MRI images</article-title><source>IEEE Trans. Med. Imaging</source><volume>35</volume><year>2016</year><fpage>1240</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.1109/TMI.2016.2538465</pub-id><pub-id pub-id-type="pmid">26960222</pub-id></element-citation></ref><ref id="bib0115"><label>23</label><element-citation publication-type="journal" id="sbref0115"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Tan</surname><given-names>E.</given-names></name><name><surname>Jiang</surname><given-names>X.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name><name><surname>Ni</surname><given-names>D.</given-names></name><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Lei</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>T.</given-names></name></person-group><article-title>Accurate cervical cell segmentation from overlapping clumps in pap smear images</article-title><source>IEEE Trans. Med. Imaging</source><volume>36</volume><year>2017</year><fpage>288</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1109/TMI.2016.2606380</pub-id><pub-id pub-id-type="pmid">27623573</pub-id></element-citation></ref><ref id="bib0120"><label>24</label><element-citation publication-type="journal" id="sbref0120"><person-group person-group-type="author"><name><surname>Ker</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Rao</surname><given-names>J.</given-names></name><name><surname>Lim</surname><given-names>T.</given-names></name></person-group><article-title>Deep learning applications in medical image analysis</article-title><source>IEEE Access</source><volume>6</volume><year>2018</year><fpage>9375</fpage><lpage>9389</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2017.2788044</pub-id></element-citation></ref><ref id="bib0125"><label>25</label><element-citation publication-type="journal" id="sbref0125"><person-group person-group-type="author"><name><surname>Litjens</surname><given-names>G.</given-names></name><name><surname>Kooi</surname><given-names>T.</given-names></name><name><surname>Bejnordi</surname><given-names>B.E.</given-names></name><name><surname>Setio</surname><given-names>A.A.A.</given-names></name><name><surname>Ciompi</surname><given-names>F.</given-names></name><name><surname>Ghafoorian</surname><given-names>M.</given-names></name><name><surname>van der Laak</surname><given-names>J.A.W.M.</given-names></name><name><surname>van Ginneken</surname><given-names>B.</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>C.I.</given-names></name></person-group><article-title>A survey on deep learning in medical image analysis</article-title><source>Med. Image Anal.</source><volume>42</volume><year>2017</year><fpage>60</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id><pub-id pub-id-type="pmid">28778026</pub-id></element-citation></ref><ref id="bib0130"><label>26</label><element-citation publication-type="journal" id="sbref0130"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group><article-title>Regression shrinkage and selection via the lasso</article-title><source>J. R. Stat. Soc. Ser. B</source><volume>58</volume><year>1996</year><fpage>267</fpage><lpage>288</lpage></element-citation></ref><ref id="bib0135"><label>27</label><element-citation publication-type="journal" id="sbref0135"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title><source>International Conference on Machine Learning</source><year>2015</year><fpage>448</fpage><lpage>456</lpage><comment>(Accessed 17 December 2018)</comment><ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v37/ioffe15.html" id="intr0020">http://proceedings.mlr.press/v37/ioffe15.html</ext-link></element-citation></ref><ref id="bib0140"><label>28</label><element-citation publication-type="book" id="sbref0140"><person-group person-group-type="author"><name><surname>Galea</surname><given-names>A.</given-names></name><name><surname>Capelo</surname><given-names>L.</given-names></name></person-group><chapter-title>Applied Deep Learning with Python: Use Scikit-learn, TensorFlow, and Keras to Create Intelligent Systems and Machine Learning Solutions</chapter-title><year>2018</year><publisher-name>Packt Publishing</publisher-name></element-citation></ref><ref id="bib0145"><label>29</label><element-citation publication-type="book" id="sbref0145"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D.P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group><chapter-title>Adam: A Method for Stochastic Optimization, ArXiv:1412.6980 [Cs]</chapter-title><year>2014</year><comment>(Accessed 17 December 2018)</comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980" id="intr0025">http://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib0150"><label>30</label><element-citation publication-type="journal" id="sbref0150"><person-group person-group-type="author"><name><surname>Rosenthal</surname><given-names>A.</given-names></name><name><surname>Ntziachristos</surname><given-names>V.</given-names></name><name><surname>Razansky</surname><given-names>D.</given-names></name></person-group><article-title>Model-based optoacoustic inversion with arbitrary-shape detectors</article-title><source>Med. Phys.</source><volume>38</volume><year>2011</year><fpage>4285</fpage><lpage>4295</lpage><pub-id pub-id-type="doi">10.1118/1.3589141</pub-id><pub-id pub-id-type="pmid">21859030</pub-id></element-citation></ref><ref id="bib0155"><label>31</label><element-citation publication-type="book" id="sbref0155"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Rahman Siddiquee</surname><given-names>M.M.</given-names></name><name><surname>Tajbakhsh</surname><given-names>N.</given-names></name><name><surname>Liang</surname><given-names>J.</given-names></name></person-group><chapter-title>UNet++: a nested U-net architecture for medical image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Stoyanov</surname><given-names>D.</given-names></name><name><surname>Taylor</surname><given-names>Z.</given-names></name><name><surname>Carneiro</surname><given-names>G.</given-names></name><name><surname>Syeda-Mahmood</surname><given-names>T.</given-names></name><name><surname>Martel</surname><given-names>A.</given-names></name><name><surname>Maier-Hein</surname><given-names>L.</given-names></name><name><surname>Tavares</surname><given-names>J.M.R.S.</given-names></name><name><surname>Bradley</surname><given-names>A.</given-names></name><name><surname>Papa</surname><given-names>J.P.</given-names></name><name><surname>Belagiannis</surname><given-names>V.</given-names></name><name><surname>Nascimento</surname><given-names>J.C.</given-names></name><name><surname>Lu</surname><given-names>Z.</given-names></name><name><surname>Conjeti</surname><given-names>S.</given-names></name><name><surname>Moradi</surname><given-names>M.</given-names></name><name><surname>Greenspan</surname><given-names>H.</given-names></name><name><surname>Madabhushi</surname><given-names>A.</given-names></name></person-group><source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</source><year>2018</year><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-00889-5_1</pub-id></element-citation></ref><ref id="bib0160"><label>32</label><element-citation publication-type="journal" id="sbref0160"><person-group person-group-type="author"><name><surname>Kervadec</surname><given-names>H.</given-names></name><name><surname>Bouchtiba</surname><given-names>J.</given-names></name><name><surname>Desrosiers</surname><given-names>C.</given-names></name><name><surname>Granger</surname><given-names>E.</given-names></name><name><surname>Dolz</surname><given-names>J.</given-names></name><name><surname>Ayed</surname><given-names>I.B.</given-names></name></person-group><article-title>Boundary loss for highly unbalanced segmentation</article-title><source>International Conference on Medical Imaging with Deep Learning</source><year>2019</year><fpage>285</fpage><lpage>296</lpage><comment>(Accessed 17 October 2019)</comment><ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v102/kervadec19a.html" id="intr0030">http://proceedings.mlr.press/v102/kervadec19a.html</ext-link></element-citation></ref><ref id="bib0165"><label>33</label><element-citation publication-type="journal" id="sbref0165"><person-group person-group-type="author"><name><surname>Kalousis</surname><given-names>A.</given-names></name><name><surname>Prados</surname><given-names>J.</given-names></name><name><surname>Hilario</surname><given-names>M.</given-names></name></person-group><article-title>Stability of feature selection algorithms</article-title><source>Fifth IEEE International Conference on Data Mining (ICDM&#x02019;05)</source><year>2005</year><pub-id pub-id-type="doi">10.1109/ICDM.2005.135</pub-id><comment>p. 8</comment></element-citation></ref><ref id="bib0170"><label>34</label><element-citation publication-type="journal" id="sbref0170"><person-group person-group-type="author"><name><surname>Nogueira</surname><given-names>S.</given-names></name><name><surname>Sechidis</surname><given-names>K.</given-names></name><name><surname>Brown</surname><given-names>G.</given-names></name></person-group><article-title>On the stability of feature selection algorithms</article-title><source>J. Mach. Learn. Res.</source><volume>18</volume><year>2018</year><fpage>1</fpage><lpage>54</lpage></element-citation></ref><ref id="bib0175"><label>35</label><element-citation publication-type="journal" id="sbref0175"><person-group person-group-type="author"><name><surname>Chlis</surname><given-names>N.</given-names></name><name><surname>Bei</surname><given-names>E.S.</given-names></name><name><surname>Zervakis</surname><given-names>M.</given-names></name></person-group><article-title>Introducing a stable bootstrap validation framework for reliable genomic signature extraction</article-title><source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source><volume>15</volume><year>2018</year><fpage>181</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1109/TCBB.2016.2633267</pub-id><pub-id pub-id-type="pmid">27913357</pub-id></element-citation></ref><ref id="bib0180"><label>36</label><element-citation publication-type="journal" id="sbref0180"><person-group person-group-type="author"><name><surname>Sauvola</surname><given-names>J.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name></person-group><article-title>Adaptive document image binarization</article-title><source>Pattern Recognit.</source><volume>33</volume><year>2000</year><fpage>225</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(99)00055-2</pub-id></element-citation></ref><ref id="bib0185"><label>37</label><element-citation publication-type="journal" id="sbref0185"><person-group person-group-type="author"><name><surname>Otsu</surname><given-names>N.</given-names></name></person-group><article-title>A threshold selection method from gray-level histograms</article-title><source>IEEE Trans. Syst. Man Cybern.</source><volume>9</volume><year>1979</year><fpage>62</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></element-citation></ref><ref id="bib0190"><label>38</label><element-citation publication-type="journal" id="sbref0190"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S.</given-names></name><name><surname>Sch&#x000f6;nberger</surname><given-names>J.L.</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J.</given-names></name><name><surname>Boulogne</surname><given-names>F.</given-names></name><name><surname>Warner</surname><given-names>J.D.</given-names></name><name><surname>Yager</surname><given-names>N.</given-names></name><name><surname>Gouillart</surname><given-names>E.</given-names></name><name><surname>Yu</surname><given-names>T.</given-names></name></person-group><article-title>scikit-image: image processing in Python</article-title><source>PeerJ.</source><volume>2</volume><year>2014</year><fpage>e453</fpage><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib0195"><label>39</label><element-citation publication-type="journal" id="sbref0195"><person-group person-group-type="author"><name><surname>Allman</surname><given-names>D.</given-names></name><name><surname>Reiter</surname><given-names>A.</given-names></name><name><surname>Bell</surname><given-names>M.A.L.</given-names></name></person-group><article-title>Photoacoustic source detection and reflection artifact removal enabled by deep learning</article-title><source>IEEE Trans. Med. Imaging</source><volume>37</volume><year>2018</year><fpage>1464</fpage><lpage>1477</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2829662</pub-id><pub-id pub-id-type="pmid">29870374</pub-id></element-citation></ref><ref id="bib0200"><label>40</label><element-citation publication-type="journal" id="sbref0200"><person-group person-group-type="author"><name><surname>Antholzer</surname><given-names>S.</given-names></name><name><surname>Haltmeier</surname><given-names>M.</given-names></name><name><surname>Schwab</surname><given-names>J.</given-names></name></person-group><article-title>Deep learning for photoacoustic tomography from sparse data</article-title><source>Inverse Probl. Sci. Eng.</source><volume>0</volume><year>2018</year><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1080/17415977.2018.1518444</pub-id></element-citation></ref></ref-list><bio><graphic xlink:href="fx1"/><p><bold>Angelos Karlas</bold> studied Medicine (M.D.) and Electrical and Computer Engineering (Dipl.-Ing.) at the Aristotle University of Thessaloniki, Greece. He holds a Master of Science in Medical Informatics (M.Sc.) from the same university and a Master of Research (M.Res.) in Medical Robotics and Image-Guided Intervention from Imperial College London, UK. He currently works as clinical resident at the Department for Vascular and Endovascular Surgery of the Rechts der Isar University Hospital in Munich, Germany. He holds the position of the Clinical Translation Manager at the Institute for Biological and Medical Imaging of the Helmholtz Center Munich and the Technical University of Munich, Germany, while pursuing his Ph.D. (Dr.rer.nat.) in Experimental Medicine at the same university. He also serves as the Group Leader of the Clinical Bioengineering Group at the Institute for Biological and Medical Imaging of the Technical University of Munich, Germany. His research interests are in the areas of innovative vascular imaging and image-guided vascular interventions.</p></bio><bio><graphic xlink:href="fx2"/><p><bold>Vasilis Ntziachristos</bold> received his PhD in electrical engineering from the University of Pennsylvania, USA, followed by a postdoctoral position at the Center for Molecular Imaging Research at Harvard Medical School. Afterwards, he became an Instructor and following an Assistant Professor and Director at the Laboratory for Bio-Optics and Molecular Imaging at Harvard University and Massachusetts General Hospital, Boston, USA. Currently, he is the Director of the Institute for Biological and Medical Imaging at the Helmholtz Zentrum in Munich, Germany, as well as a Professor of Electrical Engineering, Professor of Medicine and Chair for Biological Imaging at the Technical University Munich. His work focuses on novel innovative optical and optoacoustic imaging modalities for studying biological processes and dis-eases as well as the translation of these findings into the clinic.</p></bio><ack id="ack0005"><title>Acknowledgements</title><p id="par0120">N.K.C. acknowledges support from the <funding-source id="gs0005">Graduate School of Quantitative Biosciences Munich (QBM)</funding-source>. F.J.T. acknowledges financial support by the <funding-source id="gs0010">Graduate School QBM, by the Helmholtz Association</funding-source> (Incubator grant sparse2big, grant # ZT-I-0007), by the <funding-source id="gs0015"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100006603</institution-id><institution>BMBF</institution></institution-wrap></funding-source> (grant# 01IS18036A and grant# 01IS18053A) and by the <funding-source id="gs0020">Chan Zuckerberg Initiative DAF (advised fund of Silicon Valley Community Foundation)</funding-source>182835. C.M acknowledges support from the BMBF (e:Med grant MicMode-I2T). This project has received funding from the <funding-source id="gs0025">European Research Council (ERC)</funding-source> under the European Union&#x02019;s Horizon 2020 research and innovation program under grant agreement No 694968 (PREMSOT) and was supported by the <funding-source id="gs0030">DZHK (German Centre for Cardiovascular Research)</funding-source> and by the <funding-source id="gs0035">Helmholtz Zentrum M&#x000fc;nchen, funding program &#x0201c;Physician Scientists for Groundbreaking Projects&#x0201d;</funding-source>.</p></ack></back></article>