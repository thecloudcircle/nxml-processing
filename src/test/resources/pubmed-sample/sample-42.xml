
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="iso-abbrev">PLoS Comput. Biol</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">32569292</article-id><article-id pub-id-type="pmc">7347231</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1007918</article-id><article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-01759</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Animal Management</subject><subj-group><subject>Animal Performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Support Vector Machines</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Sociality</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Sociality</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal Behavior</subject><subj-group><subject>Animal Sociality</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Rodents</subject><subj-group><subject>Mice</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Statistical Methods</subject><subj-group><subject>Multivariate Analysis</subject><subj-group><subject>Principal Component Analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical Methods</subject><subj-group><subject>Multivariate Analysis</subject><subj-group><subject>Principal Component Analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</article-title><alt-title alt-title-type="running-head">Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8115-7488</contrib-id><name><surname>Ivanenko</surname><given-names>A.</given-names></name><role content-type="https://casrai.org/credit/">Formal analysis</role><role content-type="https://casrai.org/credit/">Investigation</role><role content-type="https://casrai.org/credit/">Software</role><role content-type="https://casrai.org/credit/">Validation</role><role content-type="https://casrai.org/credit/">Writing &#x02013; original draft</role><role content-type="https://casrai.org/credit/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2561-3458</contrib-id><name><surname>Watkins</surname><given-names>P.</given-names></name><role content-type="https://casrai.org/credit/">Formal analysis</role><role content-type="https://casrai.org/credit/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>van Gerven</surname><given-names>M. A. J.</given-names></name><role content-type="https://casrai.org/credit/">Writing &#x02013; original draft</role><role content-type="https://casrai.org/credit/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff004"><sup>4</sup></xref></contrib><contrib contrib-type="author"><name><surname>Hammerschmidt</surname><given-names>K.</given-names></name><role content-type="https://casrai.org/credit/">Writing &#x02013; original draft</role><role content-type="https://casrai.org/credit/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff005"><sup>5</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9106-0356</contrib-id><name><surname>Englitz</surname><given-names>B.</given-names></name><role content-type="https://casrai.org/credit/">Conceptualization</role><role content-type="https://casrai.org/credit/">Formal analysis</role><role content-type="https://casrai.org/credit/">Resources</role><role content-type="https://casrai.org/credit/">Software</role><role content-type="https://casrai.org/credit/">Supervision</role><role content-type="https://casrai.org/credit/">Visualization</role><role content-type="https://casrai.org/credit/">Writing &#x02013; original draft</role><role content-type="https://casrai.org/credit/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Neurophysiology, Donders Institute for Brain, Cognition and Behavior, Radboud University, Nijmegen, The Netherlands</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Institute of Biology and Biomedicine, Lobachevsky State University, Nizhny Novgorod, Russia</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>CAESAR, Bonn, Germany</addr-line></aff><aff id="aff004"><label>4</label>
<addr-line>Department of Artificial Intelligence, Donders Institute for Brain, Cognition and Behavior, Radboud University, Nijmegen, The Netherlands</addr-line></aff><aff id="aff005"><label>5</label>
<addr-line>Cognitive Ethology Laboratory, German Primate Center, G&#x000f6;ttingen, Germany</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Theunissen</surname><given-names>Fr&#x000e9;d&#x000e9;ric E.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>University of California at Berkeley, UNITED STATES</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>b.englitz@donders.ru.nl</email></corresp></author-notes><pub-date pub-type="epub"><day>22</day><month>6</month><year>2020</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2020</year></pub-date><volume>16</volume><issue>6</issue><elocation-id>e1007918</elocation-id><history><date date-type="received"><day>9</day><month>10</month><year>2019</year></date><date date-type="accepted"><day>30</day><month>4</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; 2020 Ivanenko et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Ivanenko et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pcbi.1007918.pdf"/><abstract><p>Vocalizations are widely used for communication between animals. Mice use a large repertoire of ultrasonic vocalizations (USVs) in different social contexts. During social interaction recognizing the partner's sex is important, however, previous research remained inconclusive whether individual USVs contain this information. Using deep neural networks (DNNs) to classify the sex of the emitting mouse from the spectrogram we obtain unprecedented performance (77%, vs. SVM: 56%, Regression: 51%). Performance was even higher (85%) if the DNN could also use each mouse's individual properties during training, which may, however, be of limited practical value. Splitting estimation into two DNNs and using 24 extracted features per USV, spectrogram-to-features and features-to-sex (60%) failed to reach single-step performance. Extending the features by each USVs spectral line, frequency and time marginal in a semi-convolutional DNN resulted in a performance mid-way (64%). Analyzing the network structure suggests an increase in sparsity of activation and correlation with sex, specifically in the fully-connected layers. A detailed analysis of the USV structure, reveals a subset of male vocalizations characterized by a few acoustic features, while the majority of sex differences appear to rely on a complex combination of many features. The same network architecture was also able to achieve above-chance classification for cortexless mice, which were considered indistinguishable before. In summary, spectrotemporal differences between male and female USVs allow at least their partial classification, which enables sexual recognition between mice and automated attribution of USVs during analysis of social interactions.</p></abstract><abstract abstract-type="summary"><title>Author summary</title><p>Many animals communicate by producing sounds, so-called vocalizations. Mice use many different kinds of vocalizations in different social contexts. During social interaction recognizing the partner's sex is important and female mice appear to know the difference between male and female vocalizations. However, previous research had suggested that male and female vocalizations are very similar. We here show for the first time that the emitter's sex can be guessed from the vocalization alone, even single ones. The full spectrogram was the best basis for this, while reduced representations (e.g. basic properties of the vocalization) were less informative. We therefore conclude that while the information about the emitter's sex is present in the vocalization, both mice and our analysis must rely on complex properties to determine it. This novel insight is enabled by the use of recent machine learning techniques. In contrast, we show directly that a number of more basic techniques fail in this challenge. In summary, differences in the vocalizations between male and female mice allow to guess the emitter's sex, which enables sexual recognition between mice and automated analysis. This is important in studying social interactions between mice and how speech is produced and analyzed in the brain.</p></abstract><funding-group><funding-statement>BE was supported by a European Commission Marie-Sklodowska Curie grant (#660328; <ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/research/mariecurieactions/">https://ec.europa.eu/research/mariecurieactions/</ext-link>), an NWO VIDI grant (016.189.052; <ext-link ext-link-type="uri" xlink:href="https://www.nwo.nl/en/funding/our-funding-instruments/nwo/innovational-researchincentives-scheme/vidi/index.html">https://www.nwo.nl/en/funding/our-funding-instruments/nwo/innovational-researchincentives-scheme/vidi/index.html</ext-link>), and a NOW grant (ALWOP.346; <ext-link ext-link-type="uri" xlink:href="https://www.nwo.nl/en/news-and-events/news/2018/06/new-open-competition-acrossnwo-domain-science.html">https://www.nwo.nl/en/news-and-events/news/2018/06/new-open-competition-acrossnwo-domain-science.html</ext-link>) during different parts of the project period. MG was supported by a NOW VIDI grant (639.072.513; <ext-link ext-link-type="uri" xlink:href="https://www.nwo.nl/en/funding/our-funding-instruments/nwo/innovational-researchincentives-scheme/vidi/index.html">https://www.nwo.nl/en/funding/our-funding-instruments/nwo/innovational-researchincentives-scheme/vidi/index.html</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="7"/><table-count count="7"/><page-count count="27"/></counts><custom-meta-group><custom-meta><meta-name>PLOS Publication Stage</meta-name><meta-value>vor-update-to-uncorrected-proof</meta-value></custom-meta><custom-meta><meta-name>Publication Update</meta-name><meta-value>2020-07-09</meta-value></custom-meta><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All raw and most processed data and code is available as collection di.dcn.DSC_620840_0003_891 and can be downloaded at <ext-link ext-link-type="uri" xlink:href="https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891">https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All raw and most processed data and code is available as collection di.dcn.DSC_620840_0003_891 and can be downloaded at <ext-link ext-link-type="uri" xlink:href="https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891">https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Sexual identification on the basis of sensory cues provides important information for successful reproduction. When listening to a conversation, humans can typically make an educated guess about the sexes of the participants. Limited research on this topic has suggested a range of acoustic predictors, mostly the fundamental frequency but also formant measures [<xref rid="pcbi.1007918.ref001" ref-type="bibr">1</xref>].</p><p>Similar to humans, mice vocalize frequently during social interactions [<xref rid="pcbi.1007918.ref002" ref-type="bibr">2</xref>&#x02013;<xref rid="pcbi.1007918.ref006" ref-type="bibr">6</xref>]. The complexity of the vocalizations produced during social interactions can be substantial [<xref rid="pcbi.1007918.ref007" ref-type="bibr">7</xref>&#x02013;<xref rid="pcbi.1007918.ref009" ref-type="bibr">9</xref>]. Experiments replaying male mouse courtship songs to adult females suggest that at least females are able to guess the sex of other mice based on the properties of individual vocalizations [<xref rid="pcbi.1007918.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1007918.ref011" ref-type="bibr">11</xref>].</p><p>While in humans and other species sex-specific differences in body dimensions (vocal tract length, vocal fold characteristics) lead to predictable differences in vocalization [<xref rid="pcbi.1007918.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1007918.ref013" ref-type="bibr">13</xref>], the vocal tract properties of male and female mice have not been shown to differ significantly [<xref rid="pcbi.1007918.ref014" ref-type="bibr">14</xref>,<xref rid="pcbi.1007918.ref015" ref-type="bibr">15</xref>]. Hence, for mice the expected differences in male/female USVs are less predictable from physiological characteristics and classification likely relies on more complex spectral properties.</p><p>Previous research on the properties of male and female ultrasonic vocalizations (USVs) in mice [<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>] found differences in usage of vocal types, but could not identify reliable predictors of the emitter&#x02019;s sex on the basis of single vocalizations.</p><p>This raises the question, whether the methods utilized were general enough to detect complex spectrotemporal differences. In several species, related classification tasks were successfully carried out using modern classifiers, e.g. hierarchical clustering of monkey vocalization types [<xref rid="pcbi.1007918.ref017" ref-type="bibr">17</xref>], or random forests for zebra finch vocalization types [<xref rid="pcbi.1007918.ref018" ref-type="bibr">18</xref>,<xref rid="pcbi.1007918.ref019" ref-type="bibr">19</xref>], non-negative matrix factorization/clustering for mouse vocalization types [<xref rid="pcbi.1007918.ref020" ref-type="bibr">20</xref>,<xref rid="pcbi.1007918.ref021" ref-type="bibr">21</xref>] or deep learning to detect mouse USVs [<xref rid="pcbi.1007918.ref022" ref-type="bibr">22</xref>], but have not addressed the task of determining the emitter's sex from individual USVs.</p><p>Here we find that the distinction of mouse (C57Bl/6) male/female USVs can be successfully performed using advanced classification using deep learning [<xref rid="pcbi.1007918.ref023" ref-type="bibr">23</xref>]: A custom-developed Deep Neural Network (DNN) reaches an average accuracy of 77%, substantially exceeding the performance of linear (ridge regression, 51%) or nonlinear (support vector machines, SVM [<xref rid="pcbi.1007918.ref024" ref-type="bibr">24</xref>], 56%) classifiers, which could be further improved to 85%, if properties of individual mice are available and can be included in the classifier.</p><p>Our DNN exploits a complex combination of differences between male/female USVs, which individually are insufficient for classification due to a high degree of within-sex variability. An analysis of the acoustic properties contributing to classification, directly shows that for most USVs only a complex set of properties is sufficient for this task. In contrast a DNN classification on the basis of a conventional, human-rated feature set or reduced spectrotemporal properties performs much less accurately (60% and 70%, respectively). An analysis of the full network's classification strategy indicates a feature expansion in the convolutional layers, followed by a sequential sparsening and subclassification of the activity in the fully-connected layers. Applying the same classification techniques to another dataset, we can partly distinguish a (nearly) cortexless mutant strain from a wild-type strain, which had previously been considered indistinguishable on the basis of general analysis of USVs [<xref rid="pcbi.1007918.ref025" ref-type="bibr">25</xref>].</p><p>The present results indicate that the emitter's sex and/or strain can be deduced from the USV's spectrogram if sufficiently nonlinear feature combination are exploited. The ability to perform this classification provides an important building block for attributing and analyzing vocalizations during social interactions of mice. As USVs are also important biomarkers in evaluating animal models of neural diseases [<xref rid="pcbi.1007918.ref026" ref-type="bibr">26</xref>,<xref rid="pcbi.1007918.ref027" ref-type="bibr">27</xref>], in particular in social interaction, the attribution of individual USVs to their respective emitter is gaining importance.</p></sec><sec sec-type="results" id="sec002"><title>Results</title><p>We reanalyzed recordings of ultrasonic vocalizations (USVs) from single mice during a social interaction with an anesthetized mouse (N = 17, in 9 female awake, in 8 male awake, <xref ref-type="fig" rid="pcbi.1007918.g001">Fig 1A</xref>, [<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>]). The awake mouse vocalized actively in each recording (Male: 181&#x000b1;32 min<sup>-1</sup>, Female: 212&#x000b1;14 min<sup>-1</sup>, <xref ref-type="fig" rid="pcbi.1007918.g001">Fig 1B</xref>) giving a total of 10055 (female: 5723, male: 4332) automatically extracted USVs of varying spectrotemporal structure and uniquely known sex of the emitter (<xref ref-type="fig" rid="pcbi.1007918.g001">Fig 1C</xref>). Previous approaches for assessing the gender using basic analysis have not led to single-vocalization level predictability [<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>]. Here, we utilized a general framework of deep neural networks to predict the emitter's sex for single vocalizations (<xref ref-type="fig" rid="pcbi.1007918.g001">Fig 1D</xref>). After obtaining best-in-class performance on this challenge, we further investigate the basis for this performance. For this purpose, separate DNNs were trained for predicting features from spectrograms as well as sex from features, on the basis of human-classified features. Lastly, we analyze both the network's structure as well as the space of vocalizations in relation.</p><fig id="pcbi.1007918.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.g001</object-id><label>Fig 1</label><caption><title>Recording and classifying mouse vocalizations.</title><p><bold>A</bold> Mouse vocalization were recorded from a pair of mice, in which one was awake, while the other was anesthetized, allowing an unambiguous attribution of the recorded vocalizations. <bold>B</bold> Vocalization from male and female mice (recorded in separate sessions) share a lot of properties, while differing in others. The present samples were picked at random and indicate that differences exist, while other samples would look more similar. <bold>C</bold> Vocalizations were automatically segmented using a set of filtering and selection criteria (see <xref ref-type="sec" rid="sec018">Methods</xref> for details), leading to a total set of 10055 vocalizations. <bold>D</bold> We aimed to estimate the properties and the sex of its emitter for individual vocalizations. First, the ground truth for the properties were established by a human classifier. We next estimated 3 relations, Spectrogram-Properties, Properties-Sex and Spectrogram-Sex directly, using both a Deep Neural Network (DNN), support vector machines (SVM) and regularized linear regression (LR). <bold>E</bold> The properties attributed manually to individual vocalizations could take different values (rows, red number in each subpanel), illustrated here for a subset of the properties (columns). See <xref ref-type="sec" rid="sec018">Methods</xref> for a detailed list and description of the properties.</p></caption><graphic xlink:href="pcbi.1007918.g001"/></fig><sec id="sec003"><title>Basic USV features differ between the sexes, but are insufficient to distinguish single USVs</title><p>While previous approaches have indicated few differences between male and female vocalizations ([<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>], but see [<xref rid="pcbi.1007918.ref004" ref-type="bibr">4</xref>] for CBA mice), we reassess this question first using a set of nine hand-picked features, quantifying spectrotemporal properties of the USVs (see <xref ref-type="fig" rid="pcbi.1007918.g001">Fig 1E</xref> and <xref ref-type="sec" rid="sec018">Methods</xref> for details). The first three were quantified automatically, while the latter six were scored by human classifiers.</p><p>We find significant differences in multiple features (7/9, see <xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2A&#x02013;2I</xref> for details, based on Wilcoxon Signed Ranks test) between the sexes (in all figures: red = female, blue = male). This suggests that there is exploitable information about the emitter's sex. However, the substantial variability across USVs renders each feature in isolation insufficient for classifying single USVs (see percentiles in <xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2A&#x02013;2I</xref>). Next, we investigated whether the joint set of features or the raw spectrograms have some sex-specific properties that can be exploited for classification.</p><fig id="pcbi.1007918.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.g002</object-id><label>Fig 2</label><caption><title>Basic sex-dependent differences between vocalizations.</title><p>(<bold>A-I</bold>) We quantified a range of properties for single vocalizations (see <xref ref-type="sec" rid="sec018">Methods</xref> for details) and compared them across the sexes (blue: male, red: female). Most properties exhibited significant differences in median between the sexes (Wilcoxon rank sum test), except for the average frequency (<bold>B</bold>) and directionality (<bold>D</bold>). However, given the distribution of the data (box-plots, left in each panel), the variability across each property nonetheless makes it hard to use individual properties of determining the sex of the emitter. The graphs on the right for each color in each panel, show the mean and SEM. In <bold>G-I</bold>, only few USVs have values different than 0, hence the box-plots are sitting at 0. (<bold>J-M</bold>) Dimensionality reduction can reveal more complex relationships between multiple properties. We computed principal components (PCA) and t-statistic stochastic neighborhood embedding (t-SNE) for both the features (<bold>J/L</bold>) and the spectrograms (<bold>K/M</bold>). In particular, feature-based t-SNE (<bold>L</bold>) obtained some interesting groupings, which did, however, not separate well between the sexes (red vs. blue, see <xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7</xref> for more details). Each dot represents a single vocalization, after dimensionality reduction. Axes are left unlabelled, since they represent a combination of properties.</p></caption><graphic xlink:href="pcbi.1007918.g002"/></fig><p>Applying principal component analysis (PCA) to the 9-dimensional set of features and retaining three dimensions, a largely intermingled representation is obtained (<xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2J</xref>). PCA of the spectrograms provides a slightly more structured spatial layout in the most variable three dimensions (<xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2K</xref>). However, as for the features, the sexes overlap in space with no apparent separation. The basic dimensionality reduction performed by PCA, hence, reflects the co-distribution of properties and spectrograms between the sexes. At the same time it fails to even pick-up the basic properties along which the sexes differ (see above).</p><p>Using instead t-Distributed Stochastic Neighborhood Embedding (tSNE, [<xref rid="pcbi.1007918.ref028" ref-type="bibr">28</xref>]) for dimensionality reduction, reveals a clustered structure for both features (<xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2L</xref>) and spectrograms (<xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2M</xref>). For the features the clustering is very clean, and reflects the different values of the features (e.g. different number of breaks define different clusters, or gradients within clusters (Duration/Frequency)). However, USVs from different sexes are quite close in space and co-occur in the all clusters, although density-differences (of male or female USVs) exist in individual clusters. For the spectrograms the representation is less clear, but still shows much clearer clustering than after PCA. Mapping feature properties to the local clusters visible in the t-SNE did not indicate a relation between the properties and the suggested grouping based on t-SNE.</p><p>In summary, spectrotemporal features in isolation or in basic conjunction appear insufficient to reliably classify single vocalizations by their emitter's sex. However, the prior analyses do not attempt to directly learn sex-specific spectrotemporal structure from the given data-set, but instead assume a restricted set of features. Next, we used data-driven classification algorithms to directly learn differences between male and female vocalizations.</p></sec><sec id="sec004"><title>Convolutional deep neural network identifies the emitter's sex from single USVs</title><p>Inspired by the recent successes of convolutional deep neural networks (cDNNs) in other fields, e.g. machine vision [<xref rid="pcbi.1007918.ref023" ref-type="bibr">23</xref>], we focus here on a direct classification of the emitter's sex based on the spectrograms of single USVs. The architecture chosen is a&#x02014;by now&#x02014;classical network structure of convolutional layers, followed by fully connected layers, and a single output representing the probability of a male (or conversely female) source (see <xref ref-type="sec" rid="sec018">Methods</xref> for details and <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3A</xref>). The cDNN performs surprisingly well, in comparison to other classification techniques (see below).</p><fig id="pcbi.1007918.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.g003</object-id><label>Fig 3</label><caption><title>Deep neural network reliably determines the emitter's sex from individual vocalizations.</title><p><bold>A</bold> We trained a deep neural network (DNN) with 6 convolutional and 3 fully connected layers to classify the sex of emitter from the spectrogram of the vocalization. <bold>B</bold> The network's performance on the (Female #1) test set rapidly improved to an asymptote of ~80% (dark red), clearly exceeding chance. Correspondingly the change in the network's weights (light red) progressively decreased after stabilizing after ~6k iterations. Data shown for a representative training run. <bold>C</bold> The shape of the input fields in the first convolutional layer became more reminiscent of the tuning curves in the auditory system [<xref rid="pcbi.1007918.ref050" ref-type="bibr">50</xref>,<xref rid="pcbi.1007918.ref051" ref-type="bibr">51</xref>]. Samples are representatively chosen among the entire set of 256 units in this layer. <bold>D</bold> The average performance of the DNN (cross-validation across animals) was 76.7&#x000b1;6.6%, which did not differ significantly between male and female vocalizations (p&#x0003e;0.05, Wilcoxon rank sum test). <bold>E</bold> The DNN performance by far exceeded the performance of ridge regression (regularized linear regression, blue, 50.7&#x000b1;1.6%) and support vector machines (SVM, green, 56.2&#x000b1;1.9%). Bars in light colors show the corresponding estimation with randomized labels, which are all at chance level (gray line). <bold>F</bold> The performance by the DNN was not only limited by the properties of the spectrograms (e.g. background noise, sampling, etc.) since a DNN trained on the number of breaks (right bars) performed significantly better. This control shows that the identical set of stimuli can be better separated on a simpler (but also binary) task. Light bars again show performance on randomized labels.</p></caption><graphic xlink:href="pcbi.1007918.g003"/></fig><p>During the training procedure the cDNN's parameters are adapted to optimize performance on the test set, which was not used for training (see <xref ref-type="sec" rid="sec018">Methods</xref> for details on cross-validation). Performance starts out near chance level (red, <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3B</xref>), with the initial iterations leading to substantial changes in weights (light red). It takes ~6k iterations before the cDNN settles close to its final, asymptotic performance (here 80%, see below for average).</p><p>The learning process modifies the properties of the DNN throughout all layers. In the first layer, the neurons parameters can be visualized in the stimulus domain, i.e. similar to spatial receptive fields for real neurons. Initial random configurations adapt to become more classical local filters, e.g. shaped like local patches or lines (<xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3C</xref>). This behavior is well documented for visual classification tasks, however, it is important to verify it for the current, limited set of auditory stimuli.</p><p>The cDNN classified single USVs into their emitter's sex at 76.7&#x000b1;6.6% (median&#x000b1;SEM, crossvalidation performed across animals, i.e. leave-one-out, n = 17 runs, <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3D</xref>). Classification performance did not differ significantly between male and female vocalizing mice (p&#x0003e;0.05, Wilcoxon signed ranks test).</p><p>In comparison with more classical classification techniques, such as regularized linear regression (Ridge, blue, <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3E</xref>) and support vector machines (SVM, green), the DNN (red) performs significantly better (Ridge: 50.7&#x000b1;1.6%; SVM: 56.2&#x000b1;1.9%; DNN: 76.7&#x000b1;6.6%; all comparisons: p&#x0003c;0.01). Shuffling the sexes of all vocalizations leads to chance performance, and thus indicates that the performance is not based on overlearning properties of a random set (<xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3E</xref>, light colors). Instead it has to rely on the characteristic property within one sex. Further, classification is significantly better than chance for 15/17 animals (see below and <xref ref-type="supplementary-material" rid="pcbi.1007918.s001">S1A Fig</xref> for contributions by individual animals).</p><p>Lastly, to verify that the general properties of the spectrograms do not pose a sex-independent limit on classification performance, we evaluated the performance of the same network on another complex task: to determine whether a vocalization has a break or not. On the latter task the network can do significantly better than on classifying the emitter's sex, reaching 93.3% (p&#x0003c;0.001, Wilcoxon signed ranks test, <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3F</xref>).</p><p>Altogether, the estimated cDNN clearly outperforms classical competitors on the task and reaches a substantial performance, which could potentially be further improved, e.g. by the addition of more data for refining the DNN's parameters.</p></sec><sec id="sec005"><title>Including individual vocalization properties improves classification accuracy</title><p>The network trained above did not have access to properties of vocalization of individual mice. We next asked to what degree individual properties can aid the training of a DNN to improve the attribution of vocalizations to an emitter. This was realized by using a randomly selected set of USVs for crossvalidation, such that the training set contained USVs from every individual.</p><p>The resulting classification performance increases further to 85.1&#x000b1;2.9% (median across mice), and now all mice are recognized better than chance (<xref ref-type="supplementary-material" rid="pcbi.1007918.s001">S1B Fig</xref>). We also tested to which degree the individual properties suffice for predicting the identity of the mouse instead of sex (see <xref ref-type="sec" rid="sec018">Methods</xref> for details on the output). The average performance is 46.4&#x000b1;7.5% (median across animals, <xref ref-type="supplementary-material" rid="pcbi.1007918.s001">S1C Fig</xref>), i.e. far above chance performance (5.9% if sex is unknown, or at most 12.5% if the sex is known), which is also reflected in all of the individual animals (M3 has the largest p-value at 0.0002, binomial test against chance level).</p><p>Hence, while not fully identifiable, individual mice appear to shape their vocalizations in characteristic ways, which contributes to the increased performance of the DNN trained using crossvalidation on random test sets (<xref ref-type="supplementary-material" rid="pcbi.1007918.s001">S1B Fig</xref>). It should be emphasized, however, that information on the individual vocalization properties will not be available in every experiment and thus this improvement has only limited applicability.</p></sec><sec id="sec006"><title>USV features are insufficient to explain cDNN performance on sex classification</title><p>Simple combinations of features were insufficient to identify the emitter's sex from individual USVs, as demonstrated above (<xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2</xref>). However, it could be that more complex combinations of the same features would be sufficient to reach similar performance as for the spectrogram-based classification (<xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3</xref>). We address this question by predicting the sex from the features alone&#x02014;as classified by a human&#x02014;using another DNN (see <xref ref-type="sec" rid="sec018">Methods</xref> for details and below). Further, we check whether the human-classified features can be learned by a cDNN. Together, these two steps provide a stepwise classification of sex from spectrograms.</p><p>Starting with the second point, we estimated separate cDNNs for 4 of the 6 features, i.e. direction, number of breaks, number of peaks and spectral breadth of activation ('broadband'). The remaining two features&#x02014;tremolo and complexity&#x02014;were omitted, since most vocalizations scored very low in these values, thus creating a very skewed training set of the networks. A near optimal&#x02014;but uninteresting&#x02014;learning outcome is then a flat, near zero classification. The network structure of these cDNNs was chosen identical to the sex-classification network, for simpler comparison (<xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4A</xref>).</p><fig id="pcbi.1007918.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.g004</object-id><label>Fig 4</label><caption><title>Features alone are insufficient to explain the DNN classification performance.</title><p><bold>A</bold> Features of individual vocalizations can also be measured using dedicated convolutional DNNs, one per feature, with identical architecture as for sex classification (see <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3A</xref>). <bold>B-E</bold> Classification performance for different properties was robust, ranging between 57.0 and 82.0% on average (maroon) and depending on the individual value of each property (red). We trained networks for direction ({-1,0,1}, <bold>B</bold>), the number of breaks ({0&#x02013;3}, <bold>C</bold>), the number of peaks ({0&#x02013;3}, <bold>D</bold>) and the degree of broadband activation ([0,1], <bold>E</bold>). For the other 2 properties (complex and tremolo), most values were close to 0 and thus networks did not have sufficient training data for these. The light gray lines indicate chance performance, which depends on the number of choices for each property. The light blue bars indicate the distributions of values, also in %. <bold>F</bold> Using a non-convolutional DNN, we investigated how predictable features alone would be, i.e. without any information about the precise spectral structure of each vocalization. <bold>G</bold> Prediction performance was above chance (maroon, 59.6&#x000b1;3.0%) but less than the prediction of sex on the basis of the raw spectrograms (see <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3</xref>). The gray line indicates chance performance. <bold>H</bold> Feature-based prediction of sex with DNNs performed similarly compared to ridge regresson (blue) and SVM (red, see main text for statistics). <bold>I</bold> Duration, volume and the level of broadband activation were the most significant linear predictors for sex, when using ridge regression. <bold>J</bold> Using a semi-convolutional DNN, we investigated the combined predictability of the same features as above, plus 3 statistics of the stimulus (each a vector), i.e. the marginal of the spectrogram in time and frequency, as well as the spectral line, i.e. the sequence of frequencies of maximal amplitude per time-bin. <bold>K</bold> The average performance of the semi-convolutional DNN (64.5%) stays substantially lower than the 2D cDNN (see <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3D</xref>). USVs of both sexes were predicted with similar accuracy. <bold>L</bold> The average performance of the semi-convolutional DNN is not significantly larger than ridge regression (61.9%) or SVM (62.7%) on the same data, due to the large variability across the sexes (see Panel <bold>K</bold>).</p></caption><graphic xlink:href="pcbi.1007918.g004"/></fig><p>Average performance for all four features was far above chance (<xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4B&#x02013;4E</xref>, maroon, respective chance levels shown in gray), with some variability across the different values of each property (<xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4B&#x02013;4E</xref>, red). This variability is likely a consequence of the distribution of values in the overall set of USVs (<xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4B&#x02013;4E</xref>, light blue, scaled also in percentage) in combination with their inherent difficulty in prediction as well as higher variability in human assessment. Except for the broadband classification (82.0&#x000b1;0.4%), the classification performance for the features (Direction: 73.0&#x000b1;0.5%; Breaks: 68.6&#x000b1;0.4%; Peaks: 57.0&#x000b1;1.0%) stayed below the one for sex (76.7&#x000b1;0.9%).</p><p>Next, we estimated a DNN without convolutional layers for predicting the emitter's sex from a basic set of 9 features (see <xref ref-type="sec" rid="sec018">Methods</xref> for details, and <xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4F</xref>). The overall performance of the DNN was above chance (59.6&#x000b1;3.0%, <xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4G</xref>, maroon), but remained well below the full spectrogram performance (76.7%). The DNN performed similarly on these features as SVM (57.5&#x000b1;3.7%) and ridge regression (61.7&#x000b1;3.0%) (<xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4H</xref>), suggesting that the non-convolutional DNN on the features did not have a substantial advantage, pointing to the relevance of the convolutional layers in the context of the spectrogram data. For ridge regression, the contribution of the different features in predicting the emitter's sex can be assessed directly (<xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4I</xref>), highlighting the USVs' duration, volume and spectral breadth as the most distinguishing properties (compare also [<xref rid="pcbi.1007918.ref004" ref-type="bibr">4</xref>]). Volume appears surprising, given that the relative position between the microphone and the freely moving mouse was uncontrolled, although there may still be an average effect based on the sex-related differences in vocalization intensity or head orientation.</p><p>Finally, we investigated whether describing each USV by a more encompassing set of extracted features would allow higher prediction quality. In addition to the 9 basic features above we included 15 additional extracted features (see <xref ref-type="sec" rid="sec018">Methods</xref> for full description) and in particular certain spectrotemporal, 1D features of vocalizations. Briefly, for the latter we chose the fundamental frequency line (frequency with maximal intensity per time point in spectrogram, dim = 100), the marginal frequency content (dim = 233), the marginal intensity progression (dim = 100). Together with a total of 24 extracted, single value properties (see <xref ref-type="sec" rid="sec018">Methods</xref> for list and description), each USV was described by a 457 dimensional vector. For the DNN, the three 1D properties were fed each into a separate 1D convolutional stack (see <xref ref-type="sec" rid="sec018">Methods</xref> for details), and then combined with the 24 features as input to the subsequent fully connected layers. We refer to this network as a semi-convolutional DNN.</p><p>The performance of this network (64.5&#x000b1;2.1%) was significantly higher than the basic feature network (59.3&#x000b1;3.0%, p&#x0003c;0.05, Wilcoxon signed ranks test), however, did not reach the performance of the full spectrogram network. We also ran corresponding ridge regression and SVM estimates for completeness, whose performance remained on average below the semi-convolutional DNN, which was, however, nonsignificant.</p><p>While the estimation of certain features is thus possible from the raw spectrograms, their predictiveness for the emitter's sex stays comparatively low for both simple and advanced prediction algorithms. While the inclusion of further spectrotemporal features improved the performance, it still stayed below the cDNN performance for full spectrograms. A sequential combination of the two networks&#x02014;i.e. raw-to-feature followed directly by feature-to-gender&#x02014;would perform worse than either of the two. Hence, we hypothesize that the direct sex classification from spectrograms must rely on a different set of local or global features, not well captured in the present set of features.</p></sec><sec id="sec007"><title>Classification of different strains</title><p>For social interactions, typically same strain animals are used. However, in understanding the neural basis of USV production, mutant animals are of great value. Recently, [<xref rid="pcbi.1007918.ref025" ref-type="bibr">25</xref>] analyzed social vocalizations of <italic>Emx1-CRE;Esco2</italic> mice, which lack the hippocampus and nearly all of cortex. Contrary to expectation, they concluded that their USVs did not different significantly, questioning the role of cortex in the patterning of individual USVs. We reanalyzed their dataset using the same DNN-architecture as described above, finding significant differences between WT and mutant mice on the basis of their USVs (63.4&#x000b1;5.3% correct excluding individual properties (<xref ref-type="fig" rid="pcbi.1007918.g005">Fig 5A</xref>), again outpacing linear and nonlinear classification methods (Ridge regression (blue), 51.0&#x000b1;1.5%, p = 0.017) and support vector machines (SVM, green, 55.0&#x000b1;4.1%, p = 0.089, i.e. close, but not significant at the p&#x0003c;0.05 level), <xref ref-type="fig" rid="pcbi.1007918.g005">Fig 5B</xref>). Including individual properties again improved performance substantially (74.2% with individual properties, i.e. via random test-sets, <xref ref-type="supplementary-material" rid="pcbi.1007918.s002">S2A Fig</xref>), although this may often not be a desirable or feasible option.</p><fig id="pcbi.1007918.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.g005</object-id><label>Fig 5</label><caption><title>Deep neural network partly determines the emitter's strain from individual vocalizations.</title><p>We trained a deep neural network (DNN) with 6 convolutional and 3 fully connected layers to classify the strain (WT vs. cortexless) from the spectrograms of each vocalization (see <xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3A</xref> for a schematic of the network structure). <bold>A</bold> The average performance of the DNN (cross-validation across recordings) was 63.4&#x000b1;5.3%. WT vocalizations were classified with an accuracy that did not differ statistically. <bold>B</bold> The DNN performance exceeded the performance of ridge regression (regularized linear regression, blue, 51.0&#x000b1;1.5%) and support vector machines (SVM, green, 55.0.2&#x000b1;4.1%). Bars in light colors show the corresponding estimation with randomized labels, which are all at chance level (gray line).</p></caption><graphic xlink:href="pcbi.1007918.g005"/></fig><p>While the classification is not as clear as between male/female vocalizations, we suggest that the previous conclusion regarding the complete lack of distinguishability between WT and <italic>Emx1-CRE;Esco2</italic> needs to be revisited.</p></sec><sec id="sec008"><title>Separation of sexes and representation of stimuli increases across layers</title><p>While the sex classification cDNN's substantial improvement in performance over alternative methods is remarkable (<xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3</xref>), we do not have direct insight into the internal principles that it applies to achieve this performance. We investigated the network's structure by analyzing its activity and stimulus representation across layers ('deconvolution', [<xref rid="pcbi.1007918.ref029" ref-type="bibr">29</xref>]), using the tf_cnnvis package [<xref rid="pcbi.1007918.ref030" ref-type="bibr">30</xref>]. Briefly, we performed two sets of analyses: (i) the across-layer evolution of neuronal activity for individual vocalizations in relation to gender classification, and (ii) the stimulus representation across and within layers in relation to gender classification, for details see <xref ref-type="sec" rid="sec018">Methods</xref>.</p><p>First, we investigated the patterns of neuronal activation as a function of layer in the network (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6A&#x02013;6D</xref>). We illustrate the representation for two randomly drawn vocalizations (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6A</xref> top: Female example; bottom: Male example). In the convolutional layers (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6B</xref>), the activation pattern transitions from a localized, stimulus-aligned representation to a coarsened representation, following the progressive neighborhood integration in combination with the stride (set to [<xref rid="pcbi.1007918.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1007918.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1007918.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1007918.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1007918.ref001" ref-type="bibr">1</xref>] across the layers). As the activation reaches the fully connected layers, all spatial relations to the stimulus are discarded.</p><fig id="pcbi.1007918.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.g006</object-id><label>Fig 6</label><caption><title>Structural analysis of network activity and representation.</title><p>Across the layers of the network (top, columns in B/F) the activity progressively dissociated from the image level (compare A and B) (left, A/E). The stimuli (<bold>A</bold>, samples, top: female; bottom: male) are initially encoded spatially in the early convolutional layers (<bold>B</bold>, left, CV), but progressively lead to more general activation. In the fully connected layers (<bold>B</bold>, right, FC), the representation becomes non-spatial. Concurrently, the sparsity (<bold>C</bold>) and within-sex correlation between FC representations increases (<bold>D</bold>, red/blue) towards the output layer, while across-sex correlation decreases (<bold>D</bold>, purple). The average correlation, however, stays limited to ~0.2, and thus the final performance is only achieved in the last step of pooling onto the output units. Using deconvolution [<xref rid="pcbi.1007918.ref029" ref-type="bibr">29</xref>], the network representation was also studied across layers. The representation of individual stimuli (<bold>E</bold>) became more faithful to the original across layers (<bold>F</bold>, from left to right, top: female, bottom: male sample). Correlating the deconvolved stimulus with the original stimulus exhibited a fast rise to an asymptotic value (<bold>G</bold>). In parallel, the similarity of the representation between the neurons of a layer improved through the network (<bold>H</bold>). In all plots in the right column, the error bars indicate 2 SEMs, which are vanishingly small due to the large number of vocalizations/neurons each point is based on.</p></caption><graphic xlink:href="pcbi.1007918.g006"/></fig><p>The sparsity of representation across layers of the network increased significantly, going from convolutional to fully connected (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6C</xref>, Female: red; Male: blue; sparsity was computed as 1 - #[active units]/#[total units], ANOVA, p indistinguishable from 0, for n's see # of USVs in <xref ref-type="sec" rid="sec018">Methods</xref>). This finding bears some resemblance with cortical networks, where higher level representations become sparser (e.g. [<xref rid="pcbi.1007918.ref031" ref-type="bibr">31</xref>]). Also, the correlation between activation patterns of same-sex vocalizations increased strongly and significantly across layers (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6D</xref>, red and blue). Conversely, the correlation across different-sex activation patterns became significantly more negative, in particular in the last fully connected layer (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6D</xref>, purple). Together, these correlations form a good basis for classification as male/female, by the weights of the output layer (top, right).</p><p>Second, we investigated the stimulus representation as a function of layer (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6E&#x02013;6H</xref>). The representation of the original stimulus (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6E</xref>) became successively more accurate (see <xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6G</xref> below) stepping through the layers of the network (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6F</xref>, left to right). While lower layers exhibited still some ambiguity regarding the detailed shape, the stimulus representation in higher layers reached a plateau around convolutional layer 4/5, at 0.4 for males and ~0.46 for females (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6G</xref>). This correlation appears lower than indicated visually, however, some contamination remained around the vocalization. Across neurons in a layer, this representation stabilized across layers, reaching a near-identical representation on the highest level (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6H</xref>).</p><p>In summary, both the separation of the sexes and the representation of the stimuli improved as a function of the layer, suggesting a step-wise extraction of classification relevant properties. While the convolutional layers appear to mostly expand the representation of the stimuli, the fully connected layers then become progressively more selective for the final classification.</p></sec><sec id="sec009"><title>Complex combination of acoustic properties identifies emitter's sex</title><p>Lastly, it would be insightful to understand differences between male and female vocalizations on the level of their acoustic features. As the original space of the vocalizations is high-dimensional (~10000 given our resolution of the spectrogram), we performed a dimensionality reduction using t-SNE [<xref rid="pcbi.1007918.ref028" ref-type="bibr">28</xref>] to investigate the structure of the space of vocalizations and their relation to emitter sex and acoustic features.</p><p>The vocalizations' t-SNE representation in three dimensions exhibits both particular large scale structure as well as some local clustering (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7A</xref>). Male (blue) and female (red) vocalizations are not easily separable but can form local clusters or at least exhibit differences in local density. The most salient cluster is formed by male vocalizations (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7A</xref>, bottom left), which contains almost no female vocalizations. Examples from this cluster (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7B</xref>, bottom three) are similar in appearance and differ from vocalizations outside this cluster (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7B</xref>, top two). Importantly, vocalizations in this cluster do not arise from a single male, but all male mice contribute to the cluster.</p><fig id="pcbi.1007918.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.g007</object-id><label>Fig 7</label><caption><title>In-depth analysis of vocalization space indicates complex combination of properties distinguishing emitter sex.</title><p><bold>A</bold> Low-dimensional representation of the entire set of vocalizations (t-SNE transform of the spectrograms from 10^4 to 3 dimensions) shows limited clustering and structuring, and some separation between male (blue) and female (red) emitters. See also <xref ref-type="supplementary-material" rid="pcbi.1007918.s003">S1 Video</xref>, which is a dynamic version of the present figure, revolving all plots for clarity. <bold>B</bold> Individual samples of vocalizations, where the bottom three originate from the separate, large male cluster in the lower left of <bold>A</bold>. They all have a similar, long low-frequency call, combined with a higher-frequency, delayed call. The male cluster contains vocalizations from all male mice, and is hence not just an individual property. This indicates that a subset of vocalizations is rather characteristic for its emitter's sex. <bold>C</bold> The difference (bottom) between male (top left) and female (top right) densities indicates interwoven subregions of space dominated by one sex, i.e. blue subregions indicate male-dominant vocalization types, and red subregions female dominant. <bold>D</bold> Restricting to a subset of clearly identifiable vocalizations (based on DNN output certainty, &#x0003c;0.1 (female) and &#x0003e;0.9 (male)) provides only limited improvement in separation, indicating that the DNN decides based on a complex combination of subregions/spectrogram properties. <bold>E</bold> Mean frequency of the vocalization exhibits local neighborhoods on the tSNE representation, in particular linking the dominantly male cluster with exceptionally low frequencies. <bold>F</bold> Similarly, the frequency range of vocalizations in the dominantly male cluster is comparably high. <bold>G</bold> Lastly, the typical duration of the dominantly male cluster lies in a middle range, while not being exclusive in this respect.</p></caption><graphic xlink:href="pcbi.1007918.g007"/></fig><p>The local density of male and female USVs already appears different on the large scale (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7C</xref>, top), mostly due to the lack of the male cluster (described above). Taking the difference in local density between male and female USVs (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7C</xref>, bottom), shows that differences also exist throughout the overall space of vocalizations (indicated by the presence of red and blue regions, which would not be present in the case of matched densities). These differences in local density can be the basis for classification, e.g. performed by a DNN.</p><p>Performing nearest neighbor decoding [<xref rid="pcbi.1007918.ref032" ref-type="bibr">32</xref>] using leave-one-out crossvalidation on the t-SNE representation yielded a prediction accuracy of 62.0%. This indicates that the dimensionality reduction performed by t-SNE is useful for the decoding of emitter sex, while not nearly sufficient to attain the performance of the DNN on the same task.</p><p>The classification performed by the DNN can help to identify typical representatives of male and female vocalizations. For this purpose we restrict the USVs to those that the DNN assigned as 'clearly female' (&#x0003c;0.1, red) and 'clearly male' (&#x0003e;0.9, blue), which leads to slightly better visual separation and significantly better nearest neighbor decoding (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7D</xref>, 66.8%).</p><p>Basic acoustic properties could help to explain both the t-SNE representation as well as provide an intuition to different usage of USVs between the sexes. Among a larger set tested, we here present mean frequency (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7E</xref>), frequency range (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7F</xref>) and duration (<xref ref-type="fig" rid="pcbi.1007918.g007">Fig 7G</xref>). Mean frequency shows a clear neighborhood mapping on the t-SNE representation. The male cluster appears to be dominated by low average frequency, and also a wide frequency range and a mid-range duration (~100 ms).</p><p>Together, we interpret this to indicate that the DNNs classification is largely based on a complex combination of acoustic properties, while only a subset of vocalizations is characterized by a limited set of properties.</p></sec></sec><sec sec-type="conclusions" id="sec010"><title>Discussion</title><p>The analysis of social communication in mice is a challenging task, which has gained traction over the past years. In the present study we have built a set of cDNNs which are capable of classifying the emitter's sex from single vocalizations with substantial accuracy. We find that this performance is dominated by sex-specific features, but individual differences in vocalization are also detectable and contribute to the overall performance. The cDNN classification vastly outperforms more traditional classifiers or DNNs trained on predefined features. Combining tSNE based dimensionality reduction with the DNN classification we conclude that a subset of USVs is rather clearly identifiable, while for the majority the sex rests on a complex set of properties. Our results are consistent with the behavioral evidence that mice are able to detect the sex on the basis of single vocalizations.</p><sec id="sec011"><title>Comparison with previous work on sex differences in vocalization</title><p>Previous work has approached the task of differentiating the sexes from USVs using different techniques and different datasets. The results of the present study are particularly interesting, as a previous, yet very different analysis of the same general data set did not indicate a difference between the sexes [<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>]. The latter automatically calculated 8 acoustic features for each USV and applied clustering analysis with subsequent linear discriminant analysis. Their analysis identified 3 clusters and the features contributing most to the discrimination were duration, frequency jumps and change. We find a similar mapping of features to clusters, however, both Hammerschmidt et al. [<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>] and ourselves did not find salient differences relating to the emitter's sex using these analysis techniques, i.e. either for dimensionality reduction (<xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2</xref>) and linear classification (<xref ref-type="fig" rid="pcbi.1007918.g003">Fig 3</xref>).</p><p>Extending the approach of using explicitly extracted parameters, we tested a substantially larger set of properties (see <xref ref-type="sec" rid="sec018">Methods</xref>), describing each vocalization in 457 dimensions. The properties were composed of 6 human-scored features, XXX automatically extracted composite features, and the fundamental frequency line and its intensity marginals. The above set was classified using both regression, SVM and a semi-convolutional DNN. We would have expected this large set of properties to sufficiently describe each USV, and thus allow classification at similar accuracy with the semi-convolutional DNN as the full DNN. To our surprise this was not the case, leading to a significantly lower performance (<xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4</xref>). We hypothesize that the full DNN picks up subtle differences or composite properties that were not captured by our hand-designed set of properties, despite its breadth, and inclusion of rather general properties as the fundamental frequency line and its intensity marginals.</p><p>While the human scored properties only formed a small subset of the full set of properties, we note that human scoring has its own biases (e.g. variable decision threshold through the lengthy classification process, ~60h &#x0003e; 1 week).</p><p>Previous studies on other data sets have led to mixed results, with some studies finding differences in USV properties [<xref rid="pcbi.1007918.ref033" ref-type="bibr">33</xref>], others finding no structural differences but only on the level of vocalization rates [<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>,<xref rid="pcbi.1007918.ref034" ref-type="bibr">34</xref>]. However, to our knowledge, no study was so far able to detect differences on the level of single vocalizations, as in the present analysis.</p></sec><sec id="sec012"><title>Comparison to previous work on automated classification of vocalizations</title><p>While there does not exist much advanced work on sexual classification using mouse vocalizations, there has been considerable advances in other vocalization-based classification tasks. Motivating our choice of technique for the present task, an overview is provided below for the tasks of USV detection, repertoire analysis, and individual identification (the latter not for mice).</p><p>Several commercial (e.g. Avisoft SASLab Pro, or Noldus UltraVox XT) and non-commercial systems (e.g. [<xref rid="pcbi.1007918.ref007" ref-type="bibr">7</xref>]) were able to perform USV detection with various (and often not systematically tested) degrees of success. Recently, multiple studies have advanced the state of the art, in particular DeepSqueak [<xref rid="pcbi.1007918.ref022" ref-type="bibr">22</xref>], based also on DNNs. The authors performed a quantitative comparison of the performance and robustness of USV detection, suggesting that their DNN approach is well suited for this task and superior to other approaches (see Fig 2 in [<xref rid="pcbi.1007918.ref022" ref-type="bibr">22</xref>]).</p><p>Both DeepSqueak and another recent package MUPET [<xref rid="pcbi.1007918.ref020" ref-type="bibr">20</xref>] include the possibility to extract a repertoire of basic vocalization types. While DeepSqueak converges to a more limited, less redundant set of basic vocalization types, the classification remains non-discrete in both cases. This apparent lack of clear base categories, may be reflected in our finding of a lack of simple distinguishing properties between male and female USVs. Nonetheless, MUPET was able to successfully differentiate different mouse strains using their extracted repertoires. Other techniques for classification of USV types include SVMs and Random Forest classifiers (e.g. [<xref rid="pcbi.1007918.ref035" ref-type="bibr">35</xref>]), although the study does not provide a quantitative comparison between SVM and RF performance.</p><p>In other species, various groups have successfully built repertoires or identified individuals using various data analysis techniques. Fuller [<xref rid="pcbi.1007918.ref017" ref-type="bibr">17</xref>] used hierarchical clustering to obtain a full decomposition of the vocal repertoire of Male Blue Monkeys. Elie &#x00026; Theunissen [<xref rid="pcbi.1007918.ref018" ref-type="bibr">18</xref>] applied LDA directly to PCA-preprocessed spectrogram data of zebra finch vocalization to assess their discriminability. Later the same group [<xref rid="pcbi.1007918.ref019" ref-type="bibr">19</xref>] applied LDA, QDA and RF to assess the discriminability of vocalizations among individuals. While these results are insightful in their own right, we think the degree of intermixing in the present data-set would not allow linear or quadratic techniques to be successful, although it would be worthwhile to test RF based classification on the present dataset.</p></sec><sec id="sec013"><title>Contributions of sex-specific and individual properties</title><p>While the main focus of the present study was on identifying the sex of the emitter from single USVs, the results also demonstrate that individual differences in vocalization properties contribute to the identification of the sex. Previous research has been conflicting on this issue, with a recent synthesis proposed that suggests a limited capability for learning and individual differentiation [<xref rid="pcbi.1007918.ref036" ref-type="bibr">36</xref>]. From the perspective of classifying the sex, this is undesired, and we therefore checked to which degree the network takes individual properties into account when classifying sex. The resulting performance when testing the network only on entirely unused individuals is lower (~77%), but it is well known that it will underestimate the true performance (since a powerful general estimator will tend to overfit the training set, e.g. [<xref rid="pcbi.1007918.ref037" ref-type="bibr">37</xref>]). Hence, if we expand our study to a larger set of mice, we predict that the sex-specific performance would increase to around ~80 (assuming that the performance is roughly between the performances for 'individual properties used' and 'individual properties interfering'). More generally, however, the contribution of individual properties provides an interesting insight in its own right, regarding the developmental component of USV specialization, as the animals are otherwise genetically identical.</p></sec><sec id="sec014"><title>Optimality of classification performance</title><p>The spectrotemporal structure of male and female USVs could be inherently overlapping to some degree and hence preclude perfect classification. How close the present classification comes to the best possible classification is not easy to assess. In human speech recognition, scores of speech intelligibility or other classifications can be readily obtained (using psychophysical experiments) to estimate the limits of performance, at least with respect to humans, often considered the gold standard. For comparison, human sex can be obtained with very high accuracy from vocalizations (DNN-based, 96.7%, [<xref rid="pcbi.1007918.ref038" ref-type="bibr">38</xref>]), although we suspect classification was based on longer audio segments than those presently used. For mice, a behavioral experiment would have to be conducted, to estimate their limits of classifying the sex or identity of another mouse based on its USVs. Such an estimate would naturally be limited by the inherent variability of mouse behavior in the confines of an experimental setup. In addition, the present data did not include spatial tracking, hence, our classification may be hampered by the possibility that the animal switched between different types of interactions with the anesthetized conspecific.</p></sec><sec id="sec015"><title>Biological versus laboratory relevance</title><p>Optimizing the chances of mating is essential for successfully passing on one's genes. While there is agreement that one important function of mouse vocalizations is courtship [<xref rid="pcbi.1007918.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1007918.ref003" ref-type="bibr">3</xref>,<xref rid="pcbi.1007918.ref006" ref-type="bibr">6</xref>,<xref rid="pcbi.1007918.ref034" ref-type="bibr">34</xref>], to which degree USVs contribute to the identification of a conspecific has not been fully resolved, partly due to the difficulty in attributing vocalizations to single animals during social interaction. Aside from the ethological value of deducing the sex from a vocalization, the present system also provides value for laboratory settings. The mouse strain used in the present setting is an often used laboratory strain, and in the context of social analysis the present set of analysis tools will be useful, in particular in combination with new, more refined tools for spatially attributing vocalizations to individual mice [<xref rid="pcbi.1007918.ref004" ref-type="bibr">4</xref>].</p></sec><sec id="sec016"><title>Generalization to other strains, social interactions and USV sequences</title><p>The present experimental dataset was restricted to only two strains and two types of interaction. It has been shown previously that different strains [<xref rid="pcbi.1007918.ref020" ref-type="bibr">20</xref>,<xref rid="pcbi.1007918.ref039" ref-type="bibr">39</xref>] and different social contexts [<xref rid="pcbi.1007918.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1007918.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1007918.ref033" ref-type="bibr">33</xref>] influence the vocalization behavior of mice. As the present study depended partially on the availability of human-classified USV features, we chose to work with more limited sets here. While not tested here, we expect that the performance of the current classifier would be reduced if directly applied to a larger set of strains and interactions. However, retraining a generalized version of the network, predicting both sex, strain and type of interaction would be straightforward and is planned as a follow-up study. In particular the addition of different strains would allow the network to be used as a general tool in research.</p><p>A potential generalization would be the classification of whole sequences ('bouts') of vocalizations, rather than the present single vocalization approach. This could help to further improve classification accuracy, by making more information and also the inter-USV periods available to the classifier. However, in particular during social interaction, there may be a mixture of vocalizations from multiple animals in close temporal succession, which need to be classified individually. As this is not known for each sequence of USVs, a multilayered approach would be necessary, i.e. first classify a sequence, and if the certainty of classification is low, then reclassify subsequences until the certainty per sequence is optimized.</p></sec><sec id="sec017"><title>Towards a complete analysis of social interactions in mice</title><p>The present set of classifiers provides an important building block for the quantitative analysis of social vocalizations in mice and other animals. However, there remain a number of generalizations to reach its full potential. Aside from adding additional strains and behavioral contexts, the extension to longer sequences of vocalizations is highly relevant. While we presently demonstrate that single vocalizations already provide a surprising amount of information about the sex and individual, we agree with previous studies that sequences of USVs play an important role in mouse communication [<xref rid="pcbi.1007918.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1007918.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1007918.ref040" ref-type="bibr">40</xref>]. While neither of these studies differentiated between the sexes, we consider a combination of the two approaches an essential step of further automatizing and objectifying the analysis of mouse vocalizations.</p></sec></sec><sec sec-type="materials|methods" id="sec018"><title>Methods</title><p>We recorded vocalizations from mice and subsequently analyzed their structure to automatically classify the mouse's sex and spectrotemporal properties from individual vocalizations. All experiments were performed with permission of the local authorities (Bezirksregierung Braunschweig) in accordance with the German Animal Protection Law. Data and analysis tools relevant to this publication are available to reviewers (and the general public after publication) via the Donders repository (see Data Availability Statement)</p><sec id="sec019"><title>Data acquisition</title><p>The present analysis was applied to two datasets, one recording from male/female mice and one from cortex-deficient mutants, described below. The latter is covered as supplementary material to keep the presentation in the main manuscript easy to follow. Both datasets were recorded using AVISOFT RECORDER 4.1 (Avisoft Bioacoustics, Berlin Germany) sampled at 300 kHz. The microphone (UltraSoundGate CM16) was connected to a preamplifier (UltraSoundGate 116), which was connected to a computer.</p></sec><sec id="sec020"><title>Male/Female experiment</title><p>The recordings constitute a subset of the recordings collected in a previous study [<xref rid="pcbi.1007918.ref016" ref-type="bibr">16</xref>]. Briefly, C57BL/6NCrl female and male mice (&#x0003e;8w), were housed in groups of five in standard (Type II long) plastic cages, with food and water ad libitum. The resident-intruder paradigm was used to elicit ultrasonic vocalizations (USVs) from male and female &#x02018;residents&#x02019;. Resident mice (males and females) were first habituated to the room: Mice in their own home cage were placed on the desk in the recording room for 60 seconds. Subsequently, an unfamiliar intruder mouse was placed into the home cage of the resident, and the vocalization behavior was recorded for 3 min. Anesthetized females were used as 'intruders' to ensure that only the resident mouse was awake and could emit calls. Intruder mice were anaesthetized with an intraperitoneal (i.p.) injection of 0.25% tribromoethanol (Sigma-Aldrich, Munich, Germany) in the dose 0.125 mg/g of body weight. Overall, 10055 vocalizations were recorded from 17 mice. Mice were only recorded once, such that 9 female and 8 male mice contributed to the data set. Male mice produced 542&#x000b1;97 (Mean &#x000b1; SEM) vocalizations, while female mice produced 636&#x000b1;43 calls over the recording period of 3 min.</p></sec><sec id="sec021"><title>WT/Cortexless paradigm</title><p>The recordings are a subset of those collected in a previous study [<xref rid="pcbi.1007918.ref025" ref-type="bibr">25</xref>]. Briefly, each male mouse was housed in isolation one day before the experiment in a Macrolon 2 cage. During the recordings, these cages were placed in a sound-attenuated Styrofoam box. After 3 minutes, a female (Emx1-CRE;Esco2fl/fl) was introduced in the cage with the male and the vocalizations recorded for 4 minutes.</p><p>Overall, 4427 vocalizations were recorded from 12 mice (6 WT and 6 <italic>Emx1-CRE;Esco2</italic>).</p></sec><sec id="sec022"><title>Data processing and extraction of vocalizations</title><p>An automated procedure was used to detect and extract vocalizations from the continuous sound recordings. The procedure was based on existing code [<xref rid="pcbi.1007918.ref007" ref-type="bibr">7</xref>], but extended in several ways to optimize extraction of both simple and complex vocalizations. The quality of extraction was manually checked on a randomly chosen subset of the vocalizations. We here describe all essential steps of the extraction algorithm for completeness (the code is provided in the repository alongside this manuscript, see above).</p><p>The continuous recording was first high-pass filtered (4th order Butterworth filter, 25 kHz) before transforming it into a spectrogram (using the norm of the fast fourier transform for 500 samples (1.67 ms) windows using 50% consecutive window overlap, leading to an effective spectrogram sampling rate of ~1.2 kHz). Spectrograms were converted to a sparse representation by setting all values below the 70th percentile to zero (corresponding to a 55&#x000b1;5.2 dB threshold below the respective maximum in each recording), which eliminated most close-to-silent background noise bins. Vocalizations were identified using a combination of multiple criteria (defined below), i.e. exceeding a certain spectral power, maintaining spectral continuity, and lie above a frequency threshold. Vocalizations that followed each other at intervals &#x0003c;15 ms were subsequently merged again and treated as a single vocalization. For later classification, each vocalization was represented as a 100&#x000d7;100 matrix, encoded as uint8. Hence, vocalizations longer than 100 ms (11.7%) were truncated to 100 ms.</p><p>The three criteria above were defined as follows:</p><list list-type="bullet"><list-item><p><italic>Spectral energy</italic>: The distribution of spectral energies was computed across the entire spectrogram, and only time-points were kept in which any of the bins exceeded 99.8% of the distribution (manually estimated). While this threshold seems high, we verified manually that this threshold did not exclude any clearly recognizable vocalizations. It reflects the relative rarity of bins containing energy from a vocalization.</p></list-item><list-item><p><italic>Spectral continuity</italic>: We tracked the maximum position across frequencies for each time-step, and computed the size of the difference in frequency location between time-steps. These differences were then accumulated, centered at each time-point for 3.3 ms (4 steps), in both directions. Their minimum was compared to a threshold of 15, which corresponds to ~3.4 kHz/ms. Hence, the vocalization was accepted if the central line did not change too much. The threshold was set generously, in order to also include more broadband vocalizations. Manual inspection indicated that no vocalizations were missed due to a too stringent spectral purity criterion.</p></list-item><list-item><p><italic>Frequency threshold</italic>: Despite the high-pass filtering some low-frequency environmental noises can contaminate higher frequency regions. We then excluded all vocalizations whose mean spectral energy was &#x0003c;25 kHz.</p></list-item></list><p>Only if these three properties were fulfilled, we included a segment of the data into the set of vocalizations. The segmentation code is included in the above repository (MATLAB Code: VocCollector.m), with the parameters set as follows: SpecDiscThresh = 0.8, MergeClose = 0.015.</p></sec><sec id="sec023"><title>Dimensionality reduction</title><p>For initial exploratory analysis we performed dimensionality reduction on the human-classified features and the full spectrograms. Both principal component analysis (PCA) and t-statistic neighborhood embedding (tSNE, [<xref rid="pcbi.1007918.ref028" ref-type="bibr">28</xref>]) were applied to both datasets (see <xref ref-type="fig" rid="pcbi.1007918.g002">Fig 2</xref>). tSNE was run in Matlab using the original implementation from [<xref rid="pcbi.1007918.ref028" ref-type="bibr">28</xref>], using the following parameters (initial reduction using PCA to dimension 9 for features and 100 for spectrograms; perplexity: 30).</p></sec><sec id="sec024"><title>Classification</title><p>We performed automatic classification of the emitter's sex, as well as several properties of individual vocalizations (see below). While the sex was directly known, the ground truth for most of the properties was estimated by two independent human evaluators, who had no access to the emitting sex during scoring.</p><sec id="sec025"><title>Vocalization properties</title><p>For each vocalization we extracted a range of properties on multiple levels, which should serve to characterize each vocalization at different scales of granularity. The code for assigning these properties is provided in the above repository (MATLAB Code: VocAnalyzer.m). Prior to extraction of the properties, the vocalization's spectrogram was filtered using a level set curvature flow technique [<xref rid="pcbi.1007918.ref041" ref-type="bibr">41</xref>] using an implementation available online by Pragya Sharma (<ext-link ext-link-type="uri" xlink:href="http://github.com/psharma15/Min-Max-Image-Noise-Removal">github.com/psharma15/Min-Max-Image-Noise-Removal</ext-link>). This removes background noise while leaving the vocalization parts nearly unchanged. We also only worked with the spectrogram &#x0003e;25 kHz to deemphasize background noise from movement related sound, while keeping all USVs, which&#x02014;to our experience&#x02014;are never below 25 kHz for male-female social interactions. Below the spectrogram of a USV is referred to as <italic>S(f</italic>,<italic>t) = |STFT(f</italic>,<italic>t)|</italic><sup><italic>2</italic></sup>, where <italic>STFT</italic> denotes the short-term fourier transform.</p><list list-type="bullet"><list-item><p><italic>Fundamental Frequency Line</italic>: Mouse USVs essentially never show harmonics, since the first harmonic would typically be located very close or above the recorded frequency range (here: 125kHz). Hence, at a given time, there is only one dominant frequency in the spectrogram. We next used an edge detection algorithm (MATLAB: edge(Spectrogram,'Prewitt')) to locate all locations of the spectrogram where there is a transition from background to any sound. Next, the temporal range of the USV was limited to time-bins where at least one edge was found. The frequency of the fundamental for each point in time was then identified as the frequency bin with maximal amplitude. If no edge was identified between two bins with an edge, the corresponding bins' frequencies were set to 0, indicating a break in the vocalization. If the frequency in a single bin between two adjacent bins with an edge differed by more than 5kHz, the value was replaced by the interpolation (this can occur if the amplitude inside a fundamental is briefly lowered). The fundamental frequency line is a 1-dim. function referred to as <italic>FF(t)</italic>. For input to the DNN, was <italic>FF(t)</italic> shortened or lengthened to 100 ms, the latter by adding zeros.</p></list-item><list-item><p><italic>Fundamental Energy Line</italic>: The value of <italic>S(FF(t)</italic>,<italic>t)</italic> for all <italic>t</italic> for which <italic>FF(t)</italic> is defined and &#x0003e;0. The fundamental amplitude line is a 1-dim. function referred to as <italic>FE(t)</italic>. Since the USVs only have a single spectral component per time, <italic>FE(t)</italic> is here used as the temporal marginal.</p></list-item><list-item><p><italic>Spectral Marginal</italic>: The spectral marginal was computed as the temporal average per frequency bin of <italic>FE(t)</italic>, where <italic>FE(t)&#x0003e;0</italic>. To make it a useful input for a convolutional DNN, the same, complete frequency range was used for each USV, which had a dimension of 233.</p></list-item><list-item><p><italic>Spectral Width</italic>: defined as <italic>SW = max(FF(t))&#x02014;min(FF(t))</italic>. This estimate was chosen over conventional approaches using the spectral marginal, to focus on the USV and ignore surrounding noise.</p></list-item><list-item><p><italic>Duration</italic>: time <italic>T</italic> from first to last time-bin of the spectral line (including time-bins with frequency equal to 0 in between).</p></list-item><list-item><p><italic>Starting Frequency</italic>: <italic>FF(0)</italic>.</p></list-item><list-item><p><italic>Ending Frequency</italic>: <italic>FF(T)</italic>, where <italic>T</italic> is the length of <italic>FF</italic>.</p></list-item><list-item><p><italic>Minimal Frequency</italic>: min(<italic>FF(t)</italic>), computed over all <italic>t</italic> where <italic>FF(t)&#x0003e;0</italic>).</p></list-item><list-item><p><italic>Maximal Frequency</italic>: max(<italic>FF(t)</italic>), computed over all <italic>t</italic> where <italic>FF(t)&#x0003e;0</italic>).</p></list-item><list-item><p><italic>Average Frequency</italic>: we included two estimates here, (1) the average frequency of <italic>FF(t)</italic>, computed over all <italic>t</italic> where <italic>FF(t)&#x0003e;0</italic>) and (2) the intensity-weighted average across the raw spectrogram, computed as <inline-formula id="pcbi.1007918.e001"><alternatives><graphic xlink:href="pcbi.1007918.e001.jpg" id="pcbi.1007918.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p></list-item><list-item><p><italic>Temporal Skewness</italic>: skewness of <italic>FE(t)</italic>.</p></list-item><list-item><p><italic>Temporal Kurtosis</italic>: kurtosis of <italic>FE(t)</italic>.</p></list-item><list-item><p>Spectral Skewness: skewness of the spectral marginal (i.e. across frequency)</p></list-item><list-item><p><italic>Spectral Kurtosis</italic>: kurtosis of the spectral marginal (i.e. across frequency)</p></list-item><list-item><p><italic>Direction</italic>: the sign of the mean of single-step differences of <italic>FF(t)</italic> for neighboring time-bins, for all <italic>t</italic> where <italic>FF(t)&#x0003e;0</italic>.</p></list-item><list-item><p><italic>Spectral Flatness (Wiener Entropy)</italic>: The Wiener Entropy WE [<xref rid="pcbi.1007918.ref042" ref-type="bibr">42</xref>] was computed as the temporal average of the Wiener Entropy for each time-bin of the USV, i.e.: <inline-formula id="pcbi.1007918.e002"><alternatives><graphic xlink:href="pcbi.1007918.e002.jpg" id="pcbi.1007918.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mi>W</mml:mi><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>&#x0003e;</mml:mo></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives></inline-formula> where G denotes the geometric mean, i.e. <inline-formula id="pcbi.1007918.e003"><alternatives><graphic xlink:href="pcbi.1007918.e003.jpg" id="pcbi.1007918.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x0220f;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>.</p></list-item><list-item><p><italic>Spectral Salience</italic>: Spectral salience was computed as the temporal average of the ratios between the largest, off-zero peak and the peak at 0 of the autocorrelation across frequencies for each time bin.</p></list-item><list-item><p><italic>Tremolo</italic>: i.e. whether there is a sinusoidal variation in frequency of <italic>FF(t)</italic>. To assess the tremolo we applied the spectral salience estimate to <italic>FF(t)</italic>.</p></list-item><list-item><p><italic>Spectral Energy</italic>: the average of <italic>FE(t)</italic>.</p></list-item><list-item><p><italic>Spectral Purity</italic>: average of the instantaneous spectral purity, defined as <inline-formula id="pcbi.1007918.e004"><alternatives><graphic xlink:href="pcbi.1007918.e004.jpg" id="pcbi.1007918.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p></list-item></list><p>In addition, the following 6 properties were also scored by two human evaluators:</p><list list-type="bullet"><list-item><p><italic>Direction</italic>: whether the vocalization is composed mostly of flat, descending or ascending pieces, values are 0,-1,1, respectively</p></list-item><list-item><p><italic>Peaks</italic>: number of clearly identifiable peaks, values: integers &#x02265; 0.</p></list-item><list-item><p><italic>Breaks</italic>: number of times the main fundamental frequency line is disconnected, values: integers &#x02265; 0</p></list-item><list-item><p><italic>Broadband</italic>: whether the frequency content is narrow (0) or broadband (1). Values: [0,1]</p></list-item><list-item><p><italic>Tremolo</italic>: whether there is a sinusoidal variation on the main whistle frequency. Values: [0,1]</p></list-item><list-item><p><italic>Complexity</italic>: whether the overall vocalization is simple or complex. Values: [0,1]</p></list-item></list></sec><sec id="sec026"><title>Performance evaluation</title><p>The quality of classification was assessed using cross-validation, i.e. by training on a subset of the data (90%) and evaluating the performance on a separate test set (remaining 10%). For significance assessment, we divided the overall set into 10 non-overlapping test sets (permutation draw, with their corresponding training sets), and performed 10 individual estimates. Performance was assessed as percent correct, i.e. the number of correct classifications in comparison with the total number of the same type. We performed another control, where the test set was just one recording session, which allowed us to verify that the classification was based on sex rather than individual (see <xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4B</xref>). Here, the size of the test set was determined by the number of vocalizations of each individual.</p></sec></sec><sec id="sec027"><title>Deep neural network classification</title><p>For classification of sexes, features and individuals, we implemented several neural networks of standard architectures containing multiple convolutional and fully connected layers (&#x00441;DNNs, see details below). The networks were implemented and trained in Python 3.6 using the TensorFlow toolkit [<xref rid="pcbi.1007918.ref043" ref-type="bibr">43</xref>]. Estimations were run on single GPUs, a NVIDIA GTX1070 (8 GB) and a NVIDIA RTX2080 (8 GB).</p><p>The networks were trained using standard techniques, i.e. regularisation of parameters using batch-normalization [<xref rid="pcbi.1007918.ref044" ref-type="bibr">44</xref>] and dropout [<xref rid="pcbi.1007918.ref045" ref-type="bibr">45</xref>]. Batch normalization was applied for all network layers, both convolutional and fully connected. The sizes and the number of features for convolutional kernels were selected according to the parameters commonly used for natural images processing networks (specific architectural details are displayed in each figure). For training, stochastic optimization was performed using ADAM [<xref rid="pcbi.1007918.ref046" ref-type="bibr">46</xref>]. The rectified linear unit (ReLU) was used as the activation function for all layers except for the output layer. For the output layer, the sigmoid activation function was used. The cross entropy loss function was used for minimization. The initial weight values of all layers were set using Xavier initialization [<xref rid="pcbi.1007918.ref047" ref-type="bibr">47</xref>].</p></sec><sec id="sec028"><title>Spectrogram-to-Sex, Spectrogram-to-Cortexless, Spectrogram-to-Individual and Spectrogram-to-Features Networks</title><p>In total, 8 convolutional networks taking spectrograms as their input were trained: <italic>Spectrogram-to-Sex</italic>, Spectrogram-to-Cortexless, 2 <italic>Spectrogram-to-Individual (for &#x02018;cortexless&#x02019; and &#x02018;gender&#x02019; individual data sets)</italic> and 4 <italic>Spectrogram-to-Features</italic> networks including direction (<italic>Spectrogram-to-Direction</italic>), number of peaks (<italic>Spectrogram-to-Peaks</italic>), number of breaks (<italic>Spectrogram-to-Breaks)</italic> and broadband property (<italic>Spectrogram-to-Broadband</italic>) detection networks.</p><p>The networks consisted of 6 convolutional and 3 fully connected layers. The detailed layer properties are represented in <xref rid="pcbi.1007918.t001" ref-type="table">Table 1</xref>. The output layer of <italic>Spectrogram-to-Sex</italic> network contained single element, representing the detected sex (the probability of being male). The <italic>Spectrogram-to-Direction</italic>, <italic>Spectrogram-to-Peaks and Spectrogram-to-Breaks</italic> networks&#x02019; output layers contained the number of elements equal to the number of detected classes. Since only a few samples had a number of peaks or breaks more than 3, the data set was restricted to 4 classes: no peaks (breaks), 1 peak (break), 2 peaks (breaks), 3 or more peaks (breaks). So, the output layers of <italic>Spectrogram-to-Peaks and Spectrogram-to-Breaks</italic> networks consisted of 4 elements representing each class. The <italic>Spectrogram-to-Broadband</italic> network output layer contained a single element representing <italic>Broadband</italic> value.</p><table-wrap id="pcbi.1007918.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.t001</object-id><label>Table 1</label><caption><title>The architecture of Spectrogram-to-Sex and Spectrogram-to-Features networks.</title></caption><alternatives><graphic id="pcbi.1007918.t001g" xlink:href="pcbi.1007918.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Layer</th><th align="left" rowspan="1" colspan="1">Properties</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Convolutional 1</td><td align="left" rowspan="1" colspan="1">Kernel size: 10x10, 256 units, stride = 2</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 2</td><td align="left" rowspan="1" colspan="1">Kernel size: 5x5, 256 units, stride = 2</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 3</td><td align="left" rowspan="1" colspan="1">Kernel size: 3x3, 256 units, stride = 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 4</td><td align="left" rowspan="1" colspan="1">Kernel size: 3x3, 256 units, stride = 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 5</td><td align="left" rowspan="1" colspan="1">Kernel size: 3x3, 256 units, stride = 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 6</td><td align="left" rowspan="1" colspan="1">Kernel size: 3x3, 256 units, stride = 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 1</td><td align="left" rowspan="1" colspan="1">120 (80<xref ref-type="table-fn" rid="t001fn001">*</xref>) units</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 2</td><td align="left" rowspan="1" colspan="1">120 (80<xref ref-type="table-fn" rid="t001fn001">*</xref>) units</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 3</td><td align="left" rowspan="1" colspan="1">120 (80<xref ref-type="table-fn" rid="t001fn001">*</xref>) units</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t001fn001"><p>* For Spectrogram-to-Broadband feature network we used 80-unit fully connected layers</p></fn></table-wrap-foot></table-wrap></sec><sec id="sec029"><title>Features-to-Sex network</title><p>The networks used for classification of sexes based on the spectrogram features consisted of 4 fully connected layers. The first layer received inputs from 9-dimensional vectors representing feature values: duration, average frequency, average volume, direction. number of peaks, number of breaks, &#x0201c;broadband&#x0201d; value, &#x0201c;vibrato&#x0201d; value, &#x0201c;complex&#x0201d; value (the latter 6 were determined by human classification). The output layer contained single element, representing the detected sex (the probability of being male). The detailed layer properties are represented in <xref rid="pcbi.1007918.t002" ref-type="table">Table 2</xref>.</p><table-wrap id="pcbi.1007918.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.t002</object-id><label>Table 2</label><caption><title>The architecture of Feature-to-Sex network.</title></caption><alternatives><graphic id="pcbi.1007918.t002g" xlink:href="pcbi.1007918.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Layer</th><th align="left" rowspan="1" colspan="1">Size</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Fully connected 1</td><td align="left" rowspan="1" colspan="1">70 units</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 2</td><td align="left" rowspan="1" colspan="1">70 units</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 3</td><td align="left" rowspan="1" colspan="1">70 units</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 4</td><td align="left" rowspan="1" colspan="1">70 units</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec030"><title>Extended features-to-Sex network</title><p>This emitter-sex classification network combines convolutional and non-convolutional processing steps: features were directly input to the fully-connected layers, while separate convolutional layer-stacks were trained for three 1D quantities, i.e. the marginal spectrum (233 dimensional), the marginal intensity over time (100 dimensional) and the maximum frequency line (100 data points). The convolutional sub-networks otherwise had an identical architecture with 5 layers (see <xref ref-type="fig" rid="pcbi.1007918.g004">Fig 4J</xref> and <xref rid="pcbi.1007918.t003" ref-type="table">Table 3</xref>). Their outputs were combined with all 24 discrete acoustic feature data (see above) to form the input to the subsequent, fully connected, 3 layer sub-network. The output layer contained a single element, representing the detected sex (the probability of being male).</p><table-wrap id="pcbi.1007918.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.t003</object-id><label>Table 3</label><caption><title>The architecture of Extended Features-to-Sex network.</title><p>Note that the network contains 3 parallel convolutional sub-networks of the architecture described in the table.</p></caption><alternatives><graphic id="pcbi.1007918.t003g" xlink:href="pcbi.1007918.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Layer</th><th align="left" rowspan="1" colspan="1">Properties</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Convolutional 1</td><td align="left" rowspan="1" colspan="1">Kernel size: 7, 32 units, stride = 2</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 2</td><td align="left" rowspan="1" colspan="1">Kernel size: 5, 32 units, stride = 2</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 3</td><td align="left" rowspan="1" colspan="1">Kernel size: 3, 32 units, stride = 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 4</td><td align="left" rowspan="1" colspan="1">Kernel size: 3, 32 units, stride = 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Convolutional 5</td><td align="left" rowspan="1" colspan="1">Kernel size: 3, 32 units, stride = 1</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 1</td><td align="left" rowspan="1" colspan="1">90 units</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 2</td><td align="left" rowspan="1" colspan="1">90 units</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected 3</td><td align="left" rowspan="1" colspan="1">90 units</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec031"><title>Input data preparation and augmentation</title><p>The original source spectrograms were represented as N&#x000d7;233 images where N is the rounded duration of the spectrogram in milliseconds. The values were in the range [0,255]. To represent the spectrograms as 100&#x000d7;100 images they were cut to 100 ms threshold and rescaled to M&#x000d7;100 size keeping the original aspect ratio, M is the scaled thresholded duration. The rescaled matrix was aligned to the left side of the resulting image.</p><p>For DNNs, we implemented on-the-fly data augmentation to enlarge the input dataset. For each 2D spectrogram image being fed to the network during training session a set of modifications were applied including start and end times clip (up to 10% percent of original duration), intensity (volume) amplification with a random coefficient drawn from the range [0.5, 1.5] and the addition of gaussian noise with the variance randomly drawn from the range [0, 0.01]. The same algorithm of augmentation was applied for 1D spectral line data and time marginal data (<italic>Extended-Features-To-Gender network</italic>), with no amplification. For marginal frequency data the augmentation included intensity amplification with coefficient drawn from the range [0.8, 1.2] and gaussian noise only. We used the scikit-image package [<xref rid="pcbi.1007918.ref048" ref-type="bibr">48</xref>] routines for implementing data augmentation operations.</p><p>Different approaches were used to compensate for the asymmetry in the occurrence of different classes (e.g. for there were 32% more female than male vocalizations), we used different procedures for the three classification methods: For Ridge and SVM, the number of male and female vocalizations was equalized in the training sets by reducing the size of the bigger sets. For DNNs, a loss-function was used with weighting according to the occurrence in the different classes.</p></sec><sec id="sec032"><title>Training protocols</title><p>For <italic>Spectrogram-to-Sex</italic>, <italic>Spectrogram-to-Individual</italic> and the training protocol included 3 stages with different training parameters set. (<xref rid="pcbi.1007918.t004" ref-type="table">Table 4</xref>). For <italic>Spectrogram-to-Features</italic> and <italic>Features-to-Gender</italic> the training protocols included 2 stages (Tables <xref rid="pcbi.1007918.t005" ref-type="table">5</xref> and <xref rid="pcbi.1007918.t006" ref-type="table">6</xref>). <italic>Extended-Features-to-Gender</italic> network was trained in 4 stages(<xref rid="pcbi.1007918.t007" ref-type="table">Table 7</xref>). Batch size was set to 64 samples for all networks.</p><table-wrap id="pcbi.1007918.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.t004</object-id><label>Table 4</label><caption><title><italic>Spectrogram-to-Sex</italic> and <italic>Spectrogram-to-Individual</italic> networks training protocol.</title></caption><alternatives><graphic id="pcbi.1007918.t004g" xlink:href="pcbi.1007918.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Stage</th><th align="left" rowspan="1" colspan="1">Epochs</th><th align="left" rowspan="1" colspan="1">Learning rate</th><th align="left" rowspan="1" colspan="1">Dropout probability</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">40</td><td align="left" rowspan="1" colspan="1">0.001</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">25</td><td align="left" rowspan="1" colspan="1">0.0001</td><td align="left" rowspan="1" colspan="1">0.5</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">25</td><td align="left" rowspan="1" colspan="1">0.00001</td><td align="left" rowspan="1" colspan="1">0.7</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pcbi.1007918.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.t005</object-id><label>Table 5</label><caption><title><italic>Spectrogram-to-Features</italic> networks training protocol.</title></caption><alternatives><graphic id="pcbi.1007918.t005g" xlink:href="pcbi.1007918.t005"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Stage</th><th align="left" rowspan="1" colspan="1">Epochs</th><th align="left" rowspan="1" colspan="1">Learning rate</th><th align="left" rowspan="1" colspan="1">Dropout probability</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">40</td><td align="left" rowspan="1" colspan="1">0.001</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">25</td><td align="left" rowspan="1" colspan="1">0.0001</td><td align="left" rowspan="1" colspan="1">0.5</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pcbi.1007918.t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.t006</object-id><label>Table 6</label><caption><title><italic>Features-to-Gender</italic> network training protocol.</title></caption><alternatives><graphic id="pcbi.1007918.t006g" xlink:href="pcbi.1007918.t006"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Stage</th><th align="left" rowspan="1" colspan="1">Epochs</th><th align="left" rowspan="1" colspan="1">Learning rate</th><th align="left" rowspan="1" colspan="1">Dropout probability</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">150</td><td align="left" rowspan="1" colspan="1">0.001</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">200</td><td align="left" rowspan="1" colspan="1">Quadratic decay from 0.001 to 10<sup>&#x02212;5</sup></td><td align="left" rowspan="1" colspan="1">0.5</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pcbi.1007918.t007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007918.t007</object-id><label>Table 7</label><caption><title><italic>Extended-Features-to-Gender</italic> network training protocol (single individual test set).</title></caption><alternatives><graphic id="pcbi.1007918.t007g" xlink:href="pcbi.1007918.t007"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Stage</th><th align="left" rowspan="1" colspan="1">Epochs</th><th align="left" rowspan="1" colspan="1">Learning rate</th><th align="left" rowspan="1" colspan="1">Dropout probability</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">50</td><td align="left" rowspan="1" colspan="1">0.001</td><td align="left" rowspan="1" colspan="1">0.0</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1">0.0001</td><td align="left" rowspan="1" colspan="1">0.3</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1">0.00001</td><td align="left" rowspan="1" colspan="1">0.5</td></tr><tr><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1">0.000002</td><td align="left" rowspan="1" colspan="1">0.5</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec033"><title>Analysis of activation patterns and stimulus representation</title><p>The network's representation as a function of layer was analyzed on the basis of a standard deconvolution library <italic>tf_cnnvis</italic> [<xref rid="pcbi.1007918.ref030" ref-type="bibr">30</xref>], implementing activity and representation ('deconvolution') analysis introduced earlier [<xref rid="pcbi.1007918.ref029" ref-type="bibr">29</xref>]. Briefly, the deconvolution analysis works by applying the transposed layer weight matrix to the output of the convolutional layer (feature map), taking into account strides and padding settings. Applied sequentially to the given and all preceding layers, it produces an approximate reconstruction of the original image for each neuron, allowing to relate particular parts of the image to the set of features detected by the layer.</p><p>The routines provided by the library were integrated into the sex classification code to produce activation and deconvolution maps for all spectrograms of the dataset, which were then subjected to further analysis (see <xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6</xref>).</p><p>For the activations, we computed the per-layer sparsity (i.e. fraction of activation per stimulus, separated by emitter sex) as well as the per-layer correlation between activation patterns within and across emitter sex. The latter serves as an indicator of the degree of separation between the sexes (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6</xref>, middle).</p><p>For the deconvolution results (i.e. approximations of per-neuron stimulus representation), we computed two different correlations: First, the correlation of each neurons representation with the actual stimulus, again separated by layer and sex of the emitter. Second, the correlation of between the representation of neurons within a layer, across all layers and sex by the emitter (<xref ref-type="fig" rid="pcbi.1007918.g006">Fig 6</xref>, bottom).</p></sec><sec id="sec034"><title>Linear regression and support vector machines</title><p>To assess the contribution of basic linear prediction to the classification performance, we performed regularized (<italic>ridge</italic>) regression by direct implementation of the normal equations in MATLAB. The performance was generally close to chance and did not depend much on the regularization parameter.</p><p>To check whether simple nonlinearities in the input space could account for the classification performance of the DNN we used support vector machine classification [<xref rid="pcbi.1007918.ref024" ref-type="bibr">24</xref>,<xref rid="pcbi.1007918.ref049" ref-type="bibr">49</xref>], using their implementation in MATLAB (<italic>svmtrain</italic>, <italic>svmclassify</italic>, <italic>fitcsvm</italic>, <italic>predict</italic>). We used the quadratic kernel for the spectrogram-based classification, and the linear (dot-product) kernel for the feature-based classification. The performance was above chance, however, much poorer than for DNN classification.</p><p>For both methods the evaluation was done just as for the DNN estimation, by using single individual test sets excluded from the training data (see Figs <xref ref-type="fig" rid="pcbi.1007918.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1007918.g005">5</xref> for comparative results).</p></sec><sec id="sec035"><title>Statistical analysis</title><p>Generally, nonparametric tests were used to avoid distributional assumptions, i.e. Wilcoxon's rank sum test for two group comparisons, and Kruskal-Wallis for single factor analysis of variance. When data were normally distributed, we checked that statistical conclusions were the same for the corresponding test, i.e. t-test or ANOVA, respectively. Effect sizes were computed as the variance accounted by a given factor, divided by the total variance. Error bars indicate 1 SEM (standard error of the mean). Post-hoc pairwise multiple comparisons were assessed using Bonferroni correction. All statistical analyses were performed using the statistics toolbox in MATLAB (The Mathworks, Natick).</p></sec></sec><sec sec-type="supplementary-material" id="sec036"><title>Supporting information</title><supplementary-material content-type="local-data" id="pcbi.1007918.s001"><label>S1 Fig</label><caption><title>Classification based on individual properties can support classification of sex.</title><p><bold>A</bold> Comparing the performance of the network across individual indicates that the sex-classification is better than chance in 15/17 animals. The light gray line indicates chance, and the dark gray line average performance. <bold>B</bold> If individual properties are included in the classification (through the use of random testset during crossvalidation, 10x), overall performance increases to 85.1% (median across animals). <bold>C</bold> We trained another DNN to classify mouse identity, achieving above chance performance for almost all mice, indicating directly that differences between individual mice can also contribute to the classification of sex. For classification of individuals, the chance level is at 100/N<sub>mice</sub>%.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1007918.s001.tif"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pcbi.1007918.s002"><label>S2 Fig</label><caption><title>Same layout as <xref ref-type="supplementary-material" rid="pcbi.1007918.s001">S1 Fig</xref>.</title><p>1 for the classification of wild-type vs. cortexless animals.</p><p>(TIF)</p></caption><media xlink:href="pcbi.1007918.s002.tif"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pcbi.1007918.s003"><label>S1 Video</label><caption><title>Three-dimensional representation of the entire set of vocalizations (t-SNE transform of the spectrograms from 10^4 to 3 dimensions) shows limited clustering and structuring, and some separation between male (blue) and female (red) emitters.</title><p>(MP4)</p></caption><media xlink:href="pcbi.1007918.s003.mp4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>The authors would like to thank Hugo Huijs for manually classifying the entire set of vocalizations.</p></ack><ref-list><title>References</title><ref id="pcbi.1007918.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Pisanski</surname><given-names>K</given-names></name>, <name><surname>Jones</surname><given-names>BC</given-names></name>, <name><surname>Fink</surname><given-names>B</given-names></name>, <name><surname>O&#x02019;Connor</surname><given-names>JJM</given-names></name>, <name><surname>DeBruine</surname><given-names>LM</given-names></name>, <name><surname>R&#x000f6;der</surname><given-names>S</given-names></name>, <etal>et al</etal>
<article-title>Voice parameters predict sex-specific body morphology in men and women</article-title>. <source>Animal Behaviour</source>. <year>2016</year>;<volume>112</volume>: <fpage>13</fpage>&#x02013;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2015.11.008</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Chabout</surname><given-names>J</given-names></name>, <name><surname>Sarkar</surname><given-names>A</given-names></name>, <name><surname>Dunson</surname><given-names>DB</given-names></name>, <name><surname>Jarvis</surname><given-names>ED</given-names></name>. <article-title>Male mice song syntax depends on social contexts and influences female preferences</article-title>. <source>Front Behav Neurosci</source>. <year>2015</year>;<volume>9</volume>: <fpage>76</fpage>
<pub-id pub-id-type="doi">10.3389/fnbeh.2015.00076</pub-id>
<pub-id pub-id-type="pmid">25883559</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Heckman</surname><given-names>J</given-names></name>, <name><surname>McGuinness</surname><given-names>B</given-names></name>, <name><surname>Celikel</surname><given-names>T</given-names></name>, <name><surname>Englitz</surname><given-names>B</given-names></name>. <article-title>Determinants of the mouse ultrasonic vocal structure and repertoire</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2016</year>;<volume>65</volume>: <fpage>313</fpage>&#x02013;<lpage>325</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.03.029</pub-id>
<pub-id pub-id-type="pmid">27060755</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Heckman</surname><given-names>JJ</given-names></name>, <name><surname>Proville</surname><given-names>R</given-names></name>, <name><surname>Heckman</surname><given-names>GJ</given-names></name>, <name><surname>Azarfar</surname><given-names>A</given-names></name>, <name><surname>Celikel</surname><given-names>T</given-names></name>, <name><surname>Englitz</surname><given-names>B</given-names></name>. <article-title>High-precision spatial localization of mouse vocalizations during social interaction</article-title>. <source>Sci Rep</source>. <year>2017</year>;<volume>7</volume>: <fpage>3017</fpage>
<pub-id pub-id-type="doi">10.1038/s41598-017-02954-z</pub-id>
<pub-id pub-id-type="pmid">28592832</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Neunuebel</surname><given-names>JP</given-names></name>, <name><surname>Taylor</surname><given-names>AL</given-names></name>, <name><surname>Arthur</surname><given-names>BJ</given-names></name>, <name><surname>Egnor</surname><given-names>SER</given-names></name>. <article-title>Female mice ultrasonically interact with males during courtship displays.</article-title>
<source>elife</source>. <year>2015</year>;<volume>4</volume>
<pub-id pub-id-type="doi">10.7554/eLife.06203</pub-id>
<pub-id pub-id-type="pmid">26020291</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Portfors</surname><given-names>CV</given-names></name>, <name><surname>Perkel</surname><given-names>DJ</given-names></name>. <article-title>The role of ultrasonic vocalizations in mouse communication</article-title>. <source>Curr Opin Neurobiol</source>. <year>2014</year>;<volume>28</volume>: <fpage>115</fpage>&#x02013;<lpage>120</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2014.07.002</pub-id>
<pub-id pub-id-type="pmid">25062471</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Holy</surname><given-names>TE</given-names></name>, <name><surname>Guo</surname><given-names>Z</given-names></name>. <article-title>Ultrasonic songs of male mice</article-title>. <source>PLoS Biol</source>. <year>2005</year>;<volume>3</volume>: <fpage>e386</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id>
<pub-id pub-id-type="pmid">16248680</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Warren</surname><given-names>MR</given-names></name>, <name><surname>Clein</surname><given-names>RS</given-names></name>, <name><surname>Spurrier</surname><given-names>MS</given-names></name>, <name><surname>Roth</surname><given-names>ED</given-names></name>, <name><surname>Neunuebel</surname><given-names>JP</given-names></name>. <article-title>Ultrashort-range, high-frequency communication by female mice shapes social interactions</article-title>. <source>Sci Rep</source>. <year>2020</year>;<volume>10</volume>: <fpage>2637</fpage>
<pub-id pub-id-type="doi">10.1038/s41598-020-59418-0</pub-id>
<pub-id pub-id-type="pmid">32060312</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Sangiamo</surname><given-names>DT</given-names></name>, <name><surname>Warren</surname><given-names>MR</given-names></name>, <name><surname>Neunuebel</surname><given-names>JP</given-names></name>. <article-title>Ultrasonic signals associated with different types of social behavior of mice</article-title>. <source>Nat Neurosci</source>. <year>2020</year>;<volume>23</volume>: <fpage>411</fpage>&#x02013;<lpage>422</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-020-0584-z</pub-id>
<pub-id pub-id-type="pmid">32066980</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Hammerschmidt</surname><given-names>K</given-names></name>, <name><surname>Radyushkin</surname><given-names>K</given-names></name>, <name><surname>Ehrenreich</surname><given-names>H</given-names></name>, <name><surname>Fischer</surname><given-names>J</given-names></name>. <article-title>Female mice respond to male ultrasonic &#x0201c;songs&#x0201d; with approach behaviour</article-title>. <source>Biol Lett</source>. <year>2009</year>;<volume>5</volume>: <fpage>589</fpage>&#x02013;<lpage>592</lpage>. <pub-id pub-id-type="doi">10.1098/rsbl.2009.0317</pub-id>
<pub-id pub-id-type="pmid">19515648</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Shepard</surname><given-names>KN</given-names></name>, <name><surname>Liu</surname><given-names>RC</given-names></name>. <article-title>Experience restores innate female preference for male ultrasonic vocalizations</article-title>. <source>Genes Brain Behav</source>. <year>2011</year>;<volume>10</volume>: <fpage>28</fpage>&#x02013;<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1111/j.1601-183X.2010.00580.x</pub-id>
<pub-id pub-id-type="pmid">20345895</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Markova</surname><given-names>D</given-names></name>, <name><surname>Richer</surname><given-names>L</given-names></name>, <name><surname>Pangelinan</surname><given-names>M</given-names></name>, <name><surname>Schwartz</surname><given-names>DH</given-names></name>, <name><surname>Leonard</surname><given-names>G</given-names></name>, <name><surname>Perron</surname><given-names>M</given-names></name>, <etal>et al</etal>
<article-title>Age- and sex-related variations in vocal-tract morphology and voice acoustics during adolescence</article-title>. <source>Horm Behav</source>. <year>2016</year>;<volume>81</volume>: <fpage>84</fpage>&#x02013;<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1016/j.yhbeh.2016.03.001</pub-id>
<pub-id pub-id-type="pmid">27062936</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Pfefferle</surname><given-names>D</given-names></name>, <name><surname>Fischer</surname><given-names>J</given-names></name>. <article-title>Sounds and size: identification of acoustic variables that reflect body size in hamadryas baboons, Papio hamadryas</article-title>. <source>Animal Behaviour</source>. <year>2006</year>;<volume>72</volume>: <fpage>43</fpage>&#x02013;<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2005.08.021</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Mahrt</surname><given-names>E</given-names></name>, <name><surname>Agarwal</surname><given-names>A</given-names></name>, <name><surname>Perkel</surname><given-names>D</given-names></name>, <name><surname>Portfors</surname><given-names>C</given-names></name>, <name><surname>Elemans</surname><given-names>CPH</given-names></name>. <article-title>Mice produce ultrasonic vocalizations by intra-laryngeal planar impinging jets</article-title>. <source>Curr Biol</source>. <year>2016</year>;<volume>26</volume>: <fpage>R880</fpage>&#x02013;<lpage>R881</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2016.08.032</pub-id>
<pub-id pub-id-type="pmid">27728788</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Roberts</surname><given-names>LH</given-names></name>. <article-title>The rodent ultrasound production mechanism</article-title>. <source>Ultrasonics</source>. <year>1975</year>;<volume>13</volume>: <fpage>83</fpage>&#x02013;<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1016/0041-624x(75)90052-9</pub-id>
<pub-id pub-id-type="pmid">1167711</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Hammerschmidt</surname><given-names>K</given-names></name>, <name><surname>Radyushkin</surname><given-names>K</given-names></name>, <name><surname>Ehrenreich</surname><given-names>H</given-names></name>, <name><surname>Fischer</surname><given-names>J</given-names></name>. <article-title>The structure and usage of female and male mouse ultrasonic vocalizations reveal only minor differences</article-title>. <source>PLoS ONE</source>. <year>2012</year>;<volume>7</volume>: <fpage>e41133</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0041133</pub-id>
<pub-id pub-id-type="pmid">22815941</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Fuller</surname><given-names>JL</given-names></name>. <article-title>The vocal repertoire of adult male blue monkeys (Cercopithecus mitis stulmanni): a quantitative analysis of acoustic structure</article-title>. <source>Am J Primatol</source>. <year>2014</year>;<volume>76</volume>: <fpage>203</fpage>&#x02013;<lpage>216</lpage>. <pub-id pub-id-type="doi">10.1002/ajp.22223</pub-id>
<pub-id pub-id-type="pmid">24130044</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Elie</surname><given-names>JE</given-names></name>, <name><surname>Theunissen</surname><given-names>FE</given-names></name>. <article-title>The vocal repertoire of the domesticated zebra finch: a data-driven approach to decipher the information-bearing acoustic features of communication signals</article-title>. <source>Anim Cogn</source>. <year>2016</year>;<volume>19</volume>: <fpage>285</fpage>&#x02013;<lpage>315</lpage>. <pub-id pub-id-type="doi">10.1007/s10071-015-0933-6</pub-id>
<pub-id pub-id-type="pmid">26581377</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Elie</surname><given-names>JE</given-names></name>, <name><surname>Theunissen</surname><given-names>FE</given-names></name>. <article-title>Zebra finches identify individuals using vocal signatures unique to each call type</article-title>. <source>Nat Commun</source>. <year>2018</year>;<volume>9</volume>: <fpage>4026</fpage>
<pub-id pub-id-type="doi">10.1038/s41467-018-06394-9</pub-id>
<pub-id pub-id-type="pmid">30279497</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Van Segbroeck</surname><given-names>M</given-names></name>, <name><surname>Knoll</surname><given-names>AT</given-names></name>, <name><surname>Levitt</surname><given-names>P</given-names></name>, <name><surname>Narayanan</surname><given-names>S</given-names></name>. <article-title>MUPET-Mouse Ultrasonic Profile ExTraction: A Signal Processing Tool for Rapid and Unsupervised Analysis of Ultrasonic Vocalizations</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>94</volume>: <fpage>465</fpage>&#x02013;<lpage>485.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.005</pub-id>
<pub-id pub-id-type="pmid">28472651</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Zala</surname><given-names>SM</given-names></name>, <name><surname>Reitschmidt</surname><given-names>D</given-names></name>, <name><surname>Noll</surname><given-names>A</given-names></name>, <name><surname>Balazs</surname><given-names>P</given-names></name>, <name><surname>Penn</surname><given-names>DJ</given-names></name>. <article-title>Sex-dependent modulation of ultrasonic vocalizations in house mice (Mus musculus musculus)</article-title>. <source>PLoS ONE</source>. <year>2017</year>;<volume>12</volume>: <fpage>e0188647</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0188647</pub-id>
<pub-id pub-id-type="pmid">29236704</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Coffey</surname><given-names>KR</given-names></name>, <name><surname>Marx</surname><given-names>RG</given-names></name>, <name><surname>Neumaier</surname><given-names>JF</given-names></name>. <article-title>DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title>. <source>Neuropsychopharmacology</source>. <year>2019</year>;<volume>44</volume>: <fpage>859</fpage>&#x02013;<lpage>868</lpage>. <pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id>
<pub-id pub-id-type="pmid">30610191</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>: <fpage>436</fpage>&#x02013;<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Vapnik</surname><given-names>V</given-names></name>. <article-title>Support-vector networks</article-title>. <source>Mach Learn</source>. <year>1995</year>;<volume>20</volume>: <fpage>273</fpage>&#x02013;<lpage>297</lpage>. <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Hammerschmidt</surname><given-names>K</given-names></name>, <name><surname>Whelan</surname><given-names>G</given-names></name>, <name><surname>Eichele</surname><given-names>G</given-names></name>, <name><surname>Fischer</surname><given-names>J</given-names></name>. <article-title>Mice lacking the cerebral cortex develop normal song: insights into the foundations of vocal learning</article-title>. <source>Sci Rep</source>. <year>2015</year>;<volume>5</volume>: <fpage>8808</fpage>
<pub-id pub-id-type="doi">10.1038/srep08808</pub-id>
<pub-id pub-id-type="pmid">25744204</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Zampieri</surname><given-names>BL</given-names></name>, <name><surname>Fernandez</surname><given-names>F</given-names></name>, <name><surname>Pearson</surname><given-names>JN</given-names></name>, <name><surname>Stasko</surname><given-names>MR</given-names></name>, <name><surname>Costa</surname><given-names>ACS</given-names></name>. <article-title>Ultrasonic vocalizations during male-female interaction in the mouse model of Down syndrome Ts65Dn</article-title>. <source>Physiol Behav</source>. <year>2014</year>;<volume>128</volume>: <fpage>119</fpage>&#x02013;<lpage>125</lpage>. <pub-id pub-id-type="doi">10.1016/j.physbeh.2014.02.020</pub-id>
<pub-id pub-id-type="pmid">24534182</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Chang</surname><given-names>Y-C</given-names></name>, <name><surname>Cole</surname><given-names>TB</given-names></name>, <name><surname>Costa</surname><given-names>LG</given-names></name>. <article-title>Behavioral phenotyping for autism spectrum disorders in mice</article-title>. <source>Curr Protoc toxicol</source>. <year>2017</year>;<volume>72</volume>: <fpage>11.22.1</fpage>&#x02013;<lpage>11.22.21</lpage>. <pub-id pub-id-type="doi">10.1002/cptx.19</pub-id>
<pub-id pub-id-type="pmid">28463420</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Maaten</surname><given-names>LJPVD</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <source>Visualizing High-Dimensional Data using t-SNE</source>. <year>2008</year> pp. <fpage>2579</fpage>&#x02013;<lpage>2605</lpage>.</mixed-citation></ref><ref id="pcbi.1007918.ref029"><label>29</label><mixed-citation publication-type="book"><name><surname>Zeiler</surname><given-names>MD</given-names></name>, <name><surname>Fergus</surname><given-names>R</given-names></name>. <chapter-title>Visualizing and Understanding Convolutional Networks</chapter-title> In: <name><surname>Fleet</surname><given-names>D</given-names></name>, <name><surname>Pajdla</surname><given-names>T</given-names></name>, <name><surname>Schiele</surname><given-names>B</given-names></name>, <name><surname>Tuytelaars</surname><given-names>T</given-names></name>, editors. <source>Computer Vision&#x02013;ECCV</source>
<year>2014</year>
<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; 2014. pp. <fpage>818</fpage>&#x02013;<lpage>833</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-10590-1_53</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Bhagyesh</surname><given-names>V</given-names></name>, <name><surname>Falak</surname><given-names>S</given-names></name>. <source>CNN Visualization</source>. <year>2017</year>;</mixed-citation></ref><ref id="pcbi.1007918.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Chechik</surname><given-names>G</given-names></name>, <name><surname>Anderson</surname><given-names>MJ</given-names></name>, <name><surname>Bar-Yosef</surname><given-names>O</given-names></name>, <name><surname>Young</surname><given-names>ED</given-names></name>, <name><surname>Tishby</surname><given-names>N</given-names></name>, <name><surname>Nelken</surname><given-names>I</given-names></name>. <article-title>Reduction of information redundancy in the ascending auditory pathway</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>51</volume>: <fpage>359</fpage>&#x02013;<lpage>368</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.030</pub-id>
<pub-id pub-id-type="pmid">16880130</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Cover</surname><given-names>T</given-names></name>, <name><surname>Hart</surname><given-names>P</given-names></name>. <article-title>Nearest neighbor pattern classification</article-title>. <source>IEEE Trans Inform Theory</source>. <year>1967</year>;<volume>13</volume>: <fpage>21</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1109/TIT.1967.1053964</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Burke</surname><given-names>K</given-names></name>, <name><surname>Screven</surname><given-names>LA</given-names></name>, <name><surname>Dent</surname><given-names>ML</given-names></name>. <article-title>CBA/CaJ mouse ultrasonic vocalizations depend on prior social experience</article-title>. <source>PLoS ONE</source>. <year>2018</year>;<volume>13</volume>: <fpage>e0197774</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0197774</pub-id>
<pub-id pub-id-type="pmid">29874248</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Guo</surname><given-names>Z</given-names></name>, <name><surname>Holy</surname><given-names>TE</given-names></name>. <article-title>Sex selectivity of mouse ultrasonic songs</article-title>. <source>Chem Senses</source>. <year>2007</year>;<volume>32</volume>: <fpage>463</fpage>&#x02013;<lpage>473</lpage>. <pub-id pub-id-type="doi">10.1093/chemse/bjm015</pub-id>
<pub-id pub-id-type="pmid">17426047</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Vogel</surname><given-names>AP</given-names></name>, <name><surname>Tsanas</surname><given-names>A</given-names></name>, <name><surname>Scattoni</surname><given-names>ML</given-names></name>. <article-title>Quantifying ultrasonic mouse vocalizations using acoustic analysis in a supervised statistical machine learning framework</article-title>. <source>Sci Rep</source>. <year>2019</year>;<volume>9</volume>: <fpage>8100</fpage>
<pub-id pub-id-type="doi">10.1038/s41598-019-44221-3</pub-id>
<pub-id pub-id-type="pmid">31147563</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Arriaga</surname><given-names>G</given-names></name>, <name><surname>Jarvis</surname><given-names>ED</given-names></name>. <article-title>Mouse vocal communication system: are ultrasounds learned or innate?</article-title>
<source>Brain Lang</source>. <year>2013</year>;<volume>124</volume>: <fpage>96</fpage>&#x02013;<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2012.10.002</pub-id>
<pub-id pub-id-type="pmid">23295209</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Sahani</surname><given-names>M</given-names></name>, <name><surname>Linden</surname><given-names>J</given-names></name>. <article-title>How Linear are Auditory Cortical Responses?</article-title>
<source>NIPS Proceedings</source>. <year>2003</year>;</mixed-citation></ref><ref id="pcbi.1007918.ref038"><label>38</label><mixed-citation publication-type="other">Buyukyilmaz M, Cibikdiken AO. Voice gender recognition using deep learning. Proceedings of 2016 International Conference on Modeling, Simulation and Optimization Technologies and Applications (MSOTA2016). Paris, France: Atlantis Press; 2016. <pub-id pub-id-type="doi">10.2991/msota-16.2016.90</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Sugimoto</surname><given-names>H</given-names></name>, <name><surname>Okabe</surname><given-names>S</given-names></name>, <name><surname>Kato</surname><given-names>M</given-names></name>, <name><surname>Koshida</surname><given-names>N</given-names></name>, <name><surname>Shiroishi</surname><given-names>T</given-names></name>, <name><surname>Mogi</surname><given-names>K</given-names></name>, <etal>et al</etal>
<article-title>A role for strain differences in waveforms of ultrasonic vocalizations during male-female interaction</article-title>. <source>PLoS ONE</source>. <year>2011</year>;<volume>6</volume>: <fpage>e22093</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0022093</pub-id>
<pub-id pub-id-type="pmid">21818297</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref040"><label>40</label><mixed-citation publication-type="other">Zakaria J, Rotschafer S, Mueen A, Razak K, Keogh E. Mining Massive Archives of Mice Sounds with Symbolized Representations. In: Ghosh J, Liu H, Davidson I, Domeniconi C, Kamath C, editors. Proceedings of the 2012 SIAM international conference on data mining. Philadelphia, PA: Society for Industrial and Applied Mathematics; 2012. pp. 588&#x02013;599. <pub-id pub-id-type="doi">10.1137/1.9781611972825.51</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Malladi</surname><given-names>R</given-names></name>, <name><surname>Sethian</surname><given-names>JA</given-names></name>. <article-title>A unified approach to noise removal, image enhancement, and shape recovery</article-title>. <source>IEEE Trans Image Process</source>. <year>1996</year>;<volume>5</volume>: <fpage>1554</fpage>&#x02013;<lpage>1568</lpage>. <pub-id pub-id-type="doi">10.1109/83.541425</pub-id>
<pub-id pub-id-type="pmid">18290072</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Johnston</surname><given-names>JD</given-names></name>. <article-title>Transform coding of audio signals using perceptual noise criteria</article-title>. <source>IEEE J Select Areas Commun</source>. <year>1988</year>;<volume>6</volume>: <fpage>314</fpage>&#x02013;<lpage>323</lpage>. <pub-id pub-id-type="doi">10.1109/49.608</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Abadi</surname><given-names>M</given-names></name>, <name><surname>Agarwal</surname><given-names>A</given-names></name>, <name><surname>Barham</surname><given-names>P</given-names></name>, <name><surname>Brevdo</surname><given-names>E</given-names></name>, <name><surname>Chen</surname><given-names>Z</given-names></name>, <name><surname>Citro</surname><given-names>C</given-names></name>, <etal>et al</etal>
<source>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</source>. <year>2015</year>.</mixed-citation></ref><ref id="pcbi.1007918.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Ioffe</surname><given-names>S</given-names></name>, <name><surname>Szegedy</surname><given-names>C</given-names></name>. <source>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</source>. <year>2015</year>.</mixed-citation></ref><ref id="pcbi.1007918.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Srivastava</surname><given-names>N</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>, <name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Salakhutdinov</surname><given-names>R</given-names></name>. <source>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</source>. <year>2014</year> pp. <fpage>1929</fpage>&#x02013;<lpage>1958</lpage>.</mixed-citation></ref><ref id="pcbi.1007918.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Kingma</surname><given-names>D</given-names></name>, <name><surname>Ba</surname><given-names>J</given-names></name>. <source>Adam: A Method for Stochastic Optimization</source>. <year>2014</year>.</mixed-citation></ref><ref id="pcbi.1007918.ref047"><label>47</label><mixed-citation publication-type="journal"><name><surname>Glorot</surname><given-names>Xavier</given-names></name>, <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>. <source>Understanding the difficulty of training deep feedforward neural networks</source>. <year>2010</year>; <fpage>249</fpage>&#x02013;<lpage>256</lpage>.</mixed-citation></ref><ref id="pcbi.1007918.ref048"><label>48</label><mixed-citation publication-type="journal"><name><surname>van der Walt</surname><given-names>S</given-names></name>, <name><surname>Sch&#x000f6;nberger</surname><given-names>JL</given-names></name>, <name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name>, <name><surname>Boulogne</surname><given-names>F</given-names></name>, <name><surname>Warner</surname><given-names>JD</given-names></name>, <name><surname>Yager</surname><given-names>N</given-names></name>, <etal>et al</etal>
<article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source>. <year>2014</year>;<volume>2</volume>: <fpage>e453</fpage>
<pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>
<pub-id pub-id-type="pmid">25024921</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref049"><label>49</label><mixed-citation publication-type="journal"><name><surname>Ben-Hur</surname><given-names>A</given-names></name>, <name><surname>Ong</surname><given-names>CS</given-names></name>, <name><surname>Sonnenburg</surname><given-names>S</given-names></name>, <name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name>, <name><surname>R&#x000e4;tsch</surname><given-names>G</given-names></name>. <article-title>Support vector machines and kernels for computational biology</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>4</volume>: <fpage>e1000173</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1000173</pub-id>
<pub-id pub-id-type="pmid">18974822</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref050"><label>50</label><mixed-citation publication-type="journal"><name><surname>Ahrens</surname><given-names>MB</given-names></name>, <name><surname>Linden</surname><given-names>JF</given-names></name>, <name><surname>Sahani</surname><given-names>M</given-names></name>. <article-title>Nonlinearities and contextual influences in auditory cortical responses modeled with multilinear spectrotemporal methods</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>1929</fpage>&#x02013;<lpage>1942</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3377-07.2008</pub-id>
<pub-id pub-id-type="pmid">18287509</pub-id></mixed-citation></ref><ref id="pcbi.1007918.ref051"><label>51</label><mixed-citation publication-type="journal"><name><surname>Englitz</surname><given-names>B</given-names></name>, <name><surname>Ahrens</surname><given-names>M</given-names></name>, <name><surname>Tolnai</surname><given-names>S</given-names></name>, <name><surname>R&#x000fc;bsamen</surname><given-names>R</given-names></name>, <name><surname>Sahani</surname><given-names>M</given-names></name>, <name><surname>Jost</surname><given-names>J</given-names></name>. <article-title>Multilinear models of single cell responses in the medial nucleus of the trapezoid body</article-title>. <source>Network</source>. <year>2010</year>;<volume>21</volume>: <fpage>91</fpage>&#x02013;<lpage>124</lpage>. <pub-id pub-id-type="doi">10.3109/09548981003801996</pub-id>
<pub-id pub-id-type="pmid">20735339</pub-id></mixed-citation></ref></ref-list></back><sub-article id="pcbi.1007918.r001" article-type="aggregated-review-documents"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007918.r001</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Graham</surname><given-names>Lyle J.</given-names></name><role>Deputy Editor</role></contrib><contrib contrib-type="author"><name><surname>Theunissen</surname><given-names>Fr&#x000e9;d&#x000e9;ric E.</given-names></name><role>Associate Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2020 Graham, Theunissen</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Graham, Theunissen</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007918" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">4 Dec 2019</named-content>
</p><p>Dear Dr Ivanenko,</p><p>Thank you very much for submitting your manuscript 'Classification of mouse ultrasonic vocalizations using deep learning' for review by PLOS Computational Biology. Your manuscript has been fully evaluated by the PLOS Computational Biology editorial team and in this case also by independent peer reviewers. The reviewers appreciated the attention to an important problem, but raised some substantial concerns about the manuscript as it currently stands. While your manuscript cannot be accepted in its present form, we are willing to consider a revised version in which the issues raised by the reviewers have been adequately addressed. We cannot, of course, promise publication at that time.</p><p>Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p><p>Your revisions should address the specific points made by each reviewer. Please return the revised version within the next 60 days. If you anticipate any delay in its return, we ask that you let us know the expected resubmission date by email at <email>ploscompbiol@plos.org</email>. Revised manuscripts received beyond 60 days may require evaluation and peer review similar to that applied to newly submitted manuscripts.</p><p>In addition, when you are ready to resubmit, please be prepared to provide the following:</p><p>(1) A detailed list of your responses to the review comments and the changes you have made in the manuscript. We require a file of this nature before your manuscript is passed back to the editors.</p><p>(2) A copy of your manuscript with the changes highlighted (encouraged). We encourage authors, if possible to show clearly where changes have been made to their manuscript e.g. by highlighting text.</p><p>(3) A striking still image to accompany your article (optional). If the image is judged to be suitable by the editors, it may be featured on our website and might be chosen as the issue image for that month. These square, high-quality images should be accompanied by a short caption. Please note as well that there should be no copyright restrictions on the use of the image, so that it can be published under the Open-Access license and be subject only to appropriate attribution.</p><p>Before you resubmit your manuscript, please consult our Submission Checklist to ensure your manuscript is formatted correctly for PLOS Computational Biology: <ext-link ext-link-type="uri" xlink:href="http://www.ploscompbiol.org/static/checklist.action">http://www.ploscompbiol.org/static/checklist.action</ext-link>. Some key points to remember are:</p><p>- Figures uploaded separately as TIFF or EPS files (if you wish, your figures may remain in your main manuscript file in addition).</p><p>- Supporting Information uploaded as separate files, titled Dataset, Figure, Table, Text, Protocol, Audio, or Video.</p><p>- Funding information in the 'Financial Disclosure' box in the online system.</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com">https://pacev2.apexcovantage.com</ext-link>&#x000a0;PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>.</p><p>To enhance the reproducibility of your results, we recommend that you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/submission-guidelines#loc-materials-and-methods">here</ext-link>.&#x000a0;</p><p>We are sorry that we cannot be more positive about your manuscript at this stage, but if you have any concerns or questions, please do not hesitate to contact us.</p><p>Sincerely,</p><p>Fr&#x000e9;d&#x000e9;ric E. Theunissen</p><p>Associate Editor</p><p>PLOS Computational Biology</p><p>Lyle Graham</p><p>Deputy Editor</p><p>PLOS Computational Biology</p><p>A link appears below if there are any accompanying review attachments. If you believe any reviews to be missing, please contact <email>ploscompbiol@plos.org</email> immediately:</p><p>[LINK]</p><p>Dear Alexander and Bernhard,</p><p>Both I and the reviewers found that your use of the DNN for classifying SUV is interesting - although I still find it somewhat incremental. Both reviewers have addressed minor points that you should address. I am very much in agreement with reviewer 1 that the only cross-validation test that is correct is what you call the single recording test set. Your paper should not use any other tests - period! This is true not only for the DNN but with your comparisons with other methods like linear regression. Other wise you are admitting that you have a "random- effect" the identity of each mouse but ignoring it. It is as if you knew that you need to do mixed-effect modeling but refrain from doing so. In terms of the analysis of vocalizations, this point was very explicitly made by Mundry and Sommer (Anim Behavior 2007) as applied to linear classifiers - they call their approach permuted DFA. Note also that this is the type of cross-validation that my lab has always performed both with Hyenas and with Zebra Finches. I have also not looked at what is usually done for mouse for USV but the 9 bio acoustical features you choose are quite simple - I would like you try a much more complete set of features that include a better description of the spectral enveloppe (spectral mean, quartiles, etc), temporal envelope and fundamental frequencies (we have used such features in our work but they are quite commonly used by bioacoustibcians). For the fundamental frequency, you might even to PC of the time-varying fundamental (eg. Dahl A, Sherlock BR, Campos JJ, Theunissen FE, 2014.</p><p>Best wishes,</p><p>Frederic.</p><p>Reviewer's Responses to Questions</p><p><bold>Comments to the Authors:</bold></p><p><bold>Please note here if the review is uploaded as an attachment.</bold></p><p>Reviewer #1: Review: PCOMPBIOL-D-19-01759</p><p>Ivanenko et. Al., have developed several DNNs (Deep Neural Networks) capable of classifying individual USVs into discrete categories with accuracy not previously achievable. Most notably they are capable of classifying the sex of the emitting mouse ~78% of the time. They were also able to extend this technique to classify USVs from a transgenic mouse, as well as classifying USVs by sever experimenter defined acoustic properties. The work reported here is extremely thorough and described in excellent detail, and the underlying method could prove extremely useful for researchers hoping to analyze vocalizations during opposite sex interactions. Computational methods of identification have significant benefits over competing methods, such as multi-microphone arrays, namely zero cost and ease of implementation. Although Dr. Englitz appears to be working on that solution in parallel, as well as considering combining the methods in the future. The technique also has some drawbacks. This is only useful for studying opposite-sex interactions, and 78% accuracy may be too low to effectively study syllable ordering, or syntax. The precise pattern of vocalizations is thought to be an important feature of social vocalization, and miss-categorizing 22% of syllables could drastically change results.</p><p>I have no major concerns</p><p>Minor concerns and suggestions</p><p>Calling the network 84% accurate is a bit misleading. DNNs are really meant to classify new information, and the network is only 78% accurate when tested against a novel mouse. Although the authors admit this, they lean heavily on the 84% number throughout the manuscripts.</p><p>The title feels a little broad, and perhaps should include classification of sex or genotype.</p><p>This manuscript seems to have two main threads. The first is proof that information about the sex of emitter mice is contained within individual USVs. I believe the authors do an excellent job proving this point, and they show clearly that the sex information is embedded in a complex combination of many features, including individual variation in USV production. While this work is thorough and well done, my enthusiasm for it is not particularly high. The features underlying classification are still unknown and I&#x02019;m not exactly sure why it would be useful for mice identify sex based on a single call. It feels like this thread occupies the bulk of the text and sort of occludes the authors more important contribution.</p><p>Namely, the authors have laid the groundwork for a universal sex classifier for mouse USVs. The current network is ~78% accurate within novel animals of the same strain, a major improvement to all existing methods I am aware of. The great thing about neural networks is they aren&#x02019;t fixed or permanent. They can be shared and retrained by researchers around the world who have already collected datasets from separate strains or transgenic animals. Unfortunately, I couldn&#x02019;t find the final networks among the available data. The Authors seem to have made all of the code and raw files available, so it should be possible to re-train the networks from scratch, but that seems like an unnecessary deterrent to wider adoption/refining of the technique.</p><p>Typos</p><p>79: feature combination s (I&#x02019;m not sure about this)</p><p>98: Sentence cuts off</p><p>905: vocalization..</p><p>Reviewer #2: USV &#x02013; plos computational biology</p><p>The authors propose an interesting approach to sex discrimination in USV. They rightly identify that this a very challenging line of investigation.</p><p>Automated methods were used to automatically extracted USVs - please provide validation data on these methods as automated extraction is not a straightforward process and rarely as accurate as claimed.</p><p>In presenting the final outcomes, it would be helpful to report the metrics resulting from the rerun analysis as this more likely represents what would be observed in a novel cohort. Ie. performance is reduced.</p><p>Re-analyzing previously published data which reported no differences between groups, and then reporting different outcomes, is an important process.</p><p>Wording of this phrase needs revising &#x0201c;As intruders we used anaesthetized females to ensure that only the resident mouse could emit calls.&#x0201d;</p><p>More information on the WT/Cortexless paradigm is needed in the main document rather than making the reader find it in the supplementary materials.</p><p>The authors mention that data were derived from a subset of both cohorts. How were these subsets selected? Were only the &#x02018;best &#x02018;recordings selected, was there any bias in this process or were they randomly selected?</p><p>The number of calls appears sufficient but is 17 representative of mice in general?</p><p>DNN are a great way forward in this field, but as in other fields, understanding the machinations within the convolution and connected layers and how the output represents a concrete concept is challenging.</p><p>**********</p><p><bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold></p><p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Computational Biology</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p><p>Reviewer #1: Yes</p><p>Reviewer #2: Yes</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p><p>Reviewer #1: Yes: Kevin R Coffey</p><p>Reviewer #2: No</p></body></sub-article><sub-article id="pcbi.1007918.r002" article-type="author-comment"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007918.r002</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007918" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">5 Mar 2020</named-content>
</p><supplementary-material content-type="local-data" id="pcbi.1007918.s004"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">ResponseLetter_USVClassify.docx</named-content></p></caption><media xlink:href="pcbi.1007918.s004.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></body></sub-article><sub-article id="pcbi.1007918.r003" article-type="editor-report"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007918.r003</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Graham</surname><given-names>Lyle J.</given-names></name><role>Deputy Editor</role></contrib><contrib contrib-type="author"><name><surname>Theunissen</surname><given-names>Fr&#x000e9;d&#x000e9;ric E.</given-names></name><role>Associate Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2020 Graham, Theunissen</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Graham, Theunissen</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007918" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">12 Mar 2020</named-content>
</p><p>Dear Mr Ivanenko,</p><p>Thank you very much for submitting your manuscript "Classifying sex and strain from mouse ultrasonic vocalizations using deep learning" for consideration at PLOS Computational Biology. As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. The reviewers appreciated the attention to an important topic. Based on the reviews, we are likely to accept this manuscript for publication, providing that you modify the manuscript according to the review recommendations.</p><p>Dear Alexander and Bernhard,</p><p>As you know from our previous correspondence you have performed an interesting analysis and showed that the USV in mice show some degree of sexual dimorphism but that this is exhibited in a complex acoustic signature that can best be extracted from a spectrographic representation. I still have some comments that I would like you to address. The mostly relate to your methods. You also use descriptions and language that are not completely in-line with what bio-acousticians use. I like your analyses of your networks.</p><p>Minor comments:</p><p>1. Methods: &#x0201c;Spectrograms were converted to a sparse representation by setting all values below 70th percentile to zero, which eliminated most close to silent background noise bins&#x0201d;. It is common to talk about thresholds in dB. So I suggest stating the corresponding dB threshold (e.g. This floor corresponds to a xx dB thresholds below peak).</p><p>2. &#x0201c;Hence, vocalizations longer than 100 ms (11.7%) were included shortened.&#x0201d; Reword: &#x0201c;Hence, vocalizations longer than 100 ms () were truncated to 100&#x02026;&#x0201d;. (I assume the end was deleted &#x02013; otherwise use the correct words to explain how it was shortened).</p><p>3. I know that you already provided more details on the NN but I am here also asking you to provide more details for the calculation of the auditory features. You should also use descriptions (and terms) that are well understood among bioacousticians. I am assuming that [0,1] means any value between 0 and 1? You figure 1 seems to say that these are attributed by visual inspection (by hand) but only 1, 0.5 and 0???:</p><p>a. E.g. how is the broadband defined? And how do you go from 0 to 1? Why not use Weiner Entropy which might be more common in sound analyses?</p><p>b. Similarly for complexity? Is this the Weiner entropy?</p><p>c. Similarly for tremolo? Specific the equation that you used.</p><p>d. What is the average Frequency of a vocalization? The spectral mean? Or the average fundamental? These are very different. You should probably have both. You do talk about the mean spectral energy for detecting vocalizations. If that is what you use, use the same term.</p><p>e. Spectral mean requires a distribution. How is this one estimated?</p><p>f. Also Power requires a frequency range &#x02013; just all energy above 25 kHz. Normalized by duration? dB is a unitless measure (relative) so dB2 is meaningless. Both amplitude and power can be expressed in dB.</p><p>4. I don&#x02019;t think that any of the measures should be assigned by hand. Ok for Direction, Peaks and Breaks but Broadband and Complexity should be quantified. Why not use measures such as spectral bandwidth and Weiner entropy? You could also do something like pitch saliency.</p><p>5. I still find your choice of acoustic features somewhat limited. I think that some of the most experienced bioacousticians might wince and I would not like that given that my name will also be on your report as the associate editor. I would also add fundamental. You could do the mean but also do a time-varying fundamental (that you can express with PC for dimensionality reduction if you want). My lab as a Python toolbox that you can find in <ext-link ext-link-type="uri" xlink:href="http://github.com:/theunissenlab/BioSoundTutorial">github.com:/theunissenlab/BioSoundTutorial</ext-link> that you might find useful to efficiently extract features. But don&#x02019;t take my word for it &#x02013; you can also see what has been published by other groups. One issue is that mice USV are late in the game and that most folks that have analyzed them are not bioacousticians so you will need to get inspired by analyses in other animals.</p><p>Please prepare and submit your revised manuscript within 30 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email.&#x000a0;</p><p>When you are ready to resubmit, please upload the following:</p><p>[1] A letter containing a detailed list of your responses to all review comments, and a description of the changes you have made in the manuscript. Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out</p><p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p><p>Important additional instructions are given below your reviewer comments.</p><p>Thank you again for your submission to our journal. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p><p>Sincerely,</p><p>Fr&#x000e9;d&#x000e9;ric E. Theunissen</p><p>Associate Editor</p><p>PLOS Computational Biology</p><p>Lyle Graham</p><p>Deputy Editor</p><p>PLOS Computational Biology</p><p>***********************</p><p>A link appears below if there are any accompanying review attachments. If you believe any reviews to be missing, please contact <email>ploscompbiol@plos.org</email> immediately:</p><p>[LINK]</p><p>Dear Alexander and Bernhard,</p><p>As you know from our previous correspondence you have performed an interesting analysis and showed that the USV in mice show some degree of sexual dimorphism but that this is exhibited in a complex acoustic signature that can best be extracted from a spectrographic representation. I still have some comments that I would like you to address. The mostly relate to your methods. You also use descriptions and language that are not completely in-line with what bio-acousticians use. I like your analyses of your networks.</p><p>Minor comments:</p><p>1. Methods: &#x0201c;Spectrograms were converted to a sparse representation by setting all values below 70th percentile to zero, which eliminated most close to silent background noise bins&#x0201d;. It is common to talk about thresholds in dB. So I suggest stating the corresponding dB threshold (e.g. This floor corresponds to a xx dB thresholds below peak).</p><p>2. &#x0201c;Hence, vocalizations longer than 100 ms (11.7%) were included shortened.&#x0201d; Reword: &#x0201c;Hence, vocalizations longer than 100 ms () were truncated to 100&#x02026;&#x0201d;. (I assume the end was deleted &#x02013; otherwise use the correct words to explain how it was shortened).</p><p>3. I know that you already provided more details on the NN but I am here also asking you to provide more details for the calculation of the auditory features. You should also use descriptions (and terms) that are well understood among bioacousticians. I am assuming that [0,1] means any value between 0 and 1? You figure 1 seems to say that these are attributed by visual inspection (by hand) but only 1, 0.5 and 0???:</p><p>a. E.g. how is the broadband defined? And how do you go from 0 to 1? Why not use Weiner Entropy which might be more common in sound analyses?</p><p>b. Similarly for complexity? Is this the Weiner entropy?</p><p>c. Similarly for tremolo? Specific the equation that you used.</p><p>d. What is the average Frequency of a vocalization? The spectral mean? Or the average fundamental? These are very different. You should probably have both. You do talk about the mean spectral energy for detecting vocalizations. If that is what you use, use the same term.</p><p>e. Spectral mean requires a distribution. How is this one estimated?</p><p>f. Also Power requires a frequency range &#x02013; just all energy above 25 kHz. Normalized by duration? dB is a unitless measure (relative) so dB2 is meaningless. Both amplitude and power can be expressed in dB.</p><p>4. I don&#x02019;t think that any of the measures should be assigned by hand. Ok for Direction, Peaks and Breaks but Broadband and Complexity should be quantified. Why not use measures such as spectral bandwidth and Weiner entropy? You could also do something like pitch saliency.</p><p>5. I still find your choice of acoustic features somewhat limited. I think that some of the most experienced bioacousticians might wince and I would not like that given that my name will also be on your report as the associate editor. I would also add fundamental. You could do the mean but also do a time-varying fundamental (that you can express with PC for dimensionality reduction if you want). My lab as a Python toolbox that you can find in <ext-link ext-link-type="uri" xlink:href="http://github.com:/theunissenlab/BioSoundTutorial">github.com:/theunissenlab/BioSoundTutorial</ext-link> that you might find useful to efficiently extract features. But don&#x02019;t take my word for it &#x02013; you can also see what has been published by other groups. One issue is that mice USV are late in the game and that most folks that have analyzed them are not bioacousticians so you will need to get inspired by analyses in other animals.</p><p><underline>Figure Files:</underline></p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <underline><ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com/" xlink:type="simple">https://pacev2.apexcovantage.com</ext-link></underline>.&#x000a0;PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <underline><email xlink:type="simple">figures@plos.org</email></underline>.</p><p><underline>Data Requirements:</underline></p><p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link ext-link-type="uri" xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p><p><underline>Reproducibility:</underline></p><p>To enhance the reproducibility of your results, PLOS recommends that you deposit laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see <underline><ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plospathogens/s/submission-guidelines" xlink:type="simple">http://journals.plos.org/ploscompbiol/s/submission-guidelines#loc-materials-and-methods</ext-link></underline></p></body></sub-article><sub-article id="pcbi.1007918.r004" article-type="author-comment"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007918.r004</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007918" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">18 Apr 2020</named-content>
</p><supplementary-material content-type="local-data" id="pcbi.1007918.s005"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">ResponseLetter2_USVClassify.docx</named-content></p></caption><media xlink:href="pcbi.1007918.s005.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></body></sub-article><sub-article id="pcbi.1007918.r005" article-type="editor-report"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007918.r005</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Graham</surname><given-names>Lyle J.</given-names></name><role>Deputy Editor</role></contrib><contrib contrib-type="author"><name><surname>Theunissen</surname><given-names>Fr&#x000e9;d&#x000e9;ric E.</given-names></name><role>Associate Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2020 Graham, Theunissen</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Graham, Theunissen</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="rel-obj005" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007918" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">30 Apr 2020</named-content>
</p><p>Dear Mr Ivanenko,</p><p>We are pleased to inform you that your manuscript 'Classifying sex and strain from mouse ultrasonic vocalizations using deep learning' has been provisionally accepted for publication in PLOS Computational Biology.</p><p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. A member of our team will be in touch with a set of requests.</p><p>Please note that your manuscript will not be scheduled for publication until you have made the required changes, so a swift response is appreciated.</p><p>IMPORTANT: The editorial review process is now complete. PLOS will only permit corrections to spelling, formatting or significant scientific errors from this point onwards. Requests for major changes, or any which affect the scientific understanding of your work, will cause delays to the publication date of your manuscript.</p><p>Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us now if you or your institution is planning to press release the article. All press must be co-ordinated with PLOS.</p><p>Thank you again for supporting Open Access publishing; we are looking forward to publishing your work in PLOS Computational Biology.&#x000a0;</p><p>Best regards,</p><p>Fr&#x000e9;d&#x000e9;ric E. Theunissen</p><p>Associate Editor</p><p>PLOS Computational Biology</p><p>Lyle Graham</p><p>Deputy Editor</p><p>PLOS Computational Biology</p><p>***********************************************************</p><p>dear Bernhard,</p><p>Thank you for your patience and carefully addressing all of my questions and recommendations.</p><p>Best wishes,</p><p>F</p></body></sub-article><sub-article id="pcbi.1007918.r006" article-type="editor-report"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007918.r006</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Graham</surname><given-names>Lyle J.</given-names></name><role>Deputy Editor</role></contrib><contrib contrib-type="author"><name><surname>Theunissen</surname><given-names>Fr&#x000e9;d&#x000e9;ric E.</given-names></name><role>Associate Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2020 Graham, Theunissen</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Graham, Theunissen</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="rel-obj006" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007918" related-article-type="reviewed-article"/></front-stub><body><p>
<named-content content-type="letter-date">4 Jun 2020</named-content>
</p><p>PCOMPBIOL-D-19-01759R2 </p><p>Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</p><p>Dear Dr Englitz,</p><p>I am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course.</p><p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. </p><p>Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p><p>Thank you again for supporting PLOS Computational Biology and open-access publishing. We are looking forward to publishing your work! </p><p>With kind regards,</p><p>Laura Mallard</p><p>PLOS Computational Biology | Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom <email>ploscompbiol@plos.org</email> | Phone +44 (0) 1223-442824 | <ext-link ext-link-type="uri" xlink:href="http://ploscompbiol.org">ploscompbiol.org</ext-link> | @PLOSCompBiol</p></body></sub-article></article>