
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30646611</article-id><article-id pub-id-type="pmc">6359327</article-id><article-id pub-id-type="doi">10.3390/s19020315</article-id><article-id pub-id-type="publisher-id">sensors-19-00315</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Local Parallel Cross Pattern: A Color Texture Descriptor for Image Retrieval</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9124-761X</contrib-id><name><surname>Feng</surname><given-names>Qinghe</given-names></name><xref ref-type="aff" rid="af1-sensors-19-00315">1</xref></contrib><contrib contrib-type="author"><name><surname>Hao</surname><given-names>Qiaohong</given-names></name><xref ref-type="aff" rid="af2-sensors-19-00315">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2164-6858</contrib-id><name><surname>Sbert</surname><given-names>Mateu</given-names></name><xref ref-type="aff" rid="af2-sensors-19-00315">2</xref><xref ref-type="aff" rid="af3-sensors-19-00315">3</xref></contrib><contrib contrib-type="author"><name><surname>Yi</surname><given-names>Yugen</given-names></name><xref ref-type="aff" rid="af4-sensors-19-00315">4</xref></contrib><contrib contrib-type="author"><name><surname>Wei</surname><given-names>Ying</given-names></name><xref ref-type="aff" rid="af1-sensors-19-00315">1</xref><xref rid="c1-sensors-19-00315" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Dai</surname><given-names>Jiangyan</given-names></name><xref ref-type="aff" rid="af5-sensors-19-00315">5</xref><xref rid="c1-sensors-19-00315" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-19-00315"><label>1</label>College of Information Science and Engineering, Northeastern University, Shenyang 110004, China; <email>1510377@stu.neu.edu.cn</email></aff><aff id="af2-sensors-19-00315"><label>2</label>College of Intelligence and Computing, Tianjin University, Tianjin 300350, China; <email>qiaohonghao@gmail.com</email></aff><aff id="af3-sensors-19-00315"><label>3</label>Institute of Informatics and Applications, University of Girona, 17017 Girona, Spain; <email>mateusbert@mac.com</email></aff><aff id="af4-sensors-19-00315"><label>4</label>School of Software, Jiangxi Normal University, Nanchang 330022, China; <email>yiyg510@jxnu.edu.cn</email></aff><aff id="af5-sensors-19-00315"><label>5</label>School of Computer Engineering, Weifang University, Weifang 261061, China</aff><author-notes><corresp id="c1-sensors-19-00315"><label>*</label>Correspondence: <email>weiying@ise.neu.edu.cn</email> (Y.W.); <email>daijy@wfu.edu.cn</email> (J.D.); Tel.: +86-024-83688326 (Y.W.)</corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>1</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2019</year></pub-date><volume>19</volume><issue>2</issue><elocation-id>315</elocation-id><history><date date-type="received"><day>02</day><month>12</month><year>2018</year></date><date date-type="accepted"><day>11</day><month>1</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 by the authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Riding the wave of visual sensor equipment (e.g., personal smartphones, home security cameras, vehicle cameras, and camcorders), image retrieval (IR) technology has received increasing attention due to its potential applications in e-commerce, visual surveillance, and intelligent traffic. However, determining how to design an effective feature descriptor has been proven to be the main bottleneck for retrieving a set of images of interest. In this paper, we first construct a six-layer color quantizer to extract a color map. Then, motivated by the human visual system, we design a local parallel cross pattern (LPCP) in which the local binary pattern (LBP) map is amalgamated with the color map in &#x0201c;parallel&#x0201d; and &#x0201c;cross&#x0201d; manners. Finally, to reduce the computational complexity and improve the robustness to image rotation, the LPCP is extended to the uniform local parallel cross pattern (ULPCP) and the rotation-invariant local parallel cross pattern (RILPCP), respectively. Extensive experiments are performed on eight benchmark datasets. The experimental results validate the effectiveness, efficiency, robustness, and computational complexity of the proposed descriptors against eight state-of-the-art color texture descriptors to produce an in-depth comparison. Additionally, compared with a series of Convolutional Neural Network (CNN)-based models, the proposed descriptors still achieve competitive results.</p></abstract><kwd-group><kwd>visual sensor</kwd><kwd>image retrieval</kwd><kwd>human visual system</kwd><kwd>local parallel cross pattern</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-19-00315"><title>1. Introduction</title><p>Since a huge number of image corpora have been produced by visual sensor equipment<sc>,</sc> an increasing demand for efficient encoding and indexing of these image corpora has attracted the attention of a considerable number of researchers [<xref rid="B1-sensors-19-00315" ref-type="bibr">1</xref>,<xref rid="B2-sensors-19-00315" ref-type="bibr">2</xref>,<xref rid="B3-sensors-19-00315" ref-type="bibr">3</xref>,<xref rid="B4-sensors-19-00315" ref-type="bibr">4</xref>,<xref rid="B5-sensors-19-00315" ref-type="bibr">5</xref>,<xref rid="B6-sensors-19-00315" ref-type="bibr">6</xref>,<xref rid="B7-sensors-19-00315" ref-type="bibr">7</xref>]. Thanks to the investigators&#x02019; breakthroughs, a myriad of methods [<xref rid="B8-sensors-19-00315" ref-type="bibr">8</xref>,<xref rid="B9-sensors-19-00315" ref-type="bibr">9</xref>,<xref rid="B10-sensors-19-00315" ref-type="bibr">10</xref>,<xref rid="B11-sensors-19-00315" ref-type="bibr">11</xref>,<xref rid="B12-sensors-19-00315" ref-type="bibr">12</xref>,<xref rid="B13-sensors-19-00315" ref-type="bibr">13</xref>,<xref rid="B14-sensors-19-00315" ref-type="bibr">14</xref>,<xref rid="B15-sensors-19-00315" ref-type="bibr">15</xref>,<xref rid="B16-sensors-19-00315" ref-type="bibr">16</xref>,<xref rid="B17-sensors-19-00315" ref-type="bibr">17</xref>,<xref rid="B18-sensors-19-00315" ref-type="bibr">18</xref>,<xref rid="B19-sensors-19-00315" ref-type="bibr">19</xref>,<xref rid="B20-sensors-19-00315" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-00315" ref-type="bibr">21</xref>,<xref rid="B22-sensors-19-00315" ref-type="bibr">22</xref>,<xref rid="B23-sensors-19-00315" ref-type="bibr">23</xref>,<xref rid="B24-sensors-19-00315" ref-type="bibr">24</xref>,<xref rid="B25-sensors-19-00315" ref-type="bibr">25</xref>,<xref rid="B26-sensors-19-00315" ref-type="bibr">26</xref>,<xref rid="B27-sensors-19-00315" ref-type="bibr">27</xref>,<xref rid="B28-sensors-19-00315" ref-type="bibr">28</xref>] have continuously been developed for encoding and indexing.</p><p>In early work, the local binary pattern (LBP) [<xref rid="B8-sensors-19-00315" ref-type="bibr">8</xref>], a grayscale texture descriptor, was first proposed for encoding the center pixel and its neighborhood pixels. Afterwards, owing to the disadvantage of losing global information, the LBP was extended to the LBP variance (LBPV) [<xref rid="B9-sensors-19-00315" ref-type="bibr">9</xref>] which was amalgamated with global rotation-invariant matching for texture classification. Further, in order to completely detail the local differences among the central pixel and its neighborhood pixels, the completed local binary pattern (CLBP) operator [<xref rid="B10-sensors-19-00315" ref-type="bibr">10</xref>] was designed for rotation-invariant feature representation. The local derivative pattern (LDP) [<xref rid="B11-sensors-19-00315" ref-type="bibr">11</xref>] was then produced by refining the magnitude difference in local neighborhoods. Along another line, taking into account the situation of non-uniform lighting conditions, the local ternary pattern (LTP) [<xref rid="B12-sensors-19-00315" ref-type="bibr">12</xref>] was introduced, and it was combined with kernel principal component analysis (KPCA) to improve its robustness to illumination. Later, the LTP was further modified into the local tetra pattern (LTrP) [<xref rid="B13-sensors-19-00315" ref-type="bibr">13</xref>] by using the first-order derivatives in the vertical and horizontal directions. After that, the LTP was again extended to the local maximum edge binary patterns (LMEBP) [<xref rid="B14-sensors-19-00315" ref-type="bibr">14</xref>], and the LMEBP were combined with the Gabor transform used for image retrieval and object tracking. Besides these, to achieve robustness to uniform and Gaussian noises, the noise-resistant LBP (NRLBP) [<xref rid="B15-sensors-19-00315" ref-type="bibr">15</xref>] was constructed to preserve local structure information. Inspired by the fusion strategy, the local neighborhood difference pattern (LNDP) [<xref rid="B16-sensors-19-00315" ref-type="bibr">16</xref>] was concatenated with the LBP map to integrate the local intensity difference information and the local binary information in a parallel manner. However, all the above methods are confined within grayscale image processing, so the major drawback of these construction processes is the inevitable loss of color information.</p><p>In recent years, a series of color texture descriptors [<xref rid="B17-sensors-19-00315" ref-type="bibr">17</xref>,<xref rid="B18-sensors-19-00315" ref-type="bibr">18</xref>,<xref rid="B19-sensors-19-00315" ref-type="bibr">19</xref>,<xref rid="B20-sensors-19-00315" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-00315" ref-type="bibr">21</xref>,<xref rid="B22-sensors-19-00315" ref-type="bibr">22</xref>,<xref rid="B23-sensors-19-00315" ref-type="bibr">23</xref>,<xref rid="B24-sensors-19-00315" ref-type="bibr">24</xref>,<xref rid="B25-sensors-19-00315" ref-type="bibr">25</xref>,<xref rid="B26-sensors-19-00315" ref-type="bibr">26</xref>,<xref rid="B27-sensors-19-00315" ref-type="bibr">27</xref>,<xref rid="B28-sensors-19-00315" ref-type="bibr">28</xref>,<xref rid="B29-sensors-19-00315" ref-type="bibr">29</xref>] have been sequentially developed for color image processing. Among them, the local oppugnant color texture pattern (LOCTP) [<xref rid="B17-sensors-19-00315" ref-type="bibr">17</xref>], a variant of the multi component LTrP, was combined with the colored pattern appearance model (CPAM) in the YCbCr, HSV, and RGB color spaces. In reference [<xref rid="B18-sensors-19-00315" ref-type="bibr">18</xref>], according to the adder and decoder concepts, the multi-channel adder local binary pattern (maLBP) and the multi-channel decoder local binary pattern (mdLBP) were designed to combine the LBP maps in the R, G, and B components. After that, a class of pairwise-based local binary patterns [<xref rid="B19-sensors-19-00315" ref-type="bibr">19</xref>,<xref rid="B20-sensors-19-00315" ref-type="bibr">20</xref>] and a series of color-edge approaches [<xref rid="B21-sensors-19-00315" ref-type="bibr">21</xref>,<xref rid="B22-sensors-19-00315" ref-type="bibr">22</xref>,<xref rid="B23-sensors-19-00315" ref-type="bibr">23</xref>] were introduced to classify and retrieve natural color images. Recently, on the basis of intra- and inter-channel encoding concepts, the opponent color local binary patterns (OCLBP) were proposed by M&#x000e4;enp&#x000e4;&#x000e4; et al. [<xref rid="B24-sensors-19-00315" ref-type="bibr">24</xref>]. Further, in reference [<xref rid="B25-sensors-19-00315" ref-type="bibr">25</xref>], Bianconi et al. extended the OCLBP to the improved opponent color local binary patterns (IOCLBP), in which the point-to-point thresholding was replaced by point-to-average thresholding. Considering the graph-based fusion framework, the bag-of-words of local features and the color local Haar binary pattern were integrated by Li et al. [<xref rid="B26-sensors-19-00315" ref-type="bibr">26</xref>]. Quite recently, in order to systematically analyze the robustness to illumination changes, a bag of color texture descriptors [<xref rid="B27-sensors-19-00315" ref-type="bibr">27</xref>] was studied under 46 lighting conditions. At the same time, with the help of a non-linear support vector machine, the orthogonal combination of local binary patterns (OC-LBP) was concatenated with the color histogram (CH) [<xref rid="B28-sensors-19-00315" ref-type="bibr">28</xref>]. </p><p>In this paper, we present the main following contributions:
<list list-type="order"><list-item><p>We design a six-layer color quantizer that is applied to quantize the a* and b* components for color map extraction.</p></list-item><list-item><p>We construct a local parallel cross pattern (LPCP) in which the LBP map and the color map are integrated into a whole framework.</p></list-item><list-item><p>We further extend the LPCP to the uniform local parallel cross pattern (ULPCP) and the rotation-invariant local parallel cross pattern (RILPCP) to reduce the computational complexity and achieve robustness to image rotation.</p></list-item><list-item><p>We benchmark the comparative experiments with eight state-of-the-art color texture descriptors on eight benchmark datasets to illustrate the effectiveness, efficiency, robustness, and computational complexity of the proposed descriptors.</p></list-item><list-item><p>We additionally develop a weight-based optimization scheme that shows better improvement.</p></list-item></list></p><p>The rest of this paper is organized as follows. <xref ref-type="sec" rid="sec2-sensors-19-00315">Section 2</xref> briefly introduces the local binary pattern and the color distribution prior in the L*a*b* color space. <xref ref-type="sec" rid="sec3-sensors-19-00315">Section 3</xref> details the feature representation. The experiments and discussion are presented in <xref ref-type="sec" rid="sec4-sensors-19-00315">Section 4</xref>. <xref ref-type="sec" rid="sec5-sensors-19-00315">Section 5</xref> concludes this paper and indicates future directions.</p></sec><sec id="sec2-sensors-19-00315"><title>2. Related Work</title><sec id="sec2dot1-sensors-19-00315"><title>2.1. Local Binary Pattern</title><p>In reference [<xref rid="B8-sensors-19-00315" ref-type="bibr">8</xref>], Ojala et al. first designed the local binary pattern (LBP) for texture feature representation. Given a gray pixel <italic>G</italic>(<italic>i</italic>, <italic>j</italic>), the computational results among <italic>G</italic>(<italic>i</italic>, <italic>j</italic>) and its neighbors <italic>G<sub>x</sub></italic>(<italic>i</italic>, <italic>j</italic>) are encoded as the LBP value. The formula for the LBP value in pixel <italic>G</italic>(<italic>i</italic>, <italic>j</italic>) is as follows:
<disp-formula id="FD1-sensors-19-00315"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>&#x003d1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>x</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2-sensors-19-00315"><label>(2)</label><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>&#x003d1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>G<sub>x</sub></italic>(<italic>i</italic>, <italic>j</italic>) is a pixel <italic>G</italic>(<italic>i</italic>, <italic>j</italic>)&#x02019;s <italic>x</italic>-th neighbor, and <italic>n</italic> and <italic>r</italic> represent the number of neighbors and the radius of the neighborhood, respectively.</p><p>To reduce the computational complexity, Ojala et al. [<xref rid="B8-sensors-19-00315" ref-type="bibr">8</xref>] defined the uniform LBP, in which each LBP pattern has, at most, two bitwise changes among its neighbors. The measure operator &#x0201c;U&#x0201d; of an LBP pattern is defined as the number of bitwise changes. Mathematically, the uniform LBP is defined as follows:
<disp-formula id="FD3-sensors-19-00315"><label>(3)</label><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>&#x003d1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003d1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>&#x003d1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003d1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If U(<italic>LBP<sub>n</sub></italic><sub>,<italic>r</italic></sub>(<italic>i</italic>,<italic>j</italic>)) &#x02264; 2, then <italic>LBP<sub>n</sub></italic><sub>,<italic>r</italic></sub>(<italic>i</italic>,<italic>j</italic>) is classified as the uniform LBP pattern; otherwise, <italic>LBP<sub>n</sub></italic><sub>,<italic>r</italic></sub>(<italic>i</italic>,<italic>j</italic>) belongs to the non-uniform LBP pattern.</p><p>To achieve robustness to image rotation, Pietik&#x000e4;inen et al. [<xref rid="B29-sensors-19-00315" ref-type="bibr">29</xref>] designed the rotation-invariant LBP, in which all types of the same transition were considered as one pattern. The measure operator ROR (<italic>LBP<sub>n</sub></italic>,<italic><sub>r</sub></italic>(<italic>i</italic>, <italic>j</italic>), <italic>x</italic>) is defined as a circular bitwise right shift for <italic>x</italic> times on the <italic>n</italic>-bit number <italic>LBP<sub>n</sub></italic><sub>,<italic>r</italic></sub>(<italic>i</italic>, <italic>j</italic>). Mathematically, the rotation-invariant LBP is expressed as follows:<disp-formula id="FD4-sensors-19-00315"><label>(4)</label><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>ROR</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For details, please refer to references [<xref rid="B8-sensors-19-00315" ref-type="bibr">8</xref>,<xref rid="B29-sensors-19-00315" ref-type="bibr">29</xref>,<xref rid="B30-sensors-19-00315" ref-type="bibr">30</xref>]. For simplicity, referring to reference [<xref rid="B19-sensors-19-00315" ref-type="bibr">19</xref>], <italic>n</italic> and <italic>r</italic> were set to 8 and 1, respectively. In the rest of this paper, we refer to the LBP map as <italic>LBP</italic>(<italic>i</italic>, <italic>j</italic>), the uniform LBP map as <italic>ULBP</italic>(<italic>i</italic>, <italic>j</italic>), and the rotation-invariant LBP map as <italic>RILBP</italic>(<italic>i</italic>, <italic>j</italic>).</p></sec><sec id="sec2dot2-sensors-19-00315"><title>2.2. The Selection of the Color Space</title><p>The selection of the color space is acknowledged as an important preprocessing stage [<xref rid="B1-sensors-19-00315" ref-type="bibr">1</xref>]. Currently, RGB, HSV, HIS, CMYK, YUV, and L*a*b* are widely adopted in feature representation. Among these, the most commonly used color space is RGB. However, the inferiority of the RGB color space can be summarized as follows: (1) the yellow is lost; (2) there is a plethora from green to blue; and (3) it is not suited for the visual perception mechanism. Different from RGB, the superiority of the L*a*b* color space can be summarized as follows: (1) the L*a*b* remedies the missing yellow in RGB; (2) there is no plethora from green to blue; and (3) it is suited for the visual perception mechanism. Besides this, L*a*b* provides excellent decoupling between color (represented by the a* and b* components) and intensity (represented by the L* component) [<xref rid="B31-sensors-19-00315" ref-type="bibr">31</xref>]. Therefore, all images are transformed from RGB to the L*a*b* color space in the preprocessing stage. Referring to reference [<xref rid="B32-sensors-19-00315" ref-type="bibr">32</xref>], the standard RGB to L*a*b* transformation is carried out as follows:
<disp-formula id="FD5-sensors-19-00315"><label>(5)</label><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>L</mml:mi><mml:mo>*</mml:mo><mml:mo>=</mml:mo><mml:mn>116</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>for</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x0003e;</mml:mo><mml:mn>0.08856</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>L</mml:mi><mml:mo>*</mml:mo><mml:mo>=</mml:mo><mml:mn>903.3</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>for</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x02264;</mml:mo><mml:mn>0.08856</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD6-sensors-19-00315"><label>(6)</label><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>*</mml:mo><mml:mo>=</mml:mo><mml:mn>500</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>X</mml:mi><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD7-sensors-19-00315"><label>(7)</label><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mo>*</mml:mo><mml:mo>=</mml:mo><mml:mn>500</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>X</mml:mi><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
with
<disp-formula id="FD8-sensors-19-00315"><label>(8)</label><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>for</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>u</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>0.08856</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>7.78</mml:mn><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>for</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>u</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>0.08856</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where
<disp-formula id="FD9-sensors-19-00315"><label>(9)</label><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>X</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Z</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.412453</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.357580</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.180423</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.212671</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.715160</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.072169</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.019334</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.119193</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.950227</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>R</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>G</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>X<sub>n</sub></italic>, <italic>Y<sub>n</sub></italic>, and <italic>Z<sub>n</sub></italic> are set to 0.950450, 1.000000, and 1.088754.</p></sec><sec id="sec2dot3-sensors-19-00315"><title>2.3. Color Distribution Prior Knowledge in the L*a*b* Color Space</title><p>In reference [<xref rid="B1-sensors-19-00315" ref-type="bibr">1</xref>], the color distribution prior knowledge in the L*a*b* color space was analyzed and summarized for different color image sets. In <xref ref-type="fig" rid="sensors-19-00315-f001">Figure 1</xref>a,b, an example of the Stex database [<xref rid="B33-sensors-19-00315" ref-type="bibr">33</xref>] is illustrated. It can be seen that the frequency of pixels is mainly concentrated in the middle range of the a* and b* components. To validate the consistency of this prior knowledge, extensive experiments were performed on different color image sets, and these results show that the prior knowledge is consistent. </p><p>Further, the stability of this prior knowledge was also studied when the image database was changed. Examples of 50% and 10% of the Stex image datasets are presented in <xref ref-type="fig" rid="sensors-19-00315-f001">Figure 1</xref>c&#x02013;f. From those figures, except for the frequency of pixels, we can easily see that the pixels are still distributed in the middle range of the a* and b* components. These phenomena illustrate that the prior knowledge is stable.</p><p>The reason for this was studied more deeply in reference [<xref rid="B20-sensors-19-00315" ref-type="bibr">20</xref>]. As depicted in <xref ref-type="fig" rid="sensors-19-00315-f001">Figure 1</xref>a&#x02013;f, the frequency of pixels in the a* and b* components gradually declines from the middle to both sides, but the saturation in the a* and b* components inversely goes up from the middle to both sides. Through extensive experiments, we propose that the color probability distribution has a negative correlation with the saturation of the a* and b* components because higher saturation occurs with a lower frequency.</p></sec></sec><sec id="sec3-sensors-19-00315"><title>3. Feature Representation</title><sec id="sec3dot1-sensors-19-00315"><title>3.1. Six-Layer Color Quantizer</title><p>Inspired by the color distribution prior knowledge in the L*a*b* color space, a novel six-layer color quantizer was designed, as shown in <xref ref-type="fig" rid="sensors-19-00315-f002">Figure 2</xref>, in which each layer includes a set of bins and its corresponding indices. In the proposed quantizer, the original range [&#x02212;128, +127] is first divided into two equal bins, 2<sup>8</sup>/3, on both sides and two refined bins, 2<sup>7</sup>/3, in the middle. Sequentially, the indices are named 0, 1, 2, and 3 at layer 1. Second, to further refine the two middle bins, 2<sup>7</sup>/3, they are uniformly divided into four equal bins, 2<sup>6</sup>/3, from layers 1 to 2. Meanwhile, the remaining bins are copied from the layer 1 to 2. Third, the operators &#x0201c;Copy&#x0201d; and &#x0201c;Divide&#x0201d; are continuously repeated until the two middle bins at the layer 6. Finally, combining the layer 1 to 6, we construct a six-layer color quantizer. The quantization layers in the a* and b* components are denoted <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub>, where <italic>W</italic><sub>a*</sub>, <italic>W</italic><sub>b*</sub>
<inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {1, 2, &#x02026;, 6}, and the indices are denoted <italic>&#x00174;</italic><sub>a*</sub> and <italic>&#x00174;</italic><sub>b*</sub>, <italic>&#x00174;</italic><sub>a*</sub>
<inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {0, 1, &#x02026;, <italic>&#x01e84;</italic><sub>a*</sub>} and <italic>&#x00174;</italic><sub>b*</sub>
<inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {0, 1, &#x02026;, <italic>&#x01e84;</italic><sub>b*</sub>}, where <italic>&#x01e84;</italic><sub>a*</sub> = 2(<italic>W</italic><sub>a*</sub> + 1) &#x02013; 1 and <italic>&#x01e84;</italic><sub>b*</sub> = 2(<italic>W</italic><sub>b*</sub> + 1) &#x02013; 1 respectively. </p><p>Referring to reference [<xref rid="B34-sensors-19-00315" ref-type="bibr">34</xref>], we discuss the quantization error under different quantization layers, <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub>. <xref ref-type="fig" rid="sensors-19-00315-f003">Figure 3</xref> shows the quantization errors on Stex, 50% of Stex, and 10% of Stex. From the figures, it can be seen that along with the refinement of layers W<sub>a*</sub> and W<sub>b*</sub>, where W<sub>a*</sub>, W<sub>b*</sub>
<inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {1, 2, &#x02026;, 6}, the quantization error decreases obviously. This phenomenon illustrates the effectiveness of the proposed quantizer. Moreover, we also note that the values of the quantization errors on Stex, 50% of Stex, and 10% of Stex are extremely close to each other. This phenomenon confirms the stability and consistency of the six-layer color quantizer. In particular, the quantization error from layers 5 to 6 decreases only slightly on the three datasets. These results demonstrate that stopping the quantization layers <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub> at the 6th layer is appropriate.</p><p>Furthermore, referring to the visual perception mechanism in reference [<xref rid="B35-sensors-19-00315" ref-type="bibr">35</xref>], the original range [0, +100] in the L* component is divided into three bins, namely, [0, +25], [+26, +75], and [+76, +100]. Herein, the quantization layer of the L* component is denoted <bold><italic>W</italic><sub>L*</sub></bold>, where <italic>W</italic><sub>L*</sub> = 1, and the indices of the three bins are denoted <italic>&#x00174;</italic><sub>L*</sub>, <italic>&#x00174;</italic><sub>L*</sub>
<inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {0, 1, &#x02026;, <italic>&#x01e84;</italic><sub>L*</sub>}, where <italic>&#x01e84;</italic><sub>L*</sub> = 2<italic>W</italic><sub>L*</sub>. For a pixel (<italic>i</italic>, <italic>j</italic>) in image <italic>I</italic>, we combine the indices of <italic>W</italic><sub>L*</sub>, <italic>W</italic><sub>a*</sub>, and <italic>W</italic><sub>b*</sub> to construct the color map <italic>C</italic>(<italic>i</italic>, <italic>j</italic>), and the index of <italic>C</italic>(<italic>i</italic>, <italic>j</italic>) is denoted <italic>&#x00108;</italic>, <italic>&#x00108;</italic>
<inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {0, 1, &#x02026;, <italic>&#x0010c;</italic>}, where <italic>&#x0010c;</italic> = 3 &#x000d7; 2(<italic>W</italic><sub>a*</sub> + 1) &#x000d7; 2(<italic>W</italic><sub>b*</sub> + 1) &#x02212; 1. </p></sec><sec id="sec3dot2-sensors-19-00315"><title>3.2. Local Parallel Cross Pattern</title><p>As elucidated in Gray&#x02019;s Anatomy [<xref rid="B36-sensors-19-00315" ref-type="bibr">36</xref>], the human visual system is an important pathway that codes low-layer visual cues to construct the high-layer semantics perception in parallel and cross manners. On the basis of the human visual system, we propose a novel local parallel cross pattern (LPCP) to integrate the color map and the LBP map as a unified framework in &#x0201c;parallel&#x0201c; and &#x0201c;cross&#x0201c; manners. </p><p>Given an original map <italic>I</italic>(<italic>i</italic>, <italic>j</italic>), the central point and its eight neighbors are denoted <italic>I</italic><sub>0</sub>(<italic>i</italic>, <italic>j</italic>) and <italic>I</italic><italic><sub>k</sub></italic>(<italic>i</italic>, <italic>j</italic>), where <italic>k</italic>
<inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {1, 2, &#x02026;, 8}. Firstly, we extract the LBP map <italic>LBP</italic>(<italic>i</italic>, <italic>j</italic>) (see <xref ref-type="sec" rid="sec2dot1-sensors-19-00315">Section 2.1</xref>) and the color map <italic>C</italic>(<italic>i</italic>, <italic>j</italic>) ( see <xref ref-type="sec" rid="sec3dot1-sensors-19-00315">Section 3.1</xref>). Secondly, all eight neighbors of the LBP map and the color map are mutually crossed to construct the LBP and color cross maps. Thirdly, we calculate the frequency of each neighborhood in the LBP and color cross maps to construct the LBP and color frequency maps, respectively. Finally, the values of the maximum frequency in the LBP and color frequency maps are considered the feature vectors, and the central values in the LBP and color frequency maps are flagged as the indices. For clarity, <xref ref-type="fig" rid="sensors-19-00315-f004">Figure 4</xref> presents a detailed schematic diagram of the local parallel cross pattern, in which LPCP is encoded as <italic>LPCP<sub>LBP</sub></italic>(3) = 4 and <italic>LPCP<sub>color</sub></italic>(7) = 5. Mathematically, LPCP is defined as follows:
<disp-formula id="FD10-sensors-19-00315"><label>(10)</label><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>max</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>Fr</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD11-sensors-19-00315"><label>(11)</label><mml:math id="mm18"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>max</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>Fr</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where Fr{&#x000b7;} represents the frequency of each neighbor. In particular, when the value of <italic>LBP</italic><sub>0</sub>(<italic>i</italic>, 
<italic>j</italic>) is equal to <italic>C</italic><sub>0</sub>(<italic>i</italic>, <italic>j</italic>), <italic>LBP</italic><sub>0</sub>(<italic>i</italic>, <italic>j</italic>) and <italic>C</italic><sub>0</sub>(<italic>i</italic>, <italic>j</italic>) can still be separated and encoded as <italic>LPCP<sub>LBP</sub></italic>(<italic>LBP</italic><sub>0</sub>(<italic>i</italic>, <italic>j</italic>)) and <italic>LPCP<sub>color</sub></italic>(<italic>C</italic><sub>0</sub>(<italic>i</italic>, <italic>j</italic>)), respectively. Herein, the feature dimensions of <italic>LPCP<sub>LBP</sub></italic> and <italic>LPCP<sub>color</sub></italic> are 256 and 3 &#x000d7; 2(<italic>W</italic><sub>a*</sub> + 1) &#x000d7; 2(<italic>W</italic><sub>b*</sub> + 1). Given a color image dataset <italic>T</italic>, the optimal quantization layers <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub> are calculated based upon the retrieval accuracy score. This process is defined as the maximization problem as follows:<disp-formula id="FD12-sensors-19-00315"><label>(12)</label><mml:math id="mm19"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>Acc</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where Acc(<italic>T</italic>|<italic>W</italic><sub>a*</sub>, <italic>W</italic><sub>b*</sub>) represents the retrieval accuracy score. We provide the optimal color quantization layers <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub> in <xref ref-type="sec" rid="sec4dot4-sensors-19-00315">Section 4.4</xref>.</p><p>To reduce the computational complexity, we define the uniform local parallel cross pattern (ULPCP) in which <italic>ULBP</italic>(<italic>i</italic>, <italic>j</italic>) replaces <italic>LBP</italic>(<italic>i</italic>, <italic>j</italic>). To achieve robustness to image rotation, we design the rotation-invariant local parallel cross pattern (RILPCP) in which <italic>RILBP</italic>(<italic>i</italic>, <italic>j</italic>) replaces <italic>LBP</italic>(<italic>i</italic>, <italic>j</italic>).</p><p>As presented in <xref ref-type="fig" rid="sensors-19-00315-f005">Figure 5</xref>, RILPCP achieves exactly the same feature encoding in spite of the image rotation. For <xref ref-type="fig" rid="sensors-19-00315-f005">Figure 5</xref>a, we first extract the RILBP map and the color map. Then, according to Equations (10) and (11), RILPCP is encoded as [RILPCP<italic><sub>RILBP</sub></italic>(3) = 4; RILPCP<italic><sub>color</sub></italic>(7) = 4]. When <xref ref-type="fig" rid="sensors-19-00315-f005">Figure 5</xref>a is rotated 90&#x000b0; to <xref ref-type="fig" rid="sensors-19-00315-f005">Figure 5</xref>b, we can also extract the same RILBP and color map because both color and RILBP are rotation invariant. Further, RILPCP is still calculated as [RILPCP<italic><sub>RILBP</sub></italic>(3) = 4; RILPCP<italic><sub>color</sub></italic>(7) = 4]. In fact, an image can be rotated by an arbitrary degree.</p></sec></sec><sec id="sec4-sensors-19-00315"><title>4. Experiments and Discussion</title><sec id="sec4dot1-sensors-19-00315"><title>4.1. Distance Metric</title><p>The distance metric is considered as the measure of similarity between a query image and a database image. In our retrieval framework, we first convert the query image and database images into feature vectors, and we then calculate the distance measure between the query image and database images. Referring to references [<xref rid="B1-sensors-19-00315" ref-type="bibr">1</xref>,<xref rid="B18-sensors-19-00315" ref-type="bibr">18</xref>,<xref rid="B20-sensors-19-00315" ref-type="bibr">20</xref>,<xref rid="B21-sensors-19-00315" ref-type="bibr">21</xref>,<xref rid="B37-sensors-19-00315" ref-type="bibr">37</xref>], the Extended Canberra Distance is exploited, and it is given as
<disp-formula id="FD13-sensors-19-00315"><label>(13)</label><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003bd;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>F<sub>d</sub></italic>, <italic>F<sub>q</sub></italic>, and <italic>k</italic> respectively denote the image in the database, the query image by the user, and the feature vector length, and <italic>R</italic> is the measurement result between the query image <italic>F<sub>q</sub></italic> and the image <italic>F<sub>d</sub></italic> in the database. Further, <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003bd;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003bd;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec4dot2-sensors-19-00315"><title>4.2. Evaluation Criteria</title><p>In this section, we introduce the most popular evaluation criteria, such as the precision rate, the recall rate, the average precision rate (APR) value, the average recall rate (ARR) value, and the precision-recall (PR) curve to validate the proposed descriptors. </p><p>First, the precision and recall rates are formulated as follows:<disp-formula id="FD14-sensors-19-00315"><label>(14)</label><mml:math id="mm23"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003b1;</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003b1;</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD15-sensors-19-00315"><label>(15)</label><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003b2;</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003b2;</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mi>&#x003be;</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD16-sensors-19-00315"><label>(16)</label><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02260;</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm333"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003b2;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm334"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003b1;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm336"><mml:mrow><mml:mrow><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the total number of images in the same category, the total number of returned images, and the category information. <inline-formula><mml:math id="mm335"><mml:mrow><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as the binarized function. If <italic>&#x003be;</italic>(<italic>&#x003b7;</italic>(<italic>F<sub>q</sub></italic>), <italic>&#x003b7;</italic>(<italic>F<sub>d</sub></italic>))) = 1, then <italic>&#x003b7;</italic>(<italic>F<sub>q</sub></italic>) and <italic>&#x003b7;</italic>(<italic>F<sub>d</sub></italic>) are determined to be in the same 
category; if <italic>&#x003be;</italic>(<italic>&#x003b7;</italic>(<italic>F<sub>q</sub></italic>), <italic>&#x003b7;</italic>(<italic>F<sub>d</sub></italic>))) = 0, then <italic>&#x003b7;</italic>(<italic>F<sub>q</sub></italic>) and <italic>&#x003b7;</italic>(<italic>F<sub>d</sub></italic>) do not belong to the same category. </p><p>Then, the average precision rate (APR) and average recall rate (ARR) values are given by the following equations:<disp-formula id="FD17-sensors-19-00315"><label>(17)</label><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD18-sensors-19-00315"><label>(18)</label><mml:math id="mm27"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msubsup><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28"><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29"><mml:mrow><mml:mover accent="true"><mml:mi>N</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are the total number of query images and the <inline-formula><mml:math id="mm30"><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>th query image, respectively.</p><p>Finally, the precision-recall (PR) curve can be considered an ancillary criterion that evaluates the dynamic precision by computing the threshold recall. Mathematically, the PR curve is defined as follows:
<disp-formula id="FD19-sensors-19-00315"><label>(19)</label><mml:math id="mm31"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003b2;</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>&#x003c4;</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x022c5;</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>N<sub>&#x003b2;</sub></italic> and <italic>N<sub>&#x003c4;</sub></italic> are the total number of images in the same category and the number of retrieval images at the recall of <inline-formula><mml:math id="mm444"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm32"><mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {1, 2, &#x02026;, <italic>N<sub>&#x003b1;</sub></italic> &#x02212; 1}. </p></sec><sec id="sec4dot3-sensors-19-00315"><title>4.3. Image Databases</title><p>In our experiments, the eight benchmark databases reported in <xref rid="sensors-19-00315-t001" ref-type="table">Table 1</xref>, comprising one natural object image database (Coil-100 [<xref rid="B38-sensors-19-00315" ref-type="bibr">38</xref>]), two facial image databases (Face95 [<xref rid="B39-sensors-19-00315" ref-type="bibr">39</xref>] and Face96 [<xref rid="B40-sensors-19-00315" ref-type="bibr">40</xref>]), and five color textural image databases (Outex-00031 [<xref rid="B41-sensors-19-00315" ref-type="bibr">41</xref>], Outex-00032 [<xref rid="B41-sensors-19-00315" ref-type="bibr">41</xref>], Outex-00033 [<xref rid="B41-sensors-19-00315" ref-type="bibr">41</xref>], Outex-00034 [<xref rid="B41-sensors-19-00315" ref-type="bibr">41</xref>], and MIT-VisTex [<xref rid="B42-sensors-19-00315" ref-type="bibr">42</xref>]), were used to provide a comprehensive evaluation.</p><p>The Coil-100 (No. 1) database was produced by a charge-coupled device (CCD)-camera (Sony XC-77P). It has 7200 images in 100 objects. Each object contains 72 images, each with a size of 128 &#x000d7; 128 in JPG format. Because each image was collected by rotating an object at 5 degrees, the Coil-100 not only evaluates the effectiveness, but also investigates the robustness to rotation. Some samples are depicted in <xref ref-type="fig" rid="sensors-19-00315-f006">Figure 6</xref>a in which each row represents the same semantic category. </p><p>The Face 95 (No. 2) and Face 96 (No. 3) databases were collected by an S-VHS camcorder. Among them, the Face 95 consists of 1440 images in 72 male and female subjects. For each subject, there are 20 images, each with a size of 180 &#x000d7; 200 in JPG format. The Face 96 has 91 male and female subjects. For each subject, there are 1814 images with a size of 196 &#x000d7; 196 in JPG format. Specifically, all images are variations of head turns, head scales, face expressions, and illumination changes. Some samples are presented in <xref ref-type="fig" rid="sensors-19-00315-f006">Figure 6</xref>b,c. </p><p>The Outex-00031 (No. 4), Outex-00032 (No. 5), Outex-00033 (No. 6), and Outex-00034 (No. 7) databases were produced by a charge-coupled device (CCD)-camera (Sony DXC-775P), and the MIT-VisTex (No. 8) was collected from real-world photographs and videos. Some samples of these databases are presented in <xref ref-type="fig" rid="sensors-19-00315-f006">Figure 6</xref>d&#x02013;h. As documented in <xref rid="sensors-19-00315-t001" ref-type="table">Table 1</xref>, the Outex-00031, Outex-00032, and Outex-00033 consist of 2720 images in 68 categories. Each category includes 40 images, each with a size of 128 &#x000d7; 128 in BMP format. Differently, the Outex-00034 has 4048 images in 204 categories. Each category includes 20 images, each with a size of 128 &#x000d7; 128 in BMP format. Next, the MIT-VisTex consists of 640 images in 40 categories. Each category includes 16 images, each with a size of 128 &#x000d7; 128 in PPM format. There are resolution differences on Outex-00031, noise differences on Outex-00032, blur differences on Outex-00033, and illumination differences on Outex-00034. Thus, these four databases were used to evaluate the robustness to resolution, noise, blur, and illumination.</p><p>In addition, all above databases can be freely downloaded from the corresponding websites. To guarantee accuracy and reproducibility, we chose all images as the query images in the dataset. Referring to references [<xref rid="B1-sensors-19-00315" ref-type="bibr">1</xref>,<xref rid="B18-sensors-19-00315" ref-type="bibr">18</xref>,<xref rid="B20-sensors-19-00315" ref-type="bibr">20</xref>,<xref rid="B37-sensors-19-00315" ref-type="bibr">37</xref>], if not specified, the total number of returned images was set to 10 in all of the following experiments.</p></sec><sec id="sec4dot4-sensors-19-00315"><title>4.4. Evaluation of Color Quantization Layers</title><p><xref rid="sensors-19-00315-t002" ref-type="table">Table 2</xref> shows the highest APR values of LPCP, RILPCP, and ULPCP with the optimal color quantization layers (<italic>W</italic><sub>a*</sub>, <italic>W</italic><sub>b*</sub>) on the eight databases. The best values are shown in bold. On Coil-100, three facts are noted: (1) LPCP achieves the highest APR value of 99.47% when (<italic>W</italic><sub>a*</sub> = 6, <italic>W</italic><sub>b*</sub> = 5); (2) RILPCP achieves the highest APR value of 99.44% when (<italic>W</italic><sub>a*</sub> = 5, <italic>W</italic><sub>b*</sub> = 5); and (3) ULPCP yields the highest APR value of 99.52% when (<italic>W</italic><sub>a*</sub> = 4, <italic>W</italic><sub>b*</sub> = 6). Next, on Face 95 and Face 96, LPCP, RILPCP, and ULPCP all achieve their top APR values when (<italic>W</italic><sub>a*</sub> = 6, <italic>W</italic><sub>b*</sub> = 6). Similarly, on the remaining color textural databases, the corresponding color quantization layers <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub> result in the best accuracy scores. As noted above, it can be easily summarized that when LPCP, RILPCP, and ULPCP are used on the same image database, the coefficients <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub> are extremely close to each other. These phenomena again confirm the stability and consistency of the color distribution prior knowledge. Moreover, it is noteworthy that a single color quantization layer cannot be suitable for all image databases. In the following experiments, the optimal color quantization layers, <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub>, were adaptively selected according to the different databases.</p></sec><sec id="sec4dot5-sensors-19-00315"><title>4.5. Comparison with LBP-Based Descriptors</title><p><xref rid="sensors-19-00315-t003" ref-type="table">Table 3</xref> reports comparisons among the proposed descriptors and the LBP-based descriptors in terms of the APR and ARR. The best {APR, ARR} values are shown in bold. As documented in this table, the {APR, ARR} values of LPCP are far better than those of LBP by {9.64%, 1.34%} on Coil-100, {28.88%, 14.43%} on Face95, {32.10%, 16.12%} on Face96, {11.63%, 2.90%} on Outex-00031, {15.44%, 3.86%} on Outex-00032, {12.26%, 3.10%} on Outex-00033, {38.07%, 19.04%} on Outex-00034, and {4.96%, 3.10%} on MIT-VisTex. Analogous to the proposed LPCP, the {APR, ARR} values of ULPCP and RILPCP are also definitely higher than those of ULBP and RILBP on all eight databases. Meanwhile, three points can be observed below: (1) ULPCP has the maximum {APR, ARR} value of {99.52%, 13.82%} on Coil-100; (2) RILPCP produces the best results of {97.12%, 48.56%} on Face95, and {97.77%, 49.04%} on Face96; and (3) LPCP has the highest performance of {89.62%, 22.40%} on Outex-00031, {84.86%, 21.21%} on Outex-00032, {87.80%, 21.99%} on Outex-00033, {84.36%, 42.18%} on Outex-00034, and {98.33%, 61.46%} on MIT-VisTex. According to the above results, it can be asserted that the improvements given by the proposed descriptors are very considerable. The main reason for this is that the LBP information is amalgamated with the color information.</p></sec><sec id="sec4dot6-sensors-19-00315"><title>4.6. Comparison with Other Color Texture Descriptors</title><p>To evaluate the effectiveness, efficiency, robustness, and computational complexity, the proposed descriptors were compared with eight state-of-the-art color texture descriptors in terms of the average precision rate (APR) value, the average recall rate (ARR) value, the precision-recall (PR) curve, the feature vector length, and the memory consumption. All experiments were performed under the leave-one-out cross-validation principle. For clarity, all comparative color texture methods are summarized as follows:
<list list-type="bullet"><list-item><p>Multi-channel adder local binary patterns (mdLBP) [<xref rid="B18-sensors-19-00315" ref-type="bibr">18</xref>]: The 2048-dimensional color texture descriptor in the RGB color space.</p></list-item><list-item><p>Multi-channel decoded local binary patterns (maLBP) [<xref rid="B18-sensors-19-00315" ref-type="bibr">18</xref>]: The 1024-dimensional color texture descriptor in the RGB color space.</p></list-item><list-item><p>Color difference histogram (CDH) [<xref rid="B21-sensors-19-00315" ref-type="bibr">21</xref>]: The 90-dimensional color histogram and the 18-dimensional edge orientation histogram in the L*a*b* color space.</p></list-item><list-item><p>Multi-texton histogram (MTH) [<xref rid="B22-sensors-19-00315" ref-type="bibr">22</xref>]: The 64-dimensional color histogram and the 18-dimensional edge orientation histogram in the HSV color space.</p></list-item><list-item><p>Micro-structure descriptor (MSD) [<xref rid="B23-sensors-19-00315" ref-type="bibr">23</xref>]: The 72-dimensional color histogram and the 6-dimensional edge orientation histogram in the HSV color space.</p></list-item><list-item><p>Opponent color local binary patterns (OCLBP) [<xref rid="B24-sensors-19-00315" ref-type="bibr">24</xref>]: The 1536-dimensional color texture descriptor in the RGB color space.</p></list-item><list-item><p>Improved opponent color local binary patterns (IOCLBP) [<xref rid="B25-sensors-19-00315" ref-type="bibr">25</xref>]: The 3072-dimensional color-texture descriptor in the RGB color space.</p></list-item><list-item><p>Orthogonal combination of local binary patterns and color histogram (OC-LBP + CH): The 12-dimensional color histogram in the L*a*b* color space and the 96-dimensional LBP variation in the gray-scale space [<xref rid="B28-sensors-19-00315" ref-type="bibr">28</xref>].</p></list-item><list-item><p>Local parallel cross pattern (LPCP).</p></list-item><list-item><p>Rotation-invariant local parallel cross pattern (RILPCP).</p></list-item><list-item><p>Uniform local parallel cross pattern (ULPCP).</p></list-item></list></p><p><xref rid="sensors-19-00315-t004" ref-type="table">Table 4</xref> lists the APR and ARR values for the proposed descriptors and the existing descriptors on the eight databases. The best values are shown in bold. On Coil-100, the {APR, ARR} value of RILPCP is significantly superior to those of mdLBP, maLBP, CDH, MTH, MSD, OCLBP, IOCLBP, and OC-LBP+CH by {7.10%, 0.98%}, {11.11%, 1.54%}, {1.26%, 0.17%}, {0.99%, 0.14%}, {1.43%, 0.20%}, {14.04%, 1.95%}, {10.32%, 1.43%}, and {3.81%, 0.53%}, respectively. Meanwhile, the {APR, ARR} value of RILPCP is slightly improved (by {+0.03%, +0.01%}) by using LPCP. In this scenario, ULPCP acquires a higher {APR, ARR} value than RILPCP: {99.44%, 13.81} compared with {99.52%, 13.82}. Similar to Coil-100, there are more competitive results on Face95, Face96, Outex-00031, Outex-00032, Outex-00033, Outex-00034, and MIT-VisTex. As a consequence of the above results, it can be summarized that the effectiveness of the proposed descriptors is demonstrated in terms of APR and ARR. Remarkably, there are rotation differences on Coil-100, resolution differences on Outex-00031, noise differences on Outex-00032, blur differences on Outex-00033, and illumination differences on Outex-00034. Thus, the robustness is also illustrated to a certain extent.</p><p><xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>a&#x02013;h depict the precision-recall (PR) curves of all the comparative descriptors on the eight databases. As we can see from <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>a, the curves of LPCP, RILPCP, and ULPCP are higher than those of former color texture descriptors. As shown in <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>b, although the curves of ULPCP, MTH, and MSD are interleaved with one another, both LPCP and RILPCP are superior to all other descriptors. One possible reason for this is that the Face95 database emphasizes the importance of the color information. As depicted in <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>c, considering the complex background, the curves of the proposed descriptors are better than all remaining descriptors on the Face96 database. Analogous to the results for the Face96 database, LPCP, RILPCP, and ULPCP are much better than other descriptors on the Outex-00031 (see the <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>d), Outex-00032 (see the <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>e), Outex-00033 (see the <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>f), and Outex-00034 (see the <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>g) databases, respectively. As depicted in <xref ref-type="fig" rid="sensors-19-00315-f007">Figure 7</xref>h, although mdLBP provides a competitive performance, LPCP achieves the highest curve. The main reason for this is that the MIT-VisTex database contains more textural structure information. Based on all the above observations and analyses, it can be easily noticed that the proposed descriptors are effective in terms of the precision-recall (PR) curve in most of the cases. </p><p><xref rid="sensors-19-00315-t005" ref-type="table">Table 5</xref> compares the computational complexity and memory cost in terms of the feature vector length by dimensionality (D) and memory consumption in kilobytes (Kb). All the experiments were performed on a 4.20 GHz four-core CPU with 16 GB of memory. Herein, analogous to RILPCP and ULPCP, 760/844/844/424/400/676/676/616 (D) and 5.94/6.59/6.59/3.31/3.13/5.28/5.28/4.81 (Kb) show that LPCP performs retrieval requiring 760 D and 5.94 Kb on Coil-100, 844 D and 6.59 Kb on Face95, 844 D and 6.59 Kb on Face96, 424 D and 3.31 Kb on Outex-00031, 400 D and 3.13 Kb on Outex-00032, 676 D and 5.28 Kb on Outex-00033, 676 D and 5.28 Kb on Outex-00034, and 616 D and 4.81 Kb on MIT-VisTex. As documented in <xref rid="sensors-19-00315-t005" ref-type="table">Table 5</xref>, the feature vector length and memory consumption of the proposed descriptors are inferior to those of CDH, MTH, MSD, and OC-LBP+CH, but they are superior to those of mdLBP, mdLBP, OCLBP, and IOCLBP. From the results, although the computational complexity is larger than those of some existing methods, there are several superiorities of LPCP, RILPCP, and ULPCP as follows:
<list list-type="order"><list-item><p>The added computational complexity is effective because the retrieval accuracy is enhanced by a large margin.</p></list-item><list-item><p>The proposed descriptors can adaptively code the color and texture information from different image databases.</p></list-item><list-item><p>The practicability and feasibility of the proposed descriptors, with acceptable feature vector length and competitive memory consumption, are well shown for a realistic system configuration.</p></list-item></list></p></sec><sec id="sec4dot7-sensors-19-00315"><title>4.7. Comparison with CNN-Based Descriptors</title><p>On a different note, we also compared the proposed descriptors with emerging CNN-based descriptors including VGGm, VGGm128, VGGm1024, VGGm2048, ALEX, GoogleNet, and Inception-v3 [<xref rid="B43-sensors-19-00315" ref-type="bibr">43</xref>]. Referring to references [<xref rid="B44-sensors-19-00315" ref-type="bibr">44</xref>,<xref rid="B45-sensors-19-00315" ref-type="bibr">45</xref>], the last fully connected layer was first extracted from the pretrained models. Then, L2 normalization was performed on the extracted fully connected layer. Finally, the distance measure was calculated on the normalized feature vector. For fairness, all images in the database were chosen as the query images, and the number of returned images was set to 10.</p><p><xref ref-type="fig" rid="sensors-19-00315-f008">Figure 8</xref> compares the proposed descriptors and the CNN-based descriptors. On the Coil-100 database, VGGm and ALEX perform slightly better than the proposed LPCP, RILPCP, and ULPCP methods. On the Face95, Face96, Outex-00031, Outex-00032, Outex-00033, and Outex-00034 databases, more significant APR values are obtained by using the proposed LPCP, RILPCP, and ULPCP methods. On the MIT-VisTex database, VGGm achieves a performance that is competitive with those of RILPCP and ULPCP, but LPCP achieves the highest APR value. Although VGGm and ALEX yield relatively competitive APR values, the superior abilities of LPCP, RILPCP, and ULPCP are revealed as follows:
<list list-type="order"><list-item><p>The CNN-based descriptors must be pretrained on a large-scale and annotated dataset (e.g., ImageNet), while the proposed LPCP, RILPCP, and ULPCP methods do not need any pretraining process. </p></list-item><list-item><p>The pretrained CNN-based descriptors are computationally expensive (e.g., cloud servers and mainframe computers), but LPCP, RILPCP, and ULPCP can be performed on almost all realistic systems (e.g., personal smartphones and home security cameras).</p></list-item><list-item><p>LPCP, RILPCP, and ULPCP are more effective than the CNN-based descriptors on six datasets out of the eight examined.</p></list-item></list></p></sec><sec id="sec4dot8-sensors-19-00315"><title>4.8. Additional Experiments on a Weight-Based Optimization Scheme</title><p>To further investigate the optimized coefficient scheme, we added weighting-based experiments on the eight databases. Referring to references [<xref rid="B35-sensors-19-00315" ref-type="bibr">35</xref>,<xref rid="B37-sensors-19-00315" ref-type="bibr">37</xref>], we defined the weighting parameter <italic>v</italic>, where <italic>v</italic>
<inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> {0, 0.01, &#x02026;, 1.00}, and the weighting local parallel cross pattern (LPCP<sub>weight</sub>) was formulated as follows:<disp-formula id="FD20-sensors-19-00315"><label>(20)</label><mml:math id="mm34"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>B</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Observe that <italic>LPCP<sub>weight</sub></italic> degenerates into <italic>LPCP<sub>LBP</sub></italic> when <italic>v</italic> = 0.00 and into <italic>LPCP<sub>color</sub></italic> when <italic>v</italic> = 1.00. Further, LPCP is derived from LPCP<sub>weight</sub> when <italic>v</italic> is set to 0.50. We also extended LPCP<sub>weight</sub> to the weighting uniform local parallel cross pattern (ULPCP<sub>weight</sub>) and the weighting rotation-invariant local parallel cross pattern (URILPCP<sub>weight</sub>). </p><p><xref ref-type="fig" rid="sensors-19-00315-f009">Figure 9</xref> shows the curves of the average precision rate (APR) under the weighting parameter <italic>v</italic> by using LPCP<sub>weight</sub>, ULPCP<sub>weight</sub>, and RILPCP<sub>weight</sub> on the eight databases. On the Coil-100, Face95, and Face96 databases, with an increasing value <italic>v</italic>, the APR values first rapidly go up and then gradually become stable. These results illustrate that a higher proportion of color information is crucial for object and facial images. On the five color textural databases, with the addition of the color information, the APR values firstly show fast growth for <italic>v</italic> near 0.00, gradually become stable, and then abruptly decrease for <italic>v</italic> near 1.00. These phenomena demonstrate that an appropriate weighting optimization scheme can achieve greater enhancements.</p><p>In summary, an optimized parameter <italic>v</italic> is not only beneficial to integrating the merits of <italic>LPCP<sub>color</sub></italic> and <italic>LPCP<sub>LBP</sub></italic>, but also yields more notable improvements.</p></sec></sec><sec id="sec5-sensors-19-00315"><title>5. Conclusions</title><p>In this paper, a color texture method called local parallel cross pattern (LPCP) was proposed for encoding the LBP and color information into a unified framework. Three major contributions were summarized in this paper. First, based on the color prior knowledge in the L*a*b* color space, we designed a six-layer color quantizer that is to be used for color map extraction. Second, inspired by the human visual system, we proposed the local parallel cross pattern (LPCP) for combining the color map and the LBP map in &#x0201c;parallel&#x0201d; and &#x0201c;cross&#x0201d; manners. Third, to improve the computational complexity and provide rotation invariant, LPCP was further extended to the uniform local parallel cross pattern (ULPCP) and the rotation-invariant local parallel cross pattern (RILPCP), respectively. We performed a series of experiments on the eight databases to evaluate the proposed descriptors. Depending on the average precision rate (APR) results, the optimal quantization layers were chosen from the six-layer color quantizer. Compared with the LBP-based descriptors, LPCP, ULPCP, and RILPCP achieved promising APR and ARR results. To evaluate the effectiveness, efficiency, robustness, and computational complexity, we performed comparative experiments among the proposed methods and eight state-of-the-art color texture descriptors in terms of the average precision rate (APR) value, the average recall rate (ARR) value, the precision-recall (PR) curve, the feature vector length, and the memory consumption. Moreover, the proposed approaches were also compared with a series of CNN-based models and achieved competitive results. Additionally, the weight-based optimization scheme yielded more notable improvements.</p><p>In the future, Locality Sensitive Hashing [<xref rid="B46-sensors-19-00315" ref-type="bibr">46</xref>] and feature selection [<xref rid="B47-sensors-19-00315" ref-type="bibr">47</xref>,<xref rid="B48-sensors-19-00315" ref-type="bibr">48</xref>,<xref rid="B49-sensors-19-00315" ref-type="bibr">49</xref>,<xref rid="B50-sensors-19-00315" ref-type="bibr">50</xref>] will be considered to cut down the computation complexity and memory consumption. Meanwhile, Query Expansion (QE) [<xref rid="B51-sensors-19-00315" ref-type="bibr">51</xref>] and Graph Fusion (GF) [<xref rid="B52-sensors-19-00315" ref-type="bibr">52</xref>] will be integrated into the image retrieval system to retrieve more target images. Moreover, normalization methods [<xref rid="B53-sensors-19-00315" ref-type="bibr">53</xref>] also will be considered to achieve illumination invariance.</p></sec></body><back><app-group><app><title>Supplementary Materials</title><supplementary-material content-type="local-data" id="sensors-19-00315-s001"><label>Supplementary File 1</label><media xlink:href="sensors-19-00315-s001.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Q.F. conceived the research idea. Q.F. performed the experiments. Q.F. wrote the paper. Q.F., Q.H., M.S., Y.Y., Y.W., and J.D. gave many suggestions and helped revise this manuscript.</p></notes><notes><title>Funding</title><p>This research was funded by the National Nature Science Foundation of China, grant number [61871106], the Fundamental Research Grant Scheme for the Central Universities, grant number [130204003], the Project of Shandong Province Higher Educational Science and Technology Program, grant number [J16LN68], the Shandong Province Natural Science Foundation, grant number [ZR2017QF011] and the National Key Technology Research and Development Programme of the Ministry of Science and Technology of China, grant number [2014BAI17B02]. </p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-19-00315"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>Q.</given-names></name><name><surname>Hao</surname><given-names>Q.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Yi</surname><given-names>Y.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Hybrid histogram descriptor: A fusion feature representation for image retrieval</article-title><source>Sensors</source><year>2018</year><volume>187</volume><elocation-id>1943</elocation-id><pub-id pub-id-type="doi">10.3390/s18061943</pub-id><pub-id pub-id-type="pmid">29914068</pub-id></element-citation></ref><ref id="B2-sensors-19-00315"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>M.</given-names></name><name><surname>Song</surname><given-names>W.</given-names></name><name><surname>Mei</surname><given-names>H.</given-names></name></person-group><article-title>Efficient retrieval of massive ocean remote sensing images via a cloud-based mean-shift algorithm</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>1693</elocation-id><pub-id pub-id-type="doi">10.3390/s17071693</pub-id><pub-id pub-id-type="pmid">28737699</pub-id></element-citation></ref><ref id="B3-sensors-19-00315"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Feng</surname><given-names>L.</given-names></name><name><surname>Qiao</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Lou</surname><given-names>W.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name></person-group><article-title>Perceptual uniform descriptor and ranking on manifold for image retrieval</article-title><source>Inf. Sci.</source><year>2017</year><volume>424</volume><fpage>235</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2017.10.010</pub-id></element-citation></ref><ref id="B4-sensors-19-00315"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smeulders</surname><given-names>A.W.M.</given-names></name><name><surname>Worring</surname><given-names>M.</given-names></name><name><surname>Santini</surname><given-names>S.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name><name><surname>Jain</surname><given-names>R.</given-names></name></person-group><article-title>Content-based image retrieval at the end of the early years</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2002</year><volume>22</volume><fpage>1349</fpage><lpage>1380</lpage><pub-id pub-id-type="doi">10.1109/34.895972</pub-id></element-citation></ref><ref id="B5-sensors-19-00315"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piras</surname><given-names>L.</given-names></name><name><surname>Giacinto</surname><given-names>G.</given-names></name></person-group><article-title>Information fusion in content based image retrieval: A comprehensive overview</article-title><source>Inf. Fusion</source><year>2017</year><volume>37</volume><fpage>50</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2017.01.003</pub-id></element-citation></ref><ref id="B6-sensors-19-00315"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Fieguth</surname><given-names>P.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Chellappa</surname><given-names>R.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name></person-group><article-title>From BoW to CNN: Two decades of texture representation for texture classification</article-title><source>Int. J. Comput. Vis.</source><year>2018</year><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1007/s11263-018-1125-z</pub-id></element-citation></ref><ref id="B7-sensors-19-00315"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fern&#x000e1;ndez</surname><given-names>A.</given-names></name><name><surname>&#x000c1;lvarez</surname><given-names>M.X.</given-names></name><name><surname>Bianconi</surname><given-names>F.</given-names></name></person-group><article-title>Texture description through histograms of equivalent patterns</article-title><source>J. Math. Imaging Vis.</source><year>2013</year><volume>45</volume><fpage>76</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1007/s10851-012-0349-8</pub-id></element-citation></ref><ref id="B8-sensors-19-00315"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ojala</surname><given-names>T.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name><name><surname>Maenpaa</surname><given-names>T.</given-names></name></person-group><article-title>Multi resolution gray-scale and rotation invariant texture classification with local binary patterns</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2002</year><volume>24</volume><fpage>971</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2002.1017623</pub-id></element-citation></ref><ref id="B9-sensors-19-00315"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>Rotation invariant texture classification using LBP variance (LBPV) with global matching</article-title><source>Pattern Recognit.</source><year>2010</year><volume>43</volume><fpage>706</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2009.08.017</pub-id></element-citation></ref><ref id="B10-sensors-19-00315"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>A completed modeling of local binary pattern operator for texture classification</article-title><source>IEEE Trans. Image Process.</source><year>2010</year><volume>19</volume><fpage>1657</fpage><lpage>1663</lpage><pub-id pub-id-type="pmid">20215079</pub-id></element-citation></ref><ref id="B11-sensors-19-00315"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Gao</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>S.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>Local derivative pattern versus local binary pattern: Face recognition with high-order local pattern descriptor</article-title><source>IEEE Trans. Image Process.</source><year>2010</year><volume>19</volume><fpage>533</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1109/TIP.2009.2035882</pub-id><pub-id pub-id-type="pmid">19887313</pub-id></element-citation></ref><ref id="B12-sensors-19-00315"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>X.</given-names></name><name><surname>Triggs</surname><given-names>B.</given-names></name></person-group><article-title>Enhanced local texture feature sets for face recognition under difficult lighting conditions</article-title><source>IEEE Trans. Image Process.</source><year>2010</year><volume>19</volume><fpage>1635</fpage><lpage>1650</lpage><pub-id pub-id-type="pmid">20172829</pub-id></element-citation></ref><ref id="B13-sensors-19-00315"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murala</surname><given-names>S.</given-names></name><name><surname>Maheshwari</surname><given-names>R.P.</given-names></name><name><surname>Balasubramanian</surname><given-names>R.</given-names></name></person-group><article-title>Local tetra patterns: A new feature descriptor for content-based image retrieval</article-title><source>IEEE Trans. Image Process.</source><year>2012</year><volume>21</volume><fpage>2874</fpage><lpage>2886</lpage><pub-id pub-id-type="doi">10.1109/TIP.2012.2188809</pub-id><pub-id pub-id-type="pmid">22514130</pub-id></element-citation></ref><ref id="B14-sensors-19-00315"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subrahmanyam</surname><given-names>M.</given-names></name><name><surname>Maheshwari</surname><given-names>R.P.</given-names></name><name><surname>Balasubramanian</surname><given-names>R.</given-names></name></person-group><article-title>Local maximum edge binary patterns: A new descriptor for image retrieval and object tracking</article-title><source>Signal Process.</source><year>2012</year><volume>92</volume><fpage>1467</fpage><lpage>1479</lpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2011.12.005</pub-id></element-citation></ref><ref id="B15-sensors-19-00315"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>J.</given-names></name><name><surname>Jiang</surname><given-names>X.</given-names></name><name><surname>Yuan</surname><given-names>J.</given-names></name></person-group><article-title>Noise-resistant local binary pattern with an embedded error-correction mechanism</article-title><source>IEEE Trans. Image Process.</source><year>2013</year><volume>22</volume><fpage>4049</fpage><lpage>4060</lpage><pub-id pub-id-type="doi">10.1109/TIP.2013.2268976</pub-id><pub-id pub-id-type="pmid">23797250</pub-id></element-citation></ref><ref id="B16-sensors-19-00315"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verma</surname><given-names>M.</given-names></name><name><surname>Raman</surname><given-names>B.</given-names></name></person-group><article-title>Local neighborhood difference pattern: A new feature descriptor for natural and texture image retrieval</article-title><source>Multimed. Tools Appl.</source><year>2018</year><volume>77</volume><fpage>11843</fpage><lpage>11866</lpage><pub-id pub-id-type="doi">10.1007/s11042-017-4834-3</pub-id></element-citation></ref><ref id="B17-sensors-19-00315"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeena Jacob</surname><given-names>I.</given-names></name><name><surname>Srinivasagan</surname><given-names>K.G.</given-names></name><name><surname>Jayapriya</surname><given-names>K.</given-names></name></person-group><article-title>Local oppugnant color texture pattern for image retrieval system</article-title><source>Pattern Recognit. Lett.</source><year>2014</year><volume>42</volume><fpage>72</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2014.01.017</pub-id></element-citation></ref><ref id="B18-sensors-19-00315"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubey</surname><given-names>S.R.</given-names></name><name><surname>Singh</surname><given-names>S.K.</given-names></name><name><surname>Singh</surname><given-names>R.K.</given-names></name></person-group><article-title>Multichannel decoded local binary patterns for content-based image retrieval</article-title><source>IEEE Trans. Image Process.</source><year>2016</year><volume>25</volume><fpage>4018</fpage><lpage>4032</lpage><pub-id pub-id-type="doi">10.1109/TIP.2016.2577887</pub-id><pub-id pub-id-type="pmid">27295674</pub-id></element-citation></ref><ref id="B19-sensors-19-00315"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>X.</given-names></name><name><surname>Xiao</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Qiao</surname><given-names>Y.</given-names></name><name><surname>Guo</surname><given-names>J.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Pairwise rotation invariant co-occurrence local binary pattern</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2014</year><volume>36</volume><fpage>2199</fpage><lpage>2213</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2014.2316826</pub-id><pub-id pub-id-type="pmid">26353061</pub-id></element-citation></ref><ref id="B20-sensors-19-00315"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>Q.</given-names></name><name><surname>Feng</surname><given-names>Q.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Sbert</surname><given-names>M.</given-names></name><name><surname>Lu</surname><given-names>W.</given-names></name><name><surname>Xu</surname><given-names>Q.</given-names></name></person-group><article-title>Pairwise cross pattern: A color-LBP descriptor for content-based image retrieval</article-title><source>Proceedings of the 19th Pacific Rim Conference on Multimedia</source><conf-loc>Hefei, China</conf-loc><conf-date>21&#x02013;22 September 2018</conf-date></element-citation></ref><ref id="B21-sensors-19-00315"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Content-based image retrieval using color difference histogram</article-title><source>Pattern Recognit.</source><year>2013</year><volume>46</volume><fpage>188</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2012.06.001</pub-id></element-citation></ref><ref id="B22-sensors-19-00315"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name></person-group><article-title>Image retrieval based on micro-structure descriptor</article-title><source>Pattern Recognit.</source><year>2011</year><volume>44</volume><fpage>2123</fpage><lpage>2133</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2011.02.003</pub-id></element-citation></ref><ref id="B23-sensors-19-00315"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Hou</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Image retrieval based on multi-texton histogram</article-title><source>Pattern Recognit.</source><year>2010</year><volume>43</volume><fpage>2380</fpage><lpage>2389</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2010.02.012</pub-id></element-citation></ref><ref id="B24-sensors-19-00315"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>M&#x000e4;enp&#x000e4;&#x000e4;</surname><given-names>T.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name></person-group><article-title>Texture analysis with local binary patterns</article-title><source>Handbook of Pattern Recognition and Computer Vision</source><publisher-name>World Scientific</publisher-name><publisher-loc>Singapore</publisher-loc><year>2005</year><fpage>197</fpage><lpage>216</lpage></element-citation></ref><ref id="B25-sensors-19-00315"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianconi</surname><given-names>F.</given-names></name><name><surname>Bello-Cerezo</surname><given-names>R.</given-names></name><name><surname>Napoletano</surname><given-names>P.</given-names></name></person-group><article-title>Improved opponent color local binary patterns: An effective local image descriptor for color texture classification</article-title><source>J. Electron. Imag.</source><year>2017</year><volume>27</volume><pub-id pub-id-type="doi">10.1117/1.JEI.27.1.011002</pub-id></element-citation></ref><ref id="B26-sensors-19-00315"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Feng</surname><given-names>L.</given-names></name><name><surname>Yu</surname><given-names>L.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Fusion framework for color image retrieval based on bag-of-words model and color local Haar binary patterns</article-title><source>J. Electron. Imag.</source><year>2016</year><volume>25</volume><pub-id pub-id-type="doi">10.1117/1.JEI.25.2.023022</pub-id></element-citation></ref><ref id="B27-sensors-19-00315"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cusano</surname><given-names>C.</given-names></name><name><surname>Napoletano</surname><given-names>P.</given-names></name><name><surname>Schettini</surname><given-names>R.</given-names></name></person-group><article-title>Evaluating color texture descriptors under large variations of controlled lighting conditions</article-title><source>J. Opt. Soc. Am. A</source><year>2016</year><volume>33</volume><fpage>17</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.33.000017</pub-id></element-citation></ref><ref id="B28-sensors-19-00315"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>C.</given-names></name><name><surname>Walia</surname><given-names>E.</given-names></name><name><surname>Kaur</surname><given-names>K.P.</given-names></name></person-group><article-title>Enhancing color image retrieval performance with feature fusion and non-linear support vector machine classifier</article-title><source>Optik</source><year>2018</year><volume>158</volume><fpage>127</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.ijleo.2017.11.202</pub-id></element-citation></ref><ref id="B29-sensors-19-00315"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name><name><surname>Ojala</surname><given-names>T.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name></person-group><article-title>Rotation-invariant texture classification using feature distributions</article-title><source>Pattern Recognit.</source><year>2000</year><volume>33</volume><fpage>43</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(99)00032-1</pub-id></element-citation></ref><ref id="B30-sensors-19-00315"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianconi</surname><given-names>F.</given-names></name><name><surname>Gonz&#x000e1;lez</surname><given-names>E.</given-names></name></person-group><article-title>Counting local n-ary patterns</article-title><source>Pattern Recognit. Lett.</source><year>2018</year><volume>177</volume><fpage>24</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2018.11.011</pub-id></element-citation></ref><ref id="B31-sensors-19-00315"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarrafzadeh</surname><given-names>O.</given-names></name><name><surname>Dehnavi</surname><given-names>A.M.</given-names></name></person-group><article-title>Nucleus and cytoplasm segmentation in microscopic images using k-means clustering and region growing</article-title><source>Adv. Biomed. Res.</source><year>2015</year><volume>4</volume><fpage>174</fpage><pub-id pub-id-type="pmid">26605213</pub-id></element-citation></ref><ref id="B32-sensors-19-00315"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>R.C.</given-names></name><name><surname>Woods</surname><given-names>R.E.</given-names></name></person-group><source>Digital Image Processing</source><edition>3rd ed.</edition><publisher-name>Publishing House of Electronics Industry</publisher-name><publisher-loc>Beijing, China</publisher-loc><year>2010</year><fpage>455</fpage><lpage>456</lpage><isbn>9787121102073</isbn></element-citation></ref><ref id="B33-sensors-19-00315"><label>33.</label><element-citation publication-type="web"><article-title>Salzburg Texture Image Database</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.wavelab.at/sources/STex/">http://www.wavelab.at/sources/STex/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2014-08-22">(accessed on 22 August 2014)</date-in-citation></element-citation></ref><ref id="B34-sensors-19-00315"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolesnikov</surname><given-names>A.</given-names></name><name><surname>Trichina</surname><given-names>E.</given-names></name><name><surname>Kauranne</surname><given-names>T.</given-names></name></person-group><article-title>Estimating the number of clusters in a numerical data set via quantization error modeling</article-title><source>Pattern Recognit.</source><year>2015</year><volume>48</volume><fpage>941</fpage><lpage>952</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2014.09.017</pub-id></element-citation></ref><ref id="B35-sensors-19-00315"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>K.</given-names></name><name><surname>Feng</surname><given-names>Q.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Kong</surname><given-names>J.</given-names></name></person-group><article-title>A novel image retrieval method based on hybrid information descriptors</article-title><source>J. Vis. Commun. Image Represent.</source><year>2014</year><volume>25</volume><fpage>1574</fpage><lpage>1587</lpage><pub-id pub-id-type="doi">10.1016/j.jvcir.2014.06.016</pub-id></element-citation></ref><ref id="B36-sensors-19-00315"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Standring</surname><given-names>S.</given-names></name></person-group><source>Gray&#x02019;s Anatomy: The Anatomical Basis of Clinical Practice</source><edition>41st ed.</edition><publisher-name>Elsevier Limited</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2016</year><fpage>686</fpage><lpage>708</lpage><isbn>9780702068515</isbn></element-citation></ref><ref id="B37-sensors-19-00315"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>J.</given-names></name><name><surname>Prasetyo</surname><given-names>H.</given-names></name><name><surname>Lee</surname><given-names>H.</given-names></name><name><surname>Yao</surname><given-names>C.</given-names></name></person-group><article-title>Image retrieval using indexed histogram of void-and-cluster block truncation coding</article-title><source>Signal Process.</source><year>2016</year><volume>123</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2015.11.009</pub-id></element-citation></ref><ref id="B38-sensors-19-00315"><label>38.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nene</surname><given-names>S.A.</given-names></name><name><surname>Nayar</surname><given-names>S.K.</given-names></name><name><surname>Murase</surname><given-names>H.</given-names></name></person-group><source>Columbia Object Image Library (COIL-100)</source><comment>Technical Report CUCS</comment><publisher-name>Department of Computer Science, Columbia University</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>1996</year></element-citation></ref><ref id="B39-sensors-19-00315"><label>39.</label><element-citation publication-type="web"><article-title>Libor Spacek&#x02019;s Facial Image Databases &#x0201c;Face 95 Image Database&#x0201d;</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://cswww.essex.ac.uk/mv/allfaces/faces95.html">https://cswww.essex.ac.uk/mv/allfaces/faces95.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2014-08-08">(accessed on 8 August 2014)</date-in-citation></element-citation></ref><ref id="B40-sensors-19-00315"><label>40.</label><element-citation publication-type="web"><article-title>Libor Spacek&#x02019;s Facial Image Databases &#x0201c;Face 96 Image Database&#x0201d;</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://cswww.essex.ac.uk/mv/allfaces/faces96.html">https://cswww.essex.ac.uk/mv/allfaces/faces96.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2014-08-08">(accessed on 8 August 2014)</date-in-citation></element-citation></ref><ref id="B41-sensors-19-00315"><label>41.</label><element-citation publication-type="web"><article-title>Outex Texture Image Database</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://lagis-vi.univ-lille1.fr/datasets/outex.html">http://lagis-vi.univ-lille1.fr/datasets/outex.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2017-10-05">(accessed on 5 October 2017)</date-in-citation></element-citation></ref><ref id="B42-sensors-19-00315"><label>42.</label><element-citation publication-type="web"><article-title>MIT Vision and Modeling Group</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://vismod.media.mit.edu/pub/">http://vismod.media.mit.edu/pub/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2014-08-12">(accessed on 12 August 2014)</date-in-citation></element-citation></ref><ref id="B43-sensors-19-00315"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Vanhoucke</surname><given-names>V.</given-names></name><name><surname>Loffe</surname><given-names>S.</given-names></name><name><surname>Shlens</surname><given-names>J.</given-names></name><name><surname>Wojna</surname><given-names>Z.</given-names></name></person-group><article-title>Rethinking the inception architecture for computer vision</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date></element-citation></ref><ref id="B44-sensors-19-00315"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Napoletano</surname><given-names>P.</given-names></name></person-group><article-title>Hand-crafted vs. learned descriptors for color texture classification</article-title><source>Proceedings of the International Workshop on Computational Color Imaging</source><conf-loc>Milan, Italy</conf-loc><conf-date>29&#x02013;31 March 2017</conf-date></element-citation></ref><ref id="B45-sensors-19-00315"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>Q.</given-names></name><name><surname>Ni</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>Deep learning based feature selection for remote sensing scene classification</article-title><source>IEEE Trans. Geosci. Remote Sens. Lett.</source><year>2015</year><volume>12</volume><fpage>2321</fpage><lpage>2325</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2015.2475299</pub-id></element-citation></ref><ref id="B46-sensors-19-00315"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Indyk</surname><given-names>P.</given-names></name><name><surname>Motwani</surname><given-names>R.</given-names></name></person-group><article-title>Approximate nearest neighbors: Towards removing the curse of dimensionality</article-title><source>Proceedings of the IEEE Conference on Multimedia Information Analysis and Retrieval</source><conf-loc>Dallas, TX, USA</conf-loc><conf-date>24&#x02013;26 May 1998</conf-date><fpage>604</fpage><lpage>613</lpage></element-citation></ref><ref id="B47-sensors-19-00315"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>W.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name><name><surname>Luo</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Fang</surname><given-names>Y.</given-names></name><name><surname>Zheng</surname><given-names>C.</given-names></name></person-group><article-title>Ordinal preserving matrix factorization for unsupervised feature selection</article-title><source>Signal Process. Image Commun.</source><year>2018</year><volume>67</volume><fpage>118</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/j.image.2018.06.005</pub-id></element-citation></ref><ref id="B48-sensors-19-00315"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Dai</surname><given-names>J.</given-names></name><name><surname>Gui</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Lei</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name></person-group><article-title>Semi-supervised ridge regression with adaptive graph-based label propagation</article-title><source>Appl. Sci.</source><year>2018</year><volume>12</volume><elocation-id>2636</elocation-id><pub-id pub-id-type="doi">10.3390/app8122636</pub-id></element-citation></ref><ref id="B49-sensors-19-00315"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>Y.</given-names></name><name><surname>Qiao</surname><given-names>S.</given-names></name><name><surname>Zhou</surname><given-names>W.</given-names></name><name><surname>Zheng</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Adaptive multiple graph regularized semi-supervised extreme learning machine</article-title><source>Soft Comput.</source><year>2018</year><volume>22</volume><fpage>3545</fpage><lpage>3562</lpage><pub-id pub-id-type="doi">10.1007/s00500-018-3109-x</pub-id></element-citation></ref><ref id="B50-sensors-19-00315"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>T.</given-names></name><name><surname>Liu</surname><given-names>F.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Yi</surname><given-names>Y.</given-names></name></person-group><article-title>Unsupervised feature selection by regularized matrix factorization</article-title><source>Neurocomputing</source><year>2018</year><volume>273</volume><fpage>593</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2017.08.047</pub-id></element-citation></ref><ref id="B51-sensors-19-00315"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chum</surname><given-names>O.</given-names></name><name><surname>Mikulik</surname><given-names>M.</given-names></name><name><surname>Perdoch</surname><given-names>M.</given-names></name><name><surname>Matas</surname><given-names>J.</given-names></name></person-group><article-title>Total recall II: Query expansion revisited</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Colorado Springs, CO, USA</conf-loc><conf-date>20&#x02013;25 June 2011</conf-date><fpage>889</fpage><lpage>896</lpage></element-citation></ref><ref id="B52-sensors-19-00315"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Yang</surname><given-names>M.</given-names></name><name><surname>Cour</surname><given-names>T.</given-names></name><name><surname>Yu</surname><given-names>K.</given-names></name><name><surname>Metaxas</surname><given-names>D.</given-names></name></person-group><article-title>Query specific rank fusion for image retrieval</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>37</volume><fpage>803</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2014.2346201</pub-id><pub-id pub-id-type="pmid">26353295</pub-id></element-citation></ref><ref id="B53-sensors-19-00315"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cernadas</surname><given-names>E.</given-names></name><name><surname>Fern&#x000e1;ndez-Delgado</surname><given-names>M.</given-names></name><name><surname>Gonz&#x000e1;lez-Rufino</surname><given-names>E.</given-names></name></person-group><article-title>Influence of normalization and color space to color texture classification</article-title><source>Pattern Recognit.</source><year>2017</year><volume>61</volume><fpage>120</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2016.07.002</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-19-00315-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The frequency of pixels on Stex and its subsets: (<bold>a</bold>,<bold>b</bold>) Stex, (<bold>c</bold>,<bold>d</bold>) 50% of Stex, and (<bold>e</bold>,<bold>f</bold>) 10% of Stex.</p></caption><graphic xlink:href="sensors-19-00315-g001a"/><graphic xlink:href="sensors-19-00315-g001b"/></fig><fig id="sensors-19-00315-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The details of the six-layer color quantizer.</p></caption><graphic xlink:href="sensors-19-00315-g002"/></fig><fig id="sensors-19-00315-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The quantization errors under different quantization layers, <italic>W</italic><sub>a*</sub> and <italic>W</italic><sub>b*</sub>, on Stex and its subsets: (<bold>a</bold>) Stex, (<bold>b</bold>) 50% of Stex, and (<bold>c</bold>) 10% of Stex.</p></caption><graphic xlink:href="sensors-19-00315-g003"/></fig><fig id="sensors-19-00315-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Schematic diagram of the local parallel cross pattern (LPCP).</p></caption><graphic xlink:href="sensors-19-00315-g004"/></fig><fig id="sensors-19-00315-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Illustration of the rotation invariance of the rotation-invariant local parallel cross pattern (RILPCP).</p></caption><graphic xlink:href="sensors-19-00315-g005"/></fig><fig id="sensors-19-00315-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Some sample images from the eight databases: (<bold>a</bold>) Coil-100; (<bold>b</bold>) Face95; (<bold>c</bold>) Face96; (<bold>d</bold>) Outex-00031; (<bold>e</bold>) Outex-00032; (<bold>f</bold>) Outex-00033; (<bold>g</bold>) Outex-00034; and (<bold>h</bold>) MIT-VisTex.</p></caption><graphic xlink:href="sensors-19-00315-g006a"/><graphic xlink:href="sensors-19-00315-g006b"/></fig><fig id="sensors-19-00315-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>The precision-recall curves of eleven descriptors on the eight databases: (<bold>a</bold>) Coil-100; (<bold>b</bold>) Face95; (<bold>c</bold>) Face96; (<bold>d</bold>) Outex-00031; (<bold>e</bold>) Outex-00032; (<bold>f</bold>) Outex-00033; (<bold>g</bold>) Outex-00034; and (<bold>h</bold>) MIT-VisTex.</p></caption><graphic xlink:href="sensors-19-00315-g007a"/><graphic xlink:href="sensors-19-00315-g007b"/></fig><fig id="sensors-19-00315-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>The average precision rate (APR) comparisons among the proposed descriptors and the CNN-based descriptors on the eight databases.</p></caption><graphic xlink:href="sensors-19-00315-g008"/></fig><fig id="sensors-19-00315-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>The average precision rate (APR) under the weight value <italic>v</italic> by using LPCP, RILPCP, and ULPCP on the eight databases.</p></caption><graphic xlink:href="sensors-19-00315-g009"/></fig><table-wrap id="sensors-19-00315-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-00315-t001_Table 1</object-id><label>Table 1</label><caption><p>Summary of image databases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">No.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Image Size</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Class</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Images in Each Class</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Images Total</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Format</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Website</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">Coil-100 (Rotation)</td><td align="center" valign="middle" rowspan="1" colspan="1">128 &#x000d7; 128</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">72</td><td align="center" valign="middle" rowspan="1" colspan="1">7200</td><td align="center" valign="middle" rowspan="1" colspan="1">JPG</td><td valign="middle" align="left" rowspan="1" colspan="1">
<uri xlink:href="http://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php">http://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php</uri>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">Face95</td><td align="center" valign="middle" rowspan="1" colspan="1">180 &#x000d7; 200</td><td align="center" valign="middle" rowspan="1" colspan="1">72</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">1440</td><td align="center" valign="middle" rowspan="1" colspan="1">JPG</td><td valign="middle" align="left" rowspan="1" colspan="1">
<uri xlink:href="https://cswww.essex.ac.uk/mv/allfaces/faces95.html">https://cswww.essex.ac.uk/mv/allfaces/faces95.html</uri>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">Face96</td><td align="center" valign="middle" rowspan="1" colspan="1">196 &#x000d7; 196</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">19 or 20</td><td align="center" valign="middle" rowspan="1" colspan="1">1814</td><td align="center" valign="middle" rowspan="1" colspan="1">JPG</td><td valign="middle" align="left" rowspan="1" colspan="1">
<uri xlink:href="https://cswww.essex.ac.uk/mv/allfaces/faces96.html">https://cswww.essex.ac.uk/mv/allfaces/faces96.html</uri>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">Outex-00031<break/>(Scale)</td><td align="center" valign="middle" rowspan="1" colspan="1">128 &#x000d7; 128</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">2720</td><td align="center" valign="middle" rowspan="1" colspan="1">BMP</td><td valign="middle" align="left" rowspan="1" colspan="1">
<uri xlink:href="http://lagis-vi.univ-lille1.fr/datasets/outex.html">http://lagis-vi.univ-lille1.fr/datasets/outex.html</uri>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">Outex-00032<break/>(Noise)</td><td align="center" valign="middle" rowspan="1" colspan="1">128 &#x000d7; 128</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">2720</td><td align="center" valign="middle" rowspan="1" colspan="1">BMP</td><td valign="middle" align="left" rowspan="1" colspan="1">
<uri xlink:href="http://lagis-vi.univ-lille1.fr/datasets/outex.html">http://lagis-vi.univ-lille1.fr/datasets/outex.html</uri>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">Outex-00033<break/>(Blur)</td><td align="center" valign="middle" rowspan="1" colspan="1">128 &#x000d7; 128</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">2720</td><td align="center" valign="middle" rowspan="1" colspan="1">BMP</td><td valign="middle" align="left" rowspan="1" colspan="1">
<uri xlink:href="http://lagis-vi.univ-lille1.fr/datasets/outex.html">http://lagis-vi.univ-lille1.fr/datasets/outex.html</uri>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">Outex-00034<break/>(illumination)</td><td align="center" valign="middle" rowspan="1" colspan="1">128 &#x000d7; 128</td><td align="center" valign="middle" rowspan="1" colspan="1">204</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">4080</td><td align="center" valign="middle" rowspan="1" colspan="1">BMP</td><td valign="middle" align="left" rowspan="1" colspan="1">
<uri xlink:href="http://lagis-vi.univ-lille1.fr/datasets/outex.html">http://lagis-vi.univ-lille1.fr/datasets/outex.html</uri>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIT-VisTex</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128 &#x000d7; 128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">640</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PPM</td><td valign="middle" align="left" style="border-bottom:solid thin" rowspan="1" colspan="1">
<uri xlink:href="http://vismod.media.mit.edu/pub/VisTex/">http://vismod.media.mit.edu/pub/VisTex/</uri>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-00315-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-00315-t002_Table 2</object-id><label>Table 2</label><caption><p>The highest average precision rate (APR) values of LPCP, RILPCP, and uniform local parallel cross pattern (ULPCP) with the optimal color quantization layers (<italic>W</italic><sub>a*</sub>, <italic>W</italic><sub>b*</sub>) on the eight databases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Performance</th><th colspan="8" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Dataset</th></tr><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Coil-100 </th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Face95</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Face96</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Outex-00031</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Outex-00032</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Outex-00033</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Outex-00034</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MIT-VisTex</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">LPCP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<italic>W</italic><sub>a*</sub>, <italic>W</italic><sub>b*</sub>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6, 5)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6, 6)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6, 6)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(6, 1)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(5, 1)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(6, 4)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(6, 4)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(5, 4)</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>89.62</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>84.86</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>87.80</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>84.36</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>98.33</bold>
</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RILPCP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<italic>W</italic><sub>a*</sub>, <italic>W</italic><sub>b*</sub>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5, 5)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(6, 6)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(6, 6)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5, 1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5, 1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6, 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(4, 4)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5, 2)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>97.12</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>97.77</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.22</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ULPCP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<italic>W</italic><sub>a*</sub>, <italic>W</italic><sub>b*</sub>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>(4, 6)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6, 6)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6, 6)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5, 1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5, 1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6, 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5, 4)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(3, 2)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.52</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.39</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-00315-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-00315-t003_Table 3</object-id><label>Table 3</label><caption><p>The performance comparisons among the proposed descriptors and the local binary pattern (LBP)-based descriptors on the eight databases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Performance</th><th colspan="8" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Coil-100 </th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Face95</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Face96</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00031</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00032</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00033</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00034</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIT-VisTex</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">LBP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">89.83</td><td align="center" valign="middle" rowspan="1" colspan="1">63.45</td><td align="center" valign="middle" rowspan="1" colspan="1">64.93</td><td align="center" valign="middle" rowspan="1" colspan="1">77.99</td><td align="center" valign="middle" rowspan="1" colspan="1">69.42</td><td align="center" valign="middle" rowspan="1" colspan="1">75.54</td><td align="center" valign="middle" rowspan="1" colspan="1">46.29</td><td align="center" valign="middle" rowspan="1" colspan="1">93.37</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.36</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RILBP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">84.53</td><td align="center" valign="middle" rowspan="1" colspan="1">59.78</td><td align="center" valign="middle" rowspan="1" colspan="1">61.19</td><td align="center" valign="middle" rowspan="1" colspan="1">76.57</td><td align="center" valign="middle" rowspan="1" colspan="1">67.00</td><td align="center" valign="middle" rowspan="1" colspan="1">74.13</td><td align="center" valign="middle" rowspan="1" colspan="1">45.93</td><td align="center" valign="middle" rowspan="1" colspan="1">89.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.09</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ULBP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">85.40</td><td align="center" valign="middle" rowspan="1" colspan="1">58.25</td><td align="center" valign="middle" rowspan="1" colspan="1">59.42</td><td align="center" valign="middle" rowspan="1" colspan="1">76.03</td><td align="center" valign="middle" rowspan="1" colspan="1">65.68</td><td align="center" valign="middle" rowspan="1" colspan="1">73.04</td><td align="center" valign="middle" rowspan="1" colspan="1">45.51</td><td align="center" valign="middle" rowspan="1" colspan="1">90.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.77</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">LPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.47</td><td align="center" valign="middle" rowspan="1" colspan="1">92.33</td><td align="center" valign="middle" rowspan="1" colspan="1">97.03</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>89.62</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.86</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>87.80</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.36</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>98.33</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>22.40</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>21.21</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>21.99</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>42.18</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>61.46</bold>
</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RILPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.44</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>97.12</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>97.77</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.53</td><td align="center" valign="middle" rowspan="1" colspan="1">84.40</td><td align="center" valign="middle" rowspan="1" colspan="1">87.55</td><td align="center" valign="middle" rowspan="1" colspan="1">84.06</td><td align="center" valign="middle" rowspan="1" colspan="1">97.22</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>48.56</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>49.04</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.76</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ULPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>99.52</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">96.65</td><td align="center" valign="middle" rowspan="1" colspan="1">97.45</td><td align="center" valign="middle" rowspan="1" colspan="1">89.44</td><td align="center" valign="middle" rowspan="1" colspan="1">84.07</td><td align="center" valign="middle" rowspan="1" colspan="1">87.59</td><td align="center" valign="middle" rowspan="1" colspan="1">84.20</td><td align="center" valign="middle" rowspan="1" colspan="1">97.39</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>13.82</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.87</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-00315-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-00315-t004_Table 4</object-id><label>Table 4</label><caption><p>The performance comparisons between the proposed descriptors and the existing descriptors in terms of APR and ARR on the eight databases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Performance</th><th colspan="8" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Dataset</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Coil-100</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Face95</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Face96</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00031</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00032</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00033</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outex-00034</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIT-VisTex</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">mdLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">92.34</td><td align="center" valign="middle" rowspan="1" colspan="1">72.97</td><td align="center" valign="middle" rowspan="1" colspan="1">79.09</td><td align="center" valign="middle" rowspan="1" colspan="1">69.51</td><td align="center" valign="middle" rowspan="1" colspan="1">59.60</td><td align="center" valign="middle" rowspan="1" colspan="1">65.63</td><td align="center" valign="middle" rowspan="1" colspan="1">48.66</td><td align="center" valign="middle" rowspan="1" colspan="1">97.05</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.83</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.65</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">maLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">88.33</td><td align="center" valign="middle" rowspan="1" colspan="1">67.94</td><td align="center" valign="middle" rowspan="1" colspan="1">73.99</td><td align="center" valign="middle" rowspan="1" colspan="1">70.42</td><td align="center" valign="middle" rowspan="1" colspan="1">60.24</td><td align="center" valign="middle" rowspan="1" colspan="1">65.42</td><td align="center" valign="middle" rowspan="1" colspan="1">44.53</td><td align="center" valign="middle" rowspan="1" colspan="1">95.80</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.87</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">CDH</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.18</td><td align="center" valign="middle" rowspan="1" colspan="1">94.69</td><td align="center" valign="middle" rowspan="1" colspan="1">94.97</td><td align="center" valign="middle" rowspan="1" colspan="1">85.49</td><td align="center" valign="middle" rowspan="1" colspan="1">79.65</td><td align="center" valign="middle" rowspan="1" colspan="1">82.90</td><td align="center" valign="middle" rowspan="1" colspan="1">74.03</td><td align="center" valign="middle" rowspan="1" colspan="1">94.03</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.77</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">MTH</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.45</td><td align="center" valign="middle" rowspan="1" colspan="1">89.15</td><td align="center" valign="middle" rowspan="1" colspan="1">92.28</td><td align="center" valign="middle" rowspan="1" colspan="1">80.74</td><td align="center" valign="middle" rowspan="1" colspan="1">74.63</td><td align="center" valign="middle" rowspan="1" colspan="1">79.98</td><td align="center" valign="middle" rowspan="1" colspan="1">65.10</td><td align="center" valign="middle" rowspan="1" colspan="1">91.97</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.48</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">MSD</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">98.01</td><td align="center" valign="middle" rowspan="1" colspan="1">92.97</td><td align="center" valign="middle" rowspan="1" colspan="1">89.94</td><td align="center" valign="middle" rowspan="1" colspan="1">77.16</td><td align="center" valign="middle" rowspan="1" colspan="1">72.46</td><td align="center" valign="middle" rowspan="1" colspan="1">76.16</td><td align="center" valign="middle" rowspan="1" colspan="1">66.32</td><td align="center" valign="middle" rowspan="1" colspan="1">92.20</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.63</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">OCLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">85.40</td><td align="center" valign="middle" rowspan="1" colspan="1">64.40</td><td align="center" valign="middle" rowspan="1" colspan="1">64.55</td><td align="center" valign="middle" rowspan="1" colspan="1">80.76</td><td align="center" valign="middle" rowspan="1" colspan="1">69.25</td><td align="center" valign="middle" rowspan="1" colspan="1">77.99</td><td align="center" valign="middle" rowspan="1" colspan="1">56.13</td><td align="center" valign="middle" rowspan="1" colspan="1">92.42</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.76</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">IOCLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">89.12</td><td align="center" valign="middle" rowspan="1" colspan="1">66.47</td><td align="center" valign="middle" rowspan="1" colspan="1">73.24</td><td align="center" valign="middle" rowspan="1" colspan="1">83.84</td><td align="center" valign="middle" rowspan="1" colspan="1">74.54</td><td align="center" valign="middle" rowspan="1" colspan="1">80.75</td><td align="center" valign="middle" rowspan="1" colspan="1">73.58</td><td align="center" valign="middle" rowspan="1" colspan="1">95.59</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.75</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">OC-LBP+CH</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">95.63</td><td align="center" valign="middle" rowspan="1" colspan="1">80.50</td><td align="center" valign="middle" rowspan="1" colspan="1">88.67</td><td align="center" valign="middle" rowspan="1" colspan="1">85.07</td><td align="center" valign="middle" rowspan="1" colspan="1">75.93</td><td align="center" valign="middle" rowspan="1" colspan="1">81.33</td><td align="center" valign="middle" rowspan="1" colspan="1">72.18</td><td align="center" valign="middle" rowspan="1" colspan="1">92.20</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.63</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">LPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.47</td><td align="center" valign="middle" rowspan="1" colspan="1">92.33</td><td align="center" valign="middle" rowspan="1" colspan="1">97.03</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>89.62</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.86</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>87.80</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.36</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>98.33</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>22.40</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>21.21</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>21.99</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>42.18</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>61.46</bold>
</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RILPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.44</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>97.12</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>97.77</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.53</td><td align="center" valign="middle" rowspan="1" colspan="1">84.40</td><td align="center" valign="middle" rowspan="1" colspan="1">87.55</td><td align="center" valign="middle" rowspan="1" colspan="1">84.06</td><td align="center" valign="middle" rowspan="1" colspan="1">97.22</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>48.56</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>49.04</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.76</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ULPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">APR (%)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>99.52</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">96.65</td><td align="center" valign="middle" rowspan="1" colspan="1">97.45</td><td align="center" valign="middle" rowspan="1" colspan="1">89.44</td><td align="center" valign="middle" rowspan="1" colspan="1">84.07</td><td align="center" valign="middle" rowspan="1" colspan="1">87.59</td><td align="center" valign="middle" rowspan="1" colspan="1">84.20</td><td align="center" valign="middle" rowspan="1" colspan="1">97.39</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARR (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>13.82</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.87</td></tr></tbody></table></table-wrap><table-wrap id="sensors-19-00315-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-19-00315-t005_Table 5</object-id><label>Table 5</label><caption><p>Feature vector length (D) and memory consumption (Kb) among the proposed descriptors and other previous descriptors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Vector Length (D)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Memory Consumption (Kb)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">mdLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">2048</td><td align="center" valign="middle" rowspan="1" colspan="1">16.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">maLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">1024</td><td align="center" valign="middle" rowspan="1" colspan="1">8.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CDH</td><td align="center" valign="middle" rowspan="1" colspan="1">108</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MTH</td><td align="center" valign="middle" rowspan="1" colspan="1">82</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MSD</td><td align="center" valign="middle" rowspan="1" colspan="1">78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OCLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">1535</td><td align="center" valign="middle" rowspan="1" colspan="1">11.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IOCLBP</td><td align="center" valign="middle" rowspan="1" colspan="1">3072</td><td align="center" valign="middle" rowspan="1" colspan="1">24.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OC-LBP+CH</td><td align="center" valign="middle" rowspan="1" colspan="1">108</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">760/844/844/424/400/676/676/616</td><td align="center" valign="middle" rowspan="1" colspan="1">5.94/6.59/6.59/3.31/3.13/5.28/5.28/4.81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RILPCP</td><td align="center" valign="middle" rowspan="1" colspan="1">468/624/624/180/180/372/336/252</td><td align="center" valign="middle" rowspan="1" colspan="1">3.66/4.88/4.88/1.41/1.41/2.91/2.63/1.97</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ULPCP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">479/647/647/203/203/395/419/203</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.74/5.05/5.05/1.59/1.59/3.09/3.27/1.59</td></tr></tbody></table></table-wrap></floats-group></article>