
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="discussion"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31447752</article-id><article-id pub-id-type="pmc">6692451</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2019.01833</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Opinion</subject></subj-group></subj-group></article-categories><title-group><article-title>Automatic Micro-Expression Analysis: Open Challenges</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Guoying</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="c001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/132055/overview"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Xiaobai</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/458816/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>School of Information and Technology, Northwest University</institution>, <addr-line>Xi'an</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>Center for Machine Vision and Signal Analysis, University of Oulu</institution>, <addr-line>Oulu</addr-line>, <country>Finland</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Carl Senior, Aston University, United Kingdom</p></fn><fn fn-type="edited-by"><p>Reviewed by: Amy Dawel, Australian National University, Australia</p></fn><corresp id="c001">*Correspondence: Guoying Zhao <email>guoying.zhao@oulu.fi</email></corresp><fn fn-type="other" id="fn001"><p>This article was submitted to Cognitive Science, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>07</day><month>8</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>10</volume><elocation-id>1833</elocation-id><history><date date-type="received"><day>06</day><month>5</month><year>2019</year></date><date date-type="accepted"><day>24</day><month>7</month><year>2019</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2019 Zhao and Li.</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Zhao and Li</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><kwd-group><kwd>micro-expression</kwd><kwd>facial expression</kwd><kwd>automatic analysis</kwd><kwd>computer vision</kwd><kwd>machine learning</kwd></kwd-group><funding-group><award-group><funding-source id="cn001">National Natural Science Foundation of China<named-content content-type="fundref-id">10.13039/501100001809</named-content></funding-source></award-group><award-group><funding-source id="cn002">Academy of Finland<named-content content-type="fundref-id">10.13039/501100002341</named-content></funding-source></award-group></funding-group><counts><fig-count count="0"/><table-count count="0"/><equation-count count="0"/><ref-count count="28"/><page-count count="4"/><word-count count="2826"/></counts></article-meta></front><body><p>Micro-expressions, the fleeting and involuntary facial expression, often occurring in high-stake situations when people try to conceal or mask their true feelings, became well-known since 1960s, from the work of Haggard and Isaacs (<xref rid="B6" ref-type="bibr">1966</xref>) in which micro-expression was firstly termed as micromomentary facial expressions, and later from the work of Ekman and Friesen (<xref rid="B5" ref-type="bibr">1969</xref>).</p><p>Micro-expressions are too short (1/25 to 1/2 s) and subtle for human eyes to perceive. Study (Ekman, <xref rid="B3" ref-type="bibr">2002</xref>) shows that for micro-expression recognition tasks, ordinary people without training only perform slightly better than chance on average. So computer vision and machine learning methods for automatic micro-expression analysis become appealing. Pfister et al. (<xref rid="B19" ref-type="bibr">2011</xref>) started pioneering research on spontaneous micro-expression recognition with the first publically available spontaneous micro-expression dataset: SMIC, and achieved very promising results that compare favorably with the human accuracy. Since then micro-expression study in computer vision field has been attracting attentions from more and more researchers. A number of works have been contributing to the automatic micro-expression analysis from the aspects of new datasets collection (from emotion level annotation to action unit level annotation; Li et al., <xref rid="B13" ref-type="bibr">2013</xref>; Davison et al., <xref rid="B2" ref-type="bibr">2018</xref>), micro-expression recognition (from signal apex frame recognition to whole video recognition; Wang et al., <xref rid="B22" ref-type="bibr">2015</xref>; Liu et al., <xref rid="B15" ref-type="bibr">2016</xref>; Li Y. et al., <xref rid="B14" ref-type="bibr">2018</xref>; Huang et al., <xref rid="B9" ref-type="bibr">2019</xref>) and micro-expression detection (from micro-expression peak detection to micro-expression onset and offset detection; Patel et al., <xref rid="B18" ref-type="bibr">2015</xref>; Xia et al., <xref rid="B23" ref-type="bibr">2016</xref>; Jain et al., <xref rid="B11" ref-type="bibr">2018</xref>). First completed system integrating micro-expression recognition and detection toward reading hidden emotions (Li X. et al., <xref rid="B12" ref-type="bibr">2018</xref>) has been reported by MIT Technology Review (2015) and achieved increasing attention, in which the machine learning method obtained 80.28% for three class (positive/negative/surprise) recognition for 71 micro-expression video clips recorded from eight subjects and 57.49% for five class (happiness, disgust, surprise, repression, and other) recognition for 247 micro-expression video clips recorded from 26 subjects (Li X. et al., <xref rid="B12" ref-type="bibr">2018</xref>), which has outperformed the recognition capability of human subjects (Li X. et al., <xref rid="B12" ref-type="bibr">2018</xref>).</p><p>However, there are still many open challenges which need to be considered in the future research. Several main challenges related with micro-expression study are discussed in details in the following.</p><sec id="s1"><title>Datasets</title><p>Data are a central part in micro-expression research. Even though there have been more datasets collected and released, from the first SMIC (Li et al., <xref rid="B13" ref-type="bibr">2013</xref>), to CASME (Yan et al., <xref rid="B25" ref-type="bibr">2013</xref>), CASME II (Yan et al., <xref rid="B24" ref-type="bibr">2014</xref>), SAMM (Davison et al., <xref rid="B2" ref-type="bibr">2018</xref>), MEVIEW dataset (Husak et al., <xref rid="B10" ref-type="bibr">2017</xref>), and CAS(ME)<sup>2</sup> (Qu et al., <xref rid="B21" ref-type="bibr">2018</xref>), including more subjects, higher resolution, and more videos, the scale of current datasets is just hundreds of micro-expression videos captured from 30 to 40 subjects, and there still lacks high quality, naturally collected and well-annotated large scale micro-expression data captured by different sensors for training efficient deep learning methods, which is a big obstacle for the research. As inducing and labeling micro-expression data from scratch is extremely challenging and time consuming, it is not feasible for any single research group to gather data scale of larger than tens of thousands of samples. One possible option for future micro-expression data construction work could be utilizing the vast source of YouTube videos and mining with some video tagging techniques for candidate clips then follow with human labeling. Another option could be collaborative and parallel data collection and labeling through cloud sourcing.</p><p>Moreover, one potential application of micro-expression analysis is lie detection. When lying, more contradictory behaviors could be found in verbal and non-verbal signals (Navarro and Karlins, <xref rid="B17" ref-type="bibr">2008</xref>), perhaps more micro-expressions could appear. Therefore, new datasets containing not only facial expression and micro-expression, but also audio speech could be beneficial for micro-expression study.</p></sec><sec id="s2"><title>Action Units Detection of Micro-Expressions</title><p>Facial Action Coding System (FACS) is an anatomically based system for measuring facial movements (Ekman and Friesen, <xref rid="B4" ref-type="bibr">1978</xref>), which is used to describe visually distinguishable facial activity on the basis of many unique action units (AUs). In most of the previous work (Wang et al., <xref rid="B22" ref-type="bibr">2015</xref>; Li X. et al., <xref rid="B12" ref-type="bibr">2018</xref>), micro-expressions were recognized from the whole face without action unit study, and only positive and negative micro-expressions, or limited number of micro-expressions were classified. Instead of directly recognizing a certain number of prototypical expressions as in most of the previous research, AUs can provide an intermediate meaningful abstraction of facial expressions, and carry lots of information which can help better detect and understand people's feelings. Even though AU detection has been taken into consideration for macro-expression analysis (Zhao et al., <xref rid="B28" ref-type="bibr">2016</xref>, <xref rid="B27" ref-type="bibr">2018</xref>; Han et al., <xref rid="B7" ref-type="bibr">2018</xref>; Zhang et al., <xref rid="B26" ref-type="bibr">2018</xref>), including pain detection and pain intensity estimation (Prkachin and Solomon, <xref rid="B20" ref-type="bibr">2008</xref>; Lucey et al., <xref rid="B16" ref-type="bibr">2011</xref>), rare work has been done for AUs in micro-expressions. Future study could pay more attention to explore the relationship between AUs and micro-expressions. For example: is there fixed mapping between the onset of a certain AU (or a sequence of AU combinations) and one micro-expression category, just like the criteria for AU and facial expression correspondence listed in FACS manual? The category of concerned micro-expression emotions is not necessarily limited to the prototypical basic emotions, i.e., happiness, sadness, surprise, anger, disgust and fear, but could also consider other emotions which are out of the above mentioned basic emotion scope, yet very useful for real-world applications, like nervousness, disagreement and contempt. Besides, except those most common emotional AUs (that are considered to be closely related with emotional expressions), e.g., AU1, AU4, and AU12, other AUs which were formally considered as &#x0201c;irrelevant to emotions&#x0201d; also worth more exploration, as studies found that some (e.g., eye blinks and eye gaze change) are employed as disguise behaviors to cover true feelings thus frequently occur WITH the onset of micro-expressions.</p></sec><sec id="s3"><title>Realistic Situations</title><p>Most of the existing efforts on micro-expression analysis have been made to classify the basic micro-expressions collected in highly controlled environments, e.g., from frontal view (without view changes), with stable and bright lighting conditions (without illumination variations), whole face visible (without occlusion). Such conditions are very difficult to reproduce in real-world applications and tools trained on such data usually do not generalize well to natural recordings made in unconstrained settings. Effective algorithms for recognizing naturally occurring micro-expressions which are robust to realistic situations with the capability to deal with pose changes, illumination variations and poor quality of videos, recorded in-the-wild environment must be developed.</p></sec><sec id="s4"><title>Macro- and Micro- Expressions</title><p>Previous work about facial expression has concerned with either micro- or macro-expressions. For most early micro-expression works, it has been assumed that there are just micro-expressions in a video clip. For example, in the collection of most micro-expression datasets (Li et al., <xref rid="B13" ref-type="bibr">2013</xref>; Yan et al., <xref rid="B25" ref-type="bibr">2013</xref>, <xref rid="B24" ref-type="bibr">2014</xref>; Davison et al., <xref rid="B2" ref-type="bibr">2018</xref>; Qu et al., <xref rid="B21" ref-type="bibr">2018</xref>), subjects were asked to try their best to keep a neutral face when watching emotional movie clips. In this way, the conflict of felt emotion elicited by the movie clip and the strong intention to suppress any facial expression could induce micro-expressions. The consequence in the collected videos is that, if there is micro-expression in the recorded video, it is unlikely to have other natural facial expressions. But in most cases in real life, this is not true. Micro-expressions can appear when there is a macro-expression as well, for example, when people smile, they might furrow forehead very quickly and shortly, which show their true feeling (Ekman and Friesen, <xref rid="B5" ref-type="bibr">1969</xref>). Future studies could also concern the relationship of macro and micro-expressions, and explore methods that can detect and distinguish these two when they co-occur or even overlap with each other in one scenario, which would be very helpful to understand people's feelings and intentions more accurately.</p></sec><sec id="s5"><title>Context Clues and Multi-Modality Learning</title><p>In social interactions, people interpret other's emotions and situations based on many things (Huang et al., <xref rid="B8" ref-type="bibr">2018</xref>): people in the interaction, their speech, facial expression, cloths, body pose, gender, age, surrounding environments, social parameters, and so on. All these can be considered as contextual information. Some people are better emotion readers, as they can sense others' emotion more accurately than the rest. These people usually pick up subtle clues from multiple aspects, not only the facial expressions (Navarro and Karlins, <xref rid="B17" ref-type="bibr">2008</xref>). One original motivation for the study of micro-expression is to explore people's suppressed and concealed emotions, but we shouldn't forget that micro-expression is only one of the many clues for such purpose. Future studies should try broaden the scope and consider combining micro-expression with other contextual behaviors, e.g., eye blink, eye gaze change, hand gesture change, or even whole body posture, in order to achieve better understanding of people's hidden emotions on a fuller scope.</p><p>Recent psychological research demonstrates that emotions are a multimodal procedure which can be expressed in various ways. &#x0201c;Visual scenes, voices, bodies, other faces, cultural orientation, and even words shape how emotions is perceived in a face&#x0201d; (Barrett et al., <xref rid="B1" ref-type="bibr">2011</xref>). As well emotional data can be recorded with different sensors, e.g., color camera, near infrared camera, depth camera, or physiological sensors, for recording emotional behaviors or bodily changes. This also applies to the study of micro-expression and suppressed or hidden emotion. One single modality could be unreliable, as one certain behavior pattern could be just related to physiological uncomfort or personal habit, but has nothing to do with emotional states. So only when multiple cues are considered together we could achieve more reliable emotion recognition. There is very little investigation in this respect so far, and future micro-expression studies could consider combining multi-modality data for micro-expression and hidden emotion recognition.</p></sec><sec id="s6"><title>Analysis for Multiple Persons in Interactions</title><p>The current micro-expressions research focuses on single person watching affective movies or advertisements, which is reasonable in the early stage for making challenging tasks easier and more feasible. Later it is surely that the research will be shifting toward more realistic and challenging interaction environments where multiple persons are involved. Natural interactions will induce more natural and spontaneous emotional responses in terms of facial expressions and micro-expressions, but the scenario will also become very complicated. It would be very interesting to explore not only individual level of emotional changes, but also the interpersonal co-occurrence (e.g., mimicry or contagion), and the affective dynamics of the whole group.</p></sec><sec sec-type="discussion" id="s7"><title>Discussion</title><p>We have discussed the progress and the open challenges in automatic micro-expression analysis. Solving these issues needs interdisciplinary expertise. The collaboration of machine learning, psychology, cognition and social behavior is necessary for advancing the in-depth investigation of micro-expressions and related applications in real world.</p></sec><sec id="s8"><title>Author Contributions</title><p>All authors listed have made a substantial, direct and intellectual contribution to the work, and approved it for publication.</p><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><fn-group><fn fn-type="financial-disclosure"><p><bold>Funding</bold>. This work was partially supported by the National Natural Science Foundation of China (Grants No. 61772419), Infotech Oulu and Academy of Finland (ICT 2023 project with grant No. 313600).</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>L. F.</given-names></name><name><surname>Mesquita</surname><given-names>B.</given-names></name><name><surname>Gendron</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Context in emotion perception</article-title>. <source>Curr. Dir. Psychol. Sci.</source>
<volume>20</volume>, <fpage>286</fpage>&#x02013;<lpage>290</lpage>. <pub-id pub-id-type="doi">10.1177/0963721411422522</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davison</surname><given-names>A. K.</given-names></name><name><surname>Lansley</surname><given-names>C.</given-names></name><name><surname>Costen</surname><given-names>N.</given-names></name><name><surname>Tan</surname><given-names>K.</given-names></name><name><surname>Yap</surname><given-names>M. H.</given-names></name></person-group> (<year>2018</year>). <article-title>SAMM: a spontaneous micro-facial movement dataset</article-title>. <source>IEEE Trans. Affect. Comput.</source>
<volume>9</volume>, <fpage>116</fpage>&#x02013;<lpage>129</lpage>. <pub-id pub-id-type="doi">10.1109/TAFFC.2016.2573832</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name></person-group> (<year>2002</year>). <source>Microexpression Training Tool (METT)</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>University California</publisher-name>.</mixed-citation></ref><ref id="B4"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Friesen</surname><given-names>W.</given-names></name></person-group> (<year>1978</year>). <source>Facial Action Coding System: A Technique for the Measurement of Facial Movement Consulting</source>. <publisher-loc>Palo Alto, CA</publisher-loc>: <publisher-name>Consulting Psychologists Press</publisher-name>.</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Friesen</surname><given-names>W. V.</given-names></name></person-group> (<year>1969</year>). <article-title>Nonverbal leakage and clues to deception</article-title>. <source>Psychiatry</source>
<volume>32</volume>, <fpage>88</fpage>&#x02013;<lpage>106</lpage>. <pub-id pub-id-type="pmid">5779090</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haggard</surname><given-names>E.</given-names></name><name><surname>Isaacs</surname><given-names>K.</given-names></name></person-group> (<year>1966</year>). <article-title>Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy</article-title>, in <source>Methods of Research in Psychotherapy</source>, eds <person-group person-group-type="editor"><name><surname>Gottschalk</surname><given-names>L. A.</given-names></name><name><surname>Auerbach</surname><given-names>A. H.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Appleton-Century-Crofts</publisher-name>, <fpage>154</fpage>&#x02013;<lpage>165</lpage>.</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Han</surname><given-names>S.</given-names></name><name><surname>Meng</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Reilly</surname><given-names>J.</given-names></name><name><surname>Cai</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Optimizing filter size in convolutional neural networks for facial action unit recognition</article-title>, in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>), <fpage>5070</fpage>&#x02013;<lpage>5078</lpage>.</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Dhall</surname><given-names>A.</given-names></name><name><surname>Goecke</surname><given-names>R.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>Multi-modal framework for analyzing the affect of a group of people</article-title>. <source>IEEE Trans. Multimedia</source>
<volume>20</volume>, <fpage>2706</fpage>&#x02013;<lpage>2721</lpage>. <pub-id pub-id-type="doi">10.1109/TMM.2018.2818015</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>S.-J.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Feng</surname><given-names>X.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>Discriminative spatiotemporal local binary pattern with revisited integral projection for spontaneous facial micro-expression recognition</article-title>. <source>IEEE Trans. Affect. Comput</source>. <volume>10</volume>, <fpage>32</fpage>&#x02013;<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1109/TAFFC.2017.2713359</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Husak</surname><given-names>P.</given-names></name><name><surname>Cech</surname><given-names>J.</given-names></name><name><surname>Matas</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>Spotting facial micro-expressions &#x0201c;In the Wild&#x0201d;</article-title>, in <source>Proceedings of the 22nd Computer Vision Winter Workshop</source>, eds <person-group person-group-type="author"><name><surname>Artner</surname><given-names>N.M.</given-names></name><name><surname>Janusch</surname><given-names>I.</given-names></name><name><surname>Kropatsch</surname><given-names>W. G.</given-names></name></person-group> (<publisher-loc>Retz</publisher-loc>).</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>D. K.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>K.</given-names></name></person-group> (<year>2018</year>). <article-title>Random walk-based feature learning for micro-expression recognition</article-title>. <source>Pattern Recogn. Lett</source>. <volume>115</volume>, <fpage>92</fpage>&#x02013;<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2018.02.004</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Hong</surname><given-names>X.</given-names></name><name><surname>Moilanen</surname><given-names>A.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Pfister</surname><given-names>T.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Towards reading hidden emotions: a comparative study of spontaneous micro-expression spotting and recognition methods</article-title>. <source>IEEE Trans. Affect. Comput.</source>
<volume>9</volume>, <fpage>563</fpage>&#x02013;<lpage>577</lpage>. <pub-id pub-id-type="doi">10.1109/TAFFC.2017.2667642</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Pfister</surname><given-names>T.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>A spontaneous micro facial expression database: inducement, collection and baseline</article-title>, in <source>Proceedings of the IEEE International Conference on Face and Gesture Recognition</source> (<publisher-loc>Shanghai</publisher-loc>: <publisher-name>FG 2013</publisher-name>).</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>Can micro-expression be recognized based on single apex frame?</article-title> in <source>International Conference on Image Processing</source> (<publisher-loc>Athens</publisher-loc>: <publisher-name>ICIP</publisher-name>).</mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.-J.</given-names></name><name><surname>Zhang</surname><given-names>J.-K.</given-names></name><name><surname>Yan</surname><given-names>W.-J.</given-names></name><name><surname>Wang</surname><given-names>S.-J.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Fu</surname><given-names>X.</given-names></name></person-group> (<year>2016</year>). <article-title>A main directional mean optical flow feature for spontaneous micro-expression recognition</article-title>. <source>IEEE Trans. Affect. Comput.</source>
<volume>7</volume>, <fpage>299</fpage>&#x02013;<lpage>310</lpage>. <pub-id pub-id-type="doi">10.1109/TAFFC.2015.2485205</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lucey</surname><given-names>P.</given-names></name><name><surname>Cohn</surname><given-names>J. F.</given-names></name><name><surname>Prkachin</surname><given-names>K. M.</given-names></name><name><surname>Solomon</surname><given-names>P. E.</given-names></name><name><surname>Matthews</surname><given-names>I.</given-names></name></person-group> (<year>2011</year>). <article-title>Painful data: the unbc-mcmaster shoulder pain expression archive database</article-title>, in <source>Proceedings of the IEEE International Conference on Face and Gesture Recognition</source> (<publisher-loc>Santa Barbara, CA</publisher-loc>: <publisher-name>FG 2011</publisher-name>).</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Navarro</surname><given-names>J.</given-names></name><name><surname>Karlins</surname><given-names>M.</given-names></name></person-group> (<year>2008</year>). <source>What Every BODY Is Saying: An ex-FBI Agent's Guide to Speed Reading People</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Collins</publisher-name>.</mixed-citation></ref><ref id="B18"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>D.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Spatiotemporal integration of optical flow vectors for micro-expression detection</article-title>, in <source>Proceedings of the International Conference on Advanced Concepts for Intelligent Vision Systems</source> (<publisher-loc>Catania</publisher-loc>: <publisher-name>ACIVS</publisher-name>).</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pfister</surname><given-names>T.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Pietikainen</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Recognising spontaneous facial micro-expressions</article-title>, in <source>Proceedings of the IEEE International Conference on Computer Vision</source>
<year>2011</year> (<publisher-loc>Barcelona</publisher-loc>).</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prkachin</surname><given-names>K. M.</given-names></name><name><surname>Solomon</surname><given-names>P. E.</given-names></name></person-group> (<year>2008</year>). <article-title>The structure, reliability and validity of pain expression: evidence from patients with shoulder pain</article-title>. <source>Pain</source>
<volume>139</volume>, <fpage>267</fpage>&#x02013;<lpage>274</lpage>. <pub-id pub-id-type="doi">10.1016/j.pain.2008.04.010</pub-id><pub-id pub-id-type="pmid">18502049</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qu</surname><given-names>F.</given-names></name><name><surname>Wang</surname><given-names>S.-J.</given-names></name><name><surname>Yan</surname><given-names>W.-J.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Wu</surname><given-names>S.</given-names></name><name><surname>Fu</surname><given-names>X.</given-names></name></person-group> (<year>2018</year>). <article-title>CAS(ME)<sup>2</sup>: A database for spontaneous macro-expression and micro-expression spotting and recognition</article-title>. <source>IEEE Trans. Affect. Comput</source>. <volume>9</volume>, <fpage>424</fpage>&#x02013;<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1109/TAFFC.2017.2654440</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S.-J.</given-names></name><name><surname>Yan</surname><given-names>W.-J.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Zhou</surname><given-names>C.-G.</given-names></name><name><surname>Fu</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Micro-expression recognition using color spaces</article-title>. <source>IEEE Trans. Image Process.</source>
<volume>24</volume>, <fpage>6034</fpage>&#x02013;<lpage>6047</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2015.2496314</pub-id><pub-id pub-id-type="pmid">26540689</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>Z.</given-names></name><name><surname>Feng</surname><given-names>X.</given-names></name><name><surname>Peng</surname><given-names>J.</given-names></name><name><surname>Peng</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name></person-group> (<year>2016</year>). <article-title>Spontaneous micro-expression spotting via geometric deformation modeling</article-title>. <source>Comput. Vision Image Understand.</source>
<volume>147</volume>, <fpage>87</fpage>&#x02013;<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1016/j.cviu.2015.12.006</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>W.-J.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>S.-J.</given-names></name><name><surname>Zhao</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>Y.-J.</given-names></name><name><surname>Chen</surname><given-names>Y.-H.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>CASME II: An improved spontaneous micro-expression database and the baseline evaluation</article-title>. <source>PLoS ONE</source>
<volume>9</volume>:<fpage>e86041</fpage>. <pub-id pub-id-type="pmid">24475068</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>W.-J.</given-names></name><name><surname>Wu</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>Y.-J.</given-names></name><name><surname>Wang</surname><given-names>S.-J.</given-names></name><name><surname>Fu</surname><given-names>X.</given-names></name></person-group> (<year>2013</year>). <article-title>CASME database: a dataset of spontaneous micro-expressions collected from neutralized faces</article-title>, in <source>Proceedings of the IEEE International Conference Automatic Face and Gesture Recognition 2013</source> (<publisher-loc>Shanghai</publisher-loc>).</mixed-citation></ref><ref id="B26"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Dong</surname><given-names>W.</given-names></name><name><surname>Hu</surname><given-names>B.-G.</given-names></name><name><surname>Ji</surname><given-names>Q.</given-names></name></person-group> (<year>2018</year>). <article-title>Classifier learning with prior probabilities for facial action unit recognitionm</article-title>, in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>), <fpage>5108</fpage>&#x02013;<lpage>5116</lpage>.</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>K.</given-names></name><name><surname>Chu</surname><given-names>W.-S.</given-names></name><name><surname>Martinez</surname><given-names>A. M.</given-names></name></person-group> (<year>2018</year>). <article-title>Learning facial action units from web images with scalable weakly supervised clustering</article-title>. <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>), <fpage>2090</fpage>&#x02013;<lpage>2099</lpage>. </mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>K.</given-names></name><name><surname>Chu</surname><given-names>W.-S.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep region and multi-label learning for facial action unit detection</article-title>, in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Las Vegas, NV</publisher-loc>), <fpage>3391</fpage>&#x02013;<lpage>3399</lpage>.</mixed-citation></ref></ref-list></back></article>