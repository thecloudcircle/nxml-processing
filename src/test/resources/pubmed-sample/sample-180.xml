
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Biomed Res Int</journal-id><journal-id journal-id-type="iso-abbrev">Biomed Res Int</journal-id><journal-id journal-id-type="publisher-id">BMRI</journal-id><journal-title-group><journal-title>BioMed Research International</journal-title></journal-title-group><issn pub-type="ppub">2314-6133</issn><issn pub-type="epub">2314-6141</issn><publisher><publisher-name>Hindawi</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28848763</article-id><article-id pub-id-type="pmc">5564130</article-id><article-id pub-id-type="doi">10.1155/2017/3020627</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Robustification of Na&#x000ef;ve Bayes Classifier and Its Application for Microarray Gene Expression Data Analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5027-1613</contrib-id><name><surname>Ahmed</surname><given-names>Md. Shakil</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7434-922X</contrib-id><name><surname>Shahjaman</surname><given-names>Md.</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Rana</surname><given-names>Md. Masud</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Mollah</surname><given-names>Md. Nurul Haque</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="I1">
<sup>1</sup>Lab of Bioinformatics, Department of Statistics, University of Rajshahi, Rajshahi 6205, Bangladesh</aff><aff id="I2">
<sup>2</sup>Department of Statistics, Begum Rokeya University, Rangpur, Rangpur 5400, Bangladesh</aff><author-notes><corresp id="cor1">*Md. Shakil Ahmed: <email>shakil.statru@gmail.com</email></corresp><fn fn-type="other"><p>Academic Editor: Federico Ambrogi</p></fn></author-notes><pub-date pub-type="ppub"><year>2017</year></pub-date><pub-date pub-type="epub"><day>7</day><month>8</month><year>2017</year></pub-date><volume>2017</volume><elocation-id>3020627</elocation-id><history><date date-type="received"><day>18</day><month>3</month><year>2017</year></date><date date-type="rev-recd"><day>10</day><month>6</month><year>2017</year></date><date date-type="accepted"><day>14</day><month>6</month><year>2017</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2017 Md. Shakil Ahmed et al.</copyright-statement><copyright-year>2017</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>The na&#x000ef;ve Bayes classifier (NBC) is one of the most popular classifiers for class prediction or pattern recognition from microarray gene expression data (MGED). However, it is very much sensitive to outliers with the classical estimates of the location and scale parameters. It is one of the most important drawbacks for gene expression data analysis by the classical NBC. The gene expression dataset is often contaminated by outliers due to several steps involved in the data generating process from hybridization of DNA samples to image analysis. Therefore, in this paper, an attempt is made to robustify the Gaussian NBC by the minimum <italic>&#x003b2;</italic>-divergence method. The role of minimum <italic>&#x003b2;</italic>-divergence method in this article is to produce the robust estimators for the location and scale parameters based on the training dataset and outlier detection and modification in test dataset. The performance of the proposed method depends on the tuning parameter <italic>&#x003b2;</italic>. It reduces to the traditional na&#x000ef;ve Bayes classifier when <italic>&#x003b2;</italic> &#x02192; 0. We investigated the performance of the proposed beta na&#x000ef;ve Bayes classifier (<italic>&#x003b2;</italic>-NBC) in a comparison with some popular existing classifiers (NBC, KNN, SVM, and AdaBoost) using both simulated and real gene expression datasets. We observed that the proposed method improved the performance over the others in presence of outliers. Otherwise, it keeps almost equal performance.</p></abstract><funding-group><award-group><funding-source>HEQEP</funding-source><award-id>CP-3603, W2, R3</award-id></award-group></funding-group></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>Classification is a supervised learning approach for separation of multivariate data into various sources of populations. It has been playing significant roles in bioinformatics by class prediction or pattern recognition from molecular OMICS datasets. Microarray gene expression data analysis is one of the most important OMICS research wings for bioinformatics [<xref rid="B30" ref-type="bibr">1</xref>]. There are several classification and clustering approaches that have been addressed previously for analyzing MGED [<xref rid="B20" ref-type="bibr">2</xref>&#x02013;<xref rid="B31" ref-type="bibr">11</xref>]. The Gaussian linear Bayes classifier (LBC) is one of the most popular classifiers for class prediction or pattern recognition. However, it is not so popular for microarray gene expression data analysis, since it suffers from the inverse problem of its covariance matrix in presence of large number of genes (<italic>p</italic>) with small number of patients/samples (<italic>n</italic>) in the training dataset. The Gaussian na&#x000ef;ve Bayes classifier (NBC) overcomes this difficulty of Gaussian LBC by taking the normality and independence assumptions on the variables. If these two assumptions are violated, then the nonparametric version of NBC is suggested in [<xref rid="B2" ref-type="bibr">12</xref>]. In this case the nonparametric classification methods work well but they produce poor performance for small sample sizes or in presence of outliers. In MGED the small samples are conducted because of cost and limited specimen availability [<xref rid="B35" ref-type="bibr">13</xref>]. There are some other versions of NBC also [<xref rid="B3" ref-type="bibr">14</xref>, <xref rid="B4" ref-type="bibr">15</xref>]. However, none of them are so robust against outliers. It is one of the most important drawbacks for gene expression data analysis by the existing NBC. The gene expression dataset is often contaminated by outliers due to several steps involved in the data generating process from hybridization of DNA samples to image analysis. Therefore, in this paper, an attempt is made to robustify the Gaussian NBC by the minimum <italic>&#x003b2;</italic>-divergence method within two steps. At step-1, the minimum <italic>&#x003b2;</italic>-divergence method [<xref rid="B14" ref-type="bibr">16</xref>&#x02013;<xref rid="B16" ref-type="bibr">18</xref>] attempts to estimate the parameters for the Gaussian NBC based on the training dataset. At step-2, an attempt is made to detect the outlying data vector from the test dataset using the <italic>&#x003b2;</italic>-weight function. Then an attempt is made to propose criteria to detect the outlying components in the test data vector and the modification of outlying components by the reasonable values. It will be observed that the performance of the proposed method depends on the tuning parameter <italic>&#x003b2;</italic> and it reduces to the traditional Gaussian NBC when <italic>&#x003b2;</italic> &#x02192; 0. Therefore, we call the proposed classifier as <italic>&#x003b2;</italic>-NBC.</p><p>An attempt is made to investigate the robustness performance of the proposed <italic>&#x003b2;</italic>-NBC in a comparison with several versions of robust linear classifiers based on M-estimator [<xref rid="B10" ref-type="bibr">19</xref>, <xref rid="B29" ref-type="bibr">20</xref>], MCD (Minimum Covariance Determinant), and MVE (Minimum Volume Ellipsoid) estimators [<xref rid="B12" ref-type="bibr">21</xref>, <xref rid="B13" ref-type="bibr">22</xref>], Orthogonalized Gnanadesikan-Kettenring (OGK) estimator including MCD-A, MCD-B, and MCD-C [<xref rid="B11" ref-type="bibr">23</xref>], and Feasible Solution Algorithm (FSA) classifiers [<xref rid="B5" ref-type="bibr">24</xref>&#x02013;<xref rid="B32" ref-type="bibr">26</xref>]. We observed that the proposed <italic>&#x003b2;</italic>-NBC outperforms existing robust linear classifiers as mentioned earlier. Then we investigate the performance of the proposed method in a comparison with some popular classifiers including Support Vector Machine (SVM),<italic> k</italic>-nearest neighbors (<italic>K</italic>NN), and AdaBoost; those are widely used in gene expression data analysis [<xref rid="B7" ref-type="bibr">27</xref>&#x02013;<xref rid="B9" ref-type="bibr">29</xref>]. We observed that the proposed method improves the performance over the others in presence of outliers. Otherwise, it keeps almost equal performance.</p></sec><sec id="sec2"><title>2. Methodology</title><sec id="sec2.1"><title>2.1. Na&#x000ef;ve Bayes Classifier</title><p>The na&#x000ef;ve Bayes classifiers (NBCs) [<xref rid="B1" ref-type="bibr">30</xref>] are a family of probabilistic classifiers depending on the Bayes' theorem with independence and normality assumptions among the variables. The common rule of NBCs is to pick the hypothesis that is most probable; this is known as the maximum a posteriori (MAP) decision rule. Assume that we have a training sample of vectors {<bold>x</bold><sub><italic>jk</italic></sub> = (<italic>x</italic><sub>1<italic>jk</italic></sub>, <italic>x</italic><sub>2<italic>jk</italic></sub>,&#x02026;,<italic>x</italic><sub><italic>pjk</italic></sub>)<sup><italic>T</italic></sup>; <italic>j</italic> = 1, 2,&#x02026;, <italic>N</italic><sub><italic>k</italic></sub>} of size <italic>N</italic><sub><italic>k</italic></sub> for <italic>k</italic> = 1,2,&#x02026;, <italic>K</italic>, where <bold>x</bold><sub><italic>ijk</italic></sub> denotes the<italic> j</italic>th observation of the<italic> i</italic>th variable in the<italic> k</italic>th population/class (<italic>C</italic><sub><italic>k</italic></sub>). Then the NBCs assign a class label <inline-formula><mml:math id="M1"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for some<italic> k</italic> as follows:<disp-formula id="EEq1"><label>(1)</label><mml:math id="M2"><mml:mtable style="T6"><mml:mtr><mml:mtd><mml:maligngroup/><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:malignmark/><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.57999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="6.57999pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo mathvariant="bold">,</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="4.19899pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmax</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.57999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="6.57999pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x0220f;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="4.19899pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>For the Gaussian NBC, the density function <italic>f</italic><sub><italic>k</italic></sub>(<bold>x</bold><sub><italic>jk</italic></sub>&#x02223;<bold><italic>&#x003b8;</italic></bold><sub><italic>k</italic></sub>, <italic>C</italic><sub><italic>k</italic></sub>) of<italic> k</italic>th population/class (<italic>C</italic><sub><italic>k</italic></sub>) can be written as<disp-formula id="EEq2"><label>(2)</label><mml:math id="M3"><mml:mtable style="T&#x0221e;3"><mml:mtr><mml:mtd><mml:malignmark/><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.35999pt" depth="0.12pt"/><mml:mn mathvariant="normal">2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mspace height="6.35999pt" depth="0.12pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mspace height="6.63998pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="6.63998pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="10pt"/><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="12.87999pt" depth="7.06999pt"/><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="5.32999pt" depth="4.19899pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="5.32999pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mspace height="12.87999pt" depth="7.06999pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <bold><italic>&#x003b8;</italic></bold><sub><italic>k</italic></sub> = {<bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub>, &#x0039b;<sub><italic>k</italic></sub>}, and here <bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub> = (<italic>&#x003bc;</italic><sub>1<italic>k</italic></sub>, <italic>&#x003bc;</italic><sub>2<italic>k</italic></sub>,&#x02026;,<italic>&#x003bc;</italic><sub><italic>pk</italic></sub>)<sup><italic>T</italic></sup>, is the mean vector and the diagonal covariance matrix is<disp-formula id="eq3"><label>(3)</label><mml:math id="M4"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mn mathvariant="normal">0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="normal">0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">diag</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.24498pt" depth="4.65698pt"/><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.24498pt" depth="4.65698pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec id="sec2.2"><title>2.2. Maximum Likelihood Estimators (MLEs) for the Gaussian NBC</title><p>We assume that the prior probabilities <italic>p</italic>(<italic>C</italic><sub><italic>k</italic></sub>) are known and the maximum likelihood estimators (MLEs) <inline-formula><mml:math id="M5"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of <bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub> and &#x0039b;<sub><italic>k</italic></sub> are obtained based on the training dataset as follows:<disp-formula id="EEq3"><label>(4)</label><mml:math id="M7"><mml:mtable style="T17"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="EEq4"><label>(5)</label><mml:math id="EEq4EBAAQBBFCA"><mml:mtable><mml:mtr><mml:mtd><mml:maligngroup/><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:malignmark/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="EEq5"><label>(6)</label><mml:math id="EEq5EAAAQBBFCA"><mml:mtable><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mo>=</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.24498pt" depth="4.65698pt"/><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.24498pt" depth="4.65698pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>,&#x02009;&#x02009;<inline-formula><mml:math id="M9"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <italic>N</italic> = &#x02211;<sub><italic>k</italic>=1</sub><sup><italic>K</italic></sup><italic>N</italic><sub><italic>k</italic></sub>;&#x02009;&#x02009;<italic>i</italic> = 1,2,&#x02026;, <italic>p</italic>.</p><p>It is obvious from (<xref ref-type="disp-formula" rid="EEq1">1</xref>)-(<xref ref-type="disp-formula" rid="EEq2">2</xref>) that the Gaussian NBC depends on the mean vectors (<bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub>) and diagonal covariance matrix (&#x0039b;<sub><italic>k</italic></sub>); those are estimated by the maximum likelihood estimators (MLEs) as given in (<xref ref-type="disp-formula" rid="EEq3">4</xref>)&#x02013;(<xref ref-type="disp-formula" rid="EEq5">6</xref>) based on the training dataset. Therefore, MLE based Gaussian NBC produces misleading results in presence of outliers in the datasets. To get rid of this problem, an attempt is made to robustify the Gaussian NBC by minimum <italic>&#x003b2;</italic>-divergence method [<xref rid="B14" ref-type="bibr">16</xref>&#x02013;<xref rid="B16" ref-type="bibr">18</xref>].</p></sec><sec id="sec2.3"><title>2.3. Robustification of Gaussian NBC by the Minimum <italic>&#x003b2;</italic>-Divergence Method (Proposed)</title><sec id="sec2.3.1"><title>2.3.1. Minimum <italic>&#x003b2;</italic>-Divergence Estimators for the Gaussian NBC</title><p>Let <italic>g</italic>(<bold>x</bold><sub><italic>k</italic></sub>) be the true density and <italic>f</italic>(<bold>x</bold><sub><italic>k</italic></sub>&#x02223;<bold><italic>&#x003b8;</italic></bold><sub><italic>k</italic></sub>) be the model density for<italic> k</italic>th populations; then the <italic>&#x003b2;</italic>-divergence of two p.d.f can be defined by<disp-formula id="EEq6"><label>(7)</label><mml:math id="M10"><mml:mtable style="T&#x0221e;8"><mml:mtr><mml:mtd><mml:malignmark/><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="8.07999pt" depth="2.98001pt"/><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="8.07999pt" depth="2.98001pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="10pt"/><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mo stretchy="false">&#x0222b;</mml:mo><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="12.87999pt" depth="9.65999pt"/><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mfrac><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.98001pt"/><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.75598pt" depth="2.98001pt"/></mml:mrow></mml:mfenced><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace width="10pt"/><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:mfrac><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.98001pt"/><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.75598pt" depth="2.98001pt"/></mml:mrow></mml:mfenced><mml:mspace height="12.87999pt" depth="9.65999pt"/></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>for <italic>&#x003b2;</italic> &#x0003e; 0 and <italic>D</italic><sub><italic>&#x003b2;</italic></sub>(<italic>g</italic>(<bold>x</bold><sub><italic>k</italic></sub>), <italic>f</italic>(<bold>x</bold><sub><italic>k</italic></sub>&#x02223;<bold><italic>&#x003b8;</italic></bold><sub><italic>k</italic></sub>)) &#x02265; 0. Equality holds if and only if <italic>g</italic>(<bold>x</bold><sub><italic>k</italic></sub>) = <italic>f</italic>(<bold>x</bold><sub><italic>k</italic></sub>&#x02223;<bold><italic>&#x003b8;</italic></bold><sub><italic>k</italic></sub>) for all <bold>x</bold><sub><italic>k</italic></sub>. When <italic>&#x003b2;</italic> tends to zero, <italic>&#x003b2;</italic>-divergence reduces to Kullback Leibler (K-L) divergence; that is,<disp-formula id="EEq7"><label>(8)</label><mml:math id="M11"><mml:mtable style="T7"><mml:mtr><mml:mtd><mml:malignmark/><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">lim</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x02193;</mml:mo><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="8.07999pt" depth="2.98001pt"/><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="8.07999pt" depth="2.98001pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="10pt"/><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">&#x0222b;</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="10pt"/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="8.07999pt" depth="2.98001pt"/><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.09pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="8.07999pt" depth="2.98001pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The minimum <italic>&#x003b2;</italic>-divergence estimator is defined by<disp-formula id="EEq8"><label>(9)</label><mml:math id="M12"><mml:mtable style="T6"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:malignmark/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">agr</mml:mi><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="9.52998pt" depth="4.43001pt"/><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="9.12598pt" depth="2.984pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msubsup><mml:mspace height="9.12598pt" depth="2.984pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.52998pt" depth="4.43001pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="16.99994pt" depth="12.015pt"/><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.12598pt" depth="2.984pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msubsup><mml:mspace height="9.12598pt" depth="2.984pt"/></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mspace height="16.99994pt" depth="12.015pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>For the Gaussian density <bold><italic>&#x003b8;</italic></bold><sub><italic>k</italic></sub> = {<bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub>, &#x0039b;<sub><italic>k</italic></sub>} and the minimum <italic>&#x003b2;</italic>-divergence estimators <inline-formula><mml:math id="M13"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M14"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the mean vector <bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub> and the diagonal covariance matrix &#x0039b;<sub><italic>k</italic></sub>, respectively, are obtained iteratively as follows:<disp-formula id="EEq9"><label>(10)</label><mml:math id="M15"><mml:mtable style="T17"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:malignmark/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:malignmark/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.24498pt" depth="4.69899pt"/><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.24498pt" depth="4.69899pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where<disp-formula id="EEq11"><label>(11)</label><mml:math id="M16"><mml:mtable style="T19"><mml:mtr><mml:mtd><mml:malignmark/><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="8.4871368408203125pt"/><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.59pt"/><mml:mi>&#x003b2;</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">1</mml:mn><mml:mspace height="7.08pt" depth="2.59pt"/></mml:mrow></mml:mfenced><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="EEq12"><label>(12)</label><mml:math id="EEq12EAAA6AEAFCA"><mml:mtable><mml:mtr><mml:mtd><mml:malignmark/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="11.631pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mspace height="11.631pt" depth="4.19899pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="8.4871368408203125pt"/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mspace height="13.96999pt" depth="7.06999pt"/><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mspace height="9.75598pt" depth="4.19899pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mspace height="9.75598pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mspace height="13.96999pt" depth="7.06999pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The formulation of (<xref ref-type="disp-formula" rid="EEq9">10</xref>)&#x02013;(<xref ref-type="disp-formula" rid="EEq12">12</xref>) is straightforward as described in the previous works [<xref rid="B15" ref-type="bibr">17</xref>, <xref rid="B16" ref-type="bibr">18</xref>]. The function in (<xref ref-type="disp-formula" rid="EEq12">12</xref>) is called the <italic>&#x003b2;</italic>-weight function, which plays the key role for robust estimation of the parameters. If <italic>&#x003b2;</italic> tends to 0, then (<xref ref-type="disp-formula" rid="EEq9">10</xref>) are reduced to the classical noniterative estimates of mean and diagonal covariance matrix as given in (<xref ref-type="disp-formula" rid="EEq3">4</xref>) and (<xref ref-type="disp-formula" rid="EEq5">6</xref>), respectively. The performance of the proposed method depends on the value of the tuning parameter <italic>&#x003b2;</italic> and initialization of the Gaussian parameters <bold><italic>&#x003b8;</italic></bold><sub><italic>k</italic></sub> = {<bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub>, &#x0039b;<sub><italic>k</italic></sub>}.</p></sec><sec id="sec2.3.2"><title>2.3.2. Parameters Initialization and Breakdown Points of the Estimates</title><p>The mean vector <bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub> is initialized by the median vector, since mean and median are same for normal distribution and the median (Me) is highly robust against outliers with 50% breakdown points to estimate central value of the distribution. The median vector of<italic> k</italic>th class/population is defined as<disp-formula id="EEq13"><label>(13)</label><mml:math id="M17"><mml:mtable style="T&#x0221e;7"><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="18.18047pt" depth="13.08049pt"/><mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mspace height="18.18047pt" depth="13.08049pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The diagonal covariance matrix &#x0039b;<sub><italic>k</italic></sub> is initialized by the identity matrix (<bold>I</bold>). The iterative procedure will converge to the optimal point of the parameters, since the initial mean vector would belong to the center of the dataset with 50% breakdown points. The proposed estimators can resist the effect of more than 50% breakdown points if we can initialize the mean vector <bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub> by a vector that belongs to the good part of the dataset and the variance-covariance &#x0039b;<sub><italic>k</italic></sub> by the identity (<bold>I</bold>) matrix. More discussion about high breakdown points for the minimum <italic>&#x003b2;</italic>-divergence estimators can be found in [<xref rid="B16" ref-type="bibr">18</xref>].</p></sec><sec id="sec2.3.3"><title>2.3.3. <italic>&#x003b2;</italic>-Selection Using <italic>T</italic>-Fold Cross Validation (CV) for Parameter Estimation</title><p>To select the appropriate <italic>&#x003b2;</italic> by CV, we fix the tuning parameter <italic>&#x003b2;</italic> to <italic>&#x003b2;</italic><sub>0</sub>. The computation steps for selecting appropriate <italic>&#x003b2;</italic> by<italic> T</italic>-fold cross validation is given below.</p><p>
<statement id="step1"><title>Step 1 . </title><p>Dataset <italic>D</italic><sub><italic>k</italic></sub> = {<bold>x</bold><sub><italic>jk</italic></sub>; <italic>j</italic> = 1,2,&#x02026;, <italic>N</italic><sub><italic>k</italic></sub>} is split into <italic>T</italic> subsets; <italic>D</italic><sub><italic>k</italic></sub>(1), <italic>D</italic><sub><italic>k</italic></sub>(2),&#x02026;, <italic>D</italic><sub><italic>k</italic></sub>(<italic>T</italic>) where <italic>D</italic><sub><italic>k</italic></sub>(<italic>t</italic>) = {<bold>x</bold><sub><italic>tk</italic></sub>; <italic>t</italic> = 1,2,&#x02026;, <italic>N</italic><sub><italic>tk</italic></sub>} and &#x02211;<sub><italic>t</italic>=1</sub><sup><italic>T</italic></sup><italic>N</italic><sub><italic>tk</italic></sub> = <italic>N</italic><sub><italic>k</italic></sub>.</p></statement>
</p><p>
<statement id="step2"><title>Step 2 . </title><p>Let <italic>D</italic><sub><italic>k</italic></sub><sup><italic>c</italic></sup>(<italic>t</italic>) = {<bold>x</bold><sub><italic>sk</italic></sub>&#x02223;<bold>x</bold><sub><italic>sk</italic></sub> &#x02209; <italic>D</italic><sub><italic>k</italic></sub>(<italic>t</italic>), <italic>s</italic> = 1,2,&#x02026;, <italic>N</italic><sub><italic>tk</italic></sub><sup><italic>c</italic></sup> = (<italic>N</italic><sub><italic>k</italic></sub> &#x02212; <italic>N</italic><sub><italic>tk</italic></sub>)} for <italic>t</italic> = 1,2,&#x02026;, <italic>T</italic>.</p></statement>
</p><p>
<statement id="step3"><title>Step 3 . </title><p>Estimate <inline-formula><mml:math id="M18"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M19"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> iteratively by (<xref ref-type="disp-formula" rid="EEq9">10</xref>) based on dataset <italic>D</italic><sub><italic>k</italic></sub><sup><italic>c</italic></sup>(<italic>t</italic>).</p></statement>
</p><p>
<statement id="step4"><title>Step 4 . </title><p>Compute CV(t) using dataset <italic>D</italic><sub><italic>k</italic></sub>(<italic>t</italic>), for <inline-formula><mml:math id="M20"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02223;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>&#x02009;&#x02009;</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="M21"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02223;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mo stretchy="false">[</mml:mo><mml:mn mathvariant="normal">1</mml:mn><mml:mo>-</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>&#x02009;&#x02009;</mml:mi><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02223;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>.</p></statement>
</p><p>
<statement id="step5"><title>Step 5 . </title><p>End.</p></statement>
</p><p>Computed suitable <italic>&#x003b2; by</italic><disp-formula id="eq14"><label>(14)</label><mml:math id="M22"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="fraktur">D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.59pt"/><mml:mi>&#x003b2;</mml:mi><mml:mspace height="7.08pt" depth="2.59pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>&#x1d507;</italic><sub><italic>k</italic>,<italic>&#x003b2;</italic><sub>0</sub></sub>(<italic>&#x003b2;</italic>) =&#x02009;&#x02009;(1/<italic>N</italic><sub><italic>k</italic></sub>)&#x02211;<sub><italic>t</italic>=1</sub><sup><italic>T</italic></sup>CV<sub><italic>k</italic></sub>(<italic>t</italic>).</p><p>If the sample size (<italic>N</italic><sub><italic>k</italic></sub>) is small such that <italic>N</italic><sub><italic>tk</italic></sub><sup><italic>c</italic></sup> = (<italic>N</italic><sub><italic>k</italic></sub> &#x02212; <italic>N</italic><sub><italic>tk</italic></sub>) &#x0003c; <italic>p</italic>, then<italic> T</italic> = <italic>N</italic><sub><italic>k</italic></sub> (leave-one-out CV) can be used to select the appropriate <italic>&#x003b2;</italic>. More discussion about <italic>&#x003b2;</italic> selection also can be found in [<xref rid="B14" ref-type="bibr">16</xref>&#x02013;<xref rid="B16" ref-type="bibr">18</xref>].</p></sec><sec id="sec2.3.4"><title>2.3.4. Outlier Identification Using <italic>&#x003b2;</italic>-Weight Function</title><p>The performance of NBC for classification of an unlabeled data vector <bold>x</bold> using (<xref ref-type="disp-formula" rid="EEq1">1</xref>) not only depends on the robust estimation of the parameters but also depends on the values of <bold>x</bold> weather it is contaminated or not. The data vector <bold>x</bold> is said to be contaminated if at least one component of <bold>x</bold> = {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,&#x02026;, <italic>x</italic><sub><italic>p</italic></sub>} is contaminated by outlier. To derive a criterion of whether the unlabeled data vector <bold>x</bold> is contaminated or not, we consider <italic>&#x003b2;</italic>-weight function (<xref ref-type="disp-formula" rid="EEq12">12</xref>) and rewrite it as follows:<disp-formula id="EEq14"><label>(15)</label><mml:math id="M23"><mml:mtable style="T3"><mml:mtr><mml:mtd><mml:malignmark/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.15999pt" depth="4.61pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.15999pt" depth="4.61pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="10pt"/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mspace height="13.96999pt" depth="7.06999pt"/><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.13pt" depth="4.61pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.13pt" depth="4.61pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.13pt" depth="4.61pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.13pt" depth="4.61pt"/></mml:mrow></mml:mfenced><mml:mspace height="13.96999pt" depth="7.06999pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mspace width="10pt"/><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn mathvariant="normal">0</mml:mn><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The values of this weight function lie between 0 and 1. This weight function produces larger weight (but less than 1) if <bold>x</bold> &#x02208; <italic>C</italic><sub><italic>k</italic></sub> and smaller weight (but greater than 0) if <bold>x</bold> &#x02209; <italic>C</italic><sub><italic>k</italic></sub> or contaminated by outlier. Therefore, the <italic>&#x003b2;</italic>-weight function (<xref ref-type="disp-formula" rid="EEq14">15</xref>) can be characterized as<disp-formula id="EEq15"><label>(16)</label><mml:math id="M24"><mml:mtable style="T3"><mml:mtr><mml:mtd><mml:malignmark/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.15999pt" depth="4.61pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.15999pt" depth="4.61pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="10pt"/><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mo mathvariant="bold">&#x0003e;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo mathvariant="bold">&#x02264;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02209;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The threshold value <italic>&#x003c8;</italic><sub><italic>k</italic></sub> can be determined based on the empirical distribution of <italic>&#x003b2;</italic>-weight function as discussed in [<xref rid="B17" ref-type="bibr">31</xref>] and by the quantile values of <inline-formula><mml:math id="M25"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi/><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>&#x02223;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> for <italic>j</italic> = 1,2,&#x02026;, <italic>N</italic><sub><italic>k</italic></sub> with probability<disp-formula id="EEq16"><label>(17)</label><mml:math id="M26"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Pr</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mspace height="9.52998pt" depth="4.61pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.15999pt" depth="4.61pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.15999pt" depth="4.61pt"/></mml:mrow></mml:mfenced><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.52998pt" depth="4.61pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>&#x003d1;</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>&#x003d1;</italic> is the probability for selecting the cut-off value <italic>&#x003c8;</italic><sub><italic>k</italic></sub> and the value of <italic>&#x003d1;</italic> should lie between 0.00 and 0.05. In this paper, heuristically we choose <italic>&#x003d1;</italic> = 0.03 to fix the cut-off value <italic>&#x003c8;</italic><sub><italic>k</italic></sub> for detection of outlying data vector using (<xref ref-type="disp-formula" rid="EEq17">18</xref>). This idea was first introduced in [<xref rid="B17" ref-type="bibr">31</xref>].</p><p>Then the criteria whether the unlabeled data vector <bold>x</bold> is contaminated or not can be defined as follows:<disp-formula id="EEq17"><label>(18)</label><mml:math id="M27"><mml:mtable style="T6"><mml:mtr><mml:mtd><mml:maligngroup/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="0.0pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mspace height="4.29pt" depth="0.0pt"/></mml:mrow></mml:mfenced><mml:malignmark/><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.15999pt" depth="4.61pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.15999pt" depth="4.61pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mo mathvariant="bold">=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mo mathvariant="bold">&#x02265;</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo mathvariant="bold">&#x0003c;</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>&#x003c8;</italic> = &#x02211;<sub><italic>k</italic>=1</sub><sup><italic>K</italic></sup><italic>&#x003c8;</italic><sub><italic>k</italic></sub>.</p><p>However, in this paper, we directly choose the threshold value of <italic>&#x003c8;</italic> as follows:<disp-formula id="EEq18"><label>(19)</label><mml:math id="M28"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>&#x003c8;</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.35999pt" depth="2.59pt"/><mml:mn mathvariant="normal">1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mspace height="6.35999pt" depth="2.59pt"/></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="fraktur">D</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.45499pt"/><mml:mi mathvariant="bold">y</mml:mi><mml:mspace height="4.29pt" depth="2.45499pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mi>&#x02009;</mml:mi><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="fraktur">D</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="2.45499pt"/><mml:mi mathvariant="bold">y</mml:mi><mml:mspace height="4.29pt" depth="2.45499pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>With heuristically <italic>&#x003b7;</italic> = 0.10, where <italic>&#x1d507;</italic> is the training dataset including the unclassified data vector <bold>x</bold>, (<xref ref-type="disp-formula" rid="EEq18">19</xref>) was also used in the previous works in [<xref rid="B14" ref-type="bibr">16</xref>, <xref rid="B16" ref-type="bibr">18</xref>] to choose the threshold value for outlier detection.</p></sec><sec id="sec2.3.5"><title>2.3.5. Classification by the Proposed <italic>&#x003b2;</italic>-NBC</title><p>When the unlabeled data vector <bold>x</bold> is usual, the appropriate label/class of <bold>x</bold> can be determined using the minimum <italic>&#x003b2;</italic>-divergence estimators <inline-formula><mml:math id="M29"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> of <inline-formula><mml:math id="M30"><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x0039b;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:math></inline-formula> in the predicting equation (<xref ref-type="disp-formula" rid="EEq1">1</xref>). If the unlabeled data vector <bold>x</bold>&#x02009;&#x02009;is unusual/contaminated by outliers, then we propose a classification rule as follows. We compute the absolute difference between the outlying vector and each of mean vectors as<disp-formula id="EEq19"><label>(20)</label><mml:math id="M31"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:malignmark/><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.13pt" depth="4.61pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.13pt" depth="4.61pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.15698pt"/><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.15698pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="149.853851318359375pt"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Compute sum of the smallest<italic> r </italic>components of <bold>d</bold><sub><italic>k</italic></sub> as <italic>S</italic><sub><italic>kr</italic></sub> = <italic>d</italic><sub><italic>k</italic>(1)</sub> + <italic>d</italic><sub><italic>k</italic>(2)</sub> + &#x022ef;+<italic>d</italic><sub><italic>k</italic>(<italic>r</italic>)</sub>, where <italic>r = </italic>round (<italic>p</italic>/2). Then the unlabeled test data vector <bold>x</bold> can be classified as<disp-formula id="EEq20"><label>(21)</label><mml:math id="M32"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi><mml:mo>&#x02061;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mi>&#x02009;</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>If the outlying test vector&#x02009;&#x02009;<bold>x</bold>&#x02009;&#x02009;is classified in to class <italic>k</italic>, then its<italic> i</italic>th component is said to be outlying if <italic>d</italic><sub><italic>ki</italic></sub> &#x0003e; <italic>S</italic><sub><italic>kr</italic></sub>&#x02009;&#x02009;(<italic>i</italic> = 1,2,&#x02026;, <italic>p</italic>). Then we update <bold>x</bold>&#x02009;&#x02009;by replacing its outlying components with the corresponding mean components from the mean vector <inline-formula><mml:math id="M33"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of<italic> k</italic>th population. Let <bold>x</bold><sup><italic>&#x02217;</italic></sup> be the updated vector of <bold>x</bold>. Then we use <bold>x</bold><sup><italic>&#x02217;</italic></sup> instead of <bold>x</bold> to confirm the label/class of <bold>x</bold>&#x02009;&#x02009;using (<xref ref-type="disp-formula" rid="EEq1">1</xref>).</p></sec></sec></sec><sec id="sec3"><title>3. Simulation Study</title><sec id="sec3.1"><title>3.1. Simulated Dataset 1</title><p>To investigate the performance of our proposed (<italic>&#x003b2;</italic>-NBC) classifier in a comparison with four popular classifiers (KNN, NBC, SVM, and AdaBoost), we generated both training and test datasets from <italic>m</italic> = 2 multivariate normal distributions with different mean vectors (<bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub>, <italic>k</italic> = 1,2) of length <italic>p</italic> = 10 but common covariance matrix (&#x0039b;<sub><italic>k</italic></sub> = &#x0039b;; <italic>k</italic> = 1,2). In this simulation study, we generated <bold>N</bold><sub>1</sub> = 40 samples from the first population and <bold>N</bold><sub>2</sub> = 42 samples from the second population for both training and test datasets. We computed the training error and test error rate for all five classifiers using both original and contaminated datasets with different mean vectors {(<bold><italic>&#x003bc;</italic></bold><sub>1</sub>, <bold><italic>&#x003bc;</italic></bold><sub>2</sub> = <bold><italic>&#x003bc;</italic></bold><sub>1</sub> + <italic>t</italic>); <italic>t</italic> = 0,&#x02026;, 9}, where the other parameters remain the same for each dataset. For convenience of the presentation, we distinguish the two mean vectors in such a way in which the second mean vector is generated by adding<italic> t</italic> with each of the components of the first mean vector.</p></sec><sec id="sec3.2"><title>3.2. Simulated Dataset 2</title><p>To investigate the performance of the proposed classifier (<italic>&#x003b2;</italic>-NBC) in a comparison of the classical NBC for the classification of object into two groups, let us consider a model for generating gene expression datasets as displayed in <xref ref-type="table" rid="tab1">Table 1</xref> which was also used in Nowak and Tibshirani [<xref rid="B18" ref-type="bibr">32</xref>]. In <xref ref-type="table" rid="tab1">Table 1</xref>, the first column represents the gene expressions of normal individuals and the second column represents the gene expressions of patient individuals. First row represents the genes from group A and second row represents the genes from group B. To randomize the gene expression, Gaussian noise is added from <italic>N</italic>(0, <italic>&#x003c3;</italic><sup>2</sup>). First we generate a training gene-set using the data generating model (<xref ref-type="table" rid="tab1">Table 1</xref>) with parameters <italic>d</italic> = 5 and <italic>&#x003c3;</italic><sup>2</sup> = 1, where <italic>p</italic><sub>1</sub> = 30 genes denoted by {<italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub>,&#x02026;, <italic>A</italic><sub>30</sub>} are generated for group <bold>A</bold> and <italic>p</italic><sub>2</sub> = 30 genes denoted by {<italic>B</italic><sub>1</sub>, <italic>B</italic><sub>2</sub>,&#x02026;, <italic>B</italic><sub>30</sub>} are generated for group <bold>B</bold> with <italic>n</italic><sub>1</sub> = 30 normal individuals and <italic>n</italic><sub>2</sub> = 30 patients (e.g., cancer or any other disease). Then we generate a test gene-set using the same model with the same parameters <italic>d</italic> = 5 and <italic>&#x003c3;</italic><sup>2</sup> = 1 as before, where <italic>p</italic><sub>11</sub> = 30 genes denoted by {<italic>A</italic><sub>31</sub>, <italic>A</italic><sub>32</sub>,&#x02026;, <italic>A</italic><sub>60</sub>} are generated for group&#x02009;&#x02009;<bold>A</bold> and <italic>p</italic><sub>22</sub> = 30 genes denoted by {<italic>B</italic><sub>31</sub>, <italic>B</italic><sub>32</sub>,&#x02026;, <italic>B</italic><sub>60</sub>} are generated for group&#x02009;&#x02009;<bold>B</bold> with <italic>n</italic><sub>11</sub> = 25 normal individuals and <italic>n</italic><sub>22</sub> = 25 patients (e.g., cancer or any other disease).</p></sec><sec id="sec3.3"><title>3.3. Simulated Dataset 3</title><p>To demonstrate the performance of the proposed classifier (<italic>&#x003b2;</italic>-NBC) in a comparison of some other robust linear classifiers based on the robust estimators (MCD, MVE, OGK, MCD-A, MCD-B, MCD-C, and FSA) as mentioned earlier for the classification of object into different groups, we have generated the training and test datasets from <italic>m</italic> = 2, 3 multivariate normal distributions with variables<italic> p</italic> = 10, 5, respectively. We consider <italic>n</italic><sub>1</sub> = 40 and <italic>n</italic><sub>2</sub> = 35 (<italic>n</italic> = <italic>n</italic><sub>1</sub> + <italic>n</italic><sub>2</sub>) samples from <italic>m</italic> = 2 different multivariate normal populations <italic>N</italic><sub><italic>p</italic></sub>(<bold>&#x000b5;</bold><sub>1</sub>, &#x0039b;<sub>1</sub>) and <italic>N</italic><sub><italic>p</italic></sub>(<bold>&#x000b5;</bold><sub>2</sub>, &#x0039b;<sub>2</sub>). Here <bold>&#x000b5;</bold><sub>2</sub> = <bold>&#x000b5;</bold><sub>1</sub> + &#x02126; with &#x02126; = 0,1,&#x02026;, 10 such that <bold>&#x000b5;</bold><sub>1</sub> = <bold>&#x000b5;</bold><sub>2</sub> for &#x02126; = 0; otherwise <bold>&#x000b5;</bold><sub>1</sub> &#x02260; <bold>&#x000b5;</bold><sub>2</sub>, where the scalar number &#x02126; is the common difference between two corresponding mean components of <bold><italic>&#x003bc;</italic></bold><sub>1</sub> and <bold><italic>&#x003bc;</italic></bold><sub>2</sub>, respectively. Similarly, for generating the training and test datasets, we consider the <italic>n</italic><sub>1</sub> = 30, <italic>n</italic><sub>2</sub> = 30, and <italic>n</italic><sub>3</sub> = 30 (<italic>n</italic> = <italic>n</italic><sub>1</sub> + <italic>n</italic><sub>2</sub> + <italic>n</italic><sub>3</sub>) samples from <italic>m</italic> = 3. It is carried out with different means and common variance-covariance matrix of multivariate normal populations <italic>N</italic><sub><italic>p</italic></sub>(<bold><italic>&#x003bc;</italic></bold><sub>1</sub>, &#x0039b;<sub>1</sub>), <italic>N</italic><sub><italic>p</italic></sub>(<bold><italic>&#x003bc;</italic></bold><sub>2</sub>, &#x0039b;<sub>2</sub>), and <italic>N</italic><sub><italic>p</italic></sub>(<bold><italic>&#x003bc;</italic></bold><sub>3</sub>, &#x0039b;<sub>3</sub>). In this case we consider <bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub> = <bold><italic>&#x003bc;</italic></bold><sub><italic>k</italic></sub> + &#x02126; with &#x02126; = 0,1,&#x02026;, 10 and <italic>k</italic> = 1,2, 3 such that <bold><italic>&#x003bc;</italic></bold><sub>1</sub> = <bold><italic>&#x003bc;</italic></bold><sub>2</sub> = <bold><italic>&#x003bc;</italic></bold><sub>3</sub> for &#x02126; = 0; otherwise <bold><italic>&#x003bc;</italic></bold><sub>1</sub> &#x02260; <bold><italic>&#x003bc;</italic></bold><sub>2</sub> &#x02260; <bold><italic>&#x003bc;</italic></bold><sub>3</sub>, where the scalar number &#x02126; is the common difference among the corresponding mean components of <bold><italic>&#x003bc;</italic></bold><sub>1</sub>, <bold><italic>&#x003bc;</italic></bold><sub>2</sub>, and <bold><italic>&#x003bc;</italic></bold><sub>3</sub>, respectively.</p></sec><sec id="sec3.4"><title>3.4. Head and Neck Cancer Gene Expression Dataset</title><p>To demonstrate the performance of the proposed classifier (<italic>&#x003b2;</italic>-NBC) in a comparison with four popular classifiers (KNN, NBC, SVM, and AdaBoost) with the real gene expression dataset, we considered the head and neck cancer (HNC) gene expression dataset from the previous work [<xref rid="B19" ref-type="bibr">33</xref>]. The term head and neck cancer denotes a group of biologically comparable cancers originating from the upper aero digestive tract, including the following parts of human body: lip, oral cavity (mouth), nasal cavity, pharynx and larynx, and paranasal sinuses. This microarray gene expression dataset contains 12626 genes, where 594 genes are differentially expressed and the rest of the genes are equally expressed.</p></sec></sec><sec id="sec4"><title>4. Simulation and Real Data Analysis Results</title><sec id="sec4.1"><title>4.1. Simulation Results of Dataset 1</title><p>We have used the simulated dataset 1 to investigate the performance of the proposed method with the performance of the other popular classifiers such as classical NBC, SVM, KNN, and AdaBoost. Figures <xref ref-type="fig" rid="fig1">1(a)</xref>&#x02013;<xref ref-type="fig" rid="fig1">1(f)</xref> represent the test error rate estimated by these five classifiers against the common mean differences in absence of outliers (original dataset) and in presence of 5%, 10%, 15%, 20%, and 25% outliers in test dataset, respectively. From <xref ref-type="fig" rid="fig1">Figure 1(f)</xref> it is evident that in absence of outlier every method produces almost the same result, whereas, in presence of different levels of outliers (see Figures <xref ref-type="fig" rid="fig1">1(a)</xref>&#x02013;<xref ref-type="fig" rid="fig1">1(e)</xref>), the proposed method outperformed the other methods by producing low test error rate. <xref ref-type="table" rid="tab2"> Table 2</xref> is summarized with different performance measures (accuracy, sensitivity, specificity, positive predicted value (PPV), negative predicted value (NPV), prevalence, detection rate, detection prevalence, Matthews correlation coefficient (MCC), and misclassification error rate). All these performance measures are computed by the five methods (NBC, KNN, SVM, AdaBoost, and proposed).</p><p>From <xref ref-type="table" rid="tab2">Table 2</xref> we observed that the proposed method produces better results than the other classifiers (NBC, SVM, KNN, and AdaBoost), since it produces higher values of accuracy (&#x0003e;97%), sensitivity (&#x0003e;95%), specificity (&#x0003e;94%), PPV (&#x0003e;94%), NPV (&#x0003e;94%), and MCC (&#x0003e;94%) and lower values of prevalence and MER (&#x0003c;4%). The proportion test statistic [<xref rid="B33" ref-type="bibr">34</xref>] has been used to test the significance of several proportions produced by the five classifiers for each of the performance measures. The column 7 of <xref ref-type="table" rid="tab2">Table 2</xref> represents the <italic>p</italic> values of this test statistic. Since all the <italic>p</italic> values except MER are less than 0.01, so we can conclude that the performance results are highly statistically significant. The MER (<italic>p</italic> value &#x0003c; 0.05) is also statistically significant at 5% level of significance. So we may conclude from simulated dataset 1 that our proposed method performed better than the other classical methods for the contaminated dataset. It keeps equal performance in absence of outliers for the original dataset.</p></sec><sec id="sec4.2"><title>4.2. Simulation Results of Dataset 2</title><p>To investigate the performance of the proposed classifier (<italic>&#x003b2;</italic>-NBC) in a comparison of the classical NBC for the classification of objects into two groups, we considered the simulated dataset 2. Figures <xref ref-type="fig" rid="fig2">2(a)</xref> and <xref ref-type="fig" rid="fig2">2(b)</xref> show training and test datasets in absence of outliers, respectively. Here genes are randomly allocated in the test dataset. Figures <xref ref-type="fig" rid="fig3">3(a)</xref> and <xref ref-type="fig" rid="fig3">3(b)</xref> show the results of classified test dataset by classical and proposed NBC, respectively.</p><p>From classification results we observed that both the na&#x000ef;ve Bayes procedures and proposed method produce almost the same results with low misclassification error rates in absence of outliers. To investigate the robustness performance of our proposed method in a comparison with the conventional na&#x000ef;ve Bayes procedure for classification, we randomly contaminated 30% genes by outliers in the test gene-sets (Figures <xref ref-type="fig" rid="fig4">4(a)</xref>&#x02013;<xref ref-type="fig" rid="fig4">4(c)</xref>).</p><p>To classify sample into any one of the groups using the contaminated test gene-set (<xref ref-type="fig" rid="fig4">Figure 4(a)</xref>), we calculated the misclassification error rate by NBC and proposed method. From <xref ref-type="fig" rid="fig4">Figure 4</xref> we see that the traditional na&#x000ef;ve Bayes procedures fail to achieve correct classification (<xref ref-type="fig" rid="fig4">Figure 4(b)</xref>) and the misclassification error rate is 34%. Then we try to classify objects/patients using the proposed method which is shown in <xref ref-type="fig" rid="fig4">Figure 4(c)</xref>. It is obvious from these figures that the classification performance of the proposed method is good and the misclassification error rate is approximately 5% for test gene datasets.</p></sec><sec id="sec4.3"><title>4.3. Simulation Results of Dataset 3</title><p>We also investigated the performance of the proposed robust na&#x000ef;ve Bayes classifier in a comparison with classical na&#x000ef;ve Bayes as well as robust linear classifier based on the MVE, FSA, MCD, MCD-A, MCD-B, MCD-C, and OGK estimators of the mean vectors and covariance matrices. We computed different performance measures such as average of true positive rate (TPR), false positive rate (FPR), area under the ROC curve (AUC), and partial AUC (pAUC) based on 50 replications of the dataset to measure the performance of all classifiers. A method is said to be better than others, if it produces larger values of TPR, AUC, and pAUC and smaller values of FPR and MER.</p><p>
<xref ref-type="table" rid="tab3">Table 3</xref> shows the average values of AUC and pAUC at FPR = 0.2 based on the 50 replicated simulated datasets 3 with <italic>p</italic> = 15 for the two- (2-) class classification. The performance measures have been estimated by the classical, FSA, MCD, MVE, MCD-A, MCD-B, MCD-C, OGK, and proposed methods. They show the average estimates of AUC and pAUC for seven classifiers using simulated dataset 3 in absence and presence of outliers. We observed that in absence of outliers all the classifiers produce almost similar results. The proposed classifiers produced better result than the classical NBC and other robust estimators in presence of different levels (5%, 10%, 15%, 20%, and 25%) of outliers. Also MCD, MCD-A, MCD-B, and MCD-C show the constant performance result at the same level of outlier rate and varied for the different level of outlier rates. The ROC analysis also supported these results which are shown in Figures <xref ref-type="fig" rid="fig5">5(a)</xref>&#x02013;<xref ref-type="fig" rid="fig5">5(f)</xref>, so we may conclude that the proposed method outperformed the others.</p><p>To investigate the performance of the proposed method in a comparison with other methods (classical, FSA, MCD, MVE, MCD-A, MCD-B, MCD-C, OGK, and proposed) for multiclass (3) classification problem. We generated simulated datasets 3 based on 50 replicated with <italic>p</italic> = 5 the number of variables. The performance measures were estimated for each of these methods. <xref ref-type="table" rid="tab4"> Table 4</xref> shows the average standard error of AUC and pAUC for multiclass classification. It is revealed that the proposed robust na&#x000ef;ve Bayes classifier outperformed the classical and other robust linear classifiers in presence of outliers with false positive rate 0.2. The proposed method produces the larger values of AUC and pAUC and shows the lower values of MER and standard error of AUC and pAUC values. The performance measures using different types of MCD estimators were shown in the constant result at the same level of outlier rate. It was varied for the different levels of contamination rate.</p></sec><sec id="sec4.4"><title>4.4. Head and Neck Cancer Gene Expression Data Analysis</title><p>We also investigated the performance of the proposed method in real microarray gene expression dataset. The normalized Head and Neck cancer (HNC) dataset is considered here [<xref rid="B19" ref-type="bibr">33</xref>]. The RNA sample was extracted from the 22 normal and 22 cancer tissues for generating the HNC dataset. The Affymetrix GeneChip was used for processing RNA samples and finally got the quantified CELL file format. The Robust Multichip Analysis (RMA) and quantile normalization methods were used for processing the CELL files. The HNC dataset was 12,642 probe sets, 44 samples, and 42 significantly differentially expressed probe sets. The detailed discussion is shown in [<xref rid="B19" ref-type="bibr">33</xref>] for preprocessing of HNC dataset. We first select the differentially expressed (DE) genes whose posterior probability is more than 0.9; otherwise the genes are equally expressed (EE) using bridge R package [<xref rid="B34" ref-type="bibr">35</xref>] which is shown in <xref ref-type="fig" rid="fig6">Figure 6</xref> that shows 594 differentially expressed genes from 12626 genes. We have performed the Anderson-Darling (A-D) normality test [<xref rid="B37" ref-type="bibr">36</xref>, <xref rid="B38" ref-type="bibr">37</xref>] for the HNC dataset. The results show that a few numbers of DE genes (5%) for both normal and cancer groups break the normality assumption at 1% level of significance. Also we checked the independence assumption of DE genes using the mutual information [<xref rid="B39" ref-type="bibr">38</xref>]. We found that the mutual information for HNC dataset is 0.044 which is almost close to zero for both normal and cancer groups. So we may conclude that the DE genes almost satisfy the independence assumption. Therefore, we may assume that the HNC dataset almost satisfies the normality and independence assumption of NBC for a given class/groups.</p><p>For classification problem, we have considered half of the differentially expressed genes (594/2 = 297) as training gene-set and we identified their group using hierarchical clustering (HC). <xref ref-type="fig" rid="fig7"> Figure 7</xref> represents the dendrogram of HC of half of the differentially expressed genes for training data. The rest of the 297 differentially expressed genes are considered as a test gene-set. Then we employed both classical NBC and robust NBC (<italic>&#x003b2;</italic>-NBC) in this dataset to classify cancer genes (see Figures <xref ref-type="fig" rid="fig8">8(a)</xref>&#x02013;<xref ref-type="fig" rid="fig8">8(d)</xref>). We observed that from <xref ref-type="fig" rid="fig8">Figure 8</xref> the traditional na&#x000ef;ve Bayes procedure can not find the group of gene properly whereas our proposed method (<italic>&#x003b2;</italic>-NBC) performs better for identifying the gene group in the HNC dataset. <xref ref-type="fig" rid="fig8"> Figure 8(d)</xref> shows that the proposed classifier shows better performance for classifying the samples than the classical method (<xref ref-type="fig" rid="fig8">Figure 8(c)</xref>).</p><p>We also computed different performance measures (accuracy, sensitivity, specificity, positive predicted value (PPV), negative predicted value (NPV), prevalence, detection rate, detection, prevalence, Matthews correlation coefficient (MCC), and misclassification error rate) by the five classification methods (NBC, KNN, SVM, AdaBoost, and proposed) using HNC dataset (<xref ref-type="table" rid="tab5">Table 5</xref>). From <xref ref-type="table" rid="tab5">Table 5</xref> we have observed that the proposed classifier produces better results than the other classifiers (NBC, SVM, KNN, and AdaBoost). The proportion test [<xref rid="B33" ref-type="bibr">34</xref>] has shown that the <italic>p</italic> values &#x0003c;0.01 for the different performance results excluding MCC and MER. Then we may say that they are highly statistically significant. The MCC and MER are statistically significant at 5% level of significance because of the <italic>p</italic> values &#x0003c; 0.05. Hence, the performances of the proposed methods in real HNC data analysis are better than classical and other methods. Also this data set is contaminated by outliers reported in [<xref rid="B17" ref-type="bibr">31</xref>]. So we consider this dataset to investigate the performance of the proposed method in a comparison of some popular existing classifiers. We observed that the proposed method outperforms the others for this HNC dataset.</p></sec></sec><sec id="sec5"><title>5. Discussion</title><p>In this paper, we discussed the robustification of Gaussian NBC using the minimum <italic>&#x003b2;</italic>-divergence method within two steps. For both simulated and real data analysis, at first, the mean vectors and the diagonal covariance matrices were computed by the minimum <italic>&#x003b2;</italic>-divergence estimators for the Gaussian NBC based on the training dataset. Then outlying test data vectors were detected from the test dataset using the <italic>&#x003b2;</italic>-weight function and outlying components in each test data vector were replaced by the corresponding values of their estimated mean vectors. Then the modified test data vectors were used as the input data vectors in the proposed <italic>&#x003b2;</italic>-NBC for their class prediction or pattern recognition. The rest of the data vectors from the test dataset were directly used as the input data vectors in the proposed <italic>&#x003b2;</italic>-NBC for their class prediction or pattern recognition. We observed that the performance of the proposed method depends on the tuning parameter <italic>&#x003b2;</italic> and the initialization of the Gaussian parameters. Therefore, in this paper, we also discussed the initialization procedure for the Gaussian parameters and the <italic>&#x003b2;</italic>-selection procedure using cross validation in Sections <xref ref-type="sec" rid="sec2.3.2">2.3.2</xref> and <xref ref-type="sec" rid="sec2.3.3">2.3.3</xref>, respectively. The classifier reduces to the traditional Gaussian NBC when <italic>&#x003b2;</italic> &#x02192; 0. Therefore, we call the proposed classifier <italic>&#x003b2;</italic>-NBC. We investigated the robustness performance of the proposed <italic>&#x003b2;</italic>-NBC in a comparison of several robust versions of linear classifiers based on MCD, MVE, and OGK estimators taking the smaller number of variables/genes (<italic>p</italic>) with larger number of patients/samples (<italic>n</italic>) in the training dataset, since these types of robust classifiers also suffer from the inverse problem of its covariance matrix in presence of large number of variables/genes (<italic>p</italic>) with small number of patients/samples (<italic>n</italic>) in the training dataset. We observed that the proposed <italic>&#x003b2;</italic>-NBC outperforms the existing robust linear classifiers as early mentioned in presence of outliers. Otherwise, it keeps almost equal performance. Then we investigated the performance of the proposed method in a comparison of some popular classifiers including Support Vector Machine (SVM),<italic> K</italic>-Nearest Neighbors (<italic>K</italic>NN), and AdaBoost which are widely used for gene expression data analysis [<xref rid="B7" ref-type="bibr">27</xref>&#x02013;<xref rid="B9" ref-type="bibr">29</xref>]. In that comparison, we used both simulated and real gene expression datasets. We observed that the proposed method improves the performance over the others in presence of outliers. Otherwise, it keeps almost equal performance as before. The main advantage of the proposed classifier over the others is that it works well for both conditions of (i) <italic>p</italic> &#x0003c; <italic>n</italic> and (ii) <italic>p</italic> &#x0003e; <italic>n</italic>, and it can resist the effect of 50% breakdown points. If the dataset does not satisfy the normality assumptions, then the proposed method may show weaker performance than others in absence of outliers. However, the nonnormal dataset can transform to the normal dataset by some suitable transformation like Box-Cox transformation [<xref rid="B36" ref-type="bibr">39</xref>]. Then the proposed method would be useful to tackle the outlying problems. The proposed method may also suffer from the correlated observations. In that case, correlated observations can be transforming to the uncorrelated observations using standard principal component analysis (PCA) or singular value decomposition (SVD) based PCA. Then the proposed method would be more useful to tackle the outlying problems as before. However, in our current studied in this paper, we investigated the performance of the proposed classifier (<italic>&#x003b2;</italic>-NBC) in a comparison of some popular existing classifiers (NBC, KNN, SVM, and AdaBoost) including some robust linear classifiers (MCD, MVE, OGK, MCD-A, MCD-B, MCD-C, and FSA) using both simulated and real gene expression datasets, where simulated datasets satisfied the normality and independent assumptions. We observed that the proposed method improved the performance over the others in presence of outliers. Otherwise, it keeps almost equal performance. Usually gene expression datasets are often contaminated by outliers due to several steps involved in the data generating process from hybridization to image analysis. Therefore the proposed method would be more suitable for gene expression data analysis.</p></sec><sec id="sec6"><title>6. Conclusion</title><p>The accurate sample class prediction or pattern recognition is one of the most significant issues for MGED analysis. The na&#x000ef;ve Bayes classifier is an important and widely used method for the class prediction in bioinformatics. However, this method suffers from outlying problems to estimate the location parameters in the MGED analysis. To overcome this we proposed <italic>&#x003b2;</italic>-NBC for estimating the robust location and scale parameters. In the simulation studies 1 and 2, we showed that, in presence of outliers, the proposed <italic>&#x003b2;</italic>-NBC outperforms other popular classifiers while datasets were generated from the multivariate and univariate normal distribution, respectively, and it keeps equal performance with the other classifiers, in absence of outliers. We also investigated the robustness performance of the proposed <italic>&#x003b2;</italic>-NBC in a comparison of linear classifier using some popular robust estimators in the simulation study 3. From this simulation study we observed that the proposed <italic>&#x003b2;</italic>-NBC outperforms existing robust linear classifiers. Finally we applied in the real HNC dataset; our proposed <italic>&#x003b2;</italic>-NBC showed better performance than the other traditional classifiers. Therefore, we may conclude that, in presence of outliers, our proposed <italic>&#x003b2;</italic>-NBC outperforms other methods using both simulated and real datasets.</p></sec><sec sec-type="supplementary-material" id="supplementary-material-sec"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="f1"><caption><p>The R source code for the robust naive Bayes classifier (&#x003b2;-NBC). The robust classification of microarray gene expression data.</p></caption><media xlink:href="3020627.f1.rar" mimetype="application" mime-subtype="x-rar-compressed" orientation="portrait" id="d35e6474" position="anchor"/></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>This study was supported by HEQEP Subproject (CP-3603, W2, R3), Department of Statistics, University of Rajshahi, Rajshahi-6205, Bangladesh.</p></ack><sec><title>Additional Points</title><p>
<italic>Supplementary Materials.</italic> The source code is written in R which is available in the Supplementary Material, available online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2017/3020627">https://doi.org/10.1155/2017/3020627</ext-link>.</p></sec><sec><title>Conflicts of Interest</title><p>The authors declare that they have no conflicts of interest.</p></sec><ref-list><ref id="B30"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veer</surname><given-names>V.</given-names></name><name><surname>Laura</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Gene expression profiling predicts clinical outcome of breast cancer</article-title><source><italic>Nature</italic></source><year>2002</year><volume>415</volume><issue>6871</issue><fpage>530</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/415530a</pub-id><pub-id pub-id-type="other">2-s2.0-18244409687</pub-id><pub-id pub-id-type="pmid">11823860</pub-id></element-citation></ref><ref id="B20"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dam</surname><given-names>S.</given-names></name><name><surname>V&#x000f5;sa</surname><given-names>U.</given-names></name><name><surname>van der Graaf</surname><given-names>A.</given-names></name><name><surname>Franke</surname><given-names>L.</given-names></name><name><surname>de Magalh&#x000e3;es</surname><given-names>J. P.</given-names></name></person-group><article-title>Gene co-expression analysis for functional classification and gene&#x02013;disease predictions</article-title><source><italic>Briefings in Bioinformatics</italic></source><year>2017</year><fpage>p. bbw139</fpage><pub-id pub-id-type="doi">10.1093/bib/bbw139</pub-id></element-citation></ref><ref id="B21"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X.</given-names></name><name><surname>Yu</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Brock</surname><given-names>G. N.</given-names></name></person-group><article-title>Clustering cancer gene expression data by projective clustering ensemble</article-title><source><italic>PLOS ONE</italic></source><year>2017</year><volume>12</volume><issue>2</issue><pub-id pub-id-type="publisher-id">e0171429</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0171429</pub-id></element-citation></ref><ref id="B22"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>R. K.</given-names></name><name><surname>Sivabalakrishnan</surname><given-names>M.</given-names></name></person-group><article-title>Feature Selection of Gene Expression Data for Cancer Classification: A Review</article-title><source><italic>Procedia Computer Science</italic></source><year>2015</year><volume>50</volume><fpage>52</fpage><lpage>57</lpage></element-citation></ref><ref id="B23"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novianti</surname><given-names>P. W.</given-names></name><name><surname>Jong</surname><given-names>V. L.</given-names></name><name><surname>Roes</surname><given-names>K. C.</given-names></name><name><surname>Eijkemans</surname><given-names>M. J.</given-names></name></person-group><article-title>Meta-analysis approach as a gene selection method in class prediction: does it improve model performance? A case study in acute myeloid leukemia</article-title><source><italic>BMC Bioinformatics</italic></source><year>2017</year><volume>18</volume><issue>1</issue><pub-id pub-id-type="doi">10.1186/s12859-017-1619-7</pub-id></element-citation></ref><ref id="B24"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Song</surname><given-names>C.</given-names></name><name><surname>Miao</surname><given-names>Y.</given-names></name></person-group><article-title>The Glutathione Peroxidase Gene Family in Gossypium hirsutum: Genome-Wide Identification, Classification, Gene Expression and Functional Analysis</article-title><source><italic>Scientific Reports</italic></source><year>2017</year><volume>7</volume><pub-id pub-id-type="doi">10.1038/srep44743</pub-id></element-citation></ref><ref id="B25"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jong</surname><given-names>V. L.</given-names></name><name><surname>Novianti</surname><given-names>P. W.</given-names></name><name><surname>Roes</surname><given-names>K. C. B.</given-names></name><name><surname>Eijkemans</surname><given-names>M. J. C.</given-names></name></person-group><article-title>Selecting a classification function for class prediction with gene expression data</article-title><source><italic>Bioinformatics</italic></source><year>2016</year><volume>32</volume><issue>12</issue><fpage>1814</fpage><lpage>1822</lpage><pub-id pub-id-type="other">2-s2.0-84976507439</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btw034</pub-id><pub-id pub-id-type="pmid">26873933</pub-id></element-citation></ref><ref id="B26"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buza</surname><given-names>K.</given-names></name></person-group><article-title>Classification of gene expression data: a hubness-aware semi-supervised approach</article-title><source><italic>Computer Methods and Programs in Biomedicine</italic></source><year>2016</year><volume>127</volume><fpage>105</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2016.01.016</pub-id><pub-id pub-id-type="pmid">27000293</pub-id></element-citation></ref><ref id="B27"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Oh</surname><given-names>W. K.</given-names></name><name><surname>Zhu</surname><given-names>J.</given-names></name></person-group><article-title>Disease-specific classification using deconvoluted whole blood gene expression</article-title><source><italic>Scientific Reports</italic></source><year>2016</year><volume>6</volume><pub-id pub-id-type="other">2-s2.0-84986625688</pub-id><pub-id pub-id-type="doi">10.1038/srep32976</pub-id><pub-id pub-id-type="publisher-id">32976</pub-id></element-citation></ref><ref id="B28"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Pinello</surname><given-names>L.</given-names></name><name><surname>Yuan</surname><given-names>G.-C.</given-names></name></person-group><article-title>GiniClust: Detecting rare cell types from single-cell gene expression data with Gini index</article-title><source><italic>Genome Biology</italic></source><year>2016</year><volume>17</volume><issue>1, article no. 144</issue><pub-id pub-id-type="other">2-s2.0-84976875133</pub-id><pub-id pub-id-type="doi">10.1186/s13059-016-1010-4</pub-id></element-citation></ref><ref id="B31"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname><given-names>T. R.</given-names></name><name><surname>Slonim</surname><given-names>D. K.</given-names></name><name><surname>Tamayo</surname><given-names>P.</given-names></name><etal/></person-group><article-title>Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</article-title><source><italic>Science</italic></source><year>1999</year><volume>286</volume><issue>5439</issue><fpage>531</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1126/science.286.5439.531</pub-id><pub-id pub-id-type="other">2-s2.0-0033569406</pub-id><pub-id pub-id-type="pmid">10521349</pub-id></element-citation></ref><ref id="B2"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soria</surname><given-names>D.</given-names></name><name><surname>Garibaldi</surname><given-names>J. M.</given-names></name><name><surname>Ambrogi</surname><given-names>F.</given-names></name><name><surname>Biganzoli</surname><given-names>E. M.</given-names></name><name><surname>Ellis</surname><given-names>I. O.</given-names></name></person-group><article-title>A 'non-parametric' version of the naive Bayes classifier</article-title><source><italic>Knowledge-Based Systems</italic></source><year>2011</year><volume>24</volume><issue>6</issue><fpage>775</fpage><lpage>784</lpage><pub-id pub-id-type="other">2-s2.0-79957522106</pub-id><pub-id pub-id-type="doi">10.1016/j.knosys.2011.02.014</pub-id></element-citation></ref><ref id="B35"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>G. W.</given-names></name><name><surname>Simon</surname><given-names>R. M.</given-names></name></person-group><article-title>A random variance model for detection of differential gene expression in small microarray experiments</article-title><source><italic>Bioinformatics</italic></source><year>2003</year><volume>19</volume><issue>18</issue><fpage>2448</fpage><lpage>2455</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btg345</pub-id><pub-id pub-id-type="other">2-s2.0-0348143180</pub-id><pub-id pub-id-type="pmid">14668230</pub-id></element-citation></ref><ref id="B3"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>L.</given-names></name><name><surname>Cai</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Improving Tree augmented Naive Bayes for class probability estimation</article-title><source><italic>Knowledge-Based Systems</italic></source><year>2012</year><volume>26</volume><fpage>239</fpage><lpage>245</lpage><pub-id pub-id-type="other">2-s2.0-84155186550</pub-id><pub-id pub-id-type="doi">10.1016/j.knosys.2011.08.010</pub-id></element-citation></ref><ref id="B4"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balamurugan</surname><given-names>A. A.</given-names></name><name><surname>Rajaram</surname><given-names>R.</given-names></name><name><surname>Pramala</surname><given-names>S.</given-names></name><name><surname>Rajalakshmi</surname><given-names>S.</given-names></name><name><surname>Jeyendran</surname><given-names>C.</given-names></name><name><surname>Dinesh Surya Prakash</surname><given-names>J.</given-names></name></person-group><article-title>NB+: An improved Na&#x000ef;ve Bayesian algorithm</article-title><source><italic>Knowledge-Based Systems</italic></source><year>2011</year><volume>24</volume><issue>5</issue><fpage>563</fpage><lpage>569</lpage><pub-id pub-id-type="other">2-s2.0-79955038286</pub-id><pub-id pub-id-type="doi">10.1016/j.knosys.2010.09.007</pub-id></element-citation></ref><ref id="B14"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mollah</surname><given-names>M. N. H.</given-names></name><name><surname>Minami</surname><given-names>M.</given-names></name><name><surname>Eguchi</surname><given-names>S.</given-names></name></person-group><article-title>Exploring latent structure of mixture ICA models by the minimum <italic>&#x003b2;</italic>-divergence method</article-title><source><italic>Neural Computation</italic></source><year>2006</year><volume>18</volume><issue>1</issue><fpage>166</fpage><lpage>190</lpage><pub-id pub-id-type="other">2-s2.0-33645716208</pub-id><pub-id pub-id-type="doi">10.1162/089976606774841549</pub-id></element-citation></ref><ref id="B15"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mollah</surname><given-names>M. N. H.</given-names></name><name><surname>Eguchi</surname><given-names>S.</given-names></name><name><surname>Minami</surname><given-names>M.</given-names></name></person-group><article-title>Robust prewhitening for ICA by minimizing <italic>&#x003b2;</italic>-divergence and its application to FastICA</article-title><source><italic>Neural Processing Letters</italic></source><year>2007</year><volume>25</volume><issue>2</issue><fpage>91</fpage><lpage>110</lpage><pub-id pub-id-type="other">2-s2.0-33847314660</pub-id><pub-id pub-id-type="doi">10.1007/s11063-006-9023-8</pub-id></element-citation></ref><ref id="B16"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nurul Haque Mollah</surname><given-names>M.</given-names></name><name><surname>Sultana</surname><given-names>N.</given-names></name><name><surname>Minami</surname><given-names>M.</given-names></name><name><surname>Eguchi</surname><given-names>S.</given-names></name></person-group><article-title>Robust extraction of local structures by the minimum <italic>&#x003b2;</italic>-divergence method</article-title><source><italic>Neural Networks</italic></source><year>2010</year><volume>23</volume><issue>2</issue><fpage>226</fpage><lpage>238</lpage><pub-id pub-id-type="other">2-s2.0-73949119816</pub-id><pub-id pub-id-type="doi">10.1016/j.neunet.2009.11.011</pub-id><pub-id pub-id-type="pmid">19963342</pub-id></element-citation></ref><ref id="B10"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Randles</surname><given-names>R. H.</given-names></name><name><surname>Broffitt</surname><given-names>J. D.</given-names></name><name><surname>Ramberg</surname><given-names>J. S.</given-names></name><name><surname>Hogg</surname><given-names>R. V.</given-names></name></person-group><article-title>Generalized linear and quadratic discriminant functions using robust estimates</article-title><source><italic>Journal of the American Statistical Association</italic></source><year>1978</year><volume>73</volume><issue>363</issue><fpage>564</fpage><lpage>568</lpage><pub-id pub-id-type="other">2-s2.0-0010172861</pub-id><pub-id pub-id-type="doi">10.1080/01621459.1978.10480055</pub-id><pub-id pub-id-type="doi">10.2307/2286601</pub-id></element-citation></ref><ref id="B29"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maronna</surname><given-names>R. A.</given-names></name></person-group><article-title>Robust <italic>M</italic>-estimators of multivariate location and scatter</article-title><source><italic>The Annals of Statistics</italic></source><year>1976</year><volume>4</volume><issue>1</issue><fpage>51</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1214/aos/1176343347</pub-id><pub-id pub-id-type="other">MR0388656</pub-id></element-citation></ref><ref id="B12"><label>21</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>V.</given-names></name><name><surname>Neykov</surname><given-names>P.</given-names></name></person-group><article-title>Robust selection of variables in the discriminant analysis based on the mve and mcd estimators</article-title><conf-name>Proceedings of the Computational Statistics, COMPAST</conf-name><conf-date>1990</conf-date><conf-loc>Heidelberg, deu</conf-loc><publisher-name>Physica Verlag</publisher-name></element-citation></ref><ref id="B13"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>V.</given-names></name><name><surname>Neykov</surname><given-names>N.</given-names></name><name><surname>Neytchev</surname><given-names>P.</given-names></name></person-group><article-title>Robust two-group discrimination by bounded influence regression. A Monte Carlo simulation</article-title><source><italic>Computational Statistics and Data Analysis</italic></source><year>1994</year><volume>17</volume><issue>3</issue><fpage>289</fpage><lpage>302</lpage><pub-id pub-id-type="other">2-s2.0-38149147694</pub-id><pub-id pub-id-type="doi">10.1016/0167-9473(94)90122-8</pub-id></element-citation></ref><ref id="B11"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>V.</given-names></name><name><surname>Pires</surname><given-names>A. M.</given-names></name></person-group><article-title>Comparative performance of several robust linear discriminant analysis methods</article-title><source><italic>REVSTAT Statistical Journal</italic></source><year>2007</year><volume>5</volume><issue>1</issue><fpage>63</fpage><lpage>83</lpage><pub-id pub-id-type="other">MR2365933</pub-id></element-citation></ref><ref id="B5"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>X.</given-names></name><name><surname>Fung</surname><given-names>W. K.</given-names></name></person-group><article-title>High breakdown estimation for multiple populations with applications to discriminant analysis</article-title><source><italic>Journal of Multivariate Analysis</italic></source><year>2000</year><volume>72</volume><issue>2</issue><fpage>151</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1006/jmva.1999.1857</pub-id><pub-id pub-id-type="other">MR1740638</pub-id><pub-id pub-id-type="other">2-s2.0-0011908457</pub-id></element-citation></ref><ref id="B6"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubert</surname><given-names>M.</given-names></name><name><surname>Van Driessen</surname><given-names>K.</given-names></name></person-group><article-title>Fast and robust discriminant analysis</article-title><source><italic>Computational Statistics &#x00026; Data Analysis</italic></source><year>2004</year><volume>45</volume><issue>2</issue><fpage>301</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1016/S0167-9473(02)00299-2</pub-id><pub-id pub-id-type="other">MR2045634</pub-id></element-citation></ref><ref id="B32"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname><given-names>D. M.</given-names></name><name><surname>McLachlan</surname><given-names>G. J.</given-names></name></person-group><article-title>High-breakdown linear discriminant analysis</article-title><source><italic>Journal of the American Statistical Association</italic></source><year>1997</year><volume>92</volume><issue>437</issue><fpage>136</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.2307/2291457</pub-id><pub-id pub-id-type="other">MR1436102</pub-id><pub-id pub-id-type="other">Zbl0889.62052</pub-id><pub-id pub-id-type="other">2-s2.0-0031499747</pub-id></element-citation></ref><ref id="B7"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devi Arockia Vanitha</surname><given-names>C.</given-names></name><name><surname>Devaraj</surname><given-names>D.</given-names></name><name><surname>Venkatesulu</surname><given-names>M.</given-names></name></person-group><article-title>Gene expression data classification using Support Vector Machine and mutual information-based gene selection</article-title><source><italic>Procedia Computer Science</italic></source><year>2015</year><volume>47</volume><fpage>13</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2015.03.178</pub-id><pub-id pub-id-type="other">2-s2.0-84938595120</pub-id></element-citation></ref><ref id="B8"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parry</surname><given-names>R. M.</given-names></name><name><surname>Jones</surname><given-names>W.</given-names></name><name><surname>Stokes</surname><given-names>T. H.</given-names></name><etal/></person-group><article-title>K-Nearest neighbor models for microarray gene expression analysis and clinical outcome prediction</article-title><source><italic>Pharmacogenomics Journal</italic></source><year>2010</year><volume>10</volume><issue>4</issue><fpage>292</fpage><lpage>309</lpage><pub-id pub-id-type="other">2-s2.0-77955160229</pub-id><pub-id pub-id-type="doi">10.1038/tpj.2010.56</pub-id><pub-id pub-id-type="pmid">20676068</pub-id></element-citation></ref><ref id="B9"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>P. M.</given-names></name><name><surname>Vega</surname><given-names>V. B.</given-names></name></person-group><article-title>Boosting and microarray data</article-title><source><italic>Machine Learning</italic></source><year>2003</year><volume>52</volume><issue>1-2</issue><fpage>31</fpage><lpage>44</lpage><pub-id pub-id-type="other">2-s2.0-0038724544</pub-id><pub-id pub-id-type="doi">10.1023/A:1023937123600</pub-id></element-citation></ref><ref id="B1"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>N.</given-names></name><name><surname>Geiger</surname><given-names>D.</given-names></name><name><surname>Goldszmidt</surname><given-names>M.</given-names></name></person-group><article-title>Bayesian network classifiers</article-title><source><italic>Machine Learning</italic></source><year>1997</year><volume>29</volume><issue>2-3</issue><fpage>131</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1023/A:1007465528199</pub-id><pub-id pub-id-type="other">Zbl0892.68077</pub-id><pub-id pub-id-type="other">2-s2.0-0031276011</pub-id></element-citation></ref><ref id="B17"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mollah</surname><given-names>M. M. H.</given-names></name><name><surname>Mollah</surname><given-names>N. H.</given-names></name><name><surname>Kishino</surname><given-names>H.</given-names></name></person-group><article-title>
<italic>&#x003b2;</italic>-empirical Bayes inference and model diagnosis of microarray data</article-title><source><italic>BMC Bioinformatics</italic></source><year>2012</year><volume>13</volume><issue>1</issue><fpage>p. 135</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-13-135</pub-id><pub-id pub-id-type="other">2-s2.0-84862273610</pub-id><pub-id pub-id-type="pmid">22713095</pub-id></element-citation></ref><ref id="B18"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nowak</surname><given-names>G.</given-names></name><name><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group><article-title>Complementary hierarchical clustering</article-title><source><italic>Biostatistics</italic></source><year>2008</year><volume>9</volume><issue>3</issue><fpage>467</fpage><lpage>483</lpage><pub-id pub-id-type="other">2-s2.0-45849097530</pub-id><pub-id pub-id-type="doi">10.1093/biostatistics/kxm046</pub-id><pub-id pub-id-type="other">Zbl1143.62085</pub-id><pub-id pub-id-type="pmid">18093965</pub-id></element-citation></ref><ref id="B19"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuriakose</surname><given-names>M. A.</given-names></name><name><surname>Chen</surname><given-names>W. T.</given-names></name><name><surname>He</surname><given-names>Z. M.</given-names></name><etal/></person-group><article-title>Selection and validation of differentially expressed genes in head and neck cancer</article-title><source><italic>Cellular and Molecular Life Sciences</italic></source><year>2004</year><volume>61</volume><issue>11</issue><fpage>1372</fpage><lpage>1383</lpage><pub-id pub-id-type="other">2-s2.0-3042515568</pub-id><pub-id pub-id-type="doi">10.1007/s00018-004-4069-0</pub-id><pub-id pub-id-type="pmid">15170515</pub-id></element-citation></ref><ref id="B33"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergemann</surname><given-names>T. L.</given-names></name><name><surname>Wilson</surname><given-names>J.</given-names></name></person-group><article-title>Proportion statistics to detect differentially expressed genes: A comparison with log-ratio statistics</article-title><source><italic>BMC Bioinformatics</italic></source><year>2011</year><volume>12, article no. 228</volume><pub-id pub-id-type="other">2-s2.0-79957938857</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-12-228</pub-id></element-citation></ref><ref id="B34"><label>35</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gottardo</surname><given-names>R.</given-names></name></person-group><source><italic>Bridge: Bayesian Robust Inference for Differential Gene Expression, R package version 1.40.0</italic></source><year>2017</year></element-citation></ref><ref id="B37"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>T. W.</given-names></name><name><surname>Darling</surname><given-names>D. A.</given-names></name></person-group><article-title>Asymptotic theory of certain goodness of fit criteria based on stochastic processes</article-title><source><italic>Annals of Mathematical Statistics</italic></source><year>1952</year><volume>23</volume><fpage>193</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177729437</pub-id><pub-id pub-id-type="other">MR0050238</pub-id></element-citation></ref><ref id="B38"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>R.</given-names></name><name><surname>de la Torre</surname><given-names>L.</given-names></name><name><surname>Chang</surname><given-names>X.</given-names></name><name><surname>Mehrotra</surname><given-names>S.</given-names></name></person-group><article-title>Validation and characterization of DNA microarray gene expression data distribution and associated moments</article-title><source><italic>BMC Bioinformatics</italic></source><year>2010</year><volume>11, article no. 576</volume><pub-id pub-id-type="other">2-s2.0-78549252800</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-11-576</pub-id></element-citation></ref><ref id="B39"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hausser</surname><given-names>J.</given-names></name><name><surname>Strimmer</surname><given-names>K.</given-names></name></person-group><article-title>Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</article-title><source><italic>Journal of Machine Learning Research (JMLR)</italic></source><year>2009</year><volume>10</volume><fpage>1469</fpage><lpage>1484</lpage><pub-id pub-id-type="other">MR2534868</pub-id><pub-id pub-id-type="other">Zbl1235.62006</pub-id></element-citation></ref><ref id="B36"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Box</surname><given-names>G. E. P.</given-names></name><name><surname>Cox</surname><given-names>D. R.</given-names></name></person-group><article-title>An analysis of transformations</article-title><source><italic>Journal of the Royal Statistical Society. Series B</italic></source><year>1964</year><volume>26</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="other">MR0192611</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Misclassification error rate at different outlier levels: (a) 5% contamination rate, (b) 10% contamination rate, (c) 15% contamination rate, (d) 20% contamination rate, (e) 25% contamination rate, and (f) without contamination rate for the test dataset by the simulated dataset 1.</p></caption><graphic xlink:href="BMRI2017-3020627.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Simulation dataset 2 using data generating model (given in <xref ref-type="table" rid="tab1">Table 1</xref>): (a) training gene-set and (b) test gene-set, without contamination.</p></caption><graphic xlink:href="BMRI2017-3020627.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Classification results using (a) classical NBC and (b) proposed (<italic>&#x003b2;</italic>-NBC) method for the case without contamination based on the simulated dataset 2.</p></caption><graphic xlink:href="BMRI2017-3020627.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Classification results for the contaminated data: (a) contaminated test gene-set, (b) classified test gene-set by NBC, and (c) classified test gene-set by proposed method.</p></caption><graphic xlink:href="BMRI2017-3020627.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>ROC curve for the 2- (two-) class classification of different estimators at different percentage of outliers: (a) absence of outliers, (b) 5% outliers, (c) 10% outliers, (d) 15% outliers, (e) 20% outliers, and (f) 25% outliers.</p></caption><graphic xlink:href="BMRI2017-3020627.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Differentially expressed genes of head and neck cancer dataset using bridge.</p></caption><graphic xlink:href="BMRI2017-3020627.006"/></fig><fig id="fig7" orientation="portrait" position="float"><label>Figure 7</label><caption><p>HC Dendrogram for calculated first half of DE genes.</p></caption><graphic xlink:href="BMRI2017-3020627.007"/></fig><fig id="fig8" orientation="portrait" position="float"><label>Figure 8</label><caption><p>(a) Training gene data set; (b) test gene data set; (c) classification of gene data set by classical na&#x000ef;ve Bayes procedure; (d) classification of gene data set by proposed (<italic>&#x003b2;</italic>-na&#x000ef;ve Bayes) method.</p></caption><graphic xlink:href="BMRI2017-3020627.008"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Gene expression data generating model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" colspan="1">Gene group</th><th colspan="2" align="center" rowspan="1">Individual</th></tr><tr><th align="center" rowspan="1" colspan="1">Normal</th><th align="center" rowspan="1" colspan="1">Patient</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic>A</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>d</italic> + <italic>N</italic>(0, <italic>&#x003c3;</italic><sup>2</sup>)</td><td align="center" rowspan="1" colspan="1">&#x02212;<italic>d</italic> + <italic>N</italic>(0, <italic>&#x003c3;</italic><sup>2</sup>)</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic>B</italic>
</td><td align="center" rowspan="1" colspan="1">&#x02212;<italic>d</italic> + <italic>N</italic>(0, <italic>&#x003c3;</italic><sup>2</sup>)</td><td align="center" rowspan="1" colspan="1">
<italic>d</italic> + <italic>N</italic>(0, <italic>&#x003c3;</italic><sup>2</sup>)</td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Performance evaluation by different methods based on simulated dataset 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Prediction methods</th><th align="center" rowspan="1" colspan="1">NBC</th><th align="center" rowspan="1" colspan="1">SVM</th><th align="center" rowspan="1" colspan="1">
<italic>K</italic>NN</th><th align="center" rowspan="1" colspan="1">AdaBoost</th><th align="center" rowspan="1" colspan="1">Proposed</th><th align="center" rowspan="1" colspan="1">
<italic>p</italic> value</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Accuracy</td><td align="center" rowspan="1" colspan="1">0.55</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.82</td><td align="center" rowspan="1" colspan="1">
<bold>0.97</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">95% CI of accuracy</td><td align="center" rowspan="1" colspan="1">(0.45, 0.65)</td><td align="center" rowspan="1" colspan="1">(0.75, 0.90)</td><td align="center" rowspan="1" colspan="1">(0.77, 0.92)</td><td align="center" rowspan="1" colspan="1">(0.73, 0.89)</td><td align="center" rowspan="1" colspan="1">
<bold>(0.91</bold>, <bold>0.99)</bold></td><td align="center" rowspan="1" colspan="1">&#x02014;</td></tr><tr><td align="left" rowspan="1" colspan="1">Sensitivity</td><td align="center" rowspan="1" colspan="1">0.54</td><td align="center" rowspan="1" colspan="1">0.78</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">
<bold>0.95</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Specificity</td><td align="center" rowspan="1" colspan="1">0.62</td><td align="center" rowspan="1" colspan="1">0.94</td><td align="center" rowspan="1" colspan="1">0.97</td><td align="center" rowspan="1" colspan="1">0.76</td><td align="center" rowspan="1" colspan="1">
<bold>0.94</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">PPV</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.96</td><td align="center" rowspan="1" colspan="1">0.98</td><td align="center" rowspan="1" colspan="1">0.73</td><td align="center" rowspan="1" colspan="1">
<bold>0.94</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">NPV</td><td align="center" rowspan="1" colspan="1">0.20</td><td align="center" rowspan="1" colspan="1">0.71</td><td align="center" rowspan="1" colspan="1">0.73</td><td align="center" rowspan="1" colspan="1">0.91</td><td align="center" rowspan="1" colspan="1">
<bold>0.94</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Prevalence</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.63</td><td align="center" rowspan="1" colspan="1">0.63</td><td align="center" rowspan="1" colspan="1">0.41</td><td align="center" rowspan="1" colspan="1">
<bold>0.40</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Detection rate</td><td align="center" rowspan="1" colspan="1">0.45</td><td align="center" rowspan="1" colspan="1">0.49</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">0.37</td><td align="center" rowspan="1" colspan="1">
<bold>0.48</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Detection prevalence</td><td align="center" rowspan="1" colspan="1">0.51</td><td align="center" rowspan="1" colspan="1">0.51</td><td align="center" rowspan="1" colspan="1">0.51</td><td align="center" rowspan="1" colspan="1">0.51</td><td align="center" rowspan="1" colspan="1">
<bold>0.51</bold>
</td><td align="center" rowspan="1" colspan="1">&#x02014;</td></tr><tr><td align="left" rowspan="1" colspan="1">MCC</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.70</td><td align="center" rowspan="1" colspan="1">0.74</td><td align="center" rowspan="1" colspan="1">0.65</td><td align="center" rowspan="1" colspan="1">
<bold>0.94</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">MER</td><td align="center" rowspan="1" colspan="1">0.49</td><td align="center" rowspan="1" colspan="1">0.18</td><td align="center" rowspan="1" colspan="1">0.17</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">
<bold>0.03</bold>
</td><td align="center" rowspan="1" colspan="1">0.03</td></tr></tbody></table></table-wrap><table-wrap id="tab3" orientation="portrait" position="float"><label>Table 3</label><caption><p>Performance evaluation of different methods using average values of AUC, pAUC, and standard error of pAUC for two-class classification based on simulated dataset 3.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" rowspan="1">Two- (2-) class classification</th></tr><tr><th align="left" rowspan="1" colspan="1">Estimators</th><th align="center" rowspan="1" colspan="1">Average.AUCtest</th><th align="center" rowspan="1" colspan="1">SE.AUCtest</th><th align="center" rowspan="1" colspan="1">Average.pAUCtest</th><th align="center" rowspan="1" colspan="1">SE.pAUCtest</th></tr></thead><tbody><tr><td colspan="5" align="center" rowspan="1">Without outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.01</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.91</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.01</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.13</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">5% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.92</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.14</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.95</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.16</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.01</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">10% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.80</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.93</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.15</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.01</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">15% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.92</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.03</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.14</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">20% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.92</td><td align="center" rowspan="1" colspan="1">0.02</td><td align="center" rowspan="1" colspan="1">0.14</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.75</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.016</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.06</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.78</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.02</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.95</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.01</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.17</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.00</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">25% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.72</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.83</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.05</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.043</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.92 </bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.03</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.14</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.03</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="tab4" orientation="portrait" position="float"><label>Table 4</label><caption><p>Performance evaluation of different methods using average values of AUC, pAUC, and standard error of pAUC using dataset 3 for multiclass (3) classification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" rowspan="1">Multiclass (3) Class Classification</th></tr><tr><th align="left" rowspan="1" colspan="1">Estimators</th><th align="center" rowspan="1" colspan="1">Average.AUCtest</th><th align="center" rowspan="1" colspan="1">SE.AUCtest</th><th align="center" rowspan="1" colspan="1">Average.pAUCtest</th><th align="center" rowspan="1" colspan="1">SE.pAUCtest</th></tr></thead><tbody><tr><td colspan="5" align="center" rowspan="1">No outlier</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.89</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.90</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.03</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.13</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">5% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.82</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.87</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.88</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.03</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.12</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.01</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">10% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.77</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.82</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.87</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.04</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.12</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">15% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.76</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.82</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.83</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.12</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.11</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.86</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.04</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.11</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">20% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.67</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.80</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.03</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.82</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.09</td><td align="center" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.84</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.03</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.10</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.01</bold>
</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td colspan="5" align="center" rowspan="1">25% outliers</td></tr><tr><td colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Classical</td><td align="center" rowspan="1" colspan="1">0.72</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MVE</td><td align="center" rowspan="1" colspan="1">0.82</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">FSA</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-A</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-B</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">MCD-C</td><td align="center" rowspan="1" colspan="1">0.81</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">OGK</td><td align="center" rowspan="1" colspan="1">0.85</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.10</td><td align="center" rowspan="1" colspan="1">0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">
<bold>0.82 </bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.05</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.10</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.02</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="tab5" orientation="portrait" position="float"><label>Table 5</label><caption><p>Performance investigation using head and neck cancer data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Prediction methods</th><th align="center" rowspan="1" colspan="1">NBC</th><th align="center" rowspan="1" colspan="1">SVM</th><th align="center" rowspan="1" colspan="1">
<italic>K</italic>NN</th><th align="center" rowspan="1" colspan="1">AdaBoost</th><th align="center" rowspan="1" colspan="1">Proposed</th><th align="center" rowspan="1" colspan="1">
<italic>p</italic> value</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Accuracy</td><td align="center" rowspan="1" colspan="1">0.46</td><td align="center" rowspan="1" colspan="1">0.740</td><td align="center" rowspan="1" colspan="1">0.73</td><td align="center" rowspan="1" colspan="1">0.73</td><td align="center" rowspan="1" colspan="1">
<bold>0.76</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">95% CI of accuracy</td><td align="center" rowspan="1" colspan="1">(0.35, 0.56)</td><td align="center" rowspan="1" colspan="1">(0.63, 0.81)</td><td align="center" rowspan="1" colspan="1">(0.63, 0.81)</td><td align="center" rowspan="1" colspan="1">(0.63, 0.81)</td><td align="center" rowspan="1" colspan="1">
<bold>(0.75</bold>, <bold>0.79)</bold></td><td align="center" rowspan="1" colspan="1">&#x02014;</td></tr><tr><td align="left" rowspan="1" colspan="1">Sensitivity</td><td align="center" rowspan="1" colspan="1">0.46</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">
<bold>0.83</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Specificity</td><td align="center" rowspan="1" colspan="1">0.44</td><td align="center" rowspan="1" colspan="1">0.61</td><td align="center" rowspan="1" colspan="1">0.67</td><td align="center" rowspan="1" colspan="1">0.68</td><td align="center" rowspan="1" colspan="1">
<bold>0.77</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">PPV</td><td align="center" rowspan="1" colspan="1">0.62</td><td align="center" rowspan="1" colspan="1">0.59</td><td align="center" rowspan="1" colspan="1">0.56</td><td align="center" rowspan="1" colspan="1">0.62</td><td align="center" rowspan="1" colspan="1">
<bold>0.86</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">NPV</td><td align="center" rowspan="1" colspan="1">0.30</td><td align="center" rowspan="1" colspan="1">0.79</td><td align="center" rowspan="1" colspan="1">0.90</td><td align="center" rowspan="1" colspan="1">0.84</td><td align="center" rowspan="1" colspan="1">
<bold>0.87</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Prevalence</td><td align="center" rowspan="1" colspan="1">0.66</td><td align="center" rowspan="1" colspan="1">0.36</td><td align="center" rowspan="1" colspan="1">0.33</td><td align="center" rowspan="1" colspan="1">0.39</td><td align="center" rowspan="1" colspan="1">
<bold>0.43</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Detection rate</td><td align="center" rowspan="1" colspan="1">0.31</td><td align="center" rowspan="1" colspan="1">0.28</td><td align="center" rowspan="1" colspan="1">0.28</td><td align="center" rowspan="1" colspan="1">0.31</td><td align="center" rowspan="1" colspan="1">
<bold>0.43</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Detection prevalence</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">
<bold>0.50</bold>
</td><td align="center" rowspan="1" colspan="1">&#x02014;</td></tr><tr><td align="left" rowspan="1" colspan="1">Balanced accuracy</td><td align="center" rowspan="1" colspan="1">0.45</td><td align="center" rowspan="1" colspan="1">0.71</td><td align="center" rowspan="1" colspan="1">0.76</td><td align="center" rowspan="1" colspan="1">0.74</td><td align="center" rowspan="1" colspan="1">
<bold>0.93</bold>
</td><td align="center" rowspan="1" colspan="1">0.00</td></tr><tr><td align="left" rowspan="1" colspan="1">MCC</td><td align="center" rowspan="1" colspan="1">&#x02212;0.08</td><td align="center" rowspan="1" colspan="1">0.48</td><td align="center" rowspan="1" colspan="1">0.47</td><td align="center" rowspan="1" colspan="1">0.86</td><td align="center" rowspan="1" colspan="1">
<bold>0.90</bold>
</td><td align="center" rowspan="1" colspan="1">0.04</td></tr><tr><td align="left" rowspan="1" colspan="1">MER</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">0.27</td><td align="center" rowspan="1" colspan="1">0.25</td><td align="center" rowspan="1" colspan="1">0.08</td><td align="center" rowspan="1" colspan="1">
<bold>0.0</bold>
</td><td align="center" rowspan="1" colspan="1">0.05</td></tr></tbody></table></table-wrap></floats-group></article>