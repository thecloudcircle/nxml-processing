
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">21980334</article-id><article-id pub-id-type="pmc">3181247</article-id><article-id pub-id-type="publisher-id">PONE-D-11-04947</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0022885</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational Neuroscience</subject><subj-group><subject>Circuit Models</subject></subj-group></subj-group><subj-group><subject>Learning and Memory</subject><subject>Neural Networks</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Fine-Tuning and the Stability of Recurrent Neural Networks</article-title><alt-title alt-title-type="running-head">Fine-Tuning Recurrent Neural Networks</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>MacNeil</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author"><name><surname>Eliasmith</surname><given-names>Chris</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">
<sup>&#x0002a;</sup>
</xref></contrib></contrib-group><aff id="aff1">
<addr-line>Centre for Theoretical Neuroscience, University of Waterloo, Waterloo, Canada</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Vasilaki</surname><given-names>Eleni</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University of Sheffield, United Kingdom</aff><author-notes><corresp id="cor1">&#x0002a; E-mail: <email>celiasmith@uwaterloo.ca</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: CE DM. Performed the experiments: CE DM. Analyzed the data: DM. Contributed reagents/materials/analysis tools: CE. Wrote the paper: CE DM.</p></fn></author-notes><pub-date pub-type="collection"><year>2011</year></pub-date><pub-date pub-type="epub"><day>27</day><month>9</month><year>2011</year></pub-date><volume>6</volume><issue>9</issue><elocation-id>e22885</elocation-id><history><date date-type="received"><day>16</day><month>3</month><year>2011</year></date><date date-type="accepted"><day>6</day><month>7</month><year>2011</year></date></history><permissions><copyright-statement>MacNeil, Eliasmith.</copyright-statement><copyright-year>2011</copyright-year><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>A central criticism of standard theoretical approaches to constructing stable, recurrent model networks is that the synaptic connection weights need to be finely-tuned. This criticism is severe because proposed rules for learning these weights have been shown to have various limitations to their biological plausibility. Hence it is unlikely that such rules are used to continuously fine-tune the network <italic>in vivo</italic>. We describe a learning rule that is able to tune synaptic weights in a biologically plausible manner. We demonstrate and test this rule in the context of the oculomotor integrator, showing that only known neural signals are needed to tune the weights. We demonstrate that the rule appropriately accounts for a wide variety of experimental results, and is robust under several kinds of perturbation. Furthermore, we show that the rule is able to achieve stability as good as or better than that provided by the linearly optimal weights often used in recurrent models of the integrator. Finally, we discuss how this rule can be generalized to tune a wide variety of recurrent attractor networks, such as those found in head direction and path integration systems, suggesting that it may be used to tune a wide variety of stable neural systems.</p></abstract><counts><page-count count="16"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Persistent neural activity is typically characterized as a sustained increase in neural firing, sometimes lasting up to several seconds, and usually following a brief stimulus. It has been thought to underlie a wide variety of neural computations, including the integration of velocity commands <xref rid="pone.0022885-Robinson1" ref-type="bibr">[1]</xref>, <xref rid="pone.0022885-Seung1" ref-type="bibr">[2]</xref>, the reduction of noise <xref rid="pone.0022885-Pouget1" ref-type="bibr">[3]</xref>, tracking head direction <xref rid="pone.0022885-Goodridge1" ref-type="bibr">[4]</xref>, <xref rid="pone.0022885-Redish1" ref-type="bibr">[5]</xref>, maximizing probabilities <xref rid="pone.0022885-Deneve1" ref-type="bibr">[6]</xref>, and storing working memories <xref rid="pone.0022885-Hopfield1" ref-type="bibr">[7]</xref>, <xref rid="pone.0022885-Brody1" ref-type="bibr">[8]</xref>, <xref rid="pone.0022885-Singh1" ref-type="bibr">[9]</xref>. The most common theoretical solution for realizing persistent activity is to introduce recurrent connections into a network model <xref rid="pone.0022885-LorenteDeN1" ref-type="bibr">[10]</xref>, <xref rid="pone.0022885-Hebb1" ref-type="bibr">[11]</xref>, <xref rid="pone.0022885-Amit1" ref-type="bibr">[12]</xref>
<xref rid="pone.0022885-Goldman1" ref-type="bibr">[13]</xref>. Recently, methods have been proposed which generalize this kind of solution to any neural representation with countable degrees of freedom <xref rid="pone.0022885-Eliasmith1" ref-type="bibr">[14]</xref>.</p><p>However, as demonstrated by <xref rid="pone.0022885-Seung2" ref-type="bibr">[15]</xref>, precise tuning of recurrent connection weights is required to achieve appropriate persistent activity in this class of simple recurrent networks. A similar observation was made earlier in numerical simulations by <xref rid="pone.0022885-Zhang1" ref-type="bibr">[16]</xref>. Specifically, in the oculomotor integrator, which has long been a central experimental target for characterizing persistent activity in a biological setting <xref rid="pone.0022885-Robinson1" ref-type="bibr">[1]</xref>, <xref rid="pone.0022885-Seung1" ref-type="bibr">[2]</xref>, <xref rid="pone.0022885-Major1" ref-type="bibr">[17]</xref>, <xref rid="pone.0022885-Nikitchenko1" ref-type="bibr">[18]</xref>, it is known that the precision of the recurrent weights required to induce drifts slow enough to match the observed behavior is quite high <xref rid="pone.0022885-Mensh1" ref-type="bibr">[19]</xref>. It has been shown that the stability of the oculomotor integrator can only be achieved by tuning the weights to within 1&#x00025; of the theoretical ideal. The 1&#x00025; accuracy refers to the accuracy of tuning the unity eigenvalue of the recurrent weight matrix. It can also be expressed as the ratio of the physical connection time constant, <inline-formula><inline-graphic xlink:href="pone.0022885.e001.jpg" mimetype="image"/></inline-formula>, to system time constant <xref rid="pone.0022885-Seung1" ref-type="bibr">[2]</xref>. As a result of this small 1&#x00025; margin, it has been suggested that the physiological processes necessary to support such fine-tuning might not be available <xref rid="pone.0022885-Koulakov1" ref-type="bibr">[20]</xref>. To achieve the observed stability, various alternative mechanisms have been explored. For instance, <xref rid="pone.0022885-Fransn1" ref-type="bibr">[21]</xref> provide evidence for a single cell mechanism that relies on cholinergic modulation. However, it is unclear if this is plausible outside of the entorhinal cortex. As well, bistability <xref rid="pone.0022885-Fransn1" ref-type="bibr">[21]</xref>, <xref rid="pone.0022885-Nikitchenko1" ref-type="bibr">[18]</xref>, and multiple layers of feed-forward connections <xref rid="pone.0022885-Goldman1" ref-type="bibr">[13]</xref> have been proposed as possible mechanisms. However, the evidence supporting these more exotic possibilities in the relevant neural systems is quite weak <xref rid="pone.0022885-Goldman1" ref-type="bibr">[13]</xref>.</p><p>Consequently, it is an open problem as to how real neurobiological systems produce the observed stability. The most direct answer to this question &#x02013; that there are learning mechanisms for fine-tuning &#x02013; has also seemed implausible. Several models that have adopted such an approach require a retinal slip signal in order to tune the integrator <xref rid="pone.0022885-Arnold1" ref-type="bibr">[22]</xref>, <xref rid="pone.0022885-Arnold2" ref-type="bibr">[23]</xref>, <xref rid="pone.0022885-Turaga1" ref-type="bibr">[24]</xref>. A retinal slip signal is generated by comparing the movement of the eyes to the movement of an image on the retina. If the retinal image is moving, but the eyes (and the rest of the body) are not, an error signal is generated by the oculomotor system. However, this signal is not explicitly available to the neural integrator with known connectivity, and cannot account for development of the integrator in the dark <xref rid="pone.0022885-Collewijn1" ref-type="bibr">[25]</xref>, <xref rid="pone.0022885-Harris1" ref-type="bibr">[26]</xref>, or the role of proprioceptive feedback <xref rid="pone.0022885-Seung3" ref-type="bibr">[27]</xref>. Other models require an entirely non-physiological algorithm <xref rid="pone.0022885-Eliasmith2" ref-type="bibr">[28]</xref>, or are not able to appropriately adapt to distortions in the visual feedback <xref rid="pone.0022885-Arnold3" ref-type="bibr">[29]</xref>, <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>. Other accounts, that address neural stability more generally <xref rid="pone.0022885-Durstewitz1" ref-type="bibr">[31]</xref>, <xref rid="pone.0022885-Renart1" ref-type="bibr">[32]</xref>, have not yet been shown to apply to the oculomotor integrator, and may not have the resources to do so (see <xref ref-type="sec" rid="s4">Discussion</xref>).</p><p>Here we propose a learning rule that is able to account for available plasticity results, while being biologically plausible. Specifically, we demonstrate that our proposed rule: 1) fine-tunes the connection weights to values able to reproduce experimentally observed behavior; 2) explains the mis-tuning of the neural integrator under various conditions; and 3) relies only on known inputs to the system. We also suggest a generalization of this rule that may be exploited by a wide variety of neural systems to induce stability in higher-dimensional spaces, like those possibly used in the head-direction and path integration systems in the rat <xref rid="pone.0022885-Zhang2" ref-type="bibr">[33]</xref>, <xref rid="pone.0022885-Redish1" ref-type="bibr">[5]</xref>, <xref rid="pone.0022885-Conklin1" ref-type="bibr">[34]</xref>, <xref rid="pone.0022885-Eliasmith1" ref-type="bibr">[14]</xref>.</p></sec><sec sec-type="materials|methods" id="s2"><title>Materials and Methods</title><sec id="s2a"><title>The optimal neural integrator</title><p>To understand the results and genesis of the proposed learning rule, it is useful to begin with a standard theoretical characterization of an attractor network. The &#x0201c;optimal&#x0201d; neural integrator model used in this study is constructed using the Neural Engineering Framework (NEF) methods described in <xref rid="pone.0022885-Eliasmith2" ref-type="bibr">[28]</xref>. We refer to the network model as &#x0201c;optimal&#x0201d; because the NEF relies on the linear optimization to determine the connection weights (as described below). The resulting connection weights are similar to those derived by other methods <xref rid="pone.0022885-Seung1" ref-type="bibr">[2]</xref>, <xref rid="pone.0022885-Seung2" ref-type="bibr">[15]</xref>, <xref rid="pone.0022885-Miller1" ref-type="bibr">[35]</xref>, such that all such methods generate stable integrators. However, the learning rule is derived using the NEF formulation.</p><p>For simplicity, each neuron in the integrator is modeled as a spiking leaky integrate-and-fire (LIF) neuron, though little depends on this choice of neuron model <xref rid="pone.0022885-Eliasmith2" ref-type="bibr">[28]</xref>. The sub-threshold evolution of the LIF neuron voltage is described by<disp-formula><graphic xlink:href="pone.0022885.e002.jpg" mimetype="image" position="float"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="pone.0022885.e003.jpg" mimetype="image"/></inline-formula> is the voltage across the membrane, <inline-formula><inline-graphic xlink:href="pone.0022885.e004.jpg" mimetype="image"/></inline-formula> is the input current, <inline-formula><inline-graphic xlink:href="pone.0022885.e005.jpg" mimetype="image"/></inline-formula> is the passive membrane resistance, and <inline-formula><inline-graphic xlink:href="pone.0022885.e006.jpg" mimetype="image"/></inline-formula> is the membrane time constant. When the membrane voltage crosses a threshold <inline-formula><inline-graphic xlink:href="pone.0022885.e007.jpg" mimetype="image"/></inline-formula>, a spike is emitted, and the cell is reset to its resting state for a time period equal to the refractory time constant <inline-formula><inline-graphic xlink:href="pone.0022885.e008.jpg" mimetype="image"/></inline-formula>. The output activity of the cell is thus represented as a train of delta functions, placed at the times of spikes <inline-formula><inline-graphic xlink:href="pone.0022885.e009.jpg" mimetype="image"/></inline-formula> as <inline-formula><inline-graphic xlink:href="pone.0022885.e010.jpg" mimetype="image"/></inline-formula>. The spiking response of the cell is thus a nonlinear function of the input current <inline-formula><inline-graphic xlink:href="pone.0022885.e011.jpg" mimetype="image"/></inline-formula>, that is<disp-formula><graphic xlink:href="pone.0022885.e012.jpg" mimetype="image" position="float"/></disp-formula>where <inline-formula><inline-graphic xlink:href="pone.0022885.e013.jpg" mimetype="image"/></inline-formula> indicates the neuron model response function.</p><p>The interactions between neurons are captured by allowing spikes generated by neurons to elicit post-synaptic currents (PSCs) in the dendrites of neurons to which they project. The PSCs are modeled as exponentially decaying with a time constant of <inline-formula><inline-graphic xlink:href="pone.0022885.e014.jpg" mimetype="image"/></inline-formula>:<disp-formula><graphic xlink:href="pone.0022885.e015.jpg" mimetype="image" position="float"/><label>(2)</label></disp-formula>For the models presented here, we assume a <inline-formula><inline-graphic xlink:href="pone.0022885.e016.jpg" mimetype="image"/></inline-formula> of 100 ms, which accounts for the decay of NMDA receptor PSCs, as is typical in oculomotor models <xref rid="pone.0022885-Seung2" ref-type="bibr">[15]</xref>, <xref rid="pone.0022885-Koulakov1" ref-type="bibr">[20]</xref>. Notably, we have not included saturation in our model of the synapses. It has been suggested that even with long NMDA receptor time constants, there are plausible synaptic models that do not suffer significantly from saturation effects <xref rid="pone.0022885-Seung4" ref-type="bibr">[36]</xref>. At high firing rates, small effects from saturation are evident in such models in the form of a slight roll-off of the tuning curve. This roll-off is similar to that observed when the membrane time constant of the cells is decreased. We have found our rule to provide similar results for these kinds of tuning curves (results not shown). Nevertheless, the effects of saturation, and other cellular dynamics are not captured directly by our single cell and synaptic model.</p><p>The total current flowing into the soma of a receiving cell from the dendrites, <inline-formula><inline-graphic xlink:href="pone.0022885.e017.jpg" mimetype="image"/></inline-formula>, is thus determined by the input spike trains <inline-formula><inline-graphic xlink:href="pone.0022885.e018.jpg" mimetype="image"/></inline-formula> coming from connected neurons, that are filtered by the PSCs elicited by those spikes, and weighted by a connection weight between the receiving neuron and the input neurons <inline-formula><inline-graphic xlink:href="pone.0022885.e019.jpg" mimetype="image"/></inline-formula>:<disp-formula><graphic xlink:href="pone.0022885.e020.jpg" mimetype="image" position="float"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="pone.0022885.e021.jpg" mimetype="image"/></inline-formula> is the number of spikes from each of the <inline-formula><inline-graphic xlink:href="pone.0022885.e022.jpg" mimetype="image"/></inline-formula> neurons connected to the receiving neuron. The somatic current then causes the receiving neuron to spike, as determined by the LIF model, and the resulting spikes are passed to connected downstream neurons. This process is depicted in <xref ref-type="fig" rid="pone-0022885-g001">Figure 1a</xref>.</p><fig id="pone-0022885-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g001</object-id><label>Figure 1</label><caption><title>Model neurons used in the network.</title><p>a) The dynamics of a model neuron coupled to a PSC model provides the complete model of a single cell. Spikes arrive, are filtered by a weighted post-synaptic current and then drive a spiking nonlinearity. b) Tuning curves for 40 simulated goldfish neurons with a cellular membrane time constant, <inline-formula><inline-graphic xlink:href="pone.0022885.e023.jpg" mimetype="image"/></inline-formula>, of <inline-formula><inline-graphic xlink:href="pone.0022885.e024.jpg" mimetype="image"/></inline-formula> ms and a refractory period of <inline-formula><inline-graphic xlink:href="pone.0022885.e025.jpg" mimetype="image"/></inline-formula> ms. Maximum firing rates were picked from an even distribution ranging from 20 to 100 Hz. Direction intercepts were picked from an even distribution between &#x02212;50 and 50 degrees. The neurons were evenly split between positive and negative gains, determined by a randomly assigned encoding weight <inline-formula><inline-graphic xlink:href="pone.0022885.e026.jpg" mimetype="image"/></inline-formula>.</p></caption><graphic xlink:href="pone.0022885.g001"/></fig><p>To use this cellular model to perform integration it is essential to determine the appropriate recurrent connection weights <inline-formula><inline-graphic xlink:href="pone.0022885.e027.jpg" mimetype="image"/></inline-formula>. However, it is necessary to do so in light of the particular distribution of cellular responses found in the biological integrator. Here, we focus on the neurons involved in controlling horizontal eye movements, to make the problem 1-dimensional. In mammals, the horizontal oculomotor integrator is found in the nuclei prepositus hypoglossi (NPH). While it is possible to find characterizations of the cellular responses of these neurons <xref rid="pone.0022885-McCrea1" ref-type="bibr">[37]</xref>, the very similar, but much simpler, oculomotor system of the goldfish is our focus of study, as it is one of the best studied oculomotor systems and has thus been more fully characterized. The cells controlling horizontal eye position in the goldfish are found in the reticular column. It is generally agreed that the goldfish integrator is a good model for the mammalian integrator despite the difference in size of the corresponding networks <xref rid="pone.0022885-Seung2" ref-type="bibr">[15]</xref>, <xref rid="pone.0022885-Mensh1" ref-type="bibr">[19]</xref>.</p><p>In both mammals and fish, the relevant network of cells receives projections from earlier parts of the brain that provide a velocity command to update eye position. In addition, many of the cells in the network are connected to one another, making it naturally modeled as a recurrent network. This network turns the velocity command into an eye position command, and projects the result to the motor neurons which directly affect the relevant muscles. Thus, our model circuit consists of one population of recurrently connected neurons, which receives a velocity input signal <inline-formula><inline-graphic xlink:href="pone.0022885.e028.jpg" mimetype="image"/></inline-formula> and generates a signal representing the eye position <inline-formula><inline-graphic xlink:href="pone.0022885.e029.jpg" mimetype="image"/></inline-formula>.</p><p>To construct the model, we begin with an ensemble of 40 neurons (approximately the number found in the goldfish integrator), which have firing curves randomly distributed to reflect known tuning in the goldfish <xref rid="pone.0022885-Seung2" ref-type="bibr">[15]</xref>. This neural population is taken to represent <inline-formula><inline-graphic xlink:href="pone.0022885.e030.jpg" mimetype="image"/></inline-formula>, the actual position of the eye. This variable is encoded by the neural population using an encoding weight <inline-formula><inline-graphic xlink:href="pone.0022885.e031.jpg" mimetype="image"/></inline-formula>, to account for directional sensitivity of the neurons. Neurons in this area have monotonically increasing firing either leftwards (<inline-formula><inline-graphic xlink:href="pone.0022885.e032.jpg" mimetype="image"/></inline-formula>) or rightwards (<inline-formula><inline-graphic xlink:href="pone.0022885.e033.jpg" mimetype="image"/></inline-formula>). To fit the observed heterogeneity of neuron tuning in this area, we use a gain factor <inline-formula><inline-graphic xlink:href="pone.0022885.e034.jpg" mimetype="image"/></inline-formula>. We account for the observed background firing rates of the neurons by introducing a bias current <inline-formula><inline-graphic xlink:href="pone.0022885.e035.jpg" mimetype="image"/></inline-formula>. As a result of these considerations, for any neuron <inline-formula><inline-graphic xlink:href="pone.0022885.e036.jpg" mimetype="image"/></inline-formula> in the population, the activity produced by the neuron is given by<disp-formula><graphic xlink:href="pone.0022885.e037.jpg" mimetype="image" position="float"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="pone.0022885.e038.jpg" mimetype="image"/></inline-formula> is the LIF non-linearity described by Equation 1. In essence, Equation 4 defines how eye position information is encoded into the spike patterns of the neural population.</p><p>To determine what aspects of that information are available to subsequent neurons from this activity (i.e., to determine what is represented), we need to find a decoder <inline-formula><inline-graphic xlink:href="pone.0022885.e039.jpg" mimetype="image"/></inline-formula>. For consistency with the standard cellular model described earlier (<xref ref-type="fig" rid="pone-0022885-g001">Figure 1a</xref>), we take these decoders to be linear. This assumption, which is equivalent to having linear dendrites, is shared with most integrator models.</p><p>Optimal linear decoders can be found by minimizing the difference between the represented eye position <inline-formula><inline-graphic xlink:href="pone.0022885.e040.jpg" mimetype="image"/></inline-formula> and the actual eye position <inline-formula><inline-graphic xlink:href="pone.0022885.e041.jpg" mimetype="image"/></inline-formula> over the relevant range (see the next section):<disp-formula><graphic xlink:href="pone.0022885.e042.jpg" mimetype="image" position="float"/><label>(5)</label></disp-formula>where<disp-formula><graphic xlink:href="pone.0022885.e043.jpg" mimetype="image" position="float"/><label>(6)</label></disp-formula>The activities, <inline-formula><inline-graphic xlink:href="pone.0022885.e044.jpg" mimetype="image"/></inline-formula> in this equation are the time-average of the filtered activity <inline-formula><inline-graphic xlink:href="pone.0022885.e045.jpg" mimetype="image"/></inline-formula> (from Equation 3) for a constant input. For the population in <xref ref-type="fig" rid="pone-0022885-g001">Figure 1b</xref>, the optimization range is <inline-formula><inline-graphic xlink:href="pone.0022885.e046.jpg" mimetype="image"/></inline-formula> degrees and the resulting root-mean-square (RMS) error of this decoding over that range is 0.134 degrees over the 100 degrees of movement. Identifying both the encoding (Equation 4) and decoding (Equation 6) of interest provides a characterization of the time-varying representation of eye position for the population of neurons.</p><p>For the neural integrator model, it is also essential to determine how to recurrently connect the population to result in stable dynamics. <xref rid="pone.0022885-Eliasmith1" ref-type="bibr">[14]</xref> has shown how to determine these connection weights for arbitrary attractors. We adopt that method here, for the simple 1D case. Consider the activity of the population of neurons at a future moment in time, <inline-formula><inline-graphic xlink:href="pone.0022885.e047.jpg" mimetype="image"/></inline-formula>. To avoid confusion, let us index that activity by <inline-formula><inline-graphic xlink:href="pone.0022885.e048.jpg" mimetype="image"/></inline-formula>; i.e., <inline-formula><inline-graphic xlink:href="pone.0022885.e049.jpg" mimetype="image"/></inline-formula>. The encoding, from Equation 4, for <inline-formula><inline-graphic xlink:href="pone.0022885.e050.jpg" mimetype="image"/></inline-formula> is thus<disp-formula><graphic xlink:href="pone.0022885.e051.jpg" mimetype="image" position="float"/><label>(7)</label></disp-formula>At the present moment, the representation of eye position <inline-formula><inline-graphic xlink:href="pone.0022885.e052.jpg" mimetype="image"/></inline-formula>, given by the decoding of the neuron activities is<disp-formula><graphic xlink:href="pone.0022885.e053.jpg" mimetype="image" position="float"/><label>(8)</label></disp-formula>Since the system should be stationary without any input, it should be the case that <inline-formula><inline-graphic xlink:href="pone.0022885.e054.jpg" mimetype="image"/></inline-formula> at all positions. To enforce this constraint, we substitute Equation 8 into Equation 7, giving<disp-formula><graphic xlink:href="pone.0022885.e055.jpg" mimetype="image" position="float"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="pone.0022885.e056.jpg" mimetype="image"/></inline-formula>. We refer to the model with these weights as the &#x0201c;linear optimal&#x0201d; model, since the weights are determined by a linear least squares optimization of Equation 5.</p><p>A network with these recurrent weights will attempt to hold the present representation of eye position as long as there is no additional input. However, even given optimal weights there are many reasons that the eye position will drift. These include representational error introduced by the nonlinearities in the encoding, fluctuations in the representation of eye position, due to the non-steady nature of filtered spike trains, and the many sources of noise attributed to neural systems <xref rid="pone.0022885-Stevens1" ref-type="bibr">[38]</xref>, <xref rid="pone.0022885-Henneman1" ref-type="bibr">[39]</xref>, <xref rid="pone.0022885-Lass1" ref-type="bibr">[40]</xref>. Nevertheless, a circuit with these weights can do an excellent job as an integrator, and its performance matches well to the known properties of biological integrators <xref rid="pone.0022885-Eliasmith2" ref-type="bibr">[28]</xref>.</p><p>Note also that this network will mathematically integrate its input. If we inject additional current into the neural population, it acts as an extra change in the eye position, and will be added to the representation of eye position. Additional input will thus be summed over time (i.e., integrated) until it stops, at which point the system will attempt to hold the new representation of eye position. In short, an input proportional to eye velocity will be integrated to drive the circuit to a new eye position. The stable representation of eye position by this circuit for different velocity inputs is discussed in the <xref ref-type="sec" rid="s3">Results</xref> section.</p></sec><sec id="s2b"><title>Derivation of optimal decoders</title><p>To complete our discussion of the optimal neural integrator, in this section we describe the methods used to compute optimal linear decoders <inline-formula><inline-graphic xlink:href="pone.0022885.e057.jpg" mimetype="image"/></inline-formula> in equation 6. For generality we follow the NEF methods to determination optimal decoders under noise <xref rid="pone.0022885-Eliasmith2" ref-type="bibr">[28]</xref>. Specifically, we assume that the noise <inline-formula><inline-graphic xlink:href="pone.0022885.e058.jpg" mimetype="image"/></inline-formula> is drawn from a Gaussian, independent, identically distributed, zero mean distribution. The noise is added to the neuron activity <inline-formula><inline-graphic xlink:href="pone.0022885.e059.jpg" mimetype="image"/></inline-formula> resulting in a decoding of<disp-formula><graphic xlink:href="pone.0022885.e060.jpg" mimetype="image" position="float"/><label>(10)</label></disp-formula>To find the least squares optimal <inline-formula><inline-graphic xlink:href="pone.0022885.e061.jpg" mimetype="image"/></inline-formula>, we construct and minimize the mean square error, averaging over the expected noise and <inline-formula><inline-graphic xlink:href="pone.0022885.e062.jpg" mimetype="image"/></inline-formula>:<disp-formula><graphic xlink:href="pone.0022885.e063.jpg" mimetype="image" position="float"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="pone.0022885.e064.jpg" mimetype="image"/></inline-formula> indicates integration over the range of <inline-formula><inline-graphic xlink:href="pone.0022885.e065.jpg" mimetype="image"/></inline-formula>. This can be thought of as multiple linear regression. Because the noise is independent on each neuron, the noise averages out except when <inline-formula><inline-graphic xlink:href="pone.0022885.e066.jpg" mimetype="image"/></inline-formula>. So, the average of the <inline-formula><inline-graphic xlink:href="pone.0022885.e067.jpg" mimetype="image"/></inline-formula> noise is equal to the variance <inline-formula><inline-graphic xlink:href="pone.0022885.e068.jpg" mimetype="image"/></inline-formula> of the noise on the neurons. Thus, the error with noise becomes<disp-formula><graphic xlink:href="pone.0022885.e069.jpg" mimetype="image" position="float"/><label>(12)</label></disp-formula>Taking the derivative of the error gives<disp-formula><graphic xlink:href="pone.0022885.e070.jpg" mimetype="image" position="float"/><label>(13)</label></disp-formula>Setting the derivative to zero gives<disp-formula><graphic xlink:href="pone.0022885.e071.jpg" mimetype="image" position="float"/><label>(14)</label></disp-formula>or, in matrix form,<disp-formula><graphic xlink:href="pone.0022885.e072.jpg" mimetype="image" position="float"/></disp-formula>The decoding weights <inline-formula><inline-graphic xlink:href="pone.0022885.e073.jpg" mimetype="image"/></inline-formula> are given by<disp-formula><graphic xlink:href="pone.0022885.e074.jpg" mimetype="image" position="float"/></disp-formula>where<disp-formula><graphic xlink:href="pone.0022885.e075.jpg" mimetype="image" position="float"/></disp-formula>
<disp-formula><graphic xlink:href="pone.0022885.e076.jpg" mimetype="image" position="float"/></disp-formula>Notice that the <inline-formula><inline-graphic xlink:href="pone.0022885.e077.jpg" mimetype="image"/></inline-formula> matrix is guaranteed to be non-singular, hence invertible, because of the noise term on the diagonal. In all simulations presented here the noise was taken to have a normalized variance of 0.1.</p></sec><sec id="s2c"><title>Derivation of the learning rule</title><p>Plasticity in the neural integrator is evident across a wide variety of species, and there is strong evidence that modification of retinal slip information is able to cause the oculomotor integrator to become unstable or damped <xref rid="pone.0022885-Mensh1" ref-type="bibr">[19]</xref>, <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>. Additional support for the role of tuning in the oculomotor neural integrator in humans comes from evidence of tuning within two months of birth <xref rid="pone.0022885-Weissman1" ref-type="bibr">[41]</xref>, mis-tuning in subjects with developed blindness <xref rid="pone.0022885-Kmpf1" ref-type="bibr">[42]</xref>, and induced drift after training <xref rid="pone.0022885-Kapoula1" ref-type="bibr">[43]</xref>. While evidence from experiments with dark-reared animals has shown some development of the integrator without visual feedback <xref rid="pone.0022885-Collewijn1" ref-type="bibr">[25]</xref>, <xref rid="pone.0022885-Harris1" ref-type="bibr">[26]</xref>, ocular stability improves when animals are provided visual feedback. Consequently, there is good evidence that some form of adaptation is active in the oculomotor integrator, and it is plausible that such adaptation would be able to support fine-tuning.</p><p>The goal of this study is to determine a biologically plausible learning rule that is able to perform integration as well as the linear optimal network described above. The learning rule derived here is based on the idea that integrators should be able to exploit the corrective input signals they receive. Empirical evidence indicates that all input at the integrator itself is in the form of velocity commands <xref rid="pone.0022885-Aksay1" ref-type="bibr">[44]</xref>. While the nucleus of the optic tract has retinal slip information, it encodes this into a velocity signal when it projects to the neural integrator <xref rid="pone.0022885-Ikezu1" ref-type="bibr">[45]</xref>. Consequently, there is no explicit retinal slip signal, as assumed by past learning rules <xref rid="pone.0022885-Turaga1" ref-type="bibr">[24]</xref>, <xref rid="pone.0022885-Arnold2" ref-type="bibr">[23]</xref>.</p><p>In the oculomotor integrator, there is evidence of two classes of input: <italic>intentional</italic> and <italic>corrective</italic> saccades <xref rid="pone.0022885-Weber1" ref-type="bibr">[46]</xref>, <xref rid="pone.0022885-Glasser1" ref-type="bibr">[47]</xref>. <xref rid="pone.0022885-Park1" ref-type="bibr">[48]</xref> have argued that corrective saccades, and <italic>not</italic> an explicit retinal slip error, cause adaption in saccade magnitude. There are several characteristics of saccadic commands that can be used to distinguish between corrective and intentional saccades, including magnitude of velocity or change in position (see <xref ref-type="fig" rid="pone-0022885-g002">Figure 2</xref>). Because the eye is generally in the neighborhood of its target for corrective saccades, corrective saccade velocities tend to be smaller. And, since saccade magnitude is proportional to maximum saccade velocity <xref rid="pone.0022885-Leigh1" ref-type="bibr">[49]</xref>, it is possible to filter saccadic velocity commands based on magnitude to identify corrective saccades. The algorithm used to filter saccade velocity <inline-formula><inline-graphic xlink:href="pone.0022885.e078.jpg" mimetype="image"/></inline-formula> to give corrective saccades <inline-formula><inline-graphic xlink:href="pone.0022885.e079.jpg" mimetype="image"/></inline-formula> in this model is<disp-formula><graphic xlink:href="pone.0022885.e080.jpg" mimetype="image" position="float"/><label>(15)</label></disp-formula>That is, the corrective saccade signal consist of all velocities less than 200 degrees per second.</p><fig id="pone-0022885-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g002</object-id><label>Figure 2</label><caption><title>Two methods for filtering saccade commands.</title><p>a) Eye position for a series of saccades. b) The saccade velocity, based on a). c) Filtering based on magnitude. This method uses Equation 15 to filter the velocity profile. This is the method adopted for all subsequent experiments. d) Filtering based on a change in position, where a change in position greater than 5 degree allows the subsequent velocity commands to pass through at a magnitude inversely proportional to the time elapsed after a movement.</p></caption><graphic xlink:href="pone.0022885.g002"/></fig><p>Furthermore <xref rid="pone.0022885-Seung3" ref-type="bibr">[27]</xref>, explains that retinal slip alone cannot account for learning in the dark and cannot incorporate proprioceptive feedback, which has some role in the long term adaption of ocular control <xref rid="pone.0022885-Lewis1" ref-type="bibr">[50]</xref>. An algorithm based on a corrective velocity signal has the potential to work with retinal slip, efferent feedback, and proprioceptive feedback, since any of these may drive a corrective eye movement. Small corrective saccades are known to occur in the dark <xref rid="pone.0022885-Hess1" ref-type="bibr">[51]</xref>.</p><p>Nevertheless, retinal slip plays an important role in the overall system. In most models of the oculomotor system, including the one we adopt below, corrective saccades are generated on the basis of retinal slip information. If the retinal image is moving, but there have been no self-generated movements (i.e., the retinal image is &#x0201c;slipping&#x0201d;), the system will generate corrective velocity commands to eliminate the slip. Consequently, the integrator itself has only indirect access to retinal slip information. Below, we show that this is sufficient to drive an appropriate learning rule.</p><p>Before turning to the rule itself, it is useful to first consider what is entailed by the claim that the system must be finely tuned. An integrator is able to maintain persistent activity when the sum of current from feedback connections is equal to the amount of current required to exactly represent the eye position in an open loop system. If the eye position representation determined by the feedback current and the actual eye position are plotted on normalized axes, the mapping for a perfect integrator would define a line of slope 1 though the origin (see <xref ref-type="fig" rid="pone-0022885-g003">Figure 3</xref>). This line is called the system transfer function, since it describes how the current state is transferred to future states (through feedback). A slope of 1 in the neural integrator thus indicates that the recurrent input generates exactly enough current at any given eye position to make up for the normal leak of current through the neuron membrane. In short, it means that a perfect line attractor has been achieved by the network.</p><fig id="pone-0022885-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g003</object-id><label>Figure 3</label><caption><title>Transfer functions of actual versus represented eye position for tuned, damped and unstable networks.</title><p>Eye position is normalized to lie on a range of <inline-formula><inline-graphic xlink:href="pone.0022885.e081.jpg" mimetype="image"/></inline-formula>. An exact integrator has a slope of 1, a damped integrator has a slope less than 1, and an unstable integrator has a slope greater than 1. Compare to <xref ref-type="fig" rid="pone-0022885-g009">Figure 9b</xref>.</p></caption><graphic xlink:href="pone.0022885.g003"/></fig><p>However, if the magnitude of the feedback is less than what is needed, the represented eye position will drift towards zero. This is indicated by the slope of the system transfer function being less than 1. Such systems are said to be dynamically damped. Conversely, if the feedback is greater than needed, the slope of the transfer function is greater than 1 and the system output will drift away from zero. Such systems are said to be dynamically unstable (see <xref ref-type="fig" rid="pone-0022885-g003">Figure 3</xref>).</p><p>As described earlier, the representation of eye position given by equation 8 has a definite error (for the neurons depicted in <xref ref-type="fig" rid="pone-0022885-g001">Figure 1</xref>, the RMSE is 0.134 degrees). Consequently, a perfect attractor (with slope 1) will not be achievable at all eye positions. Nevertheless, it is clear from the derivation of the linear optimal integrator that changing the decoding weights <inline-formula><inline-graphic xlink:href="pone.0022885.e082.jpg" mimetype="image"/></inline-formula> (and hence the connection weights <inline-formula><inline-graphic xlink:href="pone.0022885.e083.jpg" mimetype="image"/></inline-formula>) is equivalent to changing the represented value of the eye position in the network. Hence, changing these weights will allow us to more or less accurately approximate an exact integrator.</p><p>Given this background, it is possible to derive a learning rule that minimizes the difference between the neural representation of eye position <inline-formula><inline-graphic xlink:href="pone.0022885.e084.jpg" mimetype="image"/></inline-formula> and the actual position <inline-formula><inline-graphic xlink:href="pone.0022885.e085.jpg" mimetype="image"/></inline-formula>. Importantly, the available corrective saccade <inline-formula><inline-graphic xlink:href="pone.0022885.e086.jpg" mimetype="image"/></inline-formula> provides information about the direction in which minimization should proceed. Specifically, if <inline-formula><inline-graphic xlink:href="pone.0022885.e087.jpg" mimetype="image"/></inline-formula> is positive the estimate must be increased so as to move towards <inline-formula><inline-graphic xlink:href="pone.0022885.e088.jpg" mimetype="image"/></inline-formula>; if <inline-formula><inline-graphic xlink:href="pone.0022885.e089.jpg" mimetype="image"/></inline-formula> is negative the estimate must be decreased. More formally, we can express the error we would like to minimize as<disp-formula><graphic xlink:href="pone.0022885.e090.jpg" mimetype="image" position="float"/></disp-formula>Substituting the neural representation from Equation 8 into this expression, and then minimizing it by differentiating with respect to the decoding weights <inline-formula><inline-graphic xlink:href="pone.0022885.e091.jpg" mimetype="image"/></inline-formula> gives<disp-formula><graphic xlink:href="pone.0022885.e092.jpg" mimetype="image" position="float"/></disp-formula>
<disp-formula><graphic xlink:href="pone.0022885.e093.jpg" mimetype="image" position="float"/></disp-formula>where the subscript <inline-formula><inline-graphic xlink:href="pone.0022885.e094.jpg" mimetype="image"/></inline-formula> indexes the whole population and <inline-formula><inline-graphic xlink:href="pone.0022885.e095.jpg" mimetype="image"/></inline-formula> indexes the neuron currently being optimized. Note, however, that in a recurrent network <inline-formula><inline-graphic xlink:href="pone.0022885.e096.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0022885.e097.jpg" mimetype="image"/></inline-formula> are indexing the same neurons. In addition, the connection weight dependent on <inline-formula><inline-graphic xlink:href="pone.0022885.e098.jpg" mimetype="image"/></inline-formula> is in the postsynpatic neuron <inline-formula><inline-graphic xlink:href="pone.0022885.e099.jpg" mimetype="image"/></inline-formula>. So, despite the fact that the equation is written as an optimization of <inline-formula><inline-graphic xlink:href="pone.0022885.e100.jpg" mimetype="image"/></inline-formula>, the resulting learning rule is used to tune weights in neurons <inline-formula><inline-graphic xlink:href="pone.0022885.e101.jpg" mimetype="image"/></inline-formula> to which <inline-formula><inline-graphic xlink:href="pone.0022885.e102.jpg" mimetype="image"/></inline-formula> projects.</p><p>Importantly, it is now possible to substitute for the bracketed term using the negative of the corrective saccade. This substitution can be made because <inline-formula><inline-graphic xlink:href="pone.0022885.e103.jpg" mimetype="image"/></inline-formula> is generated by the oculomotor system so as to be proportional to, but in the opposite direction of, the difference expressed by this term (i.e., the difference between the actual and represented eye positions). Performing this substitution gives<disp-formula><graphic xlink:href="pone.0022885.e104.jpg" mimetype="image" position="float"/></disp-formula>Converting this into standard delta rule form, and including the learning rate parameter <inline-formula><inline-graphic xlink:href="pone.0022885.e105.jpg" mimetype="image"/></inline-formula>, gives<disp-formula><graphic xlink:href="pone.0022885.e106.jpg" mimetype="image" position="float"/><label>(16)</label></disp-formula>This rule indicates how the decoders themselves should change in order to minimize the error.</p><p>Unfortunately, this rule is neither in terms of the connection weights of the circuit, nor local. These two concerns can be alleviated by multiplying both sides of the expression by the encoder and gain of neurons <inline-formula><inline-graphic xlink:href="pone.0022885.e107.jpg" mimetype="image"/></inline-formula>, which receive projections from neuron <inline-formula><inline-graphic xlink:href="pone.0022885.e108.jpg" mimetype="image"/></inline-formula>
<disp-formula><graphic xlink:href="pone.0022885.e109.jpg" mimetype="image" position="float"/></disp-formula>
<disp-formula><graphic xlink:href="pone.0022885.e110.jpg" mimetype="image" position="float"/><label>(17)</label></disp-formula>The final learning rule in Equation 17 addresses both concerns. First, the NEF characterization of connection weights guarantees that the substitution of <inline-formula><inline-graphic xlink:href="pone.0022885.e111.jpg" mimetype="image"/></inline-formula> by <inline-formula><inline-graphic xlink:href="pone.0022885.e112.jpg" mimetype="image"/></inline-formula> is appropriate given the definitions of the terms (as derived in Equation 9).</p><p>Second, the right-hand side of Equation 17 is in a pseudo-Hebbian form: there is a learning rate <inline-formula><inline-graphic xlink:href="pone.0022885.e113.jpg" mimetype="image"/></inline-formula>, pre-synaptic activity <inline-formula><inline-graphic xlink:href="pone.0022885.e114.jpg" mimetype="image"/></inline-formula>, and post-synaptic activity <inline-formula><inline-graphic xlink:href="pone.0022885.e115.jpg" mimetype="image"/></inline-formula>. This last term is the effect of the corrective saccade on the somatic current of post-synaptic neuron <inline-formula><inline-graphic xlink:href="pone.0022885.e116.jpg" mimetype="image"/></inline-formula>, as described by Equation 7. Notably, this term is not the firing of a receiving neuron, but rather the subthreshold current that drives such firing (hence &#x0201c;pseudo&#x0201d; Hebbian). In other words, the same current used to drive the spiking activity of the neuron is used to update the connection weights. Consistent with this rule, it has been suggested in experimental work that post-synaptic potentials are not necessary for plasticity <xref rid="pone.0022885-Hardie1" ref-type="bibr">[52]</xref>.</p><p>However, the current and the activity are highly correlated, as the <inline-formula><inline-graphic xlink:href="pone.0022885.e117.jpg" mimetype="image"/></inline-formula> inputs must drive the neurons over threshold in order to cause the corrective saccades. Consequently, the appropriate correlations between pre- and post-synaptic firing are observed, but the postsynpatic firing does not strictly cause weight changes. As well, the rule only applies when the error term <inline-formula><inline-graphic xlink:href="pone.0022885.e118.jpg" mimetype="image"/></inline-formula> is non-zero. Hence, the corrective-saccade acts as a kind of &#x0201c;gate&#x0201d; for the connection weight changes. As a result, most accurately, the rule can be considered as a gated pseudo-Hebbian rule.</p><p>Finally, it should be noted that the integrator subject to this rule is driven by all velocity inputs as usual. Both corrective and intentional saccades determine the firing of the neurons in the integrator, and are integrated by the circuit. The mechanism that distinguishes these two kinds of saccades (<xref ref-type="fig" rid="pone-0022885-g002">figure 2</xref>), only acts to gate the learning itself, not the neural responses.</p><p>Overall, the resulting rule is biologically plausible, using only information available to neuron <inline-formula><inline-graphic xlink:href="pone.0022885.e119.jpg" mimetype="image"/></inline-formula>. This is because neuron <inline-formula><inline-graphic xlink:href="pone.0022885.e120.jpg" mimetype="image"/></inline-formula>: 1) receives a projection from neuron <inline-formula><inline-graphic xlink:href="pone.0022885.e121.jpg" mimetype="image"/></inline-formula>; 2) is able to update the weight <inline-formula><inline-graphic xlink:href="pone.0022885.e122.jpg" mimetype="image"/></inline-formula>; and 3) responds to input velocities, including <inline-formula><inline-graphic xlink:href="pone.0022885.e123.jpg" mimetype="image"/></inline-formula>, via its tuning (Equation 7). More importantly, there is no use of non-saccadic inputs (such as retinal slip). The conjunction of these properties distinguishes this rule from past proposals. We demonstrate a detailed application of this rule to the tuning of the neural integrator in the <xref ref-type="sec" rid="s3">Results</xref> section.</p></sec><sec id="s2d"><title>Generalization of the learning rule</title><p>There have been similar learning rules proposed in the literature. For example <xref rid="pone.0022885-Turaga1" ref-type="bibr">[24]</xref>, propose a learning rule that uses retinal slip in place of the corrective saccades, but has essentially the same mathematical form. They also demonstrate convergence of their rule with a Lyapunov function. In an earlier cerebellar model <xref rid="pone.0022885-Porrill1" ref-type="bibr">[53]</xref>, propose a learning rule in which an error provided by climbing fibers is used to tune the weight between incoming parallel fibers and Purkinje cells. This rule, too, has a similar mathematical form. So, we take the novelty of the proposed rule to lie more in its biological mapping than its mathematical form. In both previous models, there is an error signal provided on a different channel than the processed input. We have avoided this assumption, which is empirically more consistent with the circuitry of the oculomotor circuit, as described earlier.</p><p>More generally, there has been a wide variety of work examining Hebbian-like reinforcement learning (also called reward modulated Hebbian learning) that propose rules with a similar mathematical form to Equation 17 <xref rid="pone.0022885-Montague1" ref-type="bibr">[54]</xref>, <xref rid="pone.0022885-Rao1" ref-type="bibr">[55]</xref>, <xref rid="pone.0022885-Vasilaki1" ref-type="bibr">[56]</xref>. They are similar in the sense that the weight change is a product of an error signal, presynaptic activity and post-synaptic activity. These rules all rely on a scalar error signal that is used to drive learning. Typically this error is taken to be the reinforcement learning prediction error. But other signals are used as well. For example <xref rid="pone.0022885-Turaga1" ref-type="bibr">[24]</xref>, considers the scalar retinal slip as error, and <xref rid="pone.0022885-Porrill1" ref-type="bibr">[53]</xref> assume each parallel fibre carries a single scalar value and gets an indication of the motor error. The rule we present in Equation 17 is also only applied to scalars.</p><p>However, we can extend past work by taking advantage of the NEF decomposition used in the derivation of the previous rule. In particular, the decomposition makes it clear how we can generalize the simple rule we have derived from learning scalar functions to learning arbitrary vector functions. Consider a derivation analogous to that above, which directly replaces encoding and decoding weights (<inline-formula><inline-graphic xlink:href="pone.0022885.e124.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0022885.e125.jpg" mimetype="image"/></inline-formula>) with encoding and decoding vectors (<inline-formula><inline-graphic xlink:href="pone.0022885.e126.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0022885.e127.jpg" mimetype="image"/></inline-formula>), and replaces corrective saccades (<inline-formula><inline-graphic xlink:href="pone.0022885.e128.jpg" mimetype="image"/></inline-formula>) with a generalized error signal (<inline-formula><inline-graphic xlink:href="pone.0022885.e129.jpg" mimetype="image"/></inline-formula>). This results in a general learning rule that can be expressed as<disp-formula><graphic xlink:href="pone.0022885.e130.jpg" mimetype="image" position="float"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="pone.0022885.e131.jpg" mimetype="image"/></inline-formula> is a generalized error term (in place of <inline-formula><inline-graphic xlink:href="pone.0022885.e132.jpg" mimetype="image"/></inline-formula>, which is generated by the saccadic system).</p><p>The encoding vector <inline-formula><inline-graphic xlink:href="pone.0022885.e133.jpg" mimetype="image"/></inline-formula> can be thought of as a generalization of the &#x0201c;preferred direction&#x0201d; vector characterized by <xref rid="pone.0022885-Georgopoulos1" ref-type="bibr">[57]</xref>. Past work has shown how this generalization of the representation can capture many forms of neural representation throughout cortical and subcortical regions <xref rid="pone.0022885-Eliasmith2" ref-type="bibr">[28]</xref>. Thus, for such representations, Equation 18 suggests that the projection of an error vector onto the encoding vector can be exploited to affect weight changes of the relevant neuron. Intuitively, this suggests that the error in a vector space that can be accounted for by a given neuron, gated by its input activity, influences the relevant connection weight. This is a natural mechanism for ensuring that the neuron reduces the error that its activity affects. We demonstrate the application of this generalized learning rule to higher dimensional vector spaces after considering the oculomotor case in detail.</p></sec><sec id="s2e"><title>The oculomotor system model</title><p>Previous learning models of the oculomotor integrator <xref rid="pone.0022885-Arnold1" ref-type="bibr">[22]</xref>, <xref rid="pone.0022885-Arnold2" ref-type="bibr">[23]</xref>, <xref rid="pone.0022885-Turaga1" ref-type="bibr">[24]</xref> require a retinal slip signal to drive the learning algorithm. While this signal is available to higher centers in the brain, it does not project directly to the neural integrator. Therefore, to develop a plausible learning algorithm, it is crucial to accurately model the input to each neuron. The main component of this input is the velocity signal projected to the neural integrator. Because the generation of these velocity commands is complex in itself, it is beyond the focus of the current study. As a result, we adopt the model of the oculomotor system (OMS) developed by Dell'Osso's group <xref rid="pone.0022885-Jacobs1" ref-type="bibr">[58]</xref>, <xref rid="pone.0022885-DellOsso1" ref-type="bibr">[59]</xref>, <xref rid="pone.0022885-Wang1" ref-type="bibr">[60]</xref> to provide realistic velocity input signals. The OMS model, along with a complete description, is available for download at <ext-link ext-link-type="uri" xlink:href="http://omlab.org/software/software.html">http://omlab.org/software/software.html</ext-link>.</p><p>The OMS model contains saccadic, smooth pursuit, and fixation subsystems controlled by an internal monitor. The model uses retinal signals and an efferent copy of the motor output signals to generate motor control commands. It includes the simulation of plant dynamics, and has parameters to simulate normal ocular behavior as well as several disorders. For this study, all parameters were set for normal, healthy ocular behavior.</p><p>To test our learning algorithm, we replaced the neural integrator of the OMS model with the spiking integrator model described above. To compare the tuning of our network to the experimental results of <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>, it was necessary to modify the retinal feedback path of the OMS model to allow for the simulation of moving surroundings (see <xref ref-type="sec" rid="s3">Results</xref>). Input to the OMS model was a target position randomly selected to lie between <inline-formula><inline-graphic xlink:href="pone.0022885.e134.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0022885.e135.jpg" mimetype="image"/></inline-formula> degrees. A new position was selected once every 4 s.</p></sec><sec id="s2f"><title>Simulations</title><p>The neural integrator in this study was constructed in Simulink and embedded into the OMS model. The OMS model is available at <ext-link ext-link-type="uri" xlink:href="http://omlab.org/software/software.html">http://omlab.org/software/software.html</ext-link>, and the model used in this study is available at <ext-link ext-link-type="uri" xlink:href="http://compneuro.uwaterloo.ca/cnrglab/f/NIdemo.zip">http://compneuro.uwaterloo.ca/cnrglab/f/NIdemo.zip</ext-link>. A time step of <inline-formula><inline-graphic xlink:href="pone.0022885.e136.jpg" mimetype="image"/></inline-formula> ms was used along with the first order ODE solver provided by Simulink. All simulations were run on networks of 40 neurons for 1200 s (20 minutes) of simulated time. All inputs to the model were eye position targets chosen at random from an interval of <inline-formula><inline-graphic xlink:href="pone.0022885.e137.jpg" mimetype="image"/></inline-formula> degrees, once every 4 s. At the input and output of the integrator, the signals were normalized to a range of &#x02212;1 to 1 corresponding to eye position of <inline-formula><inline-graphic xlink:href="pone.0022885.e138.jpg" mimetype="image"/></inline-formula> to <inline-formula><inline-graphic xlink:href="pone.0022885.e139.jpg" mimetype="image"/></inline-formula> degrees. All results were collected after the 1200 s run, at which point network weights were frozen (i.e., there was no learning after 1200 s and during data collection).</p><p>The learning rule used a value of <inline-formula><inline-graphic xlink:href="pone.0022885.e140.jpg" mimetype="image"/></inline-formula> to update the weights at every time step. The value of <inline-formula><inline-graphic xlink:href="pone.0022885.e141.jpg" mimetype="image"/></inline-formula> was selected by iteratively testing the model with different values of <inline-formula><inline-graphic xlink:href="pone.0022885.e142.jpg" mimetype="image"/></inline-formula> and selecting one which allowed the connection weights to converge quickly without inducing large fluctuations in the representational error. The learning rate was kept constant across all simulations.</p><p>To appropriately characterize the behavior of the model, each simulation experiment consisted of running 30 trials each with a different, randomly generated network, allowing the collection of appropriate statistics. For each trial, a new set of tuning curves for the neurons, and a new set of input functions, were randomly generated. The parameters of the tuning curves were determined based on an even distribution of <italic>x</italic>-intercepts over <inline-formula><inline-graphic xlink:href="pone.0022885.e143.jpg" mimetype="image"/></inline-formula> degrees, maximum firing rates picked from an even distribution ranging from 20 to 100 Hz, and a random assignment of half of the neurons to positive and negative encoding weights <inline-formula><inline-graphic xlink:href="pone.0022885.e144.jpg" mimetype="image"/></inline-formula>. All neurons had a cellular membrane time constant, <inline-formula><inline-graphic xlink:href="pone.0022885.e145.jpg" mimetype="image"/></inline-formula>, of <inline-formula><inline-graphic xlink:href="pone.0022885.e146.jpg" mimetype="image"/></inline-formula> ms and a refractory period of <inline-formula><inline-graphic xlink:href="pone.0022885.e147.jpg" mimetype="image"/></inline-formula> ms. All recurrent connections had a post-synaptic current time constant of 100 ms, and were modelled with a decaying exponential.</p><p>Ten different experiments were run in this manner. The first was the linear optimal integrator described above. The connections between the neurons in the linear optimal network are defined by Equation 9. All subsequent experiments start from these weights unless otherwise specified.</p><p>Several experiments add noise to the connection weights of the linear optimal integrator over time. Noise was added to the connection weight matrix <inline-formula><inline-graphic xlink:href="pone.0022885.e148.jpg" mimetype="image"/></inline-formula> as<disp-formula><graphic xlink:href="pone.0022885.e149.jpg" mimetype="image" position="float"/><label>(19)</label></disp-formula>at each time step for a duration of 1200 s. The noise matrix <inline-formula><inline-graphic xlink:href="pone.0022885.e150.jpg" mimetype="image"/></inline-formula> is equal to a matrix <inline-formula><inline-graphic xlink:href="pone.0022885.e151.jpg" mimetype="image"/></inline-formula> randomly selected from a normal distribution <inline-formula><inline-graphic xlink:href="pone.0022885.e152.jpg" mimetype="image"/></inline-formula> and scaled by an appropriate standard deviation <inline-formula><inline-graphic xlink:href="pone.0022885.e153.jpg" mimetype="image"/></inline-formula> and number of time steps <inline-formula><inline-graphic xlink:href="pone.0022885.e154.jpg" mimetype="image"/></inline-formula>: i.e., <inline-formula><inline-graphic xlink:href="pone.0022885.e155.jpg" mimetype="image"/></inline-formula>. Thus, the noise is added as a standard Wiener process (i.e., Brownian motion).</p><p>In experiment 2, <inline-formula><inline-graphic xlink:href="pone.0022885.e156.jpg" mimetype="image"/></inline-formula> was a noise matrix with <inline-formula><inline-graphic xlink:href="pone.0022885.e157.jpg" mimetype="image"/></inline-formula>, adding 30&#x00025; noise over the 1200 s. Consequently, at the end of the 1200 s run using noise accumulated according to equation 19, the weights were perturbed by about 30&#x00025; of their original value.</p><p>The third experiment consisted of allowing the learning rule to operate on the connection weights of the integrator networks from experiment 2. That is, after being run with the above noise and no learning for 1200 s (resulting in 30&#x00025; noise), the learning rule (and no additional noise) was run for 1200 s. The fourth experiment allowed the integrator to learn while noise was continuously added to the original optimal network weights. Noise was added in the same manner as equation 19, but concurrently with learning. In this case <inline-formula><inline-graphic xlink:href="pone.0022885.e158.jpg" mimetype="image"/></inline-formula> (i.e., 10&#x00025; noise) was added over 1200 s. The fifth experiment allowed the integrator to learn with a combination of an initial disturbance of 30&#x00025; noise (after a 1200 s run) to the optimal weights and another 5&#x00025; of continuously added noise while the rule was being used. The sixth experiment examined the effects of learning starting from the linear optimal integrator, but with no noise added to the weights at all.</p><p>Experiments seven and eight were run to reproduce the results of <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>. In this study, goldfish were fixed in an aquarium where the background was controlled by a servo mechanism. The servo mechanism was programmed to rotate the background at a speed equal to eye position multiplied by a predefined gain. If the gain was in the positive direction, the network became unstable. If gain was in the negative direction, the network became damped. In our study, we directly manipulated the retinal slip feedback to simulate a moving background. Because rotation of the background in the positive direction would give the illusion of slip in the negative direction, the retinal slip in our study was modified by a gain with the opposite sign to the experimental study. We used gains of <inline-formula><inline-graphic xlink:href="pone.0022885.e159.jpg" mimetype="image"/></inline-formula> (damped) and <inline-formula><inline-graphic xlink:href="pone.0022885.e160.jpg" mimetype="image"/></inline-formula> (unstable), which compare with <inline-formula><inline-graphic xlink:href="pone.0022885.e161.jpg" mimetype="image"/></inline-formula> to <inline-formula><inline-graphic xlink:href="pone.0022885.e162.jpg" mimetype="image"/></inline-formula> in with original study. The gains were selected to be lower than a point where they caused erratic behavior which inhibited learning (also noted by <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>). We suspect larger gains were possible in the experiments because the gain operated on an external background rather than retinal slip directly. This retinal slip signal is provided directly to the OMS model, which generates the appropriate oculomotor responses that drive the integrator.</p><p>The ninth and tenth experiments demonstrate that the rule is able to account for recovery from lesions <xref rid="pone.0022885-Arnold3" ref-type="bibr">[29]</xref>. Specifically, experiment nine shows the effect of removing a randomly chosen neuron from the network. The resulting network thus has 39 neurons. Experiment ten examines the stability of the response after applying the learning rule to the lesioned network while introducing continuous 5&#x00025; noise.</p></sec><sec id="s2g"><title>Measuring drift</title><p>Two benchmarks were used to quantify the performance of the neural integrator in these experiments. The first was root-mean-square error (RMSE) between the plot of actual feedback and the exact integrator (i.e., a line of slope 1 through the origin). This is determined by comparing the represented eye position for each possible input to the actual position given that input, and taking the difference. This provides an estimate of the representational error caused by one forward pass through the neural integrator. As a result, this error is measured in degrees. The lower this error, the slower the integrator will drift over time on average.</p><p>The second measure was the time constant, <inline-formula><inline-graphic xlink:href="pone.0022885.e163.jpg" mimetype="image"/></inline-formula>, based on the average <inline-formula><inline-graphic xlink:href="pone.0022885.e164.jpg" mimetype="image"/></inline-formula> calculated from a best fit of an exponential to the response of the integrator after input pulses with a width of <inline-formula><inline-graphic xlink:href="pone.0022885.e165.jpg" mimetype="image"/></inline-formula> s, and heights of &#x02212;2, &#x02212;1, 1, and 2. This provides four evenly distributed sample drift points for each network, which are averaged to provide the final estimate.</p><p>Data was collected for 30 randomly generated networks (i.e., neuron parameters are randomly chosen as described above) and used to calculate a mean and 95&#x00025; confidence interval (using bootstrapping with 10,000 samples) for both RMSE and <inline-formula><inline-graphic xlink:href="pone.0022885.e166.jpg" mimetype="image"/></inline-formula>. For the calculation of <inline-formula><inline-graphic xlink:href="pone.0022885.e167.jpg" mimetype="image"/></inline-formula>, the absolute value was used to calculate the mean and confidence interval, and the sign was later found by summing <inline-formula><inline-graphic xlink:href="pone.0022885.e168.jpg" mimetype="image"/></inline-formula> over the 30 trials.</p></sec></sec><sec id="s3"><title>Results</title><sec id="s3a"><title>Application of the learning rule to the oculomotor integrator</title><p>To demonstrate the effectiveness of the proposed learning rule (equation 17), we present the results of the ten experiments in order to benchmark the system and reproduce a variety of plasticity observations in the oculomotor system.</p><p>The summary results of the ten experiments are shown in <xref ref-type="table" rid="pone-0022885-t001">Table 1</xref>. The time course of various example networks are described subsequently. All results in the table are averaged over 30 network models with randomly chosen neuron properties (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). <xref ref-type="fig" rid="pone-0022885-g004">Figure 4</xref> reproduces these results as a bar graph, for visual comparison. In each case, the mean and 95&#x00025; confidence intervals are presented.</p><fig id="pone-0022885-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g004</object-id><label>Figure 4</label><caption><title>Bar graphs for the experiments described in the main text.</title><p>a) RMSE and b) the magnitude of <inline-formula><inline-graphic xlink:href="pone.0022885.e169.jpg" mimetype="image"/></inline-formula> for each experiment. The error bars indicate the 95&#x00025; confidence intervals as reported in <xref ref-type="table" rid="pone-0022885-t001">Table 1</xref>.</p></caption><graphic xlink:href="pone.0022885.g004"/></fig><table-wrap id="pone-0022885-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.t001</object-id><label>Table 1</label><caption><title>RMSE and system time constant (<inline-formula><inline-graphic xlink:href="pone.0022885.e170.jpg" mimetype="image"/></inline-formula>) for the experiments described in the main text.</title></caption><alternatives><graphic id="pone-0022885-t001-1" xlink:href="pone.0022885.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td colspan="2" align="left" rowspan="1">RMSE (degrees)</td><td colspan="2" align="left" rowspan="1">
<inline-formula><inline-graphic xlink:href="pone.0022885.e171.jpg" mimetype="image"/></inline-formula> (s)</td></tr><tr><td colspan="2" align="left" rowspan="1">Experiment</td><td align="left" rowspan="1" colspan="1">Mean</td><td align="left" rowspan="1" colspan="1">CI</td><td align="left" rowspan="1" colspan="1">Mean</td><td align="left" rowspan="1" colspan="1">CI</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">Optimal</td><td align="left" rowspan="1" colspan="1">0.129</td><td align="left" rowspan="1" colspan="1">0.115&#x02013;0.138</td><td align="left" rowspan="1" colspan="1">(&#x0002b;) 41.4</td><td align="left" rowspan="1" colspan="1">31.2&#x02013;55.6</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">Noisy</td><td align="left" rowspan="1" colspan="1">2.156</td><td align="left" rowspan="1" colspan="1">1.693&#x02013;2.699</td><td align="left" rowspan="1" colspan="1">(&#x0002b;) 10.6</td><td align="left" rowspan="1" colspan="1">5.85&#x02013;18.2</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">Learned&#x0002b;Perturb<xref ref-type="table-fn" rid="nt101">1</xref>
</td><td align="left" rowspan="1" colspan="1">0.671</td><td align="left" rowspan="1" colspan="1">0.312&#x02013;1.178</td><td align="left" rowspan="1" colspan="1">(&#x0002b;) 98.7</td><td align="left" rowspan="1" colspan="1">58.5&#x02013;153</td></tr><tr><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">Learned&#x0002b;Noise<xref ref-type="table-fn" rid="nt102">2</xref>
</td><td align="left" rowspan="1" colspan="1">0.712</td><td align="left" rowspan="1" colspan="1">0.595&#x02013;0.854</td><td align="left" rowspan="1" colspan="1">(&#x02212;) 31.6</td><td align="left" rowspan="1" colspan="1">13.5&#x02013;60.1</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">Learned&#x0002b;Perturb&#x0002b;Noise<xref ref-type="table-fn" rid="nt103">3</xref>
</td><td align="left" rowspan="1" colspan="1">1.120</td><td align="left" rowspan="1" colspan="1">0.606&#x02013;1.838</td><td align="left" rowspan="1" colspan="1">(&#x0002b;) 41.4</td><td align="left" rowspan="1" colspan="1">18.9&#x02013;78.8</td></tr><tr><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">Learned&#x0002b;NoNoise<xref ref-type="table-fn" rid="nt104">4</xref>
</td><td align="left" rowspan="1" colspan="1">0.183</td><td align="left" rowspan="1" colspan="1">0.170&#x02013;0.193</td><td align="left" rowspan="1" colspan="1">(&#x0002b;) 122</td><td align="left" rowspan="1" colspan="1">88.1&#x02013;165</td></tr><tr><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">Unstable</td><td align="left" rowspan="1" colspan="1">0.382</td><td align="left" rowspan="1" colspan="1">0.364&#x02013;0.395</td><td align="left" rowspan="1" colspan="1">(&#x02212;) 15.5</td><td align="left" rowspan="1" colspan="1">13.8&#x02013;17.1</td></tr><tr><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">Damped</td><td align="left" rowspan="1" colspan="1">0.313</td><td align="left" rowspan="1" colspan="1">0.294&#x02013;0.329</td><td align="left" rowspan="1" colspan="1">(&#x0002b;) 10.9</td><td align="left" rowspan="1" colspan="1">9.19&#x02013;13</td></tr><tr><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">Lesion</td><td align="left" rowspan="1" colspan="1">0.824</td><td align="left" rowspan="1" colspan="1">0.561&#x02013;1.142</td><td align="left" rowspan="1" colspan="1">(&#x0002b;) 30.8</td><td align="left" rowspan="1" colspan="1">20.2&#x02013;46.2</td></tr><tr><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">Recovery</td><td align="left" rowspan="1" colspan="1">0.513</td><td align="left" rowspan="1" colspan="1">0.359&#x02013;0.716</td><td align="left" rowspan="1" colspan="1">(&#x02212;) 51.3</td><td align="left" rowspan="1" colspan="1">25.4&#x02013;88.1</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt101"><label>1</label><p>After an initial disturbance (30&#x00025;) to connection weights.</p></fn><fn id="nt102"><label>2</label><p>With continuous noise (10&#x00025;) added to connection weights.</p></fn><fn id="nt103"><label>3</label><p>After an initial disturbance (30&#x00025;) and continuous noise (5&#x00025;).</p></fn><fn id="nt104"><label>4</label><p>No noise.</p></fn><fn id="nt105"><label/><p>CI is the 95&#x00025; confidence interval. Positive and negative signs indicate the direction of drift; towards and away from zero respectively.</p></fn></table-wrap-foot></table-wrap><p>The root-mean-squared error (RMSE), measured in degrees, quantifies the average difference between the exact integrator transfer function (a straight line) and the estimated transfer function of the model circuit (as described in <xref ref-type="sec" rid="s2">Materials and Methods</xref>). Typically, higher RMSE means more rapid drifting (between stable points) since error accumulates more quickly. However, the transfer function is estimated using rate model approximations to the simulated spiking neurons, so this relationship is not guaranteed to hold. Consequently, we also report the absolute value of the system time constant, which is indicative of the speed at which the system drifts (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). The sign, shown in brackets, indicates the direction of drift. A negative sign indicates a drift away from midline (zero), and a positive sign indicates a drift towards midline. All time constants are in seconds.</p><p>The four experiments in which the system learns under a variety of noise profiles demonstrate the robustness of the rule. As is evident from <xref ref-type="table" rid="pone-0022885-t001">Table 1</xref>, the addition of 30&#x00025; noise to the connection weights (Noisy) increased the RMSE by over an order of magnitude. Consequently, the mean time constant was reduced from 41.4 s to 10.6 s. The time traces of the eye position for example linear Optimal, Noisy, and Learned&#x0002b;Perturb<inline-formula><inline-graphic xlink:href="pone.0022885.e172.jpg" mimetype="image"/></inline-formula> networks are shown in <xref ref-type="fig" rid="pone-0022885-g005">figure 5</xref>. As well, a comparison of the transfer functions of the exact, Optimal, and Noisy integrators is shown in <xref ref-type="fig" rid="pone-0022885-g006">Figure 6</xref>. Together, these graphs demonstrate that after the initial perturbation, the network no longer performs integration properly. However, with the introduction of the learning rule, the integrator is able to overcome the noise.</p><fig id="pone-0022885-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g005</object-id><label>Figure 5</label><caption><title>Generated eye movements of example networks.</title><p>The linear Optimal, Noisy (30&#x00025; perturbation to connection weights), and Learned&#x0002b;Perturb<inline-formula><inline-graphic xlink:href="pone.0022885.e173.jpg" mimetype="image"/></inline-formula> (after 1200 s of learning from the Noisy state) networks are shown for 30 s with the same saccade regime.</p></caption><graphic xlink:href="pone.0022885.g005"/></fig><fig id="pone-0022885-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g006</object-id><label>Figure 6</label><caption><title>A comparison of the exact integrator, linear Optimal and Noisy transfer functions over a normalized range.</title><p>The linear Optimal network is closer to the exact integrator over the range of eye positions. Although deviations of the Noisy network from the exact integrator are small, the effects on stability are highly significant (see <xref ref-type="table" rid="pone-0022885-t001">Table 1</xref> and <xref ref-type="fig" rid="pone-0022885-g005">Figure 5</xref>). Magnified regions are to aid visual comparison.</p></caption><graphic xlink:href="pone.0022885.g006"/></fig><p>In fact, as shown in <xref ref-type="fig" rid="pone-0022885-g004">Figure 4</xref> the tuned network can be more stable than the linear Optimal case (compare Learned&#x0002b;Noise<inline-formula><inline-graphic xlink:href="pone.0022885.e174.jpg" mimetype="image"/></inline-formula> or Learned&#x0002b;NoNoise<inline-formula><inline-graphic xlink:href="pone.0022885.e175.jpg" mimetype="image"/></inline-formula> to Optimal). There is no overlap in confidence intervals, making it clear this is a strong effect. In other words, using the learning rule can tune the network better than &#x0201c;optimal&#x0201d; (see <xref ref-type="sec" rid="s4">Discussion</xref>). In both cases, this improvement beyond the Optimal case occurs when there is no noise during the learning period.</p><p>Consequently we consider the rule under continuous noise. With the continuous addition of 10&#x00025; noise (Learned&#x0002b;Noise<inline-formula><inline-graphic xlink:href="pone.0022885.e176.jpg" mimetype="image"/></inline-formula>), the integrator is also able to retain a similar time constant to the linear Optimal case, though there is a slight increase in the variability of the drift over the 30 test networks. This demonstrates that the system is robust to continuous noise, but does not show that it can retune after an initial disturbance and with continuous noise.</p><p>In the case of combined initial and continuous noise (Learned&#x0002b;Perturb&#x0002b;Noise<inline-formula><inline-graphic xlink:href="pone.0022885.e177.jpg" mimetype="image"/></inline-formula>), the learning rule maintains the same mean as the linear Optimal case, though again with a slight increase in variability. We found that the continuous noise in this case had to be reduced (to 5&#x00025;) to allow retuning from the initial perturbation.</p><p>Taken together, these results suggest that the learning rule is as good as the optimization at generating and fine-tuning a stable neural integrator. In fact, with no noise (Learned&#x0002b;NoNoise<inline-formula><inline-graphic xlink:href="pone.0022885.e178.jpg" mimetype="image"/></inline-formula>), the learning rule can tune the integrator to have a much longer time constant than the linear Optimal case. This is because the model that is optimized has various assumptions about neural properties which are violated in the model (e.g., rate versus spiking neurons). In short, the learning tunes the network better than the standard optimization &#x02013; we return to this point in the discussion.</p><p>The results can also be compared to the goldfish integrator, which has empirically measured time constants that range between 29 s and 95 s, with a mean of 66 s <xref rid="pone.0022885-Mensh1" ref-type="bibr">[19, fig. 6]</xref>. As shown in <xref ref-type="table" rid="pone-0022885-t001">Table 1</xref>, this compares well with experiment 5, in which the simulation has been tuned after an initial disturbance, and constant ongoing noise of 5&#x00025; (mean 41.4 s, CI: 18.9&#x02013;78.8). To get a better understanding of the temporal behavior of the simulations as compared to the biological system, <xref ref-type="fig" rid="pone-0022885-g007">Figure 7</xref> shows a 6 s run with several saccades in both systems. The simulation effectively reproduces the kinds of responses seen in integrator neurons, and the related eye movements.</p><fig id="pone-0022885-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g007</object-id><label>Figure 7</label><caption><title>Comparison of goldfish integrator neurons from electrophysiological recordings and the simulation after tuning with the learning rule.</title><p>A single raw recording is shown on the left, along with the corresponding eye trace. Arrows indicate times of saccade (black right, grey left; adapted from <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>). The right shows 14 neurons randomly selected from the model population after tuning with the learning rule. Neurons in the model have similar kinds of responses as the example neuron. One is highlighted in grey.</p></caption><graphic xlink:href="pone.0022885.g007"/></fig><p>The results from the unstable and damped experiments reproduce the major trends observed in the experimental results, as shown in <xref ref-type="table" rid="pone-0022885-t002">Table 2</xref> and <xref ref-type="fig" rid="pone-0022885-g008">Figure 8</xref>. For the Unstable case, the learning rule demonstrates a large difference between the tuned and untuned networks, going from 41.4 s to an average value of &#x02212;15.5 s (drift is away from zero, see <xref ref-type="fig" rid="pone-0022885-g008">Figure 8</xref>). The 95&#x00025; confidence interval is also well outside that for the any of the linear Optimal or Learned cases. This compares well to the experimental change reported. The animals in <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref> were trained between 20 min and 16.5 h, with averages only reported for animals after 1 h or more of training. Simulations of that length were not feasible, and so all simulations were run for 20 min of training. Hence, slightly smaller changes are expected. However, for both the simulations and the experimental system, longer detuning resulted in faster time constants. A similarly sized change is evident in the damped case, which shows an average reduction to a time constant of 10.9 s (drift towards zero) for the simulation and 7.7 s for the experiment (see <xref ref-type="fig" rid="pone-0022885-g008">Figure 8</xref>).</p><fig id="pone-0022885-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g008</object-id><label>Figure 8</label><caption><title>A comparison of the simulated detuning experiments with experimental data <xref rid="pone.0022885-Major2" ref-type="bibr">[<bold>30</bold>]</xref>.</title><p>The top trace is for the control situation, which for the model is tuning after a 30&#x00025; perturbation and 5&#x00025; continuous noise. The middle trace shows the unstable integrator, and the bottom trace shows the damped integrator. The goldfish traces are from animals that had longer training times (6 h and 16.5 h respectively), than the model (20 min). Both the model and experiment demonstrate increased detuning with longer training times (not shown), and both show the expected detuning (drift away from midline for the unstable case, and drift towards midline in the damped case).</p></caption><graphic xlink:href="pone.0022885.g008"/></fig><table-wrap id="pone-0022885-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.t002</object-id><label>Table 2</label><caption><title>A comparison of the time constants observed in our model to experimental results.</title></caption><alternatives><graphic id="pone-0022885-t002-2" xlink:href="pone.0022885.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Simulation</td><td align="left" rowspan="1" colspan="1">Empirical Data</td></tr><tr><td align="left" rowspan="1" colspan="1">Experiment</td><td align="left" rowspan="1" colspan="1">(20 min training)</td><td align="left" rowspan="1" colspan="1">(1 h or more training)</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">6 Learned&#x0002b;Perturb<inline-formula><inline-graphic xlink:href="pone.0022885.e179.jpg" mimetype="image"/></inline-formula>/Control</td><td align="left" rowspan="1" colspan="1">41.4</td><td align="left" rowspan="1" colspan="1">66.0 <xref rid="pone.0022885-Mensh1" ref-type="bibr">[19]</xref>
</td></tr><tr><td align="left" rowspan="1" colspan="1">7 Unstable</td><td align="left" rowspan="1" colspan="1">15.1</td><td align="left" rowspan="1" colspan="1">4.3 <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>
</td></tr><tr><td align="left" rowspan="1" colspan="1">8 Damped</td><td align="left" rowspan="1" colspan="1">10.9</td><td align="left" rowspan="1" colspan="1">7.7 <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>
</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt106"><label/><p>All values are the reported <inline-formula><inline-graphic xlink:href="pone.0022885.e180.jpg" mimetype="image"/></inline-formula> in seconds.</p></fn></table-wrap-foot></table-wrap><p>
<xref ref-type="fig" rid="pone-0022885-g009">Figure 9</xref> compares the transfer functions for the Unstable, Damped, and Learned&#x0002b;Perturb&#x0002b;Noise<inline-formula><inline-graphic xlink:href="pone.0022885.e181.jpg" mimetype="image"/></inline-formula> networks. Notably, a small deviation from the transfer function of the exact integrator causes reasonably rapid unstable or damped performance. The zoomed in sections of this figure make the differences between pre and post-tuning more evident. It is crucial to show the entire transfer function, however, as it demonstrates that the time constant change is smooth across all eye positions (the transfer functions are approximately straight lines). The same is observed experimentally <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>.</p><fig id="pone-0022885-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g009</object-id><label>Figure 9</label><caption><title>A comparison of the Learned&#x0002b;Perturb&#x0002b;Noise<inline-formula><inline-graphic xlink:href="pone.0022885.e182.jpg" mimetype="image"/></inline-formula>, Unstable and Damped transfer functions.</title><p>The slope of the Unstable network is greater than 1 and that of the Damped network is less than 1. The re-tuned networks demonstrate the expected drifting behavior (see <xref ref-type="fig" rid="pone-0022885-g008">Figure 8</xref> and <xref ref-type="table" rid="pone-0022885-t001">Table 1</xref>).</p></caption><graphic xlink:href="pone.0022885.g009"/></fig><p>One noticeable difference between the experiments and simulations is the variability in the system after training. While the standard deviations for the experimental results are not available, the range of one correctly tuned experiment is reported as being from &#x02212;31 s to 15 s <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>, which is a much greater spread than observed in the simulations. There are several possible reasons for this much wider variance. While we have attempted to match the variability of the tuning curves, there are several other parameters kept constant across simulations that are likely varying in the biological system, such as synaptic time constants, and learning rates. These are fixed in the simulations, as we do not have experimental estimates of the distributions of these parameters. Nevertheless, the important features of detuning, including the direction and extent of the detuning are reproduced in the simulations.</p><p>To simulate the lesion of a single neuron, the network was tuned to the linear optimal weights before a single neuron was removed. Lesioning a neuron resulted in an increase in RMSE from 0.129 to 0.824 and a decrease in time constant to about 10 s. To demonstrate the recovery process documented by <xref rid="pone.0022885-Arnold3" ref-type="bibr">[29]</xref>, the learning rule was then run on the lesioned network under 5&#x00025; continuous noise. The system was able to recover to a system time constant of 51.3 s on average. The temporal properties of the network are shown before and after lesioning in <xref ref-type="fig" rid="pone-0022885-g010">Figure 10</xref>.</p><fig id="pone-0022885-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g010</object-id><label>Figure 10</label><caption><title>Performance of the integrator before and after lesioning the network.</title><p>Severe drift is evident after randomly removing one of the 40 neurons. After 1200 s of recovery with the learning rule under 5&#x00025; noise, the time constant improves back to pre-lesioning levels.</p></caption><graphic xlink:href="pone.0022885.g010"/></fig></sec><sec id="s3b"><title>Application of the generalized learning rule</title><p>In other work, we have shown how this characterization of the oculomotor integrator as a line attractor network can be generalized to the family of attractor networks including ring, plane, cyclic, and chaotic attractors <xref rid="pone.0022885-Eliasmith1" ref-type="bibr">[14]</xref>. These attractors have been implicated in a wide variety of biological behaviors including rat head-direction control (ring), working memory and path integration (plane), swimming and other repetitive movements (cyclic), and olfaction (chaotic). The generalized learning rule described above applies in a straightforward manner to these other cases.</p><p>For example, the ring attractor is naturally characterized as a stable function attractor (where the stabilized function is typically a &#x0201c;bump&#x0201d;), as opposed to the scalar attractor of the oculomotor system. Similarly, a 2D bump attractor, which has been used by several groups to model path integration in rat subiculum <xref rid="pone.0022885-Redish2" ref-type="bibr">[61]</xref>, <xref rid="pone.0022885-Conklin1" ref-type="bibr">[34]</xref>, can also be characterized as a function attractor in a higher dimensional space. A function space can be represented as a vector space, and so we can apply the generalized learning rule to tune this network. Example tuning curves in these function spaces are showing in <xref ref-type="fig" rid="pone-0022885-g011">figure 11</xref>.</p><fig id="pone-0022885-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g011</object-id><label>Figure 11</label><caption><title>Tuning curves in two function spaces.</title><p>a) Gaussian-like tuning curves of 20 example neurons in a one-dimensional function space (7-dimensional vector space). These are tunings representative of neurons in a head-direction ring attractor network. b) Multi-dimensional Gaussian-like tuning curves of four example neurons in a two-dimensional function space (14-dimensional vector space). These are tunings representative of neurons in a subicular path integration network.</p></caption><graphic xlink:href="pone.0022885.g011"/></fig><p>Analogous simulations to the oculomotor Learned&#x0002b;Perturb<inline-formula><inline-graphic xlink:href="pone.0022885.e183.jpg" mimetype="image"/></inline-formula> case were constructed in the Nengo neural simulation package to characterize tuning of head direction and path integrators networks (Nengo was used as it executes these simulations more quickly than Matlab. They are available at <ext-link ext-link-type="uri" xlink:href="http://compneuro.uwaterloo.ca/cnrglab/f/NINengoDemos.zip">http://compneuro.uwaterloo.ca/cnrglab/f/NINengoDemos.zip</ext-link>). Specifically, for the head direction system, neurons were randomly assigned unit encoding vectors <inline-formula><inline-graphic xlink:href="pone.0022885.e184.jpg" mimetype="image"/></inline-formula> in a 7D vector space, to define encodings as in equation 8. Initial optimal weights were calculated as defined in equation 9, using encoding and decoding vectors rather than weights (i.e. <inline-formula><inline-graphic xlink:href="pone.0022885.e185.jpg" mimetype="image"/></inline-formula>). Both the represented vector space is mapped to the 1D function space using a cyclic orthonormal basis <inline-formula><inline-graphic xlink:href="pone.0022885.e186.jpg" mimetype="image"/></inline-formula>: e.g., to get the encoding functions we compute <inline-formula><inline-graphic xlink:href="pone.0022885.e187.jpg" mimetype="image"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="pone.0022885.e188.jpg" mimetype="image"/></inline-formula> is the 1D spatial variable and <inline-formula><inline-graphic xlink:href="pone.0022885.e189.jpg" mimetype="image"/></inline-formula>. The same process is followed for the path integrator using a 14D vector space and 2D function space.</p><p>
<xref ref-type="fig" rid="pone-0022885-g012">Figure 12</xref> shows example results from these simulations, using the generalized learning rule. The models are very similar since both can be realized by different stable structures in a vector space <xref rid="pone.0022885-Eliasmith1" ref-type="bibr">[14]</xref>. Hence, the simulation setups are identical, except the head direction network has 7 dimensions and 700 neurons, and the path integration network has 14 dimensions and 1400 neurons. Neurons have the same parameters as in the oculomotor simulation, except that encoding vectors are chosen to tile the appropriate spaces (analogous to choosing encoding weights of <inline-formula><inline-graphic xlink:href="pone.0022885.e190.jpg" mimetype="image"/></inline-formula> in the oculomotor network).</p><fig id="pone-0022885-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0022885.g012</object-id><label>Figure 12</label><caption><title>Simulations of tuning attractor networks in higher dimensional spaces.</title><p>a) The input (dashed line) along with the final position of the representation after 500 ms of drift for pre-training (thick line) and post-training (thin line). b) The pre-training drift in the vector space over 500 ms at the beginning of the simulation for the bump (thick line in a). d) The drift in the vector space over 500 ms after 1200 s of training in the simulation (thin line in a). Comparing similar vector dimensions between b) and c) demonstrates a slowing of the drift. d) A 2D bump in the function space for the simulated time shown in e), after training. e) The vector drift in the 14-dimensional space over 500 ms after training.</p></caption><graphic xlink:href="pone.0022885.g012"/></fig><p>As shown in <xref ref-type="fig" rid="pone-0022885-g012">Figure 12</xref>, the same trend of improving the time constant over a 1200 s run is evident in the other networks. For the ring attractor, the time constant improved from 7.69e<inline-formula><inline-graphic xlink:href="pone.0022885.e191.jpg" mimetype="image"/></inline-formula>s to 2.9 s. The increase in time constant is evident in the decrease in the amount of drift in <xref ref-type="fig" rid="pone-0022885-g012">Figure 12</xref> between the beginning and the end of the simulation. In this figure, we have also shown the difference in drift in the function space. A similar trend, with lower time constants, was evident in the head direction network over 1200 s of training (from 7.69e<inline-formula><inline-graphic xlink:href="pone.0022885.e192.jpg" mimetype="image"/></inline-formula>s to 1.1e<inline-formula><inline-graphic xlink:href="pone.0022885.e193.jpg" mimetype="image"/></inline-formula>s). This small change in the time constant is not visually evident in plots like those for the ring attractor. Instead we have shown the representation of a stable 2D bump at the end of training. It is clear from these simulations that the number of neurons per dimension is not sufficient to achieve a similar level of stability as seen in the oculomotor integrator in the higher dimensional spaces. This is not surprising, as the number of neurons required to achieve a similar RMSE goes to the power of the number of dimensions of the space. This means that stable representations in higher-dimensional spaces are much more difficult to achieve for a given number of cells. Exploring the relationship between the number of neurons, the dimensionality of the space, and stability properties, and properly quantifying the behaviour of the learning rule in detail in these spaces remains future work.</p><p>These simulations are intended only as a proof-in-principle that the learning rule generalizes, and are clearly inaccurate regarding the biological details of both systems (e.g., neuron parameters should not be the same as the oculomotor integrator). More importantly, the generalized error needed in each simulation <inline-formula><inline-graphic xlink:href="pone.0022885.e194.jpg" mimetype="image"/></inline-formula> needs to be identified in each case. Our assumption that there is drift information analogous to the oculomotor integrator may or may not be biologically plausible. Consequently, in each case there remains important questions regarding the existence and source of the required error signals. These questions go well beyond the scope of the current paper. However, these simulations do demonstrate that the same kind of learning rule can be used to tune a wide variety of attractor networks in higher-dimensional spaces.</p></sec></sec><sec id="s4"><title>Discussion</title><p>The simulations described in this paper demonstrate one possible solution to the problem of fine-tuning in neural integrators. The oculomotor model was able to achieve and maintain finely-tuned connection weights through a biologically plausible learning algorithm. Specifically, the learning rule allowed recovery from large perturbations of connection weights, continuous perturbation of connection weights, and the lesioning of cells. Not surprisingly, these results are in agreement with other experimental findings that suggest that feedback plays an important role in the behavior of the oculomotor integrator <xref rid="pone.0022885-Kmpf1" ref-type="bibr">[42]</xref>, <xref rid="pone.0022885-Kapoula1" ref-type="bibr">[43]</xref>, <xref rid="pone.0022885-Weissman1" ref-type="bibr">[41]</xref>.</p><p>Consideration of the learning rule suggested here demonstrates that on-line fine-tuning is a viable <italic>in vivo</italic> mechanism for explaining the stability of the neural integrator. Specifically, this rule improves upon existing oculomotor learning models <xref rid="pone.0022885-Arnold3" ref-type="bibr">[29]</xref>, <xref rid="pone.0022885-Arnold1" ref-type="bibr">[22]</xref>, <xref rid="pone.0022885-Arnold2" ref-type="bibr">[23]</xref>, <xref rid="pone.0022885-Turaga1" ref-type="bibr">[24]</xref> by expressing the modification of synaptic weights in terms of information that is known to be available to each neuron. Furthermore, this rule is able to explain not only robustness to random connection weight noise, but also several experimental findings related to other forms of perturbation. For instance, unlike rules that strictly enforce stability <xref rid="pone.0022885-Renart1" ref-type="bibr">[32]</xref>, this model is able to replicate the de-tuning observations described by <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>. The learning rule is able to tune the integrator in response to distorted visual feedback in a way comparable in terms of both required training time and degree of instability/damping observed <xref rid="pone.0022885-Major2" ref-type="bibr">[30]</xref>. As well, the system is able to tune the integrator after cell death, which has been observed empirically <xref rid="pone.0022885-Arnold3" ref-type="bibr">[29]</xref>. Consequently, this rule provides a plausible mechanism for solving the fine-tuning problem, without relying on less well-established mechanisms (e.g., <xref rid="pone.0022885-Koulakov1" ref-type="bibr">[20]</xref>, <xref rid="pone.0022885-Goldman1" ref-type="bibr">[13]</xref>).</p><sec id="s4a"><title>Optimality of linear methods</title><p>Using feedback to tune the integrator results in learned connection weights that produce the same or even longer time constants than the theoretically derived linear optimal connection weights, despite a significantly larger RMSE (compare experiments one, three, four, five, and ten). This is likely because the calculation of linear optimal weights does not account for dynamics of the eye or the spiking non-linearities in the neurons (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>).</p><p>In contrast, the learning algorithm is employed alongside the simulation of the oculomotor plant and single cell dynamics, so the learned weights are calculated for a more complete model rather than an approximation to that model. The effect of these approximations is most directly demonstrated by experiment six, in which the learning rule tunes the system with no noise. In this case, the average learned time constant is three times longer than that of the linear optimal network, even though the RMSE is higher as well. This is true regardless of how much noise is assumed during the optimization process (results not shown). This suggests that typical theoretical methods for tuning connection weights are not generally &#x0201c;optimal&#x0201d; in fully spiking network models.</p></sec><sec id="s4b"><title>Empirical consequences</title><p>Despite the limitations of these theoretical optimization methods, they are important for allowing the network to be in a neighbourhood where it can be fine-tuned. This rule will not tune a completely random network with large amounts of continuous noise, for instance. As a result, one empirically testable consequence of this model is a characterization of the maximum amount of noise such a mechanism can tolerate. In particular, the system is robust under 10&#x00025; continuous noise, or under 30&#x00025; initial and 5&#x00025; continuous noise. This makes it reasonable to expect that the amount of continuous noise of this type in the system would be on the order of 5&#x02013;10&#x00025; (over twenty minutes). While this degree of robustness is significant, it remains to be seen how robust the biological integrator is to these same kinds of perturbation, and how severe intrinsic perturbations in the system are. Given our model, we suggest that the magnitude of intrinsic perturbations could be determined by examining the extent and speed of detuning when corrective saccades are inhibited or removed. For instance, under the same 10&#x00025; continuous noise for 200 minutes with no corrective saccades, the average system time constant becomes 7.68 s (confidence interval: 4.67 s&#x02013;11.8 s) in the model. We leave for future consideration careful characterization of the relationship between continuous noise, one-shot noise, learning rates, and the absence of corrective saccades.</p><p>It can also be noted that the speed with which the model converges to stability is a function of the learning rate, <inline-formula><inline-graphic xlink:href="pone.0022885.e195.jpg" mimetype="image"/></inline-formula>. Increasing this learning rate may help overcome larger amounts of noise, but there is also the potential for introducing learning instabilities with larger learning rates. The model as presented is tuned approximately at the same speed as the goldfish (see, e.g., <xref ref-type="fig" rid="pone-0022885-g008">figure 8</xref>). The empirical consequences of varying learning rate could be predicted, if methods for manipulating such rates in the biological system could be established.</p><p>A related empirical question that arises given this model is: How are corrective and intentional saccades distinguished? In the model, that distinction is made by filtering based on the magnitude of the velocity command. However, it remains an open question what the biological mechanism underlying this filtering might be. This issue is left largely unaddressed here because there are several potential means of identifying corrective saccades. For example, the learning process may require a kind of &#x0201c;activation energy&#x0201d; to initiate learning, in which case large saccades would reduce this energy and act as inhibitors for learning. It is also possible that the (amplitude independent) frequency content of saccades is used to trigger the learning process, such that intentional saccades do not cause modification of the synaptic weights. As well, the duration of the saccades can be used as a means of distinguishing intentional from corrective saccades. In the end, the magnitude filtering implemented in this study was chosen because of simplicity and lack of experimental evidence for any one of these potential mechanisms.</p><p>Notably, our particular choice of filtering method does not seem crucial. We have run single simulations with other filtering methods with similar results. For example, using the filtering by change in position (see <xref ref-type="fig" rid="pone-0022885-g002">Figure 2</xref>), the time constant of the network improved from 5.7 s to 69.5 s, similar to our chosen method. More importantly, however, the learning rule itself is independent of the method of distinguishing corrective from intentional saccades, although it assumes there is some mechanism that provides this distinction.</p></sec><sec id="s4c"><title>The generalized learning rule</title><p>Consideration of the generalized learning rule raises interesting possibilities that could be tested experimentally. Perhaps most speculatively, the rule suggests that intrinsic neuron properties play a central role in a how a particular neuron is exploited by a system. The encoding vector <inline-formula><inline-graphic xlink:href="pone.0022885.e196.jpg" mimetype="image"/></inline-formula> and gain <inline-formula><inline-graphic xlink:href="pone.0022885.e197.jpg" mimetype="image"/></inline-formula> determine how the error signal is &#x0201c;interpreted&#x0201d; by a given neuron. The mapping of input currents onto neural activity are a function of intrinsic neuron properties, like the membrane resistance and capacitance, channel density, dendritic morphology and so on. This suggests it may be possible to experimentally determine relationships between such properties and how cells are exploited in a given learning circuit.</p><p>Much less speculatively, the general structure of the rule suggests simple behavioral experiments. For example, if an error signal analogous to retinal slip is available to head-direction, path integration systems, or working memory systems, it should be possible to similarly mis-tune those systems with careful manipulation of the stimulus. If such mis-tuning is achievable, it would suggest that this kind of plasticity is broadly important for the neural control of behavior.</p><p>Returning to the saccadic system specifically, it is evident that the error signal is generated by elements of the oculomotor system external to the integrator itself. However, it is clearly the case that such a signal is self-generated by the neurobiological system as a whole (as captured by the OMS model). This signal allows for a kind of &#x0201c;self-directed organization&#x0201d; of the system. The generalization suggests that any other error signal that can be self-generated can also be exploited by this rule for tuning a network to perform other kinds of computations. Preliminary results show that this generalized rule is able to learn arbitrary non-linear vector transformations <xref rid="pone.0022885-Bekolay1" ref-type="bibr">[62]</xref>. Notably, the generalization of the error signal and the neural representation does not change the basic gated pseudo-Hebbian nature of the rule.</p><p>In addition, clear differences in the consequences of different kinds of learning arise in the case of stability. A supervised rule, such as backpropagation through time <xref rid="pone.0022885-Williams1" ref-type="bibr">[63]</xref>, requires enforcing the desired output on the state of the system (i.e., eye position), which is biologically implausible in this case as the correcty eye position is not immediately available to the integrator. An unsupervised stability rule <xref rid="pone.0022885-Renart1" ref-type="bibr">[32]</xref> will enforce stability over the range of experienced input. Thus the dynamics of the system (i.e., whether it is stable, unstable, or damped) are determined by the rule itself. In contrast, a self-directed rule, like that presented here, determines the stability of the system in an environmentally dependent way. As demonstrated by the mis-tuning experiments, the stability of the integrator is not intrinsic, but rather tied to environmental stability. Or, more accurately, tied to the system's ability to generate corrective stability signals based on environmental cues. Uncertainties regarding environmental dynamics may make it evolutionarily advantageous to prefer rules that rely on self-directed organization in many circumstances.</p></sec></sec></body><back><ack><p>We thank T. Stewart, M. Hurwitz, D. Rasmussen, T. Bekolay, X. Choo, C.H. Anderson, and Y. Tang for helpful discussions, comments, and technical support.</p></ack><fn-group><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="financial-disclosure"><p><bold>Funding: </bold>This research was supported by a National Science and Engineering Research Council of Canada (NSERC) Discovery grant, an NSERC Student Research Award (USRA), the Canada Research Chair program, and the Canadian Foundation for Innovation (CFI) and Ontario Innovation Trust (OIT) Leaders Opportunity Fund. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></fn></fn-group><ref-list><title>References</title><ref id="pone.0022885-Robinson1"><label>1</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Robinson</surname><given-names>D</given-names></name></person-group>
<year>1989</year>
<article-title>Integrating with neurons.</article-title>
<source>Annual Review of Neuroscience</source>
<volume>12</volume>
<fpage>33</fpage>
<lpage>45</lpage>
</element-citation></ref><ref id="pone.0022885-Seung1"><label>2</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name></person-group>
<year>1996</year>
<article-title>How the brain keeps the eyes still.</article-title>
<source>Proceedings of the National Academy of Sciences of the United States of America</source>
<volume>93</volume>
<fpage>13339</fpage>
<lpage>13344</lpage>
<pub-id pub-id-type="pmid">8917592</pub-id></element-citation></ref><ref id="pone.0022885-Pouget1"><label>3</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group>
<year>1998</year>
<article-title>Statistically efficient estimation using population coding.</article-title>
<source>Neural Computation</source>
<volume>10</volume>
<fpage>373</fpage>
<lpage>401</lpage>
<pub-id pub-id-type="pmid">9472487</pub-id></element-citation></ref><ref id="pone.0022885-Goodridge1"><label>4</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Goodridge</surname><given-names>JP</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name></person-group>
<year>2000</year>
<article-title>Modeling attractor deformation in the rodent headdirection system.</article-title>
<source>Journal of Neurophysiology</source>
<volume>83</volume>
<fpage>3402</fpage>
<lpage>3410</lpage>
<pub-id pub-id-type="pmid">10848558</pub-id></element-citation></ref><ref id="pone.0022885-Redish1"><label>5</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Elga</surname><given-names>AN</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name></person-group>
<year>1996</year>
<article-title>A coupled attractor model of the rodent head direction system.</article-title>
<source>Network: Computation in Neural Systems</source>
<volume>7</volume>
<fpage>671</fpage>
<lpage>685</lpage>
</element-citation></ref><ref id="pone.0022885-Deneve1"><label>6</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name></person-group>
<year>2008</year>
<article-title>Bayesian spiking neurons i: Inference.</article-title>
<source>Neural Comput</source>
<volume>20</volume>
<fpage>91</fpage>
<lpage>117</lpage>
<pub-id pub-id-type="pmid">18045002</pub-id></element-citation></ref><ref id="pone.0022885-Hopfield1"><label>7</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>
<year>1982</year>
<article-title>Neural networks and physical systems with emergent collective computational abilities.</article-title>
<source>Proceedings of the National Academy of Sciences</source>
<volume>79</volume>
<fpage>2554</fpage>
<lpage>2558</lpage>
</element-citation></ref><ref id="pone.0022885-Brody1"><label>8</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group>
<year>2003</year>
<article-title>Basic mechanisms for graded persistent activity: discrete attractors, continuous attractors, and dynamic representations.</article-title>
<source>Current Opinion in Neurobiology</source>
<volume>13</volume>
<fpage>204</fpage>
<lpage>211</lpage>
<pub-id pub-id-type="pmid">12744975</pub-id></element-citation></ref><ref id="pone.0022885-Singh1"><label>9</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Singh</surname><given-names>R</given-names></name><name><surname>Eliasmith</surname><given-names>C</given-names></name></person-group>
<year>2006</year>
<article-title>Higher-Dimensional Neurons Explain the Tuning and Dynamics of Working Memory Cells.</article-title>
<source>J Neurosci</source>
<volume>26</volume>
<fpage>3667</fpage>
<lpage>3678</lpage>
<pub-id pub-id-type="pmid">16597721</pub-id></element-citation></ref><ref id="pone.0022885-LorenteDeN1"><label>10</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lorente De N&#x000f3;</surname><given-names>R</given-names></name></person-group>
<year>1938</year>
<article-title>Analysis of the activity of the chains of internuncial neurons.</article-title>
<source>J Neurophysiol</source>
<volume>1</volume>
<fpage>207</fpage>
<lpage>244</lpage>
</element-citation></ref><ref id="pone.0022885-Hebb1"><label>11</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Hebb</surname><given-names>D</given-names></name></person-group>
<year>1949</year>
<source>The organization of behavior</source>
<publisher-name>John Wiley &#x00026; Sons</publisher-name>
</element-citation></ref><ref id="pone.0022885-Amit1"><label>12</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Amit</surname><given-names>DJ</given-names></name></person-group>
<year>1989</year>
<source>Modeling brain function: The world of attractor neural networks</source>
<publisher-loc>New York, NY</publisher-loc>
<publisher-name>Cambridge University Press</publisher-name>
</element-citation></ref><ref id="pone.0022885-Goldman1"><label>13</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Goldman</surname><given-names>MS</given-names></name></person-group>
<year>2009</year>
<article-title>Memory without feedback in a neural network.</article-title>
<source>Neuron</source>
<volume>61</volume>
<fpage>621</fpage>
<lpage>634</lpage>
<pub-id pub-id-type="pmid">19249281</pub-id></element-citation></ref><ref id="pone.0022885-Eliasmith1"><label>14</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name></person-group>
<year>2005</year>
<article-title>A unified approach to building and controlling spiking attractor networks.</article-title>
<source>Neural computation</source>
<volume>17</volume>
<fpage>1276</fpage>
<lpage>1314</lpage>
<pub-id pub-id-type="pmid">15901399</pub-id></element-citation></ref><ref id="pone.0022885-Seung2"><label>15</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Lee</surname><given-names>DD</given-names></name><name><surname>Reis</surname><given-names>BY</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group>
<year>2000</year>
<article-title>Stability of the memory of eye position in a recurrent network of conductance-based model neurons.</article-title>
<source>Neuron</source>
<volume>26</volume>
<fpage>259</fpage>
<lpage>271</lpage>
<pub-id pub-id-type="pmid">10798409</pub-id></element-citation></ref><ref id="pone.0022885-Zhang1"><label>16</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name></person-group>
<year>1996</year>
<article-title>Representation of spatial orientation by the intrinsic dynamics of the headdirection cell ensemble: A theory.</article-title>
<source>Journal of Neuroscience</source>
<volume>16</volume>
<fpage>2112</fpage>
<lpage>2126</lpage>
<pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="pone.0022885-Major1"><label>17</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Major</surname><given-names>G</given-names></name><name><surname>Tank</surname><given-names>D</given-names></name></person-group>
<year>2004</year>
<article-title>Persistent neural activity: prevalence and mechanisms.</article-title>
<source>Current Opinion in Neurobiology</source>
</element-citation></ref><ref id="pone.0022885-Nikitchenko1"><label>18</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Nikitchenko</surname><given-names>M</given-names></name><name><surname>Koulakov</surname><given-names>A</given-names></name></person-group>
<year>2008</year>
<article-title>Neural integrator: A sandpile model.</article-title>
<source>Neural computation</source>
<volume>20</volume>
<fpage>2379</fpage>
<lpage>2417</lpage>
<pub-id pub-id-type="pmid">18533820</pub-id></element-citation></ref><ref id="pone.0022885-Mensh1"><label>19</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mensh</surname><given-names>BD</given-names></name><name><surname>Aksay</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>DD</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group>
<year>2004</year>
<article-title>Spontaneous eye movements in goldfish: oculomotor integrator performance, plasticity, and dependence on visual feedback.</article-title>
<source>Vision research</source>
<volume>44</volume>
<fpage>711</fpage>
<lpage>726</lpage>
<pub-id pub-id-type="pmid">14751555</pub-id></element-citation></ref><ref id="pone.0022885-Koulakov1"><label>20</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Koulakov</surname><given-names>AA</given-names></name><name><surname>Raghavachari</surname><given-names>S</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Lisman</surname><given-names>JE</given-names></name></person-group>
<year>2002</year>
<article-title>Model for a robust neural integrator.</article-title>
<source>Nature Neuroscience</source>
<volume>5</volume>
<fpage>775</fpage>
<lpage>782</lpage>
<pub-id pub-id-type="pmid">12134153</pub-id></element-citation></ref><ref id="pone.0022885-Fransn1"><label>21</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Frans&#x000e9;n</surname><given-names>E</given-names></name><name><surname>Tahvildari</surname><given-names>B</given-names></name><name><surname>Egorov</surname><given-names>AV</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Alonso</surname><given-names>AA</given-names></name></person-group>
<year>2006</year>
<article-title>Mechanism of graded persistent cellular activity of entorhinal cortex layer v neurons.</article-title>
<source>Neuron</source>
<volume>49</volume>
<fpage>735</fpage>
<lpage>46</lpage>
<pub-id pub-id-type="pmid">16504948</pub-id></element-citation></ref><ref id="pone.0022885-Arnold1"><label>22</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Arnold</surname><given-names>DB</given-names></name><name><surname>Robinson</surname><given-names>DA</given-names></name></person-group>
<year>1992</year>
<article-title>A neural network model of the vestibulo-ocular reflex using a local synaptic learning rule.</article-title>
<source>Philosophical Transactions: Biological Sciences</source>
<volume>337</volume>
<fpage>327</fpage>
<lpage>330</lpage>
<pub-id pub-id-type="pmid">1359586</pub-id></element-citation></ref><ref id="pone.0022885-Arnold2"><label>23</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Arnold</surname><given-names>DB</given-names></name><name><surname>Robinson</surname><given-names>DA</given-names></name></person-group>
<year>1997</year>
<article-title>The oculomotor integrator: Testing of a neural network model.</article-title>
<source>Experimental Brain Research</source>
<volume>113</volume>
<fpage>57</fpage>
<lpage>74</lpage>
<pub-id pub-id-type="pmid">9028775</pub-id></element-citation></ref><ref id="pone.0022885-Turaga1"><label>24</label><element-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Turaga</surname><given-names>SC</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Seung</surname><given-names>SH</given-names></name></person-group>
<year>2006</year>
<article-title>Online Learing in a Model Neural Integrator.</article-title>
<source>Cosyne</source>
</element-citation></ref><ref id="pone.0022885-Collewijn1"><label>25</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Collewijn</surname><given-names>H</given-names></name></person-group>
<year>1977</year>
<article-title>Optokinetic and vestibulo-ocular reflexes in dark-reared rabbits.</article-title>
<source>Experimental Brain Research</source>
<volume>27</volume>
<fpage>287</fpage>
<lpage>300</lpage>
<pub-id pub-id-type="pmid">301828</pub-id></element-citation></ref><ref id="pone.0022885-Harris1"><label>26</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Harris</surname><given-names>LR</given-names></name><name><surname>Cynader</surname><given-names>M</given-names></name></person-group>
<year>1981</year>
<article-title>The eye movements of the dark-reared cat.</article-title>
<source>Experimental Brain Research</source>
<volume>44</volume>
<fpage>41</fpage>
<lpage>56</lpage>
<pub-id pub-id-type="pmid">7274362</pub-id></element-citation></ref><ref id="pone.0022885-Seung3"><label>27</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name></person-group>
<year>1998</year>
<article-title>Continuous attractors and oculomotor control.</article-title>
<source>Neural Networks</source>
<volume>11</volume>
<fpage>1253</fpage>
<lpage>1258</lpage>
<pub-id pub-id-type="pmid">12662748</pub-id></element-citation></ref><ref id="pone.0022885-Eliasmith2"><label>28</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name></person-group>
<year>2003</year>
<source>Neural engineering: computation, representation, and dynamics in neurobiological systems</source>
<publisher-name>MIT Press</publisher-name>
</element-citation></ref><ref id="pone.0022885-Arnold3"><label>29</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Arnold</surname><given-names>DB</given-names></name><name><surname>Robinson</surname><given-names>DA</given-names></name></person-group>
<year>1991</year>
<article-title>A learning network model of the neural integrator of the oculomotor system.</article-title>
<source>Biological cybernetics</source>
<volume>64</volume>
<fpage>447</fpage>
<lpage>454</lpage>
<pub-id pub-id-type="pmid">1863658</pub-id></element-citation></ref><ref id="pone.0022885-Major2"><label>30</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Major</surname><given-names>G</given-names></name><name><surname>Baker</surname><given-names>R</given-names></name><name><surname>Aksay</surname><given-names>E</given-names></name><name><surname>Mensh</surname><given-names>B</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><etal/></person-group>
<year>2004</year>
<article-title>Plasticity and tuning by visual feedback of the stability of a neural integrator.</article-title>
<source>Proceedings of the National Academy of Sciences of the United States of America</source>
<volume>101</volume>
<fpage>7739</fpage>
<lpage>7744</lpage>
<pub-id pub-id-type="pmid">15136746</pub-id></element-citation></ref><ref id="pone.0022885-Durstewitz1"><label>31</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name></person-group>
<year>2003</year>
<article-title>Self-Organizing Neural Integrator Predicts Interval Times through Climbing Activity.</article-title>
<source>J Neurosci</source>
<volume>23</volume>
<fpage>5342</fpage>
<lpage>5353</lpage>
<pub-id pub-id-type="pmid">12832560</pub-id></element-citation></ref><ref id="pone.0022885-Renart1"><label>32</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Song</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group>
<year>2003</year>
<article-title>Robust spatial working memory through homeostatic synaptic scaling in heterogeneous cortical networks.</article-title>
<source>Neuron</source>
<volume>38</volume>
</element-citation></ref><ref id="pone.0022885-Zhang2"><label>33</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name></person-group>
<year>1996</year>
<article-title>Representation of spatial orientation by the intrinsic dynamics of the headdirection cell ensemble: A theory.</article-title>
<source>Journal of Neuroscience</source>
<volume>16</volume>
<fpage>2112</fpage>
<lpage>2126</lpage>
<pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="pone.0022885-Conklin1"><label>34</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Conklin</surname><given-names>J</given-names></name><name><surname>Eliasmith</surname><given-names>C</given-names></name></person-group>
<year>2005</year>
<article-title>A controlled attractor network model of path integration in the rat.</article-title>
<source>Journal of computational neuroscience</source>
<volume>18</volume>
<fpage>March</fpage>
</element-citation></ref><ref id="pone.0022885-Miller1"><label>35</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Miller</surname><given-names>P</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group>
<year>2003</year>
<article-title>A recurrent network model of somatosensory parametric working memory in the prefrontal cortex.</article-title>
<source>Cerebral Cortex</source>
<volume>13</volume>
<fpage>1208</fpage>
<lpage>1218</lpage>
<pub-id pub-id-type="pmid">14576212</pub-id></element-citation></ref><ref id="pone.0022885-Seung4"><label>36</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Lee</surname><given-names>DD</given-names></name><name><surname>Reis</surname><given-names>BY</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group>
<year>2000</year>
<article-title>The autapse: a simple illustration of shortterm analog memory storage by tuned synaptic feedback.</article-title>
<source>Journal of computational neuroscience</source>
<volume>9</volume>
<fpage>171</fpage>
<lpage>85</lpage>
<pub-id pub-id-type="pmid">11030520</pub-id></element-citation></ref><ref id="pone.0022885-McCrea1"><label>37</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>McCrea</surname><given-names>R</given-names></name><name><surname>Cullen</surname><given-names>K</given-names></name></person-group>
<year>1992</year>
<article-title>Responses of vestibular and prepositus neurons to head movements during voluntary suppression of the vestibulo-ocular reflex.</article-title>
<source>New York Acacdemy of Sciences</source>
<fpage>379</fpage>
<lpage>395</lpage>
</element-citation></ref><ref id="pone.0022885-Stevens1"><label>38</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Stevens</surname><given-names>CF</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group>
<year>1994</year>
<article-title>Changes in reliability of synaptic function as a mechanism for plasticity.</article-title>
<source>Nature</source>
<volume>371</volume>
<fpage>704</fpage>
<lpage>707</lpage>
<pub-id pub-id-type="pmid">7935816</pub-id></element-citation></ref><ref id="pone.0022885-Henneman1"><label>39</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Henneman</surname><given-names>E</given-names></name><name><surname>Mendell</surname><given-names>L</given-names></name></person-group>
<year>1981</year>
<article-title>Functional organization of motoneuron pool and its inputs.</article-title>
<person-group person-group-type="editor"><name><surname>Brooks</surname><given-names>VB</given-names></name></person-group>
<source>Handbook of physiology: The nervous system</source>
<publisher-loc>Bethesda, MD</publisher-loc>
<publisher-name>American Physiological Society, volume 2</publisher-name>
</element-citation></ref><ref id="pone.0022885-Lass1"><label>40</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lass</surname><given-names>Y</given-names></name><name><surname>Abeles</surname><given-names>M</given-names></name></person-group>
<year>1975</year>
<article-title>Transmission of information by the axon. I: Noise and memory in the myelinated nerve fiber of the frog.</article-title>
<source>Biological Cybernetics</source>
<volume>19</volume>
<fpage>61</fpage>
<lpage>67</lpage>
<pub-id pub-id-type="pmid">172162</pub-id></element-citation></ref><ref id="pone.0022885-Weissman1"><label>41</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Weissman</surname><given-names>BM</given-names></name><name><surname>DiScenna</surname><given-names>AO</given-names></name><name><surname>Leigh</surname><given-names>RJ</given-names></name></person-group>
<year>1989</year>
<article-title>Maturation of the vestibulo-ocular reflex in normal infants during the first 2 months of life.</article-title>
<source>Neurology</source>
<volume>39</volume>
<fpage>534</fpage>
<lpage>538</lpage>
<pub-id pub-id-type="pmid">2927678</pub-id></element-citation></ref><ref id="pone.0022885-Kmpf1"><label>42</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>K&#x000f6;mpf</surname><given-names>D</given-names></name><name><surname>Piper</surname><given-names>H</given-names></name></person-group>
<year>1987</year>
<article-title>Eye movements and vestibulo-ocular reflex in the blind.</article-title>
<source>Journal of Neurology</source>
<volume>234</volume>
<fpage>337</fpage>
<lpage>341</lpage>
<pub-id pub-id-type="pmid">3612205</pub-id></element-citation></ref><ref id="pone.0022885-Kapoula1"><label>43</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kapoula</surname><given-names>Z</given-names></name><name><surname>Optican</surname><given-names>LM</given-names></name><name><surname>Robinson</surname><given-names>DA</given-names></name></person-group>
<year>1989</year>
<article-title>Visually induced plasticity of postsaccadic ocular drift in normal humans.</article-title>
<source>Journal of neurophysiology</source>
<volume>61</volume>
<fpage>879</fpage>
<lpage>891</lpage>
<pub-id pub-id-type="pmid">2723732</pub-id></element-citation></ref><ref id="pone.0022885-Aksay1"><label>44</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Aksay</surname><given-names>E</given-names></name><name><surname>Baker</surname><given-names>R</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group>
<year>2000</year>
<article-title>Anatomy and discharge properties of premotor neurons in the goldfish medulla that have eye-position signals during fixations.</article-title>
<source>Journal of Neurophysiology</source>
<volume>84</volume>
<fpage>1035</fpage>
<lpage>1049</lpage>
<pub-id pub-id-type="pmid">10938326</pub-id></element-citation></ref><ref id="pone.0022885-Ikezu1"><label>45</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Ikezu</surname><given-names>T</given-names></name><name><surname>Gendelman</surname><given-names>H</given-names></name></person-group>
<year>2008</year>
<source>Neuroimmune Pharmacology</source>
<publisher-name>Springer</publisher-name>
</element-citation></ref><ref id="pone.0022885-Weber1"><label>46</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Weber</surname><given-names>RB</given-names></name><name><surname>Daroff</surname><given-names>RB</given-names></name></person-group>
<year>1972</year>
<article-title>Corrective movements following refixation saccades: Type and control system analysis.</article-title>
<source>Vision Research</source>
<volume>12</volume>
<fpage>467</fpage>
<lpage>475</lpage>
<pub-id pub-id-type="pmid">5021911</pub-id></element-citation></ref><ref id="pone.0022885-Glasser1"><label>47</label><element-citation publication-type="other">
<person-group person-group-type="editor"><name><surname>Glasser</surname><given-names>J</given-names></name></person-group>
<year>1999</year>
<fpage>327</fpage>
<lpage>343</lpage>
<comment>(1999) Neuro-Ophthamalmology, Lippincott Williams &#x00026; Wilkins, chapter Chapter 9: Eye Movement Characteristics and Recording Techniques</comment>
</element-citation></ref><ref id="pone.0022885-Park1"><label>48</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group>
<year>2007</year>
<article-title>Corrective saccades drive saccadic adaptation independently of explicit interpretation of retinal error.</article-title>
<source>J Vis</source>
<volume>7</volume>
<fpage>142</fpage>
<lpage>142</lpage>
</element-citation></ref><ref id="pone.0022885-Leigh1"><label>49</label><element-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Leigh</surname><given-names>RJ</given-names></name><name><surname>Zee</surname><given-names>DS</given-names></name></person-group>
<year>2006</year>
<article-title>The neurology of eye movements, Oxford University Press, chapter Chapter 5: Gaze Holding and the Neural Integrator.</article-title>
<fpage>241</fpage>
<lpage>260</lpage>
<comment>4 edition</comment>
</element-citation></ref><ref id="pone.0022885-Lewis1"><label>50</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lewis</surname><given-names>RF</given-names></name><name><surname>Zee</surname><given-names>DS</given-names></name><name><surname>Gaymard</surname><given-names>BM</given-names></name><name><surname>Guthrie</surname><given-names>BL</given-names></name></person-group>
<year>1994</year>
<article-title>Extraocular muscle proprioception functions in the control of ocular alignment and eye movement conjugacy.</article-title>
<source>J Neurophysiol</source>
<volume>72</volume>
<fpage>1028</fpage>
<lpage>1031</lpage>
<pub-id pub-id-type="pmid">7983509</pub-id></element-citation></ref><ref id="pone.0022885-Hess1"><label>51</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hess</surname><given-names>K</given-names></name><name><surname>Reisine</surname><given-names>H</given-names></name><name><surname>D&#x000fc;rsteler</surname><given-names>M</given-names></name></person-group>
<year>1985</year>
<article-title>Normal eye drift and saccadic drift correction in darkness.</article-title>
<source>Neuro-ophthalmology</source>
<volume>5</volume>
<fpage>247</fpage>
<lpage>252</lpage>
</element-citation></ref><ref id="pone.0022885-Hardie1"><label>52</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hardie</surname><given-names>J</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name></person-group>
<year>2009</year>
<article-title>Synaptic depolarization is more effective than backpropagating action potentials during induction of associative long-term potentiation in hippocampal pyramidal neurons.</article-title>
<source>The Journal of neuroscience: the official journal of the Society for Neuroscience</source>
<volume>29</volume>
<fpage>3233</fpage>
<lpage>41</lpage>
<pub-id pub-id-type="pmid">19279260</pub-id></element-citation></ref><ref id="pone.0022885-Porrill1"><label>53</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Porrill</surname><given-names>J</given-names></name><name><surname>Dean</surname><given-names>P</given-names></name><name><surname>Stone</surname><given-names>JV</given-names></name></person-group>
<year>2004</year>
<article-title>Recurrent cerebellar architecture solves the motor-error problem.</article-title>
<source>Proceedings Biological sciences/The Royal Society</source>
<volume>271</volume>
<fpage>789</fpage>
<lpage>96</lpage>
</element-citation></ref><ref id="pone.0022885-Montague1"><label>54</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Montague</surname><given-names>PR</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Sejnowski</surname><given-names>T</given-names></name></person-group>
<year>1996</year>
<article-title>A Framework for Mesencephalic Predictive Hebbian Learning.</article-title>
<source>Journal of Neuroscience</source>
<volume>16</volume>
<fpage>1936</fpage>
<lpage>1947</lpage>
<pub-id pub-id-type="pmid">8774460</pub-id></element-citation></ref><ref id="pone.0022885-Rao1"><label>55</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>
<year>2001</year>
<article-title>Spike-timing-dependent Hebbian plasticity as temporal difference learning.</article-title>
<source>Neural computation</source>
<volume>13</volume>
<fpage>2221</fpage>
<lpage>37</lpage>
<pub-id pub-id-type="pmid">11570997</pub-id></element-citation></ref><ref id="pone.0022885-Vasilaki1"><label>56</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Vasilaki</surname><given-names>E</given-names></name><name><surname>Fr&#x000e9;maux</surname><given-names>N</given-names></name><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group>
<year>2009</year>
<article-title>Spike-based reinforcement learning in continuous state and action space: when policy gradient methods fail.</article-title>
<source>PLoS Computational Biology</source>
<volume>5</volume>
</element-citation></ref><ref id="pone.0022885-Georgopoulos1"><label>57</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Kettner</surname><given-names>RE</given-names></name></person-group>
<year>1986</year>
<article-title>Neuronal population coding of movement direction.</article-title>
<source>Science</source>
<volume>243</volume>
</element-citation></ref><ref id="pone.0022885-Jacobs1"><label>58</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>J</given-names></name><name><surname>Dell'Osso</surname><given-names>L</given-names></name></person-group>
<year>2004</year>
<article-title>Congenital nystagmus: Hypotheses for its genesis and complex waveforms within a behavioral ocular motor system model.</article-title>
<source>Journal of Vision</source>
<volume>4</volume>
<fpage>604</fpage>
<lpage>625</lpage>
<pub-id pub-id-type="pmid">15330705</pub-id></element-citation></ref><ref id="pone.0022885-DellOsso1"><label>59</label><element-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Dell'Osso</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name></person-group>
<year>2008</year>
<article-title>Behavioral OMS Model v1.4 OMLAB Report #070108.</article-title>
<comment><ext-link ext-link-type="uri" xlink:href="http://www.omlab.org/OMLAB_page/Teaching/teaching.html">http://www.omlab.org/OMLAB_page/Teaching/teaching.html</ext-link></comment>
</element-citation></ref><ref id="pone.0022885-Wang1"><label>60</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Dell'Osso</surname><given-names>L</given-names></name></person-group>
<year>2009</year>
<article-title>Factors influencing pursuit ability in infantile nystagmus syndrome: Target timing and foveation capability.</article-title>
<source>Vision Research</source>
<volume>49</volume>
<fpage>182</fpage>
<lpage>189</lpage>
<pub-id pub-id-type="pmid">18996408</pub-id></element-citation></ref><ref id="pone.0022885-Redish2"><label>61</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name></person-group>
<year>1999</year>
<source>Beyond the cognitive map</source>
<publisher-loc>Cambridge, MA</publisher-loc>
<publisher-name>MIT Press</publisher-name>
</element-citation></ref><ref id="pone.0022885-Bekolay1"><label>62</label><element-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Bekolay</surname><given-names>T</given-names></name></person-group>
<year>2010</year>
<article-title>Learning nonlinear functions on vectors: examples and predictions.</article-title>
<comment>Technical Report CTN&#x02013;TR&#x02013;20101217&#x02013;010, Centre for Theoretical Neuroscience, Waterloo</comment>
</element-citation></ref><ref id="pone.0022885-Williams1"><label>63</label><element-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Williams</surname><given-names>RJ</given-names></name><name><surname>Zipser</surname><given-names>D</given-names></name></person-group>
<year>1995</year>
<article-title>Back-propagation: Theory, architectures, and applications.</article-title>
<person-group person-group-type="editor"><name><surname>Chauvin</surname><given-names>Y</given-names></name><name><surname>Rumelhart</surname><given-names>DE</given-names></name></person-group>
<source>Developments in connectionist theory, Erlbaum</source>
<fpage>433</fpage>
<lpage>486</lpage>
</element-citation></ref></ref-list></back></article>