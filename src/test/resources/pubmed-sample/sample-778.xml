
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med Inform Decis Mak</journal-id><journal-id journal-id-type="iso-abbrev">BMC Med Inform Decis Mak</journal-id><journal-title-group><journal-title>BMC Medical Informatics and Decision Making</journal-title></journal-title-group><issn pub-type="epub">1472-6947</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31215427</article-id><article-id pub-id-type="pmc">6580478</article-id><article-id pub-id-type="publisher-id">833</article-id><article-id pub-id-type="doi">10.1186/s12911-019-0833-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Technical Advance</subject></subj-group></article-categories><title-group><article-title>Ear biometrics for patient identification in global health: a field study to test the effectiveness of an image stabilization device in improving identification accuracy</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Etter</surname><given-names>Lauren P.</given-names></name><address><email>laetter@bu.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ragan</surname><given-names>Elizabeth J.</given-names></name><address><email>elizabeth.ragan@bmc.org</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Campion</surname><given-names>Rachael</given-names></name><address><email>rcampion@bu.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Martinez</surname><given-names>David</given-names></name><address><email>dmartinz@bu.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3353-0617</contrib-id><name><surname>Gill</surname><given-names>Christopher J.</given-names></name><address><phone>617-358-2438</phone><email>cgill@bu.edu</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7558</institution-id><institution-id institution-id-type="GRID">grid.189504.1</institution-id><institution>College of Engineering, Boston University, </institution></institution-wrap>Boston, MA USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2183 6745</institution-id><institution-id institution-id-type="GRID">grid.239424.a</institution-id><institution>Department of Medicine, Section of Infectious Diseases, Boston Medical Center, </institution></institution-wrap>Boston, USA </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7558</institution-id><institution-id institution-id-type="GRID">grid.189504.1</institution-id><institution>Department of Global Health, </institution><institution>Boston University School of Public Health, </institution></institution-wrap>801 Massachusetts Avenue, Boston, MA 02118 USA </aff></contrib-group><pub-date pub-type="epub"><day>18</day><month>6</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>18</day><month>6</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>19</volume><elocation-id>114</elocation-id><history><date date-type="received"><day>24</day><month>10</month><year>2018</year></date><date date-type="accepted"><day>6</day><month>6</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s). 2019</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">In many low and middle-income countries (LMICs), difficulties in patient identification are a major obstacle to the delivery of longitudinal care. In absence of unique identifiers, biometrics have emerged as an attractive solution to the identification problem. We developed an mHealth App for subject identification using pattern recognition around ear morphology (Project SEARCH (Scanning EARS for Child Health). Early field work with the SEARCH App revealed that image stabilization would be required for optimum performance.</p></sec><sec><title>Methods</title><p id="Par2">To improve image capture, we designed and tested a device (the &#x02018;Donut&#x02019;), which standardizes distance, angle, rotation and lighting. We then ran an experimental trial with 194 participants to measure the impact of the Donut on identification rates. Images of the participant&#x02019;s left ear were taken both with and without use of the Donut, then processed by the SEARCH algorithm, measuring the top one and top ten most likely matches.</p></sec><sec><title>Results</title><p id="Par3">With the Donut, the top one identification rate and top ten identification rates were 99.5 and 99.5%, respectively, vs. 38.4 and 24.1%, respectively, without the Donut (<italic>P</italic>&#x000a0;&#x0003c;&#x02009;0.0001 for each comparison). In sensitivity analyses, crop technique during pre-processing of images had a powerful impact on identification rates, but this too was facilitated through the Donut.</p></sec><sec><title>Conclusions</title><p id="Par4">By standardizing lighting, angle and spatial location of the ear, the Donut achieved near perfect identification rates on a cohort of 194 participants, proving the feasibility and effectiveness of using the ear as a biometric identifier.</p></sec><sec><title>Trial registration</title><p id="Par5">This study did not include a medical intervention or assess a medical outcome, and therefore did not meet the definition of a human subjects research study as defined by FDAAA. We did not register our study under <ext-link ext-link-type="uri" xlink:href="http://clinicaltrials.gov">clinicaltrials.gov</ext-link>.</p></sec><sec><title>Electronic supplementary material</title><p>The online version of this article (10.1186/s12911-019-0833-9) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Ear biometrics</kwd><kwd>Identification</kwd><kwd>Patient identification</kwd><kwd>Global health</kwd><kwd>Public health</kwd><kwd>Electronic medical record</kwd><kwd>Image stabilization</kwd><kwd>Pattern recognition algorithm</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par14">Any public health, clinical, or research program that requires collecting and acting upon longitudinal data critically depends on accurately and repeatedly identifying individuals over time and space.</p><p id="Par15">Consider a newborn baby girl in Boston. There, her routine well-child care is delivered seamlessly because she is easily identified using multiple identifiers: home address, birth certificate, name and date of birth, insurance card, social security number, etc. Now, imagine that same infant girl in rural Zambia. As in many low and middle-income countries (LMICs), she has no formal mailing address; she is too young to have been named (in Zambia, naming is often deferred until 6 weeks of age); her family is too poor to participate in the national insurance system; and since she was delivered at home, a still-frequent occurrence in many countries, her birth was never formally registered, leaving her without a birth certificate or social security number.</p><p id="Par16">Given this reality, a Zambian infant&#x02019;s well-child care is coordinated using an &#x02018;Under Five Card&#x02019;, a 4&#x02009;&#x000d7;&#x02009;5 inch hard stock paper trifold which remains the mother&#x02019;s responsibility to retain. When cards are lost or degraded to illegibility, a child&#x02019;s vaccination history, growth curves, HIV screening data, and acute care history are irretrievably lost. In Zambia, as in many LMICs, the lack of robust patient identification renders even routine medical care fragmented, redundant, porous, and inefficient, and is a major barrier to the effective management of preventable and chronic diseases.</p><p id="Par17">Recent years have seen an explosion of interest in developing electronic medical records (EMRs) adapted for LMICs to help facilitate longitudinal care and transfer the burden of record retention from the patient back to the clinic [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR6">6</xref>]. However, the enormous potential of EMRs can only be realized once the problem of subject identification has been solved. This is far from a reality. With the growing ubiquity of smartphones with powerful processing capacity, mobile health (mHealth) applications (Apps) may be particularly suited to solving the patient identification problem [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>]. </p><p id="Par18">In Project SEARCH (Scanning EARs for Child Health), our multidisciplinary team of public health, engineering, and computer science faculty and students at Boston University (BU) has focused on solving the identification problem through pattern recognition analysis of biometric data, using ear morphology as the identifier. Biometric data offer distinct advantages over external identifiers. By definition, biometric features are intrinsic and cannot be lost, left at home, sold, or traded. And the choice of ears is logical, offering clear advantages over other biometric identifiers: finger prints are stigmatized by association with law enforcement; faces lack anonymity; and iris/retinal scans require magnification and external lighting, rendering them futile for infants. By contrast, ears are impersonal, easily imaged with a smartphone&#x02019;s camera with little to no discomfort, and are as unique as fingerprints [<xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR19">19</xref>]. </p><p id="Par19">Former work under the scope of Project SEARCH has involved three studies. In the analog proof of concept, a 3-dimensional object (the ear) was rendered into a set of stereotypical digital features. In a blinded matching exercise, an individual ear allowed for high re-identification rates (83%), proving the ear identifiable even with a relatively crude, analog approach [<xref ref-type="bibr" rid="CR20">20</xref>]. In the digital proof of concept evaluation, Project SEARCH created the first version of the SEARCH mobile app. The App was built around a powerful pattern recognition algorithm (PRA) called &#x02018;Scale Invariant Feature Transform&#x02019; (SIFT), [<xref ref-type="bibr" rid="CR21">21</xref>] and allows one to register a new subject, capture an image of the ear, render it into a digital matrix, add this to a library of digital renditions, and then later interrogate that library of renditions to obtain a match. [<xref ref-type="bibr" rid="CR22">22</xref>] The results from this experiment provided context that the SEARCH App was viable, but highlighted clear limitations. Most notably, the experiment used a set of images taken from an on-line database and did not evaluate the App&#x02019;s performance in the face of random error introduced by the image capture process. In the third experiment, the &#x02018;real-world&#x02019; evaluation, a birth cohort of 50 infants at a local pediatrics office were enrolled in a study to assess the process of image capture and how this affected the App&#x02019;s performance. The analysis yielded critical results: random error around image capture had a profound negative impact on the accuracy of identification resulting in an inadequate recapture accuracy of just 27.3% (unpublished). All previous work clearly highlighted that in order for this simplified, computationally-efficient method to work, there was a need to minimize sources of random variation during image capture.</p><p id="Par20">Results from these previous experiments set the stage for the current study, which was conceived and conducted by a team of three Boston University undergraduate engineering students in fulfilment of their senior project requirement under mentorship from professors of engineering and public health. The twin goals of this study were: (1) to design an image stabilization device that would improve the rate of re-identification by neutralizing variables of lighting, distance, and angle of image capture; and (2) to test how the use of this device improved individual identification rates using the SEARCH algorithm.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Technological background and system architecture</title><p id="Par21">Literature suggests that most ear biometric identification processes make use of 2D feature extraction techniques [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]. Under previous work through Project SEARCH, a student team from the Computer Science Department at Boston University analyzed three common feature extraction methods and their accuracy when applied to ear biometric identification: Local Binary Patterns (LBPs), Generic Fourier Descriptor (GFD), and Scale Invariant Feature Transform (SIFT). To analyze these methods, the SEARCH team used 493 ear images of 125 subjects from the IIT Delhi Ear Image Database, and found that SIFT gave the highest accuracy in identifying the top 1 match correctly (96.5%) [<xref ref-type="bibr" rid="CR22">22</xref>]. Since images were taken from an ear image database, where conditions of illumination and out of plane image capture angle are non-variable, this initial analysis did not take into account variables present in the field. Project SEARCH used the SIFT algorithm in the form of an iOS application, to test on a cohort of 50 infants in a clinical setting. In the face of random error, recapture rate plummeted to 27.3% (unpublished). The need to control for random error inherent in field settings set the stage for our current experiment.</p><p id="Par22">For our experiment, we use the Matlab algorithm developed in that initial study (with few modifications) in order to test the effect that neutralizing random error in the image capture process has on identification rates. The algorithm reads in two datasets of images (denoted first visit and second visit images, respectively), which are passed in as single precision, greyscale images. The images undergo a preprocessing step to resize and auto-crop all images passed into the system. Preprocessing limits the number of pixels that the open-source SIFT functions analyze, drastically cutting down processing time, which simulates implementation in a mobile application. After preprocessing, the algorithm reads in the first visit images to establish the cohort, and loops through the second visit images independently. A best (top one) match and a top ten list of matches for each image are determined based on a set of SIFT features, i.e. points of high contrast in the image. Each image is designated a SIFT feature vector, and the match is determined using a function that compares SIFT feature vectors. After all second visit images are read in and matched to images in the first visit dataset, the algorithm determines if the correct match was made. The correct match is determined by a function that checks if the matches made (both top one and top ten, independently) had the same image name. The name assigned to the images correspond to the participant number, and thus are the same in each dataset. An identification rate is computed (for both a top 1 match and a top 10 match), and the process repeats, using the second visit images to establish the cohort, looping independently through first visit images. The average of both processes is taken as the overall identification rate, yielding an averaged top match identification rate, and an average top ten identification rate. The algorithm runs twice, once yielding identification rates for images taken with the Donut (standardizing variables in the image capture process) and the second time yielding identification rates for images taken without the Donut (random error present). Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> provides a visual representation of the algorithm; access to the Matlab functions and main script are provided as Additional file <xref rid="MOESM1" ref-type="media">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption><p>This figure depicts high-level system architecture of the Matlab algorithm used in our experiment</p></caption><graphic xlink:href="12911_2019_833_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec4"><title>Image stabilization device design &#x02013; creating the &#x02018;Donut&#x02019;</title><p id="Par23">Although SIFT is considered a robust pattern recognition algorithm (controlling for angle and illumination variability fairly well when compared to other methods of pattern recognition), it was clear from SEARCH&#x02019;s previous experiments that images generated in a lab setting and run through the SIFT algorithm outperformed images taken in the field, where angle and illumination varied greatly between images. Recreating lab conditions from field-generated images through image processing techniques is difficult to do when dealing with clinical field settings that are unpredictable in nature. For example, movement of the patient, ambient lighting changes, and hair occlusion are some of the many factors present in the field that are unpredictable and hard to control for using image processing techniques alone. Because of this, our approach was to create a simple image stabilization system that is used during the image capture process to create lab-like conditions. In order to create an effective stabilization system for the phone during image capture, we designed and manufactured a device - a lightweight opaque cylinder with a platform on the back to attach a smartphone, a hole to allow for the phone&#x02019;s camera, and an internal 360-degree light source using energy-efficient light emitting diode (LED) strips. Understanding the use context of the device heavily informed the design process. A set of design requirements to minimize variables affecting the image capture process were developed from research, feedback from previous experiments, and heavy communication with members of the larger Project SEARCH team. Device requirements included: lightweight, accommodating to a range of ear sizes, able to standardize lighting of the image (by neutralizing external light and providing a constant and consistent source of internal light), sustainably powered, controlled for angle rotation, allowing of the attachment of varying sized phones, and aesthetically appropriate. After identifying the most important elements of the design, we created weighted metrics to rank each design feature. Informed decisions were guided by Pugh charts (Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>), our chosen tool for alternate design analysis [<xref ref-type="bibr" rid="CR24">24</xref>].<table-wrap id="Tab1"><label>Table 1</label><caption><p>Pugh Chart for Alternate Design Analysis</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2"/><th colspan="3">Concepts</th></tr><tr><th>Criteria</th><th>Weight</th><th>Diffused LED plate</th><th>Electroluminescent Ribbons</th><th>LED Strip</th></tr></thead><tbody><tr><td>Shadow</td><td>2</td><td>0</td><td>0</td><td>+</td></tr><tr><td>Power Required</td><td>1</td><td>0</td><td>0</td><td>&#x02013;</td></tr><tr><td>Shape</td><td>1</td><td>0</td><td>+</td><td>+</td></tr><tr><td>Buy/Make</td><td>1</td><td>0</td><td>0</td><td>+</td></tr><tr><td>Price</td><td>1</td><td>0</td><td>&#x02013;</td><td>0</td></tr><tr><td/><td>Weighted Total</td><td>0</td><td>0</td><td>3</td></tr></tbody></table><table-wrap-foot><p>This Pugh chart was used to rank lighting designs for our Donut. Key metrics are weighted according to importance (2 being a more important design feature than 1). We used the diffused LED design as a baseline (all zero ranks). A positive mark indicates that the design outperforms the diffused LED for the specific metric, while a negative mark indicates underperformance. Weighted totals were determined by replacing the positive and negative signs with positive and negative one values, and multiplying each value by the corresponding metric&#x02019;s weight. All values in each column were added, and totals for each design option were compared. The LED strip, with the highest total (3), was deemed the best lighting design option for the Donut</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec5"><title>Device manufacture method</title><p id="Par24">The Donut was first modeled using a Computer Aided Design software, CREO. This model was converted to an STL file, adjusting chord length and angle for optimal 3D printing. The Donut was manufactured using a Uprint 3D printer, painted black to neutralize external light, and then assembled with the remaining design features. The circuit was secured to the bottom circumference of the Donut. Foam padding with an antimicrobial leather covering were attached to the top circumference (the area in contact with the patient&#x02019;s head) in order to provide comfort and sterility. Finally, a bubble level (to standardize rotation of image capture) and Velcro (to easily mount the phone) were adhered to the back of the Donut. The final version that advanced to field testing is shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Fig. 2</label><caption><p>This figure shows the image stabilization device (the Donut). The leftmost image shows the back of the Donut, where the phone is attached. The bubble level is mounted on the top of the back of the Donut to control for angle rotation during image capture. The middle image shows looking into the Donut, the LED strip is laid along the inner base of the Donut. The right image demonstrates use of the Donut. The phone is mounted on the left, while the Donut interfaces with the participant on the right</p></caption><graphic xlink:href="12911_2019_833_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec6"><title>Clinical study methodology</title><p id="Par25">To validate the impact of the Donut on identification accuracy, we conducted a field study among BU undergraduate students, all of whom provided informed consent. This involved creating a cohort and database of ear images taken both with and without use of the Donut. After receiving IRB approval, the cohort was established by contacting a number of different student organizations across Boston University&#x02019;s campus.</p><p id="Par26">To simulate two distinct patient visits at a clinic, we constructed our experimental study to capture two sets of pictures of each participant&#x02019;s ear under different lighting conditions. To accomplish this, we stationed one study coordinator on one side of a classroom, and stationed a second study coordinator across the room facing a different direction (altering the ambient light). The participant visited the first study coordinator, where two images of their left ear were captured (one with the Donut and one without). These two images taken under identical ambient light, were designated &#x02018;first visit&#x02019; images for the corresponding datasets (with and without Donut). The participant then walked to the second study coordinator where the process was repeated &#x02013; designating &#x02018;second visit&#x02019; images for the corresponding datasets (with and without Donut). A total of four images of each participant&#x02019;s left ear were collected - two images taken with the Donut (first and second visit images), and two images taken without the Donut (first and second visit images).</p></sec><sec id="Sec7"><title>Power assumptions and statistical analysis</title><p id="Par27">Based on Project SEARCH&#x02019;s prior analog and digital proof of concept studies, we conservatively estimated the probability of a correct match at 80%: assuming that the Donut improved identification by 10%, we would need 199 participants. A type 1 error for the null hypothesis was 0.05 with 80% power. We used an uncorrected chi squared analysis to evaluate the null hypothesis that our device did not improve the identification accuracy. Once identification accuracies were determined, we used an uncorrected chi squared analysis to determine the statistical significance of using the Donut to aid the image capture process.</p><p id="Par28">In order to analyze data from our study, we modified the previously developed Matlab algorithm used by SEARCH [<xref ref-type="bibr" rid="CR22">22</xref>]. After running an image through the entire database, an &#x02018;identification&#x02019; or match is determined, which corresponds to the image stored in the database that has the most matching SIFT points, i.e., points of high contrast [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR23">23</xref>].</p><p id="Par29">Two distinct identification accuracies, equivalent to match probability, were determined by running the Matlab algorithm twice &#x02013; once using all images taken with the Donut and once using all images taken without the Donut. To analyze images taken with the Donut, two data sub-sets (&#x02018;first visit&#x02019;, with Donut; &#x02018;second visit&#x02019;, with Donut) were passed into the Matlab algorithm as single-precision, greyscale images. A pre-processing step was done, using built in Matlab functions, to crop and resize all images. Pre-processing limited the number of pixels that the algorithm analyzed, which drastically cut down processing time, allowing the algorithm to simulate the speed needed to be effectively implemented in a mobile application. We tested a number of different dimensions in order to determine the ideal crop for images both with and without the device, respectively.</p><p id="Par30">Next, the algorithm established the &#x02018;first visit&#x02019; images as the database, and looped through the &#x02018;second visit&#x02019; images independently, finding a best (top one) match and a top ten list of matches for each image, ranked in order of most likely match. After all &#x02018;second visit&#x02019; images were read in and matched to images in the database, the algorithm determined: 1) if the correct match was made (top one), or 2) if the correct match resided within the top ten most likely matches (top ten).</p><p id="Par31">An identification accuracy was computed (for both a top one match and a top ten match), and the process was repeated using the &#x02018;second visit&#x02019; images as the database, and looping independently through the &#x02018;first visit&#x02019; images. The average of both processes was taken as the overall identification accuracy, yielding an averaged top match identification accuracy, and an averaged top ten identification accuracy.</p><p id="Par32">In order to most accurately compare improvement due to solely lighting and angle we analyzed the bias introduced by an automated cropping methodology. We completed a sensitivity analysis, with the goal of assessing whether or not pre-processing the images had a different effect between the two image datasets (with and without use of Donut). To accomplish this, we ran the algorithm multiple times, changing both the crop length (corresponding to ear length) and the crop width (corresponding to ear width) of images taken with and without the Donut. We analyzed how changing these variables affected the output of top one identification accuracy. Sensitivity was quantified as the change in identification accuracy with respect to change in crop. If sensitivity values were large (&#x0003e;&#x02009;1), we deemed that the identification accuracy was largely affected by the crop. If values were largely different between the two datasets (with and without Donut), this would signify that pre-processing introduced bias between the datasets.</p></sec></sec><sec id="Sec8"><title>Results</title><sec id="Sec9"><title>Creating the Donut</title><p id="Par33">Internal lighting of the device was deemed the most important design feature. Without a way to control for variable lighting conditions, we would be unable to improve the signal-to-noise ratio, thus the rate of re-identification. An LED strip inlaid along the inner circumference of the device proved the best solution for standardizing internal lighting, notably reducing shadow more than alternatives, such as a diffused LED plate or electroluminescent ribbons. To achieve optimal lighting, we powered the device with a 9&#x02009;V battery &#x02013; chosen due to its optimized voltage for the LED strip, availability, and it being an easily replaceable power supply. In order to maintain consistent illumination, we wired the battery in series with a 33-&#x003a9; resistor, which served as a voltage regulator. As the battery runs down, light intensity stays constant above a critical threshold and then switches off (rather than slowly dimming, as with the common household flashlight). Dimensions of the device were determined from research on the focus length of cell-phone cameras and the average size of the human ear. [<xref ref-type="bibr" rid="CR25">25</xref>] These dimensions, which define the inner diameter and the depth of the device, were constrained by the average skull size of an infant (4&#x02009;in.) [<xref ref-type="bibr" rid="CR26">26</xref>]. The final specifications for the device are shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. Aesthetic appropriateness of the Donut was achieved by employing similar methodology used in the GE Adventure Series (Fig. <xref rid="Fig2" ref-type="fig">2</xref><bold>)</bold> [<xref ref-type="bibr" rid="CR24">24</xref>], [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>].<table-wrap id="Tab2"><label>Table 2</label><caption><p>Final Device Dimensions</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Inner Diameter</th><th>Depth</th><th>Battery Box Width</th><th>Battery Box Height</th><th>Battery Box Length</th></tr></thead><tbody><tr><td>Dimension (mm)</td><td char="." align="char">89</td><td char="." align="char">81</td><td char="." align="char">41</td><td char="." align="char">29</td><td char="." align="char">67.5</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec10"><title>Evaluation of the impact of the Donut on identification rates</title><p id="Par34">Complete image sets were obtained from 194 participants enrolled in the study. Cohort demographics are summarized in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref><bold>.</bold> The majority of participants were Caucasian (62%) and male (60%). All were undergraduate students at Boston University who had been invited to participate after SEARCH team members made brief presentations at classes and to on campus student groups.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Demographics of cohort</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Variables</th><th colspan="2"><italic>n</italic>&#x000a0;=&#x02009;194</th></tr></thead><tbody><tr><td rowspan="5">Race, %</td><td>Caucasian</td><td>120 (62%)</td></tr><tr><td>Asian</td><td>45 (23%)</td></tr><tr><td>Hispanic</td><td>13 (7%)</td></tr><tr><td>African American</td><td>6 (3%)</td></tr><tr><td>Other</td><td>10 (5%)</td></tr><tr><td rowspan="2">Sex, %</td><td>Male</td><td>116 (60%)</td></tr><tr><td>Female</td><td>78 (40%)</td></tr></tbody></table><table-wrap-foot><p>This table identifies the demographics of our 194 participant cohort. Both race and gender demographics are broken down by number and percent</p></table-wrap-foot></table-wrap></p><p id="Par35">After processing the images through the MatLab algorithm, the following identification accuracies were determined. Images taken with use of the Donut yielded a top one match accuracy of 95.9% and a top 10 match accuracy of 99.5%, significantly outperforming the matching rates without the Donut (Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>).<table-wrap id="Tab4"><label>Table 4</label><caption><p>Identification accuracies with and without use of the Donut</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Matched within top 10 most likely individuals (<italic>N</italic>&#x000a0;=&#x02009;194 paired images)</th><th>Matched to top ranked individual (<italic>N</italic>&#x000a0;=&#x02009;194 paired images)</th></tr></thead><tbody><tr><td>With Donut</td><td>99.5%</td><td>95.9%</td></tr><tr><td>Without Donut</td><td>38.4%</td><td>24.1%</td></tr><tr><td><italic>P</italic>-value</td><td>P&#x02009;&#x0003c;&#x02009;0.0001</td><td>P&#x02009;&#x0003c;&#x02009;0.0001</td></tr></tbody></table><table-wrap-foot><p>The identification accuracies are shown as percentages for each case: with Donut, top 10 accuracy; with Donut, top 1 accuracy; without Donut, top 10 accuracy; without Donut, top 1 accuracy</p></table-wrap-foot></table-wrap></p><p id="Par36">A sensitivity analysis was performed to determine what effect pre-processing the images had on identification rates. Figures&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a-d outline the results of the sensitivity analysis. In both cases (with and without the Donut), identification accuracy was most sensitive when changing the length of the crop (corresponding to cropping the length of the ear), while cropping the width of the ear had little to no effect on identification accuracies. There was little difference in sensitivity values between the two datasets.<fig id="Fig3"><label>Figs. 3</label><caption><p><bold>a</bold>-<bold>d</bold> These figures show the results of our sensitivity analysis. In each graph, x-axis corresponds to the dimension of the crop in pixels, while the y-axis corresponds to the identification accuracy. The data points were found by running the Matlab algorithm multiple times using a number of crop dimensions. The slope of each graph represents how the sensitivity of identification accuracy reacted to changing the crop dimension</p></caption><graphic xlink:href="12911_2019_833_Fig3_HTML" id="MO3"/></fig></p><p id="Par37">Additionally, we found that the Donut localized the ear during image capture. In order to be processed efficiently by the MatLab algorithm, images must be cropped and resized. For images taken with the Donut, we created an automatic crop in MatLab based on the edges of the Donut in the image. For images taken without the Donut, however, the ear was in a number of different spatial locations, making automatic crop dimensions difficult to determine and leading to cropping of parts of the image containing the ear, decreasing the identification accuracy.</p></sec></sec><sec id="Sec11"><title>Discussion</title><p id="Par38">In this experiment, we showed that the SEARCH algorithm&#x02019;s performance was significantly improved with the use of an image stabilization/standardization device (i.e., the Donut). A direct comparison of the results from this experiment show that, with the device, we improved the identification accuracy from just under 40 to &#x0003e;&#x02009;96% for the single most likely match, and to &#x0003e;&#x02009;99% within the top 10 most likely matches. A previously completed study was able to get an accuracy of 80%, without any device, by employing a box on the screen of the phone in which a user would place the ear. Using this prior study as our benchmark expected value of 80%, our current results significantly exceeded our expectations. As hypothesized, quality control around image capture, and the ability to standardize lighting, distance, centering, rotation, and angle of capture, is a key determinant of the accuracy of biometric matching via pattern recognition algorithms and is a problem that is readily addressed using our simple mechanical engineering solution. We view this as a major step forward in Project SEARCH, and one that allows us to move forward with improving our design in future iterations.</p><p id="Par39">Over the course of this experiment, we came across a number of notable findings. First, we found that hair occlusion within the image, in addition to lighting and rotation angle, was another factor that interfered with the image capture and identification process. While we had not planned this explicitly, it turned out that the design of the Donut tended to keep hair out of the image by consistently enclosing the ear during image capture. This is important because the pattern recognition algorithm is sensitive to all elements within the image, and hair, being mobile and therefore impossible to standardize, could be a source of a significant degree of random error. This unforeseen advantage to our device further limited the variables involved in the image capture process, likely contributing to the increased identification accuracy on images taken with our device. The implication is that future iterations of the Donut should include some method to systematically and more effectively exclude hair from the image, perhaps by including an internal shroud that would cup and therefore isolate the ear from the hair, head and neck, limiting the data for the SIFT algorithm to information just from the ear.</p><p id="Par40">Additionally, through pre-processing the images, we found that our Donut localized spatial location of the ear, while images taken without the Donut did not. Because of this, automatic crop dimensions were determined for each dataset, independently. Since the Donut localized the ear during image capture, automatic crop dimensions were easier to determine in comparison to images taken without the Donut. This further emphasizes the need to standardize how the image is centered on the ear during capture, a problem that could be solved by software modifications to the SEARCH App (e.g., by including an on-screen C-shaped guided to align with the edge of the ear while taking the image), through post-capture automatic processing of the image, through design modifications to the Donut, or potentially all three.</p><p id="Par41">Although our experiment was successful in many ways, there were clear limitations to our study that need to be addressed in future work. Major limitations to our methodology include that 1) this experiment was not done using an infant cohort, 2) identification accuracies were determined using a computer algorithm, not a fully developed smartphone application, and 3) there are newer computational methods that should also be considered and compared to SIFT&#x02019;s performance. First, the most rapid period of ear growth occurs within the first few years of an infant&#x02019;s life, and the algorithm has not been validated in a longitudinal field study on a cohort of infants. Validation in this context is a clear next step for testing the SEARCH platform. Second, our experiment tested identification rates using an algorithm after all data had been collected, and did not make use of a developed smartphone application. The application would need to be fully developed and tested in the field to prove that the capabilities of the application match that of the computer algorithm. Planning of a pragmatic field study in Zambia by assembling a longitudinal cohort of infants is currently underway. Lastly, these experiments used SIFT as the pattern recognition algorithm of choice. Since the start of SEARCH, there have been several advancements in the development of deep learning models in the field of computer vision. It should be noted that these models, although requiring a large amount of data to train, have the potential to improve performance in ear recognition. As research continues under Project SEARCH, an effort to explore these models is underway.</p></sec><sec id="Sec12"><title>Conclusions</title><p id="Par42">Our experiment adds to previous work done through Project SEARCH aimed at proving ear biometrics an accurate and reliable method of patient identification. We demonstrated viability of the SEARCH App in a real-world context, and proved that image standardization and stabilization through a device could minimize random error during image capture and thereby improve accuracy. This project has proven that our device is both necessary and effective for identifying individuals using a SIFT biometric recognition algorithm. In the future, by combining our device with an application, Project SEARCH will have a cost effective, accurate way to identify and link patients to their medical records in Zambia.</p></sec><sec sec-type="supplementary-material"><title>Additional file</title><sec id="Sec13"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="12911_2019_833_MOESM1_ESM.zip"><label>Additional file 1:</label><caption><p>Matlab functions and main script. (ZIP 4 kb)</p></caption></media></supplementary-material>
</p></sec></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>App</term><def><p id="Par6">Application</p></def></def-item><def-item><term>BU</term><def><p id="Par7">Boston University</p></def></def-item><def-item><term>CREO</term><def><p id="Par8">A computer aided design software</p></def></def-item><def-item><term>LED</term><def><p id="Par9">light emitting diode</p></def></def-item><def-item><term>LMIC</term><def><p id="Par10">Low and Middle-Income Countries</p></def></def-item><def-item><term>mHealth</term><def><p id="Par11">mobile health</p></def></def-item><def-item><term>SEARCH</term><def><p id="Par12">Scanning Ears for Child Health</p></def></def-item><def-item><term>SIFT</term><def><p id="Par13">Scale-Invariant Feature Transform</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><p>We would like to thank Professor William Hauser from the Mechanical Engineering Department at Boston University for his contributions and support throughout the study. We would also like to acknowledge and thank Dr. Sarah Bargal at the Boston University Computer Science Department for her time and support with the project, particularly in regard to the MatLab algorithm.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>The experiment was conceived, designed and executed by all LE, DM, RC, ER and CJG. Design of the device was completed by LE, DM and RC. Data collection and extraction into the algorithm was also by LE, DM and RC. LE wrote the paper, with contributions from ER and CJG. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>SEARCH was funded through a pilot grant provided by Boston University School of Public Health and through a crowd-funding campaign led by the SEARCH team. Funding for this portion of the project was used on materials to manufacture the Donut prototype. The funder played no role in the design, implementation or interpretation and reporting of this work.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></notes><notes><title>Ethics approval and consent to participate</title><p id="Par43">This study was approved by the Institutional Review Board of Boston University Medical Campus under protocol H35788. All participants provided written informed consent.</p></notes><notes><title>Consent for publication</title><p id="Par44">All participants provided consent for publication of images.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par45">The authors declare that they have no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Mitchell M, Hedt-Gauthier BL, Msellemu D, Nkaka M, Lesh N. Using electronic technology to improve clinical care - results from a before-after cluster trial to evaluate assessment and classification of sick children according to Integrated Management of Childhood Illness (IMCI) protocol in Tanzania. BMC Med Inform Decis Mak. 2013;13:95. Epub 2013/08/29. 10.1186/1472-6947-13-95. PubMed PMID: 23981292; PubMed Central PMCID: PMCPMC3766002.</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Tapsfield JB, Jane Bates M. Hospital based palliative care in sub-Saharan Africa; a six month review from Malawi. BMC Palliat Care. 2011;10:12. Epub 2011/07/12. 10.1186/1472-684X-10-12. PubMed PMID: 21740584; PubMed Central PMCID: PMCPMC3146929.</mixed-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oluoch</surname><given-names>Tom</given-names></name><name><surname>Santas</surname><given-names>Xenophon</given-names></name><name><surname>Kwaro</surname><given-names>Daniel</given-names></name><name><surname>Were</surname><given-names>Martin</given-names></name><name><surname>Biondich</surname><given-names>Paul</given-names></name><name><surname>Bailey</surname><given-names>Christopher</given-names></name><name><surname>Abu-Hanna</surname><given-names>Ameen</given-names></name><name><surname>de Keizer</surname><given-names>Nicolette</given-names></name></person-group><article-title>The effect of electronic medical record-based clinical decision support on HIV care in resource-constrained settings: A systematic review</article-title><source>International Journal of Medical Informatics</source><year>2012</year><volume>81</volume><issue>10</issue><fpage>e83</fpage><lpage>e92</lpage><pub-id pub-id-type="doi">10.1016/j.ijmedinf.2012.07.010</pub-id><pub-id pub-id-type="pmid">22921485</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>Mohammed K.</given-names></name><name><surname>Shah</surname><given-names>Seema</given-names></name><name><surname>Tandon</surname><given-names>Nikhil</given-names></name></person-group><article-title>Review of Electronic Decision-Support Tools for Diabetes Care: A Viable Option for Low- and Middle-Income Countries?</article-title><source>Journal of Diabetes Science and Technology</source><year>2011</year><volume>5</volume><issue>3</issue><fpage>553</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1177/193229681100500310</pub-id><pub-id pub-id-type="pmid">21722571</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Hotchkiss DR, Diana ML, Foreit KG. How can routine health information systems improve health systems functioning in low- and middle-income countries? Assessing the evidence base. Adv Health Care Manag 2012;12:25&#x02013;58. Epub 2012/08/17. PubMed PMID: 22894044.</mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piette</surname><given-names>John</given-names></name><name><surname>Lun</surname><given-names>KC</given-names></name><name><surname>Moura</surname><given-names>Lincoln</given-names></name><name><surname>Fraser</surname><given-names>Hamish</given-names></name><name><surname>Mechael</surname><given-names>Patricia</given-names></name><name><surname>Powell</surname><given-names>John</given-names></name><name><surname>Khoja</surname><given-names>Shariq</given-names></name></person-group><article-title>Impacts of e-health on the outcomes of care in low- and middle-income countries: where do we go from here?</article-title><source>Bulletin of the World Health Organization</source><year>2012</year><volume>90</volume><issue>5</issue><fpage>365</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.2471/BLT.11.099069</pub-id><pub-id pub-id-type="pmid">22589570</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">van Velthoven MH, Car J, Zhang Y, Marusic A. mHealth series: new ideas for mHealth data collection implementation in low- and middle-income countries. J Glob Health 2013;3(2):020101. Epub 2013/12/24. 10.7189/jogh.03.020101. PubMed PMID: 24363911; PubMed Central PMCID: PMCPMC3868820.</mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Heerden</surname><given-names>Alastair</given-names></name><name><surname>Tomlinson</surname><given-names>Mark</given-names></name><name><surname>Swartz</surname><given-names>Leslie</given-names></name></person-group><article-title>Point of care in your pocket: a research agenda for the field of m-health</article-title><source>Bulletin of the World Health Organization</source><year>2012</year><volume>90</volume><issue>5</issue><fpage>393</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.2471/BLT.11.099788</pub-id><pub-id pub-id-type="pmid">22589575</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A.K.</given-names></name><name><surname>Ross</surname><given-names>A.</given-names></name><name><surname>Prabhakar</surname><given-names>S.</given-names></name></person-group><article-title>An Introduction to Biometric Recognition</article-title><source>IEEE Transactions on Circuits and Systems for Video Technology</source><year>2004</year><volume>14</volume><issue>1</issue><fpage>4</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2003.818349</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Burge M, B W. Ear biometrics in computer vision. Proceedings of the international conference on pattern recognition. 2000:822&#x02013;6.</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Iannerelli</surname><given-names>AV</given-names></name></person-group><source>Ear identification (forensic identification series). Paramount publishing company</source><year>1989</year></element-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Yan P, KB. Ear biometrics using 2D and 3D images. Proceedings IEEE Conference Computer Vision and Pattern Recognition Workshop in Advanced 3D imaging for safety and security2005. p. 121.</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Chang K, Bowyer K, Sarkar S, B. V. Comparison and combination of ear and face images in appearance-based biometrics. IEEE Transactions Pattern Analysis Machine Intelligence2003. p. 1160&#x02013;65.</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Moreno B, AS. On the use of outer ear images for personal identification in security applications. Proceedings of IEEE 33rd annual international conference on Secruity Technology1999. p. 469&#x02013;76.</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Pun K, YM. Recent advances in ear biometrics. Proceedings of the sixth IEEE International Conference on automatic face and gesture recognition2004. p. 164&#x02013;69.</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Khursheed F, AM. time series model based personal verification using ear biometrics. 4th international conference on computer and communication Technology2013. p. 264&#x02013;68.</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Ibrahim MI, Nixon MS, SM. The effect of time on ear biometrics. IEEE 2011;2011.</mixed-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pflug</surname><given-names>A.</given-names></name><name><surname>Busch</surname><given-names>C.</given-names></name></person-group><article-title>Ear biometrics: a survey of detection, feature extraction and recognition methods</article-title><source>IET Biometrics</source><year>2012</year><volume>1</volume><issue>2</issue><fpage>114</fpage><pub-id pub-id-type="doi">10.1049/iet-bmt.2011.0003</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Abaza A, Ross A, Heber C, Harrison MAF, MSN. A survey of ear biometrics. In: Surv AC, editor.2013. p. 1&#x02013;35.</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Ragan EJ, Johnson C, Milton JN, Gill CJ. Ear biometrics for patient identification in global health: a cross-sectional study to test the feasibility of a simplified algorithm. BMC Res Notes 2016;9(1):484. Epub 2016/11/04. 10.1186/s13104-016-2287-9. PubMed PMID: 27806727; PubMed Central PMCID: PMCPMC5094067.</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Introduction to SIFT (scale-invariant feature transform) [April 2017]. Available from: <ext-link ext-link-type="uri" xlink:href="http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html">http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html</ext-link>.</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Bargal S.A. WA, Chan C. R., Howes S., Sclaroff S., Ragan E., Johnson C., Gill C.J. Image-based ear biometric smarphone app for patient identification in field settings. International Conference on Computer Visiion Theory and Application (VISAPP); Berlin, Germany2015.</mixed-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Dong</given-names></name><name><surname>Zhou</surname><given-names>Huiling</given-names></name><name><surname>Lam</surname><given-names>Kin-Man</given-names></name></person-group><article-title>High-Resolution Face Verification Using Pore-Scale Facial Features</article-title><source>IEEE Transactions on Image Processing</source><year>2015</year><volume>24</volume><issue>8</issue><fpage>2317</fpage><lpage>2327</lpage><pub-id pub-id-type="doi">10.1109/TIP.2015.2412374</pub-id><pub-id pub-id-type="pmid">25781876</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Burge S. The systems engineering toolbox 2018 [May 2018]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.burgehugheswalsh.co.uk/uploaded/1/documents/pugh-matrix-v1.1.pdf">https://www.burgehugheswalsh.co.uk/uploaded/1/documents/pugh-matrix-v1.1.pdf</ext-link>.</mixed-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niemitz</surname><given-names>Carsten</given-names></name><name><surname>Nibbrig</surname><given-names>Maike</given-names></name><name><surname>Zacher</surname><given-names>Vanessa</given-names></name></person-group><article-title>Human ears grow throughout the entire lifetime according to complicated and sexually dimorphic patterns - conclusions from a cross-sectional analysis</article-title><source>Anthropologischer Anzeiger</source><year>2007</year><volume>65</volume><issue>4</issue><fpage>391</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1127/anthranz/65/2007/391</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Nellhaus G. Head circumference from birth to eighteen years. Practical composite international and interracial graphs. Pediatrics. 1968;41(1):106&#x02013;14. Epub 1968/01/01. PubMed PMID: 5635472.</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brucker</surname><given-names>Michael J.</given-names></name><name><surname>Patel</surname><given-names>Jagruti</given-names></name><name><surname>Sullivan</surname><given-names>Patrick K.</given-names></name></person-group><article-title>A Morphometric Study of the External Ear: Age- and Sex-Related Differences</article-title><source>Plastic and Reconstructive Surgery</source><year>2003</year><volume>112</volume><issue>2</issue><fpage>647</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1097/01.PRS.0000070979.20679.1F</pub-id><pub-id pub-id-type="pmid">12900628</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Adventure series for CT [April 2018]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.gehealthcare.com/products/accessories-and-supplies/adventure-series-for-ct">https://www.gehealthcare.com/products/accessories-and-supplies/adventure-series-for-ct</ext-link>.</mixed-citation></ref></ref-list></back></article>