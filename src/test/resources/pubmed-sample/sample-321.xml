
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31963912</article-id><article-id pub-id-type="pmc">7014500</article-id><article-id pub-id-type="doi">10.3390/s20020552</article-id><article-id pub-id-type="publisher-id">sensors-20-00552</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Improving Positioning Accuracy via Map Matching Algorithm for Visual&#x02013;Inertial Odometer</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meng</surname><given-names>Juan</given-names></name><xref ref-type="aff" rid="af1-sensors-20-00552">1</xref><xref ref-type="aff" rid="af2-sensors-20-00552">2</xref><xref ref-type="aff" rid="af3-sensors-20-00552">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4058-4170</contrib-id><name><surname>Ren</surname><given-names>Mingrong</given-names></name><xref ref-type="aff" rid="af1-sensors-20-00552">1</xref><xref ref-type="aff" rid="af2-sensors-20-00552">2</xref><xref ref-type="aff" rid="af3-sensors-20-00552">3</xref><xref rid="c1-sensors-20-00552" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Pu</given-names></name><xref ref-type="aff" rid="af1-sensors-20-00552">1</xref><xref ref-type="aff" rid="af2-sensors-20-00552">2</xref><xref ref-type="aff" rid="af3-sensors-20-00552">3</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Jitong</given-names></name><xref ref-type="aff" rid="af1-sensors-20-00552">1</xref><xref ref-type="aff" rid="af2-sensors-20-00552">2</xref><xref ref-type="aff" rid="af3-sensors-20-00552">3</xref></contrib><contrib contrib-type="author"><name><surname>Mou</surname><given-names>Yuman</given-names></name><xref ref-type="aff" rid="af1-sensors-20-00552">1</xref><xref ref-type="aff" rid="af2-sensors-20-00552">2</xref><xref ref-type="aff" rid="af3-sensors-20-00552">3</xref></contrib></contrib-group><aff id="af1-sensors-20-00552"><label>1</label>College of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; <email>S201761125@emails.bjut.edu.cn</email> (J.M.); <email>wangpu@bjut.edu.cn</email> (P.W.); <email>a17812102961@163.com</email> (J.Z.); <email>mu3218509460@163.com</email> (Y.M.)</aff><aff id="af2-sensors-20-00552"><label>2</label>Engineering Research Center of Digital Community, Ministry of Education, Beijing 100124, China</aff><aff id="af3-sensors-20-00552"><label>3</label>Beijing Key Laboratory of Computational Intelligence and Intelligent Systems, Beijing 100124, China</aff><author-notes><corresp id="c1-sensors-20-00552"><label>*</label>Correspondence: <email>renmingrong@bjut.edu.cn</email>; Tel.: +86-010-6739-2264</corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2020</year></pub-date><volume>20</volume><issue>2</issue><elocation-id>552</elocation-id><history><date date-type="received"><day>27</day><month>11</month><year>2019</year></date><date date-type="accepted"><day>16</day><month>1</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; 2020 by the authors.</copyright-statement><copyright-year>2020</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>A visual&#x02013;inertial odometer is used to fuse the image information obtained by a vision sensor with the data measured by an inertial sensor and recover the motion track online in a global frame. However, in an indoor environment, geometric transformation, sparse features, illumination changes, blurring, and noise will occur, which will either cause a reduction in or failure of the positioning accuracy. To solve this problem, a map matching algorithm based on an indoor plane structure map is proposed to improve the positioning accuracy of the system; this algorithm was implemented using a conditional random field model. The output of the attitude information from the visual&#x02013;inertial odometer was used as the input of the conditional random field model. The feature function between the attitude information and the expected value was established, and the maximum probabilistic value of the attitude was estimated. Finally, the closed-loop feedback correction of the visual&#x02013;inertial system was carried out with the probabilistic attitude value. A number of experiments were designed to verify the feasibility and reliability of the positioning method proposed in this paper.</p></abstract><kwd-group><kwd>visual&#x02013;inertial odometer</kwd><kwd>indoor positioning system</kwd><kwd>conditional random field</kwd><kwd>map matching</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-20-00552"><title>1. Introduction</title><p>Mobile robots are widely used in industrial production, the service industry, and other fields. Mobile robots need to realize autonomous navigation instead of being manipulated by human beings. To achieve this purpose, positioning is one of the key technologies. The global positioning system (GPS) can be used accurately in an outdoor environment; however, in an indoor environment, the global navigation satellite system (GNSS) signal is weak; thus, it cannot complete the positioning function [<xref rid="B1-sensors-20-00552" ref-type="bibr">1</xref>]. At present, the commonly used indoor positioning technologies are laser, inertial navigation system, infrared, and wireless local area network (WLAN), but these indoor navigation technologies cannot be widely used because of problems such as allele accuracy and cost. With the rapid development of machine vision and computer technology, the performance of small industrial cameras and the microelectromechanical system (MEMS) inertial devices was continuously improved [<xref rid="B2-sensors-20-00552" ref-type="bibr">2</xref>,<xref rid="B3-sensors-20-00552" ref-type="bibr">3</xref>,<xref rid="B4-sensors-20-00552" ref-type="bibr">4</xref>]. The technology of a visual&#x02013;inertial odometer (VIO) was gradually realized in engineering applications at the theoretical verification stage.</p><p>A monocular visual&#x02013;inertial navigation algorithm was developed rapidly in recent years. In 2016, Usenko, from the Munich University of Technology, proposed a binocular visual&#x02013;inertial positioning method based on tight coupling and direct methods. Using the photometric errors and residuals of images, an objective function was established, and a semi-dense map was constructed to achieve a positional accuracy of less than 0.1% of the motion distance [<xref rid="B5-sensors-20-00552" ref-type="bibr">5</xref>]. Mur-Artal, author of ORB-SLAM, and others also proposed the addition of an inertial measurement unit (IMU) error model to the cost function of tracking and the local bundle adjustment (BA) module to improve the positioning accuracy in 2016 [<xref rid="B6-sensors-20-00552" ref-type="bibr">6</xref>]. The VINS-MONO system proposed by Shaojie&#x02019;s research group from the Hong Kong University of Science and Technology in 2017 is a VIO system based on sliding window optimization. It only optimizes the information in the sliding window and does not optimize the previous state or measurement value. Its translation error based on loop detection is less than 0.3% of the accuracy of the motion distance [<xref rid="B7-sensors-20-00552" ref-type="bibr">7</xref>]. On one hand, all of these algorithms need rich features in the scene. On the other hand, they do not optimize the previous state or measurement value. In an indoor environment, corridors, white walls, and other weak-texture scenarios, as well as the influence of the illumination intensity, make the existing fusion algorithms less robust, sharply decline the precision, or even cause location failure.</p><p>The indoor environment has spatial constraints and can be used to eliminate some incorrect positioning results. It is an additional data source to improve the accuracy and reliability of an indoor positioning system. The process of utilizing map information in the positioning process is called map matching. This method can establish a matching model through the corresponding relationship between the attitude sequence obtained by the visual&#x02013;inertial positioning system and the sequence of the state points in the map; therefore, the positioning accuracy of the system can be improved without adding any hardware [<xref rid="B8-sensors-20-00552" ref-type="bibr">8</xref>].</p><p>In an indoor map matching algorithm, points (doors), lines (grids), and surfaces (rooms) need to be constrained. In Reference [<xref rid="B9-sensors-20-00552" ref-type="bibr">9</xref>], the indoor structure model was constructed by using the indoor basic vector information, and the wireless location results were constrained by fusing the sensor information and the map information. However, the final location results were constrained by the map in this document. The smoothing filtering problem of the location results was not considered in the constraints process. At present, the common methods are based on a hidden Markov model (HMM) and a particle filter (PF) [<xref rid="B10-sensors-20-00552" ref-type="bibr">10</xref>,<xref rid="B11-sensors-20-00552" ref-type="bibr">11</xref>,<xref rid="B12-sensors-20-00552" ref-type="bibr">12</xref>], because of its non-parameterized characteristics. Here, the stochastic variables must satisfy the Gaussian distribution when solving the problem of nonlinear filtering, which provides an effective solution for the analysis of nonlinear dynamic systems. One of the major issues of the PF is the computation time, as a large number of particles are typically required to ensure a good estimation of the continuous probability distribution, particularly when dealing with noisy inertial data and large maps. The MapCraft system proposed by Xiao is the first in which a map matching algorithm based on a conditional random field is tightly coupled with the positioning system [<xref rid="B13-sensors-20-00552" ref-type="bibr">13</xref>]. The system obtains the original measurement value directly from the sensor as the input of the algorithm and fuses it with the map information. When this system is used for real-time tracking, the delay caused by the backward phase of the conditional random field will affect the efficiency of the algorithm and reduce the positioning accuracy. Therefore, in this study, we developed a map matching algorithm based on a conditional random field model, which used only the output of the position information from a visual&#x02013;inertial system as the input of the conditional random field model and constrained it with the map information. The error in the position updating process was reduced by the feedback correction method, and the positioning result of the visual&#x02013;inertial system was continuously corrected to make it more accurate. Finally, the algorithm was validated experimentally. The experimental results showed that the algorithm yielded good results with respect to improving the positioning error of the visual&#x02013;inertial system.</p></sec><sec id="sec2-sensors-20-00552"><title>2. System</title><p><xref ref-type="fig" rid="sensors-20-00552-f001">Figure 1</xref> shows the system structure diagram. The system consists of four subsystems. The first part is the visual&#x02013;inertial system, which is the main positioning system to generate the estimated trajectory. In this study, the open-source VINS-MONO system introduced in 2017 was adopted by the Shaojie Research Group. The second part involves map preprocessing, mainly in the form of the original map, which generates the map types required by the map matching algorithm. The indoor map needs to be discretized to evenly spread the discrete points over the reachable area and get the matched state points. The third part is a map matching algorithm based on the conditional random field. The estimated trajectory is fused with the data of the map processing system by the conditional random field, and an accurate matching trajectory is obtained. The matching result is used as feedback to correct the position of the VINS-MONO system at any moment. The fourth part is the output of the final corrected position trajectory.</p><sec><title>VINS-MONO System Overview</title><p>VINS-MONO is a real-time VI-SLAM system that integrates inertial sensor data and monocular vision through tight coupling. The system diagram is shown in <xref ref-type="fig" rid="sensors-20-00552-f002">Figure 2</xref>. The system can be operated in a small indoor environment or a wide range of outdoor environments with strong robustness and stability. Firstly, a robust initialization scheme is proposed to provide a relatively accurate initial value for monocular tightly coupled VIO, including the restoration of the visual structure scale, gyroscope offset calibration, and velocity and gravity estimation, which can make the system from the unknown begin to run stably. Secondly, with the combination of the pre-integrated IMU measurements and visual feature observations, a tightly coupled nonlinear optimization method is used to obtain a high-precision visual&#x02013;inertial odometer. Lastly, the system also performs closed-loop detection and global map optimization. In VIO, there is only cumulative drift on <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the yaw angle; therefore, only these four degrees of freedom are optimized. VINS-MONO has good robustness and accuracy.</p></sec></sec><sec id="sec3-sensors-20-00552"><title>3. Design of Map Matching Algorithms</title><sec id="sec3dot1-sensors-20-00552"><title>3.1. Preprocessing of Indoor Maps</title><p>Indoor maps come in a variety of formats, such as image file formats, PDF files, or CAD files. These formats cannot be directly used for map matching algorithms, and the map needs to be digitized to create mathematical models. The map in this article is in the label image file format (TIFF). We used the mapinfo software to digitize the electronic version of the indoor map, using only the wall information of the map. The digital information format features the coordinates of the end point of each line segment.</p><p>Map matching can be divided into two types: point-to-point matching and trajectory matching. The point-to-point matching method matches the position point with the position of the indoor space according to the floor plan. This method is simple and computationally efficient. Trajectory matching is the matching of the motion trajectory obtained by the initial positioning system with the geometric topology information of corners, corridors, and rooms. This method is highly accurate but computationally intensive. In this study, the map matching of the conditional random field model was adopted, and the point-to-point matching method was more suitable for the visual&#x02013;inertial positioning system. The method covered the entire indoor area with equally spaced points and stored the coordinates of each state point [<xref rid="B9-sensors-20-00552" ref-type="bibr">9</xref>]. At the same time, we removed the state point of the unreachable area. In <xref ref-type="fig" rid="sensors-20-00552-f003">Figure 3</xref>, the red points indicate a state point, while the red lines indicate an indoor structure.</p></sec><sec id="sec3dot2-sensors-20-00552"><title>3.2. Conditional Random Field Model</title><p>The conditional random field model was proposed by Lafferty et al. in 2001 [<xref rid="B14-sensors-20-00552" ref-type="bibr">14</xref>]. Conditional random fields (CRFs) are a type of discriminative undirected probabilistic graphical model. They are used to encode known relationships between observations and construct consistent interpretations, and they are often used for the labeling or parsing of sequential data, such as natural language processing or biological sequences, and in computer vision [<xref rid="B15-sensors-20-00552" ref-type="bibr">15</xref>,<xref rid="B16-sensors-20-00552" ref-type="bibr">16</xref>,<xref rid="B17-sensors-20-00552" ref-type="bibr">17</xref>,<xref rid="B18-sensors-20-00552" ref-type="bibr">18</xref>,<xref rid="B19-sensors-20-00552" ref-type="bibr">19</xref>].</p><p>A conditional random field can be viewed as an undirected graphical model or a Markov random field. Ideally, if the description of the sequence of states has conditional independence, the structure of the graph can be arbitrary. However, when building the model, we usually choose the linear-chain undirected graph structure, which is the most commonly used, as shown in <xref ref-type="fig" rid="sensors-20-00552-f004">Figure 4</xref>.</p><p>In this study, we defined <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for a given sequence of observations (a sequence of <inline-formula><mml:math id="mm3"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> observation points). <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the output state point sequence, which is the predicted robot motion trajectory. Then, the output sequence conditional probability can be defined as shown in Equation (1).
<disp-formula id="FD1-sensors-20-00552"><label>(1)</label><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2-sensors-20-00552"><label>(2)</label><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>Y</mml:mi></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (1) denotes that, under the <inline-formula><mml:math id="mm7"><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:math></inline-formula> condition, the joint distribution form of sequences <inline-formula><mml:math id="mm8"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> can be evaluated. <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a normalizing factor, which can be calculated using Equation (2). <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the feature weights that can be determined by training the model. In this study, we set both weights to one. In general, the value of the feature function is zero or one; that is, the dissatisfaction feature condition is zero; otherwise, it is one. As summarized in the review, the value of the linear-chain condition random field depends mainly on the characteristic coefficient and the feature function.</p><p>In Equation (1), both <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are feature functions; further details follow in <xref ref-type="sec" rid="sec3dot3-sensors-20-00552">Section 3.3</xref> and <xref ref-type="sec" rid="sec3dot4-sensors-20-00552">Section 3.4</xref>, but the meanings are different. A function corresponds to the corresponding meaning at each position. The same function can be added at different positions to convert the original local features functions into global feature functions. The conditional random field expression can then be rewritten as a vector form, also known as a conditional random field simplification. The previous local feature function can be uniformly written as a global feature function. Assuming that the conditional random field contains <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> transfer functions and <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> observation functions, then the conditional random field contains <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> global eigenfunctions.
<disp-formula id="FD3-sensors-20-00552"><label>(3)</label><mml:math id="mm17"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, the sum of the global feature functions at various locations <inline-formula><mml:math id="mm18"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> can be calculated as follows:<disp-formula id="FD4-sensors-20-00552"><label>(4)</label><mml:math id="mm19"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Similarly, the weight of the local function can be replaced with the global weight as follows:<disp-formula id="FD5-sensors-20-00552"><label>(5)</label><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, Equations (1) and (2) can be expressed as follows:<disp-formula id="FD6-sensors-20-00552"><label>(6)</label><mml:math id="mm21"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD7-sensors-20-00552"><label>(7)</label><mml:math id="mm22"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>y</mml:mi></mml:munder><mml:mi>exp</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We can denote weight <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in the vector form <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>; thus, the global feature function can be composed into a vector; then, <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, Equations (6) and (7) can be rewritten in vector form as follows:<disp-formula id="FD8-sensors-20-00552"><label>(8)</label><mml:math id="mm26"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD9-sensors-20-00552"><label>(9)</label><mml:math id="mm27"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>y</mml:mi></mml:munder><mml:mi>exp</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot3-sensors-20-00552"><title>3.3. Observation Probability</title><p>A motion trajectory <inline-formula><mml:math id="mm28"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> can be defined as a set of interconnected trajectory segments between two position points. The VINS-MONO system outputs a position point coordinate at every <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> time point. In order to reduce the number of candidate state points in <xref ref-type="sec" rid="sec3dot4-sensors-20-00552">Section 3.4</xref> we used the method of the robot&#x02019;s fixed length distance to extract the observation points. When the distance between the current time and the previous time is equal to a certain threshold, the visual&#x02013;inertial system output is used as the observation point <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the mathematical model, and the time of the sampling point is recorded simultaneously. As shown in <xref rid="sensors-20-00552-t001" ref-type="table">Table 1</xref>, the record of one observation point <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> includes the position coordinate <inline-formula><mml:math id="mm32"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003be;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at time <inline-formula><mml:math id="mm33"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Newson and Krumm proposed a map matching algorithm to find the optimal moving trajectory sequence based on the location of the anchor point and the error radius [<xref rid="B17-sensors-20-00552" ref-type="bibr">17</xref>]. In general, map matching associates each anchor point with all the candidate road segments located within a preset error radius, as shown in <xref ref-type="fig" rid="sensors-20-00552-f005">Figure 5</xref>. In the method based on the conditional random field model, the position coordinate point is regarded as an observation state. Each candidate path represents a hidden state; that is, a hidden state represents a candidate state point. In this study, each candidate path represented the point closest to the observation point on the candidate path. The probability of observation depends on the distance between itself and the anchor point. It is intuitively assumed that candidate state points closer to the anchor point have higher observation probability. In the real state, there is a measurement error in the distance between the anchor point and the candidate state point, generally assuming a zero-mean Gaussian distribution. For a given observation point <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and candidate state point <inline-formula><mml:math id="mm48"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the observation probability is <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and can be expressed as follows:<disp-formula id="FD10-sensors-20-00552"><label>(10)</label><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm51"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Euclidean distance between the observation point and the candidate status point, <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the standard deviation of the measured data, and <inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is only obtained when <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is connected to <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, the state can be transferred, and the potential energy is one and blocked by the obstacle. Alternatively, if the map is far away, the state transition is not performed, and all of the transfer potentials are zero.</p></sec><sec id="sec3dot4-sensors-20-00552"><title>3.4. State Transition Probability</title><p>In the map, the status point may be a free space or may be occupied by a wall or an obstacle. To perform a state transition, it is necessary to know the state points adjacent to each state point, and those adjacent to the other state points [<xref rid="B20-sensors-20-00552" ref-type="bibr">20</xref>]. In this study, the area that defined the search from the current location to the next location was called the buffer [<xref rid="B9-sensors-20-00552" ref-type="bibr">9</xref>]. The maximum conversion distance allowed in each direction was called the buffer size. Furthermore, we set the buffer size to the distance <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between two state points. In the absence of any obstacles, the state transitioned between adjacent state points. In the case of obstacles, there was a conversion in the position of two buffers. <xref ref-type="fig" rid="sensors-20-00552-f006">Figure 6</xref>a shows the state transition when an obstacle is not present, and <xref ref-type="fig" rid="sensors-20-00552-f006">Figure 6</xref>b shows the state transition when an obstacle exists. The next state is shown by pink-shaded diamonds.</p><p>The transition probability refers to the transition probability between the candidate state point from time <inline-formula><mml:math id="mm57"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> to the candidate state point at time <inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The robot at the candidate state point <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> at time <inline-formula><mml:math id="mm60"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is transferred to the candidate state point <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and the shortest path passed via the Dijkstra algorithm or the Floyd algorithm is obtained, which is taken as the candidate path. The set of candidate paths between the marked observation point <inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the next observation point <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a candidate path of one candidate state point <inline-formula><mml:math id="mm67"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the observation point <inline-formula><mml:math id="mm68"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to another candidate state point <inline-formula><mml:math id="mm69"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the observation point <inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For two adjacent candidate state points, the transition probability can be defined as follows:<disp-formula id="FD11-sensors-20-00552"><label>(11)</label><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mi>&#x003b7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Euclidean distance between two observation points, <inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the length of the shortest path of the two candidate state points <inline-formula><mml:math id="mm74"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm75"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm76"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has the same meaning as above. Considering the shortest path between candidate state points, it is possible to avoid the occurrence of detours and trajectories in the matching result.</p></sec><sec id="sec3dot5-sensors-20-00552"><title>3.5. Best Path Matching</title><p>The Viterbi algorithm was proposed by Viterbi in 1976 as one of the basic algorithms of the hidden Markov model [<xref rid="B21-sensors-20-00552" ref-type="bibr">21</xref>,<xref rid="B22-sensors-20-00552" ref-type="bibr">22</xref>]. The main problem solved by the Viterbi algorithm is to find the optimal state sequence in the sequence of state values corresponding to the sequence of observations in the case of a given model; thus, it is an optimal dynamic programming algorithm and can trace back the entire path. Suppose that <inline-formula><mml:math id="mm77"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> time needs to pass from S to E, there are <inline-formula><mml:math id="mm78"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> states <inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>11</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>12</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>13</mml:mn></mml:msub><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at each moment; then, we only need to record the shortest path corresponding to each state. At any time, it is only necessary to consider a very limited number of the shortest paths (depending on the number of states corresponding to the moment), and there is no need to consider the previous moments upward; thus, so there is no multidimensional condition problem. As shown in <xref ref-type="fig" rid="sensors-20-00552-f007">Figure 7</xref>, we set <inline-formula><mml:math id="mm80"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm81"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> as an example to illustrate the algorithm, and the red path in the figure is the shortest path finally obtained.</p><p>In this study, it was applied to solve indoor positioning and used to find the optimal path. In the above, we used Equation (10) to get the observation probability and Equation (11) to get the state transition probability. We combined these two formulas into Equation (12) to find the most likely hidden sequence (the most likely trajectory). Finally, we used the Viterbi algorithm to solve the problem. Thus, the hidden sequence looking for the maximum probability was transformed into the path selection problem with the highest normalized probability as follows:<disp-formula id="FD12-sensors-20-00552"><label>(12)</label><mml:math id="mm82"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mi>max</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:munder><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, in order to considerably reduce the computational complexity and solve the problem more conveniently, there was no need to normalize the probability, and the problem could be converted into the following formula:<disp-formula id="FD13-sensors-20-00552"><label>(13)</label><mml:math id="mm83"><mml:mrow><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mi>y</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm84"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the local feature vector.</p><p>Equation (13) was solved using the Viterbi algorithm. Initialization was performed to find the non-normalized state at the most beginning position in the conditional random field as follows:<disp-formula id="FD14-sensors-20-00552"><label>(14)</label><mml:math id="mm85"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, we recursively computed the other nodes in the conditional random field, sought the maximum conditional probability at each node, and saved the maximum path.
<disp-formula id="FD15-sensors-20-00552"><label>(15)</label><mml:math id="mm86"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD16-sensors-20-00552"><label>(16)</label><mml:math id="mm87"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003a8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>When we recursively computed the end node, the maximum conditional probability was <inline-formula><mml:math id="mm88"><mml:mrow><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mi>y</mml:mi></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the optimal path was <inline-formula><mml:math id="mm89"><mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the Viterbi algorithm obtained the optimal path as <inline-formula><mml:math id="mm90"><mml:mrow><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="sec4-sensors-20-00552"><title>4. Experimental Results and Analysis</title><sec id="sec4dot1-sensors-20-00552"><title>4.1. Implementation Details</title><p>The experiments were carried out on the 10th floor of the Science Building of Beijing University of Technology with an area of 80 m &#x000d7; 25 m. Based on the VINS-MONO system, the preliminary trajectory was estimated using Intel Realsense d435i. In <xref ref-type="fig" rid="sensors-20-00552-f008">Figure 8</xref>a, the Intel Realsense D435i is a depth camera that includes a Bosch BMI055 six-axis inertial sensor, in addition to the depth camera that measures linear accelerations and angular velocities. Each IMU data packet is time-stamped using the depth sensor hardware clock to allow temporal synchronization between the gyro, accel, and depth frames. The frame rate of the RGB camera is 30 fps, and the sampling frequency of IMU is 650 Hz. <xref ref-type="fig" rid="sensors-20-00552-f008">Figure 8</xref>b shows the area of the experiments. We implemented all of the software in the C++/Matlab language.</p><p>Generally, to build a complete mobile robot software test platform, it needs to write a series of visual codes to display the status of the robot at all times. To realize such a complex system, it needs a lot of time and a huge amount of code work, which greatly increases the cost of building the test software system. Therefore, for the sake of time, the open-source robot operating system (ROS) was used in the test system of this paper. Finally, the algorithm was implemented in Dell G3 (the processor was an Intel Core i5 8th Gen). The experiment of this paper focused on the accuracy of location and the matching rate of the map.</p><p><xref ref-type="fig" rid="sensors-20-00552-f009">Figure 9</xref> shows the preset ideal trajectory. The black box in the lower left corner is the starting point and the end point of the trajectory, which follows the arrow in the figure. The preset ideal trajectory was constructed as the ground truth, with different unmatched trajectories, then created randomly as the input of the map matching algorithm. The map matching system was applied to refine the estimated trajectory by avoiding the crossing of obstacles. After matching the input trajectory to the map by using the CRF algorithm, we compared the matched (corrected) trajectory to the ground truth in order to measure the precision, using the errors obtained by calculating the Euclidean distance between the actual position in the ground truth and the corresponding corrected position.</p></sec><sec id="sec4dot2-sensors-20-00552"><title>4.2. Real Environment Experiment</title><p>In <xref ref-type="fig" rid="sensors-20-00552-f010">Figure 10</xref>, the moving speed of the robot was 1.5 m/s, which was very slow. The initial trajectory of the VINS-MONO system output had high robustness at the initial stage, but slight wall-crossing occurred after two turns (average cumulative error of 0.91 m). As shown in <xref ref-type="fig" rid="sensors-20-00552-f011">Figure 11</xref>, as a consequence of matching the trajectory with the map, the accuracy was also improved; even though the accuracy of the VINS-MONO trajectory was already good, the map matching using CRF further improved the accuracy and the cumulative error decreased. Because of the small noise and matching error of the initial positioning system, the mismatch rate was only 0.28%.</p><p>In the experiment illustrated in <xref ref-type="fig" rid="sensors-20-00552-f012">Figure 12</xref>, complex movements such as 180&#x000b0; turns and 360&#x000b0; turns were carried out several times, which enriched the scene transformation. In addition, some lamps in the corridor were damaged, and the light and the shade of the corridor were obviously changed, while the positioning error increased gradually with an increase in time; furthermore, the phenomenon of passing through the wall was serious (average cumulative error = 3.43 m). At this time, the moving speed of the robot was 2.5 m/s, which was relatively fast. Although the primary estimated trajectory crossed a number of walls, this was corrected successfully by using the map matching algorithm, and zero obstacles were crossed by the map matched trajectory, as shown in <xref ref-type="fig" rid="sensors-20-00552-f013">Figure 13</xref>. The mismatch rate was 2.94% when the initial positioning system was noisy and the error was large. As we can see, there were still some unreasonable matching points in the corner, compared with the actual trajectory of the robot; the algorithm proposed in this paper could estimate the trajectory of the robot well and meet the requirements of room-level positioning accuracy.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-20-00552"><title>5. Conclusions</title><p>Aimed at solving the problem of the positioning accuracy of a visual&#x02013;inertial system (VIS) decreasing in some indoor areas, in this study, we applied the output position of VIS to the conditional random field model by extracting the observation points and the corresponding possible state points at a fixed distance. Moreover, we made full use of the indoor structure information. In the proposed model, the Viterbi algorithm was used to find the best matching state points of the observation points in the window, finally finding the maximum probability trajectory. It fully embodied the advantages of the map matching algorithm and the probability algorithm. This algorithm was not only applicable to the VINS-MONO system; it might also be equally applicable to other visual&#x02013;inertial systems. In this paper, the positioning accuracy was required to be high, but the positioning time was a little insufficient, which will be improved in the future research. For the actual scenarios, many experiments were carried out to obtain relatively good map matching results.</p></sec></body><back><notes><title>Author Contributions</title><p>Conceptualization, J.M. and M.R.; methodology, J.M. and M.R.; investigation, J.M., M.R., J.Z., and Y.M.; software, J.M. and J.Z.; visualization, J.M.; writing&#x02014;original draft preparation, J.M.; writing&#x02014;review and editing, J.M., M.R., and P.W.; supervision, M.R. and P.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Funding</title><p>This research was supported by the Project of Beijing Municipal Education Commission KM201610005006.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-20-00552"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>T.</given-names></name><name><surname>Yin</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>D.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Pan</surname><given-names>F.</given-names></name></person-group><article-title>A novel KGP algorithm for improving INS/GPS integrated navigation positioning accuracy</article-title><source>Sensors (Switzerland)</source><year>2019</year><volume>19</volume><elocation-id>1623</elocation-id><pub-id pub-id-type="doi">10.3390/s19071623</pub-id><pub-id pub-id-type="pmid">30987372</pub-id></element-citation></ref><ref id="B2-sensors-20-00552"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petritoli</surname><given-names>E.</given-names></name><name><surname>Leccese</surname><given-names>F.</given-names></name></person-group><article-title>High accuracy attitude and navigation system for an autonomous underwater vehicle (AUV)</article-title><source>Acta IMEKO</source><year>2018</year><volume>7</volume><fpage>3</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.21014/acta_imeko.v7i2.535</pub-id></element-citation></ref><ref id="B3-sensors-20-00552"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsheikh</surname><given-names>M.</given-names></name><name><surname>Abdelfatah</surname><given-names>W.</given-names></name><name><surname>Nourledin</surname><given-names>A.</given-names></name><name><surname>Iqbal</surname><given-names>U.</given-names></name><name><surname>Korenberg</surname><given-names>M.</given-names></name></person-group><article-title>Low-cost real-time PPP/INS integration for automated land vehicles</article-title><source>Sensors (Switzerland)</source><year>2019</year><volume>19</volume><elocation-id>4896</elocation-id><pub-id pub-id-type="doi">10.3390/s19224896</pub-id><pub-id pub-id-type="pmid">31717569</pub-id></element-citation></ref><ref id="B4-sensors-20-00552"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Yin</surname><given-names>J.</given-names></name><name><surname>Noureldin</surname><given-names>A.</given-names></name><name><surname>Iqbal</surname><given-names>U.</given-names></name></person-group><article-title>Research on an improved method for foot-mounted inertial/magnetometer pedestrian-positioning based on the adaptive gradient descent algorithm</article-title><source>Sensors (Switzerland)</source><year>2018</year><volume>18</volume><elocation-id>4105</elocation-id><pub-id pub-id-type="doi">10.3390/s18124105</pub-id><pub-id pub-id-type="pmid">30477156</pub-id></element-citation></ref><ref id="B5-sensors-20-00552"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Usenko</surname><given-names>V.</given-names></name><name><surname>Engel</surname><given-names>J.</given-names></name><name><surname>Stuckler</surname><given-names>J.</given-names></name><name><surname>Cremers</surname><given-names>D.</given-names></name></person-group><article-title>Direct visual-inertial odometer with stereo cameras</article-title><source>Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>16&#x02013;21 May 2016</conf-date></element-citation></ref><ref id="B6-sensors-20-00552"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur-Artal</surname><given-names>R.</given-names></name><name><surname>Tardos</surname><given-names>J.D.</given-names></name></person-group><article-title>Visual-Inertial Monocular SLAM with Map Reuse</article-title><source>IEEE Robot. Autom. Lett.</source><year>2017</year><volume>2</volume><fpage>796</fpage><lpage>803</lpage><pub-id pub-id-type="doi">10.1109/LRA.2017.2653359</pub-id></element-citation></ref><ref id="B7-sensors-20-00552"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>Q.</given-names></name><name><surname>Peiliang</surname><given-names>L.</given-names></name><name><surname>Shaojie</surname><given-names>S.</given-names></name></person-group><article-title>VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator</article-title><source>IEEE Trans. Robot.</source><year>2018</year><volume>34</volume><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="B8-sensors-20-00552"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Petritoli</surname><given-names>E.</given-names></name><name><surname>Leccese</surname><given-names>F.</given-names></name></person-group><article-title>Improvement of altitude precision in indoor and urban canyon navigation for small flying vehicles</article-title><source>Proceedings of the 2015 IEEE Metrology for Aerospace (MetroAeroSpace)</source><conf-loc>Benevento, Italy</conf-loc><conf-date>4&#x02013;5 June 2015</conf-date><fpage>56</fpage><lpage>60</lpage></element-citation></ref><ref id="B9-sensors-20-00552"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shang</surname><given-names>J.</given-names></name><name><surname>Hu</surname><given-names>X.</given-names></name><name><surname>Gu</surname><given-names>F.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Yu</surname><given-names>S.</given-names></name></person-group><article-title>Improvement Schemes for Indoor Mobile Location Estimation: A Survey</article-title><source>Math. Probl. Eng.</source><year>2015</year><volume>2015</volume><fpage>397298</fpage><pub-id pub-id-type="doi">10.1155/2015/397298</pub-id></element-citation></ref><ref id="B10-sensors-20-00552"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arulampalam</surname><given-names>M.S.</given-names></name><name><surname>Maskell</surname><given-names>S.</given-names></name><name><surname>Gordon</surname><given-names>N.</given-names></name><name><surname>Clapp</surname><given-names>T.</given-names></name></person-group><article-title>A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking</article-title><source>IEEE Trans. Signal Process.</source><year>2002</year><volume>50</volume><fpage>174</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1109/78.978374</pub-id></element-citation></ref><ref id="B11-sensors-20-00552"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>P.</given-names></name><name><surname>Collin</surname><given-names>J.</given-names></name><name><surname>Takala</surname><given-names>J.</given-names></name></person-group><article-title>Application of particle filters for indoor positioning using floor plans</article-title><source>Proceedings of the 2010 Ubiquitous Positioning Indoor Navigation and Location Based Service</source><conf-loc>Kirkkonummi, Finland</conf-loc><conf-date>14&#x02013;15 October 2010</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B12-sensors-20-00552"><label>12.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Maru&#x00161;i&#x00107;</surname><given-names>B.G.</given-names></name><name><surname>Maru&#x00161;i&#x00107;</surname><given-names>D.</given-names></name></person-group><source>Behavioural Maps and GIS in Place Evaluation and Design</source><publisher-name>INTECH Open Access Publisher</publisher-name><publisher-loc>Rijeka, Croatia</publisher-loc><year>2012</year></element-citation></ref><ref id="B13-sensors-20-00552"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Z.</given-names></name><name><surname>Wen</surname><given-names>H.</given-names></name><name><surname>Markham</surname><given-names>A.</given-names></name><name><surname>Trigoni</surname><given-names>N.</given-names></name></person-group><article-title>Indoor Tracking Using Undirected Graphical Models</article-title><source>IEEE Trans. Mob. Comput.</source><year>2015</year><volume>14</volume><fpage>2286</fpage><lpage>2301</lpage><pub-id pub-id-type="doi">10.1109/TMC.2015.2398431</pub-id></element-citation></ref><ref id="B14-sensors-20-00552"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lafferty</surname><given-names>J.</given-names></name><name><surname>Mccallum</surname><given-names>A.</given-names></name><name><surname>Pereira</surname><given-names>F.C.N.</given-names></name></person-group><article-title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</article-title><source>Proceedings of the 18th International Conference on Machine Learning 2001 (ICML 2001)</source><conf-loc>Berkshires, MA, USA</conf-loc><conf-date>28 June&#x02013;1 July 2001</conf-date><volume>Volume 3</volume><fpage>282</fpage><lpage>289</lpage></element-citation></ref><ref id="B15-sensors-20-00552"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>M.</given-names></name><name><surname>Karimi</surname><given-names>H.A.</given-names></name></person-group><article-title>A hidden Markov model-based map-matching algorithm for wheelchair navigation</article-title><source>J. Navig.</source><year>2009</year><volume>62</volume><fpage>383</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1017/S0373463309005347</pub-id></element-citation></ref><ref id="B16-sensors-20-00552"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lou</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Xie</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>Y.</given-names></name></person-group><article-title>Map-matching for low-sampling-rate GPS trajectories</article-title><source>Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>4&#x02013;6 November 2009</conf-date><publisher-name>ACM</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2009</year><fpage>352</fpage><lpage>361</lpage></element-citation></ref><ref id="B17-sensors-20-00552"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Newson</surname><given-names>P.</given-names></name><name><surname>Krumm</surname><given-names>J.</given-names></name></person-group><article-title>Hidden Markov map matching through noise and sparseness</article-title><source>Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>4&#x02013;6 November 2009</conf-date><publisher-name>ACM</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2009</year><fpage>336</fpage><lpage>343</lpage></element-citation></ref><ref id="B18-sensors-20-00552"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Raymond</surname><given-names>R.</given-names></name><name><surname>Morimura</surname><given-names>T.</given-names></name><name><surname>Osogami</surname><given-names>T.</given-names></name><name><surname>Hirosue</surname><given-names>N.</given-names></name></person-group><article-title>Map matching with hidden Markov model on sampled road network</article-title><source>Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)</source><conf-loc>Tsukuba, Japan</conf-loc><conf-date>11&#x02013;15 November 2012</conf-date><fpage>2242</fpage><lpage>2245</lpage></element-citation></ref><ref id="B19-sensors-20-00552"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goh</surname><given-names>C.Y.</given-names></name><name><surname>Dauwels</surname><given-names>J.</given-names></name><name><surname>Mitrovic</surname><given-names>N.</given-names></name><name><surname>Asif</surname><given-names>M.T.</given-names></name><name><surname>Oran</surname><given-names>A.</given-names></name><name><surname>Jaillet</surname><given-names>P.</given-names></name></person-group><article-title>Online map-matching based on hidden markov model for real-time traffic sensing applications</article-title><source>Proceedings of the 2012 15th International IEEE Conference on Intelligent Transportation Systems</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>16&#x02013;19 September 2012</conf-date><fpage>776</fpage><lpage>781</lpage></element-citation></ref><ref id="B20-sensors-20-00552"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bataineh</surname><given-names>S.</given-names></name><name><surname>Bahillo</surname><given-names>A.</given-names></name><name><surname>D&#x000ed;ez</surname><given-names>L.E.</given-names></name></person-group><article-title>Using of behavioral information for enhancing Conditional Random Field-based map matching</article-title><source>Proceedings of the 2017 European Navigation Conference (ENC)</source><conf-loc>Lausanne, Switzerland</conf-loc><conf-date>9&#x02013;12 May 2017</conf-date></element-citation></ref><ref id="B21-sensors-20-00552"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>M.S.</given-names></name><name><surname>Nudd</surname><given-names>G.R.</given-names></name></person-group><article-title>The Viterbi Algorithm</article-title><source>Proc. IEEE</source><year>1993</year><volume>61</volume><fpage>268</fpage><lpage>278</lpage></element-citation></ref><ref id="B22-sensors-20-00552"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname><given-names>L.R.</given-names></name></person-group><article-title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition</article-title><source>Proc. IEEE</source><year>1989</year><volume>77</volume><fpage>257</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1109/5.18626</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-20-00552-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Systematic structure diagram.</p></caption><graphic xlink:href="sensors-20-00552-g001"/></fig><fig id="sensors-20-00552-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>VINS-MONO system framework.</p></caption><graphic xlink:href="sensors-20-00552-g002"/></fig><fig id="sensors-20-00552-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>State point map.</p></caption><graphic xlink:href="sensors-20-00552-g003"/></fig><fig id="sensors-20-00552-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Linear-chain undirected graph model.</p></caption><graphic xlink:href="sensors-20-00552-g004"/></fig><fig id="sensors-20-00552-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Trace diagram.</p></caption><graphic xlink:href="sensors-20-00552-g005"/></fig><fig id="sensors-20-00552-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>State transition diagram: (<bold>a</bold>) state transition when there is no obstacle; (<bold>b</bold>) state transition in the presence of obstacles.</p></caption><graphic xlink:href="sensors-20-00552-g006"/></fig><fig id="sensors-20-00552-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Schematic representation of the optimal path solution.</p></caption><graphic xlink:href="sensors-20-00552-g007"/></fig><fig id="sensors-20-00552-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Sensors and test sites: (<bold>a</bold>) Intel Realsense D435i; (<bold>b</bold>) area of the experiments.</p></caption><graphic xlink:href="sensors-20-00552-g008"/></fig><fig id="sensors-20-00552-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Preset ideal trajectory.</p></caption><graphic xlink:href="sensors-20-00552-g009"/></fig><fig id="sensors-20-00552-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>VINS trajectory with high robustness.</p></caption><graphic xlink:href="sensors-20-00552-g010"/></fig><fig id="sensors-20-00552-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Corrected trajectory of <xref ref-type="fig" rid="sensors-20-00552-f010">Figure 10</xref>.</p></caption><graphic xlink:href="sensors-20-00552-g011"/></fig><fig id="sensors-20-00552-f012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Trajectory map with large location error of VINS-MONO.</p></caption><graphic xlink:href="sensors-20-00552-g012"/></fig><fig id="sensors-20-00552-f013" orientation="portrait" position="float"><label>Figure 13</label><caption><p>Corrected trajectory of <xref ref-type="fig" rid="sensors-20-00552-f012">Figure 12</xref>.</p></caption><graphic xlink:href="sensors-20-00552-g013"/></fig><table-wrap id="sensors-20-00552-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-20-00552-t001_Table 1</object-id><label>Table 1</label><caption><p>VINS-MONO.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">VINS-MONO Observation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm34"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">&#x003be;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">&#x003c8;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Time</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm35"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm36"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c8;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm37"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c8;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm41"><mml:mrow><mml:mo>&#x022ef;</mml:mo></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm42"><mml:mrow><mml:mo>&#x022ef;</mml:mo></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm43"><mml:mrow><mml:mo>&#x022ef;</mml:mo></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm44"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm45"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c8;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm46"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></table></table-wrap></floats-group></article>