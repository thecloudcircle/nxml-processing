
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Appl Genet</journal-id><journal-id journal-id-type="iso-abbrev">J Appl Genet</journal-id><journal-title-group><journal-title>Journal of Applied Genetics</journal-title></journal-title-group><issn pub-type="ppub">1234-1983</issn><issn pub-type="epub">2190-3883</issn><publisher><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">32996082</article-id><article-id pub-id-type="pmc">7652806</article-id><article-id pub-id-type="publisher-id">586</article-id><article-id pub-id-type="doi">10.1007/s13353-020-00586-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Animal Genetics &#x02022; Original Paper</subject></subj-group></article-categories><title-group><article-title>The application of deep learning for the classification of correct and incorrect SNP genotypes from whole-genome DNA sequencing pipelines</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2271-025X</contrib-id><name><surname>Kotlarz</surname><given-names>Krzysztof</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1086-9119</contrib-id><name><surname>Mielczarek</surname><given-names>Magda</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8530-3486</contrib-id><name><surname>Suchocki</surname><given-names>Tomasz</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9908-3007</contrib-id><name><surname>Czech</surname><given-names>Bartosz</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1764-135X</contrib-id><name><surname>Guldbrandtsen</surname><given-names>Bernt</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9688-0193</contrib-id><name><surname>Szyda</surname><given-names>Joanna</given-names></name><address><email>joanna.szyda@upwr.edu.pl</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.411200.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0694 6014</institution-id><institution>Biostatistics Group, Department of Genetics, </institution><institution>Wroclaw University of Environmental and Life Sciences, </institution></institution-wrap>Kozuchowska 7, 51-631 Wroclaw, Poland </aff><aff id="Aff2"><label>2</label>Institute of Animal Breeding, Balice, Poland </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.10388.32</institution-id><institution-id institution-id-type="ISNI">0000 0001 2240 3300</institution-id><institution>Animal Breeding Group, Department of Animal Sciences, </institution><institution>University of Bonn, </institution></institution-wrap>Bonn, Germany </aff></contrib-group><author-notes><fn fn-type="com"><p>Communicated by: Maciej Szydlowski</p></fn></author-notes><pub-date pub-type="epub"><day>29</day><month>9</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>29</day><month>9</month><year>2020</year></pub-date><pub-date pub-type="ppub"><year>2020</year></pub-date><volume>61</volume><issue>4</issue><fpage>607</fpage><lpage>616</lpage><history><date date-type="received"><day>26</day><month>8</month><year>2020</year></date><date date-type="rev-recd"><day>11</day><month>9</month><year>2020</year></date><date date-type="accepted"><day>18</day><month>9</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020, corrected publication 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">A downside of next-generation sequencing technology is the high technical error rate. We built a tool, which uses array-based genotype information to classify next-generation sequencing&#x02013;based SNPs into the correct and the incorrect calls. The deep learning algorithms were implemented via Keras. Several algorithms were tested: (i) the basic, na&#x000ef;ve algorithm, (ii) the na&#x000ef;ve algorithm modified by pre-imposing different weights on incorrect and correct SNP class in calculating the loss metric and (iii)&#x02013;(v) the na&#x000ef;ve algorithm modified by random re-sampling (with replacement) of the incorrect SNPs to match 30%/60%/100% of the number of correct SNPs. The training data set was composed of data from three bulls and consisted of 2,227,995 correct (97.94%) and 46,920 incorrect SNPs, while the validation data set consisted of data from one bull with 749,506 correct (98.05%) and 14,908 incorrect SNPs. The results showed that for a rare event classification problem, like incorrect SNP detection in NGS data, the most parsimonious na&#x000ef;ve model and a model with the weighting of SNP classes provided the best results for the classification of the validation data set. Both classified 19% of truly incorrect SNPs as incorrect and 99% of truly correct SNPs as correct and resulted in the F1 score of 0.21&#x000a0;&#x02014;&#x000a0;the highest among the compared algorithms. We conclude the basic models were less adapted to the specificity of a training data set and thus resulted in better classification of the independent, validation data set, than the other tested models.</p><sec><title>Electronic supplementary material</title><p>The online version of this article (10.1007/s13353-020-00586-0) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Classification</kwd><kwd>Keras</kwd><kwd>Next-generation sequencing</kwd><kwd>Python</kwd><kwd>SNP calling</kwd><kwd>SNP microarray</kwd><kwd>TensorFlow</kwd></kwd-group><funding-group><award-group><funding-source><institution>Wroclaw University of Environmental and Life Sciences</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Institute of Plant Genetics, Polish Academy of Sciences, Poznan 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Next-generation sequencing (NGS) technology has led to a tremendous increase in sequencing speed and a decrease in sequencing cost. It allows fast and cost-effective sequencing of whole genomes of many individuals. The downside of sequencing carried out by high-throughput processes are the significant technical (Pfeiffer et al. <xref ref-type="bibr" rid="CR22">2018</xref>; Ma et al. <xref ref-type="bibr" rid="CR18">2019</xref>) and bioinformatics (Abnizova et al. <xref ref-type="bibr" rid="CR2">2017</xref>) error rates. In particular, the very large amounts of genomes sequenced with moderate or low coverage, short-read lengths and individual genetic variation often cause numerous computational problems (Horner et al. <xref ref-type="bibr" rid="CR9">2010</xref>). Such drawbacks make utilizing NGS data for research dependent on bioinformatics tools for data editing and processing. The resulting polymorphism detected should therefore rather be regarded as an estimate of the true underlying genetic variation. Therefore, it should be kept in mind that not all of the reported polymorphisms represent true variants. In addition, some true polymorphisms remain undetected despite the availability of the whole-genome sequence. A trivial example of such situations is differences in the number and positions of detected polymorphisms resulting from the application of various bioinformatics pipelines to the same set of short-read sequence data (e.g. Hwang et al. <xref ref-type="bibr" rid="CR10">2015</xref>; Laurie et al. <xref ref-type="bibr" rid="CR15">2016</xref>). Also, in our unpublished analyses of 197 cattle genomes, with an average genome coverage of 10&#x000d7;, we observed differences between single-nucleotide polymorphism (SNP) called using GATK (McKenna et al. <xref ref-type="bibr" rid="CR19">2010</xref>), FreeBayes (Garrison and Marth <xref ref-type="bibr" rid="CR6">2012</xref>) and SAMtools (Li et al. <xref ref-type="bibr" rid="CR17">2009</xref>). The differences amounted to some 25% depending on the particular chromosome.</p><p id="Par3">Therefore, in the current study, we aimed to build a tool, which uses standard information available in the Variant Calling Format (VCF) file for the classification of SNPs into correct and incorrect detections. Based on the available data, which comprises individuals genotyped by a commercial high-density oligonucleotide microarray and with whole-genome sequence, we were able to identify correct and incorrect SNP detections. The rate of SNPs incorrectly called from NGS data&#x02014;represented by polymorphisms with discordant genotypes between a microarray and NGS&#x02014;was only 2%. Although it is a positive observation, it makes it difficult to use standard statistical tools for classification since, with such a severe imbalance in class counts, we enter a rare event data problem. For such data, a standard logistic regression-based classifier does not apply because of a bias in maximum likelihood-derived parameter and probability estimates, which is due to the strong imbalance between the observed counts of events and non-events (King and Zeng <xref ref-type="bibr" rid="CR12">2001a</xref>, <xref ref-type="bibr" rid="CR13">b</xref>). Even though a modification of logistic regression based on bootstrap and Markov chain Monte Carlo (Fr&#x000fc;hwirth-Schnatter and Wagner <xref ref-type="bibr" rid="CR5">2008</xref>) dedicated to the rare event data was applied for the analysis of the data, parameter estimation and subsequent classification did not improve. Clearly, a valid possibility to improve the model would be to add more independent variables. In practice, the number of explanatory variables is limited by the information included in VCF files. On the other hand, machine learning algorithms, including deep learning (DL), proved to provide a flexible classification tool for diverse and complex data structures (Jiang et al.&#x000a0;<xref ref-type="bibr" rid="CR11">2020</xref>). DL has been successfully implemented in many classification challenges naming, for example, the KAGGLE competition (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com">https://www.kaggle.com</ext-link>). For livestock data, the application of DL has just begun. It has mainly focussed on the prediction of animals&#x02019; genetic merit (for a review, see P&#x000e9;rez-Enciso and Zingaretti <xref ref-type="bibr" rid="CR21">2019</xref>). The use of DL-based tools for NGS variant detection and/or classification is still scarce. It has only recently been considered by Ravasio et al. (<xref ref-type="bibr" rid="CR23">2018</xref>), Singh and Bhatia (<xref ref-type="bibr" rid="CR25">2019</xref>) and Gupta and Saini (<xref ref-type="bibr" rid="CR7">2020</xref>). In the current study, we exploited the potential of DL to classify correct and incorrect SNP discoveries for the context of livestock whole-genome sequence data. Beginning from a na&#x000ef;ve classification algorithm, we further moved towards its modifications aiming to mitigate the problem of the very low number of SNPs representing the incorrect SNP class. Moreover, we also explored the continuous [0, 1] space of the distribution of SNP class probability estimated by the deep learning network in order to determine the best cutoff for SNP binary classification. Since the study aims to provide a general classifier, we used explanatory variables available in a standard VCF file.</p></sec><sec id="Sec2"><title>Materials and methods</title><sec id="Sec3"><title>Datasets</title><p id="Par4">The data comprised whole-genome DNA sequence reads of four traditional Danish Red Dairy Cattle bulls. Samples were sequenced by the Illumina HiSeq2000 platform with paired-end 100&#x000a0;bp read length with a 300 bp insert size. The total number of raw reads generated for a single animal varied between 249,478,818 and 290,364,464. This resulted in the average genome coverage of 10x. Additionally, these bulls were genotyped using the Illumina BovineHD BeadArray comprising 777,962 SNPs. This data was then utilised to compose the following subsets:<list list-type="bullet"><list-item><p id="Par5">The training data set, used for building a DL-based classifier, was composed of three (out of the four) animals.</p></list-item><list-item><p id="Par6">The validation data set, used as the independent input for the validation of the classification quality, comprised data from the fourth animal with NGS SNPs identified based on the same pipeline as in the training data set.</p></list-item></list></p></sec><sec id="Sec4"><title>SNP calling</title><p id="Par7">A pipeline for SNP calling comprised the alignment to the reference genome carried out by using BWA-MEM (Li and Durbin <xref ref-type="bibr" rid="CR16">2009</xref>) with the following default parameters: the seed length of 19, the matching score of 1, mismatch penalty of 4, gap open penalty of 6 and gap extension penalty of 1. Post-alignment processing included the conversion of SAM-formatted files into the binary (BAM) format, data indexing and marking of PCR duplicates. This step was performed using Picard (<ext-link ext-link-type="uri" xlink:href="https://broadinstitute.github.io/picard/">https://broadinstitute.github.io/picard/</ext-link>) and SAMtools. Pre-variant calling was implemented via the GATK package and included local re-alignment around INDELs by using GATK&#x02019;s RealignerTargetCreator and IndelRealigner tools followed by quality score recalibration by using BaseRecalibrator and PrintReads tools. For the actual variant calling, the UnifiedGenotyper tool from the GATK package was used. Note that the above pipeline was run only for those 772,173 SNPs genomic positions, which were also genotyped by the Illumina BovineHD BeadArray and were defined in the SNPchiMp (Nicolazzi et al. <xref ref-type="bibr" rid="CR20">2015</xref>) database.</p></sec><sec id="Sec5"><title>Correct and incorrect SNP definition</title><p id="Par8">After the exclusion of missing genotypes, the total number of the analysed polymorphic loci for each of the bulls was 764,446 and these were compared between the NGS and the microarray outputs. Correct SNPs refer to a full agreement in genotypes estimated by both technologies; incorrect SNPs were defined as mismatches involving at least one allele. For the case of multi-allelic SNPs, when two or more alternative alleles were called in the NGS output, we checked if at least one of these alleles allowed for a match with the genotype called by a microarray.</p></sec><sec id="Sec6"><title>Explanatory variables</title><p id="Par9">The following explanatory variables from a standard VCF output were considered for the classification:<list list-type="bullet"><list-item><p id="Par10">The probability of incorrectly called alternative allele (QUAL): QUAL&#x02009;=&#x02009;&#x02212;&#x02009;10log<sub>10</sub>(<italic>P</italic><sub>1</sub>), where <italic>P</italic><sub>1</sub> denotes the probability that the identified alternative allele is an incorrect detection.</p></list-item><list-item><p id="Par11">The conditional likelihood of incorrectly called alternative allele (GQ): GQ&#x02009;=&#x02009;&#x02212;&#x02009;10log<sub>10</sub>(<italic>P</italic><sub>2</sub>), where <italic>P</italic><sub>2</sub> denotes the probability that the identified genotype is an incorrect detection conditional on the position being polymorphic.</p></list-item><list-item><p id="Par12">Sequencing depth at the polymorphic site combined over all four sequenced individuals (DP).</p></list-item><list-item><p id="Par13">Sequencing depth at the polymorphic site for a given individual (DP2).</p></list-item><list-item><p id="Par14">The coded genotype (CALL): <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{CALL}=\left\{\ \begin{array}{c}1\kern0.5em \mathrm{for}\kern0.5em ./.\\ {}\kern0.75em 2\kern0.5em \mathrm{for}\kern0.5em 0/0\\ {}\kern0.75em 3\kern0.5em \mathrm{for}\kern0.5em 0/1\\ {}\kern1em 4\kern0.5em \mathrm{for}\kern0.5em 1/1\ \end{array}\right. $$\end{document}</tex-math><mml:math id="M2" display="inline"><mml:mtext>CALL</mml:mtext><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mspace width="0.25em"/><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="0.5em"/><mml:mtext>for</mml:mtext><mml:mspace width="0.5em"/><mml:mo>.</mml:mo><mml:mo>/</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.75em"/><mml:mn>2</mml:mn><mml:mspace width="0.5em"/><mml:mtext>for</mml:mtext><mml:mspace width="0.5em"/><mml:mn>0</mml:mn><mml:mo>/</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.75em"/><mml:mn>3</mml:mn><mml:mspace width="0.5em"/><mml:mtext>for</mml:mtext><mml:mspace width="0.5em"/><mml:mn>0</mml:mn><mml:mo>/</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mn>4</mml:mn><mml:mspace width="0.5em"/><mml:mtext>for</mml:mtext><mml:mspace width="0.5em"/><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.25em"/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq1.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p id="Par15">A categorical variable was constructed based on three reference bases downstream of a SNP.</p></list-item><list-item><p id="Par16">A categorical variable was constructed based on three reference bases upstream of a SNP.</p></list-item></list></p><p id="Par17">The last three variables were included into modelling in order to capture the potential sequencer errors specific to the fluorescence of particular genotypes (CALL) or the sequence closely neighbouring the polymorphic site.</p></sec><sec id="Sec7"><title>Deep learning algorithms</title><p id="Par18">The deep learning algorithms were implemented via the Keras interface (Chollet <xref ref-type="bibr" rid="CR3">2015</xref>) with the TensorFlow (Abadi et al. <xref ref-type="bibr" rid="CR1">2015</xref>) library in Python 3.7.7 on a personal computer running Windows 10, with an Intel Core i5-3210M CPU 2.50GHz (2 cores and 3-MB cache memory), 8&#x000a0;GB RAM and 120&#x000a0;GB of SSD. Prior to the implementation of the algorithm, all quantitative explanatory variables (QUAL, GQ, DP and DP2) were transformed into the standard normal distribution. The categorical explanatory variables (base trio up- and downstream of a SNP as well as CALL) were one-hot encoded. The network architecture underlying the na&#x000ef;ve algorithm (NA&#x000cf;VE) was composed of eight sequentially connected layers with gradually decreasing numbers of parameters and a dropout rate of 0.2 after the first five layers. The dropout algorithm modifies the activation matrix (<bold><italic>X</italic></bold>) estimated by every single network by randomly setting 20% of its values to zero. In the first seven layers, the rectified linear unit (<italic>ReLU</italic>) function, given by <italic>f</italic>(<italic>x</italic>)&#x02009;=&#x02009;max(0,&#x02009;<italic>x</italic><sub><italic>ij</italic></sub>), was used for activation, while the <italic>sigmoid</italic>, given by <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f(x)=\frac{1}{1+{e}^{-{x}_{ij}}} $$\end{document}</tex-math><mml:math id="M4" display="inline"><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq2.gif"/></alternatives></inline-formula> was used as the activation of the last layer. The <italic>Adam</italic> algorithm (Kingma and Ba <xref ref-type="bibr" rid="CR14">2014</xref>) implementing the stochastic gradient descent approach was used to explore the likelihood function and the <italic>binary crossentropy</italic> loss function was used to quantify the quality of the classification applied to the training data set. The Keras implementation is summarised by Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption><p>Implementation scheme for the NA&#x000cf;VE deep learning algorithm for SNP classification in Keras</p></caption><graphic xlink:href="13353_2020_586_Fig1_HTML" id="MO1"/></fig></p><p id="Par19">In order to mitigate the imbalance in class counts observed in the training data set, the above, na&#x000ef;ve, algorithm was modified by pre-imposing different weights for the correct and the incorrect SNP class while calculating the loss metric. Following the recommendations from the TensorFlow online manual, a weight for the correct SNP class was estimated as <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{1}{2}\cdotp \frac{N_{\mathrm{trueSNP}}+{N}_{\mathrm{falseSNP}}}{N_{\mathrm{trueSNP}}} $$\end{document}</tex-math><mml:math id="M6" display="inline"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x000b7;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>trueSNP</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>falseSNP</mml:mtext></mml:msub></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>trueSNP</mml:mtext></mml:msub></mml:mfrac></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq3.gif"/></alternatives></inline-formula> and for the incorrect SNP class as <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{1}{2}\cdotp \frac{N_{\mathrm{trueSNP}}+{N}_{\mathrm{falseSNP}}}{N_{\mathrm{falseSNP}}} $$\end{document}</tex-math><mml:math id="M8" display="inline"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x000b7;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>trueSNP</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>falseSNP</mml:mtext></mml:msub></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>falseSNP</mml:mtext></mml:msub></mml:mfrac></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq4.gif"/></alternatives></inline-formula>, where <italic>N</italic> represents the number of SNPs representing the respective class. This algorithm is referred to as WEIGHTED. Another modification of the NA&#x000cf;VE algorithm was realised within the frame of construction of the training data set. In particular, SNPs for the incorrect class were randomly sampled with replacement from the pool of all incorrect SNPs. This resulted in the larger number of SNPs representing the incorrect SNP class. In the OVERSAMPLED30 algorithm, the number of incorrect SNPs was equal to 30% of the number of correct SNPs; in the OVERSAMPLED60 algorithm, the number of incorrect SNPs was equal to 60% of the number of correct SNPs and in the OVERSAMPLED100 algorithm, both classes were represented by the equal numbers of SNPs.</p></sec><sec id="Sec8"><title>Classification quality metrics</title><p id="Par20">The classification approach comprises the following categories:<list list-type="bullet"><list-item><p id="Par21">True positive (TP) is defined as the situation when an incorrect SNP was classified as incorrect.</p></list-item><list-item><p id="Par22">False negative (FN) is defined as the situation when an incorrect SNP was classified as correct.</p></list-item><list-item><p id="Par23">True negative (TN) is defined as the situation when a correct SNP was classified as correct.</p></list-item><list-item><p id="Par24">False positive (FP) is defined as the situation when a correct SNP was classified as incorrect.</p></list-item></list></p><p id="Par25">Based on these categories, two summary statistics were used for the quantification of the quality of classifiers. The F1 metric is given by <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ F1=\frac{2\mathrm{TP}}{2\mathrm{TP}+\mathrm{FN}+\mathrm{FP}} $$\end{document}</tex-math><mml:math id="M10" display="inline"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>TP</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq5.gif"/></alternatives></inline-formula>, and the SUMSS metric given by <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ S\mathrm{UMSS}=\frac{\mathrm{TN}}{\mathrm{TN}+\mathrm{FP}}+\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}} $$\end{document}</tex-math><mml:math id="M12" display="inline"><mml:mi>S</mml:mi><mml:mtext>UMSS</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi>TN</mml:mi><mml:mrow><mml:mi>TN</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq6.gif"/></alternatives></inline-formula>.</p></sec><sec id="Sec9"><title>The estimation of probability cutoff</title><p id="Par26">For each analysed SNP, the original output from the last layer is a probability of a SNP being correct, resulting from the sigmoid activation function. The default true/false class assignment applies the 0.5 probability threshold. For each of the implemented algorithms (NA&#x000cf;VE, WEIGHTED and the three OVERSAMPLED), in addition to this standard threshold, probability cutoff values were also estimated based on the optimisation of F1 or SUMSS metrics respectively, using the cutpointR package (Thiele and Hirschfeld <xref ref-type="bibr" rid="CR27">2020</xref>) implemented in the R. In brief, in this package, separately for each of the five algorithms, a subset of data is sampled with replacement from the original training set of SNPs multiple times. For each such sub-sample, the probability cutoff is represented by the value which yields the highest F1/SUMSS metric. The final estimate is the bootstrap mean of cutoff values from all the bootstrap samples. In order to check the robustness of the cutoff estimates towards the initial data, cutoff estimates were validated by estimating them based on 20 sub-samples of our original training data.</p></sec></sec><sec id="Sec10"><title>Results</title><sec id="Sec11"><title>Data sets</title><p id="Par27">For the training data set, 2,274,915 SNPs from the three bulls were considered, among which 2,227,995 (97.94%) were correctly identified by the NGS platform (Table <xref rid="Tab1" ref-type="table">1</xref>). Since 24.4% of observations did not have an estimate for the conditional probability of incorrectly called alternative allele (GQ), this metric was not used as an explanatory variable in the DL algorithm. For the validation data set representing the fourth bull, 749,506 correct (98.05%) and 14,940 incorrect SNPs were used (Table <xref rid="Tab1" ref-type="table">1</xref>). A total of 24.2% of observations also did not have an estimate GQ this metric.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Characteristics of the analysed data sets</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">SNP</th><th colspan="2">Training data</th><th colspan="2">Validation data</th></tr><tr><th>Correct</th><th>Incorrect</th><th>Correct</th><th>Incorrect</th></tr></thead><tbody><tr><td>%</td><td>97.94%</td><td>2.06%</td><td>98.05%</td><td>1.95%</td></tr><tr><td colspan="5">Genotype counts</td></tr><tr><td align="justify">&#x000a0;&#x000a0;0/0</td><td>882,838</td><td>19,725</td><td>299,804</td><td>6037</td></tr><tr><td align="justify">&#x000a0;&#x000a0;0/1</td><td>571,549</td><td>12,910</td><td>193,755</td><td>4270</td></tr><tr><td align="justify">&#x000a0;&#x000a0;1/1</td><td>773,608</td><td>14,285</td><td>255,947</td><td>4633</td></tr><tr><td>Mean DP &#x000b1; SD</td><td>37.91&#x02009;&#x000b1;&#x02009;11.66</td><td>30.68&#x02009;&#x000b1;&#x02009;13.21</td><td>37.61&#x02009;&#x000b1;&#x02009;11.95</td><td>33.51&#x02009;&#x000b1;&#x02009;13.08</td></tr><tr><td>DP range</td><td>1&#x02013;587</td><td>1&#x02013;457</td><td>1&#x02013;587</td><td>1&#x02013;457</td></tr><tr><td>Mean DP2 &#x000b1; SD</td><td>9.53&#x02009;&#x000b1;&#x02009;4.16</td><td>6.20&#x02009;&#x000b1;&#x02009;4.09</td><td>9.27&#x02009;&#x000b1;&#x02009;3.59</td><td>7.13&#x02009;&#x000b1;&#x02009;4.33</td></tr><tr><td>DP2 range</td><td>1&#x02013;159</td><td>1&#x02013;113</td><td>1&#x02013;192</td><td>1&#x02013;184</td></tr><tr><td>Mean QUAL &#x000b1; SD</td><td>484.15&#x02009;&#x000b1;&#x02009;430.14</td><td>365.74&#x02009;&#x000b1;&#x02009;318.29</td><td>479.85&#x02009;&#x000b1;&#x02009;429.82</td><td>405.4&#x02009;&#x000b1;&#x02009;340.08</td></tr><tr><td>QUAL range</td><td>10.00&#x02013;3829.35</td><td>10.00&#x02013;4516.94</td><td>10.00&#x02013;3829.35</td><td>10.0&#x02013;4516.90</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec12"><title>Probabilities of SNP being incorrect, based on training data</title><p id="Par28">For each SNP, the probabilities of being incorrectly called, estimated in the training data set by the five models, are depicted in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. It is evident that the estimates differ between algorithms, with the two algorithms which mitigate the rare data problem (WEIGHTED, OVERSAMPLED) resulting in generally higher probabilities of a SNP being incorrect. Unfortunately, the examination of the probability curves shows that the NA&#x000cf;VE algorithm fails to make a distinction between truly correct and incorrect polymorphisms since the empirical probability distributions for each SNP class is quite similar. A visually best differentiation results from the implementation of OVERSAMPLED algorithms, with performance increasing with the balancing of the SNP class counts in the training data set&#x02014;i.e. the most visually distinct distributions are provided by the OVERSAMPLED100 version.<fig id="Fig2"><label>Fig. 2</label><caption><p>Probabilities of each SNP being incorrect, estimated based on the training data set, by the different algorithms</p></caption><graphic xlink:href="13353_2020_586_Fig2_HTML" id="MO2"/></fig></p><p id="Par29">Note, that in Keras, for a binary (i.e. correct/incorrect) classification, a default cutoff point is 0.5; however, for almost all the applied algorithms, the optimal cutoff points were estimated based on the F1 or SUMSS metrics deviated from this default value (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). The most difference was observed when cutoff points were optimised based on the F1 metric and varied between as low as 0.1005 for the NA&#x000cf;VE algorithm and 0.8550 for the OVERSAMPLED100 algorithm. Cutoff points estimated based on the F1 metric were always higher than those estimated based on SUMSS. However, regardless of which metric was used for the cutoff optimisation, the NA&#x000cf;VE algorithm resulted in the lowest cutoffs and the highest cutoff values corresponded to the OVERSAMPLED100 algorithm. The accuracy of the cutoff points was examined by re-estimating them based on the bootstrapped sub-samples from the training data set and was very high, as expressed by the standard deviations varying between &#x0003c;&#x02009;0.001 (NA&#x000cf;VE for SUMSS) and 0.016 (WEIGHTED for SUMSS).<fig id="Fig3"><label>Fig. 3</label><caption><p>Probability cutoff values for SNP classification into the correct or incorrect group, estimated by the different algorithms based on the optimisation either for the F1 or for SUMSS metric</p></caption><graphic xlink:href="13353_2020_586_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec13"><title>Model optimisation</title><p id="Par30">To avoid overfitting, each model was appropriately optimised. For our imbalanced data sets, not only a loss metric, but also precision <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left(\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}\right) $$\end{document}</tex-math><mml:math id="M14" display="inline"><mml:mfenced close=")" open="("><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac></mml:mfenced></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq7.gif"/></alternatives></inline-formula> and recall <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left(\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}\right) $$\end{document}</tex-math><mml:math id="M16" display="inline"><mml:mfenced close=")" open="("><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:mfenced></mml:math><inline-graphic xlink:href="13353_2020_586_Article_IEq8.gif"/></alternatives></inline-formula> metrics were used to choose the optimal number of epochs. Network optimisation was applied based on a subset constructed from 10% of SNPs, stratified (i.e. the same ratio of correct to incorrect SNPs as in the training data set) randomly sampled from the training data set (OPTset). Early stopping parameters were set to control a loss metric of the OPTset, where after 25 epochs without loss improvement, the learning process was stopped. The Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>A shows the learning process along with each epoch for the NA&#x000cf;VE algorithm. It is evident, that this baseline model did not handle our imbalanced data well. According to the loss metric, the network began to overfit after seven epochs. The NA&#x000cf;VE approach brings high precision at the cost of poor recall close to zero. The WEIGHTED algorithm achieved a much higher recall with a constant level of precision. Based on loss and recall metrics, the WEIGHTED algorithm was trimmed to 30 epochs (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>A). In relation to OVERSAMPLED30 (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">3</xref>A), OVERSAMPLED60 (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">4</xref>A) and OVERSAMPLED100 (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">5</xref>A) algorithms, learning was stopped after 70, 120 and 113 epochs respectively. However, with regard to the OVERSAMPLED algorithms, it is crucial to note that the distributions of metrics will be different because of differences in SNP class percentages between the OPTset and the training data set.</p></sec><sec id="Sec14"><title>Classification of training data</title><p id="Par31">Figure <xref rid="Fig4" ref-type="fig">4</xref> visualises the classification obtained for training data with the final parameters of the classification algorithms. Regardless of the applied probability cutoff (i.e. estimated base on the F1 or on the SUMSS metrics), both&#x02014;the NA&#x000cf;VE and especially the WEIGHTED algorithms&#x02014;do not provide a reasonable classification. Therefore, the approach based on oversampling of the incorrect SNP category emerged to be a better option. No marked differences were observed among the three oversampling schemes tested, but nominally, the best F1 metric of 0.42 was obtained for the most balanced scheme attributed to the OVERSAMPLED60 algorithm. Depending on the probability cutoff applied, this algorithm properly classifies 59.22% of incorrect SNPs and 97.42% of correct SNPs (for the F1 based cutoff estimated to 0.77) or 96.96% of incorrect SNPs and 89.56% of correct SNPs (for the SUMMS based cutoff estimated to 0.49). The latter shall be regarded as the best of the compared classification schemes. Since the probability cutoffs estimated based on the F1 metric were always higher than those based on the SUMMS metric classification based on the former always <italic>favoured</italic>, the proper assignment of correct SNPs while the classification trend based on SUMMS was the opposite.<fig id="Fig4"><label>Fig. 4</label><caption><p>Classification of training data by the different algorithms, based on the probability cutoff thresholds estimated for the F1 or SUMSS metrics. The numbers above columns represent TP&#x02014;percentages of true positive results, TN&#x02014;percentages of true negative results, F1&#x02014;values of the F1 metric</p></caption><graphic xlink:href="13353_2020_586_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec15"><title>Classification of validation data</title><p id="Par32">Obviously, the classification of validation data, depicted in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, was not as good as in the training conditions. The F1 metrics dropped down to 0.21 for NA&#x000cf;VE and WEIGHTED algorithms as well as 0.17 for all OVERSAMPLED algorithms, which was a decrease by 0.04 (NA&#x000cf;VE, WEIGHTED), 0.24 (OVERSAMPLED30, OVERSAMPLED100) and 0.25 (OVERSAMPLED60). With that, it became evident that the good performance of the OVERSAMPLED algorithms observed on training data was not robust, especially for the detection of incorrect SNPs. In particular, for training data and using the probability cutoff estimated (using the training data set) based on F1, only some 22% of such SNPs were detected by the OVERSAMPLED algorithms. This was a decrease of 33.87% for OVERSAMPLED30, by 36.39% for OVERSAMPLED100 and 37.40% for OVERDSAMPLED60 and thus much higher than the decrease of 7.13% and 6.69% observed respectively for NA&#x000cf;VE and WEIGHTED algorithms. On the other hand, the proper detection of correct SNPs was on the same level for both analysed data sets (i.e. training and validation). A very similar relation between test-based and validation-based classifications was observed when using probability cutoff values estimated based on the SUMSS metric, visualised in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. The F1 metric quantifying the overall performance of the algorithms favoured the basic models&#x02014;NA&#x000cf;VE and WEIGHTED, which yielded F1 of 0.21. All three algorithms trained based on OVERSAMPLING of the incorrect SNPs resulted in a lower F1 of 0.17.<fig id="Fig5"><label>Fig. 5</label><caption><p>Classification of validation data by the different algorithms, based on the probability cutoff thresholds estimated for the F1 or SUMSS metrics. The numbers above columns represent TP&#x02014;percentages of true positive results, TN&#x02014;percentages of true negative results, F1&#x02014;values of the F1 metric</p></caption><graphic xlink:href="13353_2020_586_Fig5_HTML" id="MO5"/></fig></p></sec></sec><sec id="Sec16"><title>Discussion and conclusions</title><p id="Par33">The classification of rare event data has been a long recognised problem in statistics. Before the era of machine learning, such data was typically attacked either by modification of input data through applying continuity corrections (Sweeting et al. <xref ref-type="bibr" rid="CR26">2004</xref>) and re-sampling (Fr&#x000fc;hwirth-Schnatter and Wagner <xref ref-type="bibr" rid="CR5">2008</xref>) or by modification of the underlying logistic regression model by estimates correction and applying different weighting for input data classes (King and Zeng <xref ref-type="bibr" rid="CR12">2001a</xref>). Unfortunately, none of those statistical-based approaches resulted in satisfactory handling of the rare event class. A modern extension of handling the problem is to apply the machine learning approach, which with its flexibility towards data structures poses a promising alternative.</p><p id="Par34">NGS-based classification of variants based on the standard output from VCF files is not a new idea. Already in <xref ref-type="bibr" rid="CR4">2013</xref>, Durtschi et al. proposed a metric based on the likelihood of three possible genotypes and read depth at a polymorphic site to classify each SNP into four categories expressing different probabilities of being a correct call. Other methods, reviewed by Heydari et al. (<xref ref-type="bibr" rid="CR8">2017</xref>), aiming not only to identify ambiguous SNP calls but also to correct the original output from variant calling pipelines based on Illumina sequencing, were either based on analysing sequence k-mers or on multiple alignments. In the context of machine learning for SNP classification based on the standard VCF output, Shringarpure et al. (<xref ref-type="bibr" rid="CR24">2017</xref>) constructed a classifier based on random forests. Deep learning was applied by Ravasio et al. (<xref ref-type="bibr" rid="CR23">2018</xref>) who estimated the SNP correctness probability and provided a SNP classification tool GARFIELD based on a multi-layer network implemented through the H2O platform (<ext-link ext-link-type="uri" xlink:href="http://www.h2o.ai">www.h2o.ai</ext-link>). The authors achieved generally high areas under the ROC, varying between 0.63 and 0.98, depending on the analysed platform and coverage. In their unpublished report deposited in the bioRxiv preprint server (2019), Singh and Bhatia applied a series of dense layers to classify data originated from the IonTorrent technology, obtaining a high F1 score of 0.94. The authors circumvented the rare nature of incorrect SNP calls by evaluating only subsets of correct and incorrect variants of equal size.</p><p id="Par35">The experience gained from our analysis of the data is that for a rare event classification problem, like incorrect SNP detection in NGS data, a more parsimonious network, which is less adapted to the specificity of a training data set, is a better, i.e. more robust, option.</p></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec17"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="13353_2020_586_MOESM1_ESM.docx"><label>ESM 1</label><caption><p>(DOCX 453&#x000a0;kb)</p></caption></media></supplementary-material></p></sec></sec></body><back><fn-group><fn><p>The original online version of this article was revised: The original version on this paper contained an error. Figure 5 was published with the same image of figure 4.</p></fn><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p><bold>Change history</bold></p><p>10/12/2020</p><p>The original version on this paper contained an error. Figure 5 was published with the same image of Fig.&#x000a0;4.</p></fn></fn-group><ack><title>Availability of data and material</title><p>The data set is available upon request to bernt.guldbrandtsen@itw.uni-bonn.de.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>K.K. performed computations involving deep learning and network diagnostics and co-designed network architecture. M.M. built and ran the variant detection pipeline. T.S. edited raw variant data. B.C. participated in network computations and co-designed network architecture. B.G. provided and edited raw data as well as participated in designing the variant detection pipeline. J.S. provided the idea for the study. All authors contributed to the writing of the manuscript.</p></notes><notes notes-type="ethics"><title>Compliance with ethical standards</title><notes id="FPar1" notes-type="COI-statement"><title>Conflict of interest</title><p id="Par36">The authors declare that they have no conflict of interest.</p></notes><notes id="FPar2"><title>Ethics approval</title><p id="Par37">This study is entirely based on in silico data and therefore ethics issues do not apply.</p></notes><notes id="FPar3"><title>Consent to participate</title><p id="Par38">Not applicable.</p></notes><notes id="FPar4"><title>Consent for publication</title><p id="Par39">Not applicable.</p></notes><notes id="FPar5"><title>Code availability</title><p id="Par40">Keras code and the trained network are available upon request from the corresponding author.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P et al. (2015) TensorFlow: large-scale machine learning on heterogeneous systems. <ext-link ext-link-type="uri" xlink:href="http://tensorflow.org">tensorflow.org</ext-link></mixed-citation></ref><ref id="CR2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abnizova</surname><given-names>I</given-names></name><name><surname>Boekhorst</surname><given-names>R</given-names></name><name><surname>Orlov</surname><given-names>YL</given-names></name></person-group><article-title>Generation, computational errors and biases in short read next sequencing</article-title><source>J Proteomics Bioinform</source><year>2017</year><volume>10</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.4172/jpb.1000420</pub-id></element-citation></ref><ref id="CR3"><mixed-citation publication-type="other">Chollet F (2015) Keras. <ext-link ext-link-type="uri" xlink:href="http://github.com/fchollet/keras">github.com/fchollet/keras</ext-link></mixed-citation></ref><ref id="CR4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durtschi</surname><given-names>J</given-names></name><name><surname>Margraf</surname><given-names>RL</given-names></name><name><surname>Coonrod</surname><given-names>EM</given-names></name><etal/></person-group><article-title>VarBin, a novel method for classifying true and false positive variants in NGS data</article-title><source>BMC Bioinformatics</source><year>2013</year><volume>14</volume><fpage>S2</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-14-S13-S2</pub-id><pub-id pub-id-type="pmid">24266885</pub-id></element-citation></ref><ref id="CR5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fr&#x000fc;hwirth-Schnatter</surname><given-names>S</given-names></name><name><surname>Wagner</surname><given-names>H</given-names></name></person-group><article-title>Marginal likelihoods for non-Gaussian models using auxiliary mixture sampling</article-title><source>Comput Stat Data An</source><year>2008</year><volume>52</volume><fpage>4608</fpage><lpage>4624</lpage><pub-id pub-id-type="doi">10.1016/j.csda.2008.03.028</pub-id></element-citation></ref><ref id="CR6"><mixed-citation publication-type="other">Garrison E, Marth G (2012) Haplotype-based variant detection from short-read sequencing arXiv 1207.3907</mixed-citation></ref><ref id="CR7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>G</given-names></name><name><surname>Saini</surname><given-names>S</given-names></name></person-group><article-title>DAVI: deep learning-based tool for alignment and single nucleotide variant identification</article-title><source>Mach Learn Sci Technol</source><year>2020</year><volume>1</volume><fpage>025013</fpage><pub-id pub-id-type="doi">10.1101/778647</pub-id></element-citation></ref><ref id="CR8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heydari</surname><given-names>M</given-names></name><name><surname>Miclotte</surname><given-names>G</given-names></name><name><surname>Demeester</surname><given-names>P</given-names></name><etal/></person-group><article-title>Evaluation of the impact of Illumina error correction tools on de novo genome assembly</article-title><source>BMC Bioinformatics</source><year>2017</year><volume>18</volume><issue>1</issue><fpage>374</fpage><pub-id pub-id-type="doi">10.1186/s12859-017-1784-8</pub-id><pub-id pub-id-type="pmid">28821237</pub-id></element-citation></ref><ref id="CR9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horner</surname><given-names>DS</given-names></name><name><surname>Pavesi</surname><given-names>G</given-names></name><name><surname>Castrignan&#x000f2;</surname><given-names>T</given-names></name><etal/></person-group><article-title>Bioinformatics approaches for genomics and post genomics applications of next-generation sequencing</article-title><source>Brief Bioinform</source><year>2010</year><volume>11</volume><issue>2</issue><fpage>181</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1093/bib/bbp046</pub-id><pub-id pub-id-type="pmid">19864250</pub-id></element-citation></ref><ref id="CR10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>I</given-names></name><etal/></person-group><article-title>Systematic comparison of variant calling pipelines using gold standard personal exome variants</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><fpage>17875</fpage><pub-id pub-id-type="doi">10.1038/srep17875</pub-id><pub-id pub-id-type="pmid">26639839</pub-id></element-citation></ref><ref id="CR11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>T</given-names></name><name><surname>Gradus</surname><given-names>JL</given-names></name><name><surname>Rosellini</surname><given-names>AJ</given-names></name></person-group><article-title>Supervised machine learning: a brief primer</article-title><source>Behav Ther</source><year>2020</year><volume>51</volume><issue>5</issue><fpage>675</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1016/j.beth.2020.05.002</pub-id><pub-id pub-id-type="pmid">32800297</pub-id></element-citation></ref><ref id="CR12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>G</given-names></name><name><surname>Zeng</surname><given-names>L</given-names></name></person-group><article-title>Logistic regression in rare events data</article-title><source>Polit Anal</source><year>2001</year><volume>9</volume><fpage>137</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1093/oxfordjournals.pan.a004868</pub-id></element-citation></ref><ref id="CR13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>G</given-names></name><name><surname>Zeng</surname><given-names>L</given-names></name></person-group><article-title>Explaining rare events in international relations</article-title><source>Int Organ</source><year>2001</year><volume>55</volume><fpage>693</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1162/00208180152507597</pub-id></element-citation></ref><ref id="CR14"><mixed-citation publication-type="other">Kingma DP, Ba J (2014) Adam: a method for stochastic optimization arXiv 1412.6980</mixed-citation></ref><ref id="CR15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurie</surname><given-names>S</given-names></name><name><surname>Fernandez-Callejo</surname><given-names>M</given-names></name><name><surname>Marco-Sola</surname><given-names>S</given-names></name><etal/></person-group><article-title>From wet-lab to variations: concordance and speed of bioinformatics pipelines for whole genome and whole exome sequencing</article-title><source>Hum Mutat</source><year>2016</year><volume>37</volume><fpage>1263</fpage><lpage>1271</lpage><pub-id pub-id-type="doi">10.1002/humu.23114</pub-id><pub-id pub-id-type="pmid">27604516</pub-id></element-citation></ref><ref id="CR16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Durbin</surname><given-names>R</given-names></name></person-group><article-title>Fast and accurate short read alignment with burrows-wheeler transform</article-title><source>Bioinformatics</source><year>2009</year><volume>25</volume><fpage>1754</fpage><lpage>1760</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp324</pub-id><pub-id pub-id-type="pmid">19451168</pub-id></element-citation></ref><ref id="CR17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Handsaker</surname><given-names>B</given-names></name><name><surname>Wysoker</surname><given-names>A</given-names></name><etal/></person-group><article-title>The sequence alignment/map format and SAMtools</article-title><source>Bioinformatics</source><year>2009</year><volume>25</volume><fpage>2078</fpage><lpage>2079</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp352</pub-id><pub-id pub-id-type="pmid">19505943</pub-id></element-citation></ref><ref id="CR18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Shao</surname><given-names>Y</given-names></name><name><surname>Tian</surname><given-names>L</given-names></name><etal/></person-group><article-title>Analysis of error profiles in deep next-generation sequencing data</article-title><source>Genome Biol</source><year>2019</year><volume>20</volume><fpage>50</fpage><pub-id pub-id-type="doi">10.1186/s13059-019-1659-6</pub-id><pub-id pub-id-type="pmid">30867008</pub-id></element-citation></ref><ref id="CR19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKenna</surname><given-names>A</given-names></name><name><surname>Hanna</surname><given-names>M</given-names></name><name><surname>Banks</surname><given-names>E</given-names></name><etal/></person-group><article-title>The genome analysis toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data</article-title><source>Genome Res</source><year>2010</year><volume>20</volume><fpage>1297</fpage><lpage>1303</lpage><pub-id pub-id-type="doi">10.1101/gr.107524.110</pub-id><pub-id pub-id-type="pmid">20644199</pub-id></element-citation></ref><ref id="CR20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolazzi</surname><given-names>EL</given-names></name><name><surname>Caprera</surname><given-names>A</given-names></name><name><surname>Nazzicari</surname><given-names>N</given-names></name><etal/></person-group><article-title>SNPchiMp v.3: integrating and standardizing single nucleotide polymorphism data for livestock species</article-title><source>BMC Genomics</source><year>2015</year><volume>16</volume><fpage>283</fpage><pub-id pub-id-type="doi">10.1186/s12864-015-1497-1</pub-id><pub-id pub-id-type="pmid">25881165</pub-id></element-citation></ref><ref id="CR21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>P&#x000e9;rez-Enciso</surname><given-names>M</given-names></name><name><surname>Zingaretti</surname><given-names>LM</given-names></name></person-group><article-title>A guide on deep learning for complex trait genomic prediction</article-title><source>Genes</source><year>2019</year><volume>10</volume><fpage>553</fpage><pub-id pub-id-type="doi">10.3390/genes10070553</pub-id></element-citation></ref><ref id="CR22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>F</given-names></name><name><surname>Gr&#x000f6;ber</surname><given-names>C</given-names></name><name><surname>Blank</surname><given-names>M</given-names></name><etal/></person-group><article-title>Systematic evaluation of error rates and causes in short samples in next-generation sequencing</article-title><source>Sci Rep</source><year>2018</year><volume>8</volume><fpage>10950</fpage><pub-id pub-id-type="doi">10.1038/s41598-018-29325-6</pub-id><pub-id pub-id-type="pmid">30026539</pub-id></element-citation></ref><ref id="CR23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravasio</surname><given-names>V</given-names></name><name><surname>Ritelli</surname><given-names>M</given-names></name><name><surname>Legati</surname><given-names>A</given-names></name><etal/></person-group><article-title>GARFIELD-NGS: genomic vARiants FIltering by dEep Learning moDels in NGS</article-title><source>Bioinformatics</source><year>2018</year><volume>34</volume><issue>17</issue><fpage>3038</fpage><lpage>3040</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty303</pub-id><pub-id pub-id-type="pmid">29668842</pub-id></element-citation></ref><ref id="CR24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shringarpure</surname><given-names>SS</given-names></name><name><surname>Mathias</surname><given-names>RA</given-names></name><name><surname>Hernandez</surname><given-names>RD</given-names></name><etal/></person-group><article-title>Using genotype array data to compare multi- and single-sample variant calls and improve variant call sets from deep coverage whole-genome sequencing data</article-title><source>Bioinformatics</source><year>2017</year><volume>33</volume><issue>8</issue><fpage>1147</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btw786</pub-id><pub-id pub-id-type="pmid">28035032</pub-id></element-citation></ref><ref id="CR25"><mixed-citation publication-type="other">Singh A, Bhatia P (2019) Intelli-NGS: intelligent NGS, a deep neural network-based artificial intelligence to delineate good and bad variant calls from IonTorrent sequencer data. bioRxiv:12.17.879403. 10.1101/2019.12.17.879403</mixed-citation></ref><ref id="CR26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweeting</surname><given-names>MJ</given-names></name><name><surname>Sutton</surname><given-names>AJ</given-names></name><name><surname>Lambert</surname><given-names>PC</given-names></name></person-group><article-title>What to add to nothing? Use and avoidance of continuity corrections in meta-analysis of sparse data</article-title><source>Stat Med</source><year>2004</year><volume>23</volume><issue>9</issue><fpage>1351</fpage><lpage>1375</lpage><pub-id pub-id-type="doi">10.1002/sim.1761</pub-id><pub-id pub-id-type="pmid">15116347</pub-id></element-citation></ref><ref id="CR27"><mixed-citation publication-type="other">Thiele C, Hirschfeld G (2020) Cutpointr: improved estimation and validation of optimal cutpoints in R arXiv 2002.09209</mixed-citation></ref></ref-list></back></article>