
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Adv Health Sci Educ Theory Pract</journal-id><journal-id journal-id-type="iso-abbrev">Adv Health Sci Educ Theory Pract</journal-id><journal-title-group><journal-title>Advances in Health Sciences Education</journal-title></journal-title-group><issn pub-type="ppub">1382-4996</issn><issn pub-type="epub">1573-1677</issn><publisher><publisher-name>Springer Netherlands</publisher-name><publisher-loc>Dordrecht</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26597452</article-id><article-id pub-id-type="pmc">4923093</article-id><article-id pub-id-type="publisher-id">9652</article-id><article-id pub-id-type="doi">10.1007/s10459-015-9652-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An investigation into the optimal number of distractors in single-best answer exams</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8067-2306</contrib-id><name><surname>Kilgour</surname><given-names>James M.</given-names></name><address><email>KilgourJM@cardiff.ac.uk</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Tayyaba</surname><given-names>Saadia</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1">Institute of Medical Education, School of Medicine, Cardiff University, The Cochrane Building, Heath Park Campus, Cardiff, CF14 4XN UK </aff></contrib-group><pub-date pub-type="epub"><day>23</day><month>11</month><year>2015</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>11</month><year>2015</year></pub-date><pub-date pub-type="ppub"><year>2016</year></pub-date><volume>21</volume><fpage>571</fpage><lpage>585</lpage><history><date date-type="received"><day>24</day><month>7</month><year>2015</year></date><date date-type="accepted"><day>12</day><month>11</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2015</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p>In UK medical schools, five-option single-best answer (SBA) questions are the most widely accepted format of summative knowledge assessment. However, writing SBA questions with four effective incorrect options is difficult and time consuming, and consequently, many SBAs contain a high frequency of implausible distractors. Previous research has suggested that fewer than five-options could hence be used for assessment, without deterioration in quality. Despite an existing body of empirical research in this area however, evidence from undergraduate medical education is sparse. The study investigated the frequency of non-functioning distractors in a sample of 480 summative SBA questions at Cardiff University. Distractor functionality was analysed, and then various question models were tested to investigate the impact of reducing the number of distractors per question on examination difficulty, reliability, discrimination and pass rates. A survey questionnaire was additionally administered to 108 students (33&#x000a0;% response rate) to gain insight into their perceptions of these models. The simulation of various exam models revealed that, for four and three-option SBA models, pass rates, reliability, and mean item discrimination remained relatively constant. The average percentage mark however consistently increased by 1&#x02013;3&#x000a0;% with the four and three-option models, respectively. The questionnaire survey revealed that the student body had mixed views towards the proposed format change. This study is one of the first to comprehensively investigate distractor performance in SBA examinations in undergraduate medical education. It provides evidence to suggest that using three-option SBA questions would maximise efficiency whilst maintaining, or possibly improving, psychometric quality, through allowing a greater number of questions per exam paper.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Assessment</kwd><kwd>Testing</kwd><kwd>Written examination</kwd><kwd>Examination reliability</kwd><kwd>Examination quality</kwd><kwd>Single-best answer exams</kwd><kwd>Undergraduate medical education</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Science+Business Media Dordrecht 2016</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p>
Multiple-choice questions (MCQs) have been used as a form of knowledge assessment since the early twentieth century (Trewin <xref ref-type="bibr" rid="CR44">2007</xref>). They are now commonplace in both undergraduate and postgraduate medical examinations (Shumway and Harden <xref ref-type="bibr" rid="CR36">2003</xref>), as they are standardised, equitable, objective, cost effective, reliable and discriminatory (Al-Rukban <xref ref-type="bibr" rid="CR2">2006</xref>). In addition, they can be used to assess both factual recall and more complex cognitive functions such as diagnostic skill, evaluation and reasoning (Epstein <xref ref-type="bibr" rid="CR13">2007</xref>). They also enable the assessment of a broad range of content, as each exam paper can contain a large number of items. This makes the MCQ format particularly suited to summative final examinations, and also national licensing tests (Epstein <xref ref-type="bibr" rid="CR13">2007</xref>; van der Vleuten <xref ref-type="bibr" rid="CR46">2000</xref>). The major drawback to the MCQ format however, is that high quality questions are difficult, time-consuming and costly to write (Al-Rukban <xref ref-type="bibr" rid="CR2">2006</xref>; Epstein <xref ref-type="bibr" rid="CR13">2007</xref>; Shumway and Harden <xref ref-type="bibr" rid="CR36">2003</xref>).</p><p>The incorrect options to a MCQ are known as distractors; they serve the purpose of diverting non-competent candidates away from the correct answer (Burton et al. <xref ref-type="bibr" rid="CR7">1991</xref>). Lowe (<xref ref-type="bibr" rid="CR23">1991</xref>) noted that &#x0201c;the mark of a good question is often the quality of the distractors&#x0201d; (Lowe <xref ref-type="bibr" rid="CR23">1991</xref>, p. 780), and this reflects the importance of these incorrect options for discrimination between examinees. As the number of high quality, functional distractors increases, so does item difficulty and discrimination (Haladyna and Downing <xref ref-type="bibr" rid="CR15">1988</xref>, <xref ref-type="bibr" rid="CR17">1993</xref>; Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>; Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>). Undergraduate medical examinations typically use a specific form of MCQ, known as single-best answer (SBA), where all of the incorrect distractors still contain some element of truth, but to a competent candidate are perceptibly inferior to the one correct response. The process of generating distractors for SBA questions is challenging and labour intensive, and this often results in unconvincing distractors being chosen (Al-Rukban <xref ref-type="bibr" rid="CR2">2006</xref>). It has been further argued by the literature that in most cases, there is a natural limit to the number of plausible distractors possible for any given topic, and that this limit is generally less than the four conventionally used in medical school SBA papers (Haladyna and Downing <xref ref-type="bibr" rid="CR17">1993</xref>; Haladyna et al. <xref ref-type="bibr" rid="CR18">2002</xref>).</p><p>There is substantial evidence to suggest that across diverse academic disciplines, the majority of MCQs used in examinations contain a high proportion of non-functioning distractors. These are distractors which either have been selected by &#x0003c;5&#x000a0;% of the target cohort (Cizek and O&#x02019;Day <xref ref-type="bibr" rid="CR8">1994</xref>; Delgado and Prieto <xref ref-type="bibr" rid="CR12">1998</xref>; Haladyna and Downing <xref ref-type="bibr" rid="CR17">1993</xref>; Rogers and Harley <xref ref-type="bibr" rid="CR30">1999</xref>; Shizuka et al. <xref ref-type="bibr" rid="CR35">2006</xref>; Sidick et al. <xref ref-type="bibr" rid="CR37">1994</xref>; Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>), or show negative discrimination statistics (Measured Progress <xref ref-type="bibr" rid="CR27">2014</xref>). A key psychometric investigation by Haladyna and Downing (<xref ref-type="bibr" rid="CR17">1993</xref>) found that in a two hundred item MCQ exam administered to US physicians, no items had four functioning distractors, and only 8.4&#x000a0;% of questions had three (Haladyna and Downing <xref ref-type="bibr" rid="CR17">1993</xref>). Given that non-functioning distractors do not significantly add to the difficulty, discrimination or reliability of exam papers (Haladyna and Downing <xref ref-type="bibr" rid="CR15">1988</xref>; Rodriguez <xref ref-type="bibr" rid="CR28">2005</xref>), it was therefore hypothesised that the removal of poor quality distractors from SBA questions, to instead use a reduced-option format, should not have an impact on exam quality. This hypothesis has been supported by numerous other studies; the earliest involved mathematical modelling (Bruno and Dirkzwager <xref ref-type="bibr" rid="CR6">1995</xref>; Grier <xref ref-type="bibr" rid="CR14">1975</xref>; Lord <xref ref-type="bibr" rid="CR22">1977</xref>; Tversky <xref ref-type="bibr" rid="CR45">1964</xref>), whilst later studies went on to use experimental approaches. These investigations revealed that a reduced-option MCQ format is equivalent or superior to the conventional format in terms of the number of functioning distractors (Trevisan et al. <xref ref-type="bibr" rid="CR42">1991</xref>; Wakefield <xref ref-type="bibr" rid="CR49">1958</xref>), exam reliability (Baghaei and Amrahi <xref ref-type="bibr" rid="CR4">2011</xref>; Costin <xref ref-type="bibr" rid="CR10">1972</xref>; Delgado and Prieto <xref ref-type="bibr" rid="CR12">1998</xref>; Rogers and Harley <xref ref-type="bibr" rid="CR30">1999</xref>; Ruch and Charles <xref ref-type="bibr" rid="CR31">1928</xref>; Shizuka et al. <xref ref-type="bibr" rid="CR35">2006</xref>; Sidick et al. <xref ref-type="bibr" rid="CR37">1994</xref>; Trevisan et al. <xref ref-type="bibr" rid="CR42">1991</xref>, <xref ref-type="bibr" rid="CR43">1994</xref>), item discrimination (Baghaei and Amrahi <xref ref-type="bibr" rid="CR4">2011</xref>; Costin <xref ref-type="bibr" rid="CR10">1972</xref>; Crehan et al. <xref ref-type="bibr" rid="CR11">1993</xref>; Delgado and Prieto <xref ref-type="bibr" rid="CR12">1998</xref>; Owen and Froman <xref ref-type="bibr" rid="CR26">1987</xref>; Shizuka et al. <xref ref-type="bibr" rid="CR35">2006</xref>; Trevisan et al. <xref ref-type="bibr" rid="CR42">1991</xref>) and difficulty (Baghaei and Amrahi <xref ref-type="bibr" rid="CR4">2011</xref>; Crehan et al. <xref ref-type="bibr" rid="CR11">1993</xref>; Delgado and Prieto <xref ref-type="bibr" rid="CR12">1998</xref>; Landrum et al. <xref ref-type="bibr" rid="CR21">1993</xref>; Owen and Froman <xref ref-type="bibr" rid="CR26">1987</xref>; Shizuka et al. <xref ref-type="bibr" rid="CR35">2006</xref>; Sidick et al. <xref ref-type="bibr" rid="CR37">1994</xref>). Several systematic reviews have also provided evidence to confirm these findings (Aamodt and McShane <xref ref-type="bibr" rid="CR1">1992</xref>; Haladyna and Downing <xref ref-type="bibr" rid="CR16">1989</xref>; Haladyna et al. <xref ref-type="bibr" rid="CR18">2002</xref>; Rodriguez <xref ref-type="bibr" rid="CR28">2005</xref>; Vyas and Supe <xref ref-type="bibr" rid="CR48">2008</xref>). Furthermore, student perceptions of a reduced-option MCQ model have also been investigated with positive results (Owen and Froman <xref ref-type="bibr" rid="CR26">1987</xref>).</p><p>This literature shows that there is evidence to support a reduced-option MCQ format, however, to date, little empirical research has been produced to investigate this phenomenon in medical education. Whilst some early studies involved the assessment of postgraduate medical training (Cizek and O&#x02019;Day <xref ref-type="bibr" rid="CR8">1994</xref>; Cizek et al. <xref ref-type="bibr" rid="CR9">1998</xref>; Haladyna and Downing <xref ref-type="bibr" rid="CR15">1988</xref>, <xref ref-type="bibr" rid="CR17">1993</xref>), and more recent studies have involved nursing (Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>) and dental students (Kolstad et al. <xref ref-type="bibr" rid="CR20">1985</xref>), there have only been three studies involving undergraduate medical students. Two of these studies were of limited scope and are therefore of limited generalisability (Schneid et al. <xref ref-type="bibr" rid="CR32">2014</xref>; Swanson et al. <xref ref-type="bibr" rid="CR39">2008</xref>). Rogausch et al. (<xref ref-type="bibr" rid="CR29">2010</xref>) conducted the most comprehensive study, which involved a sample of 737 questions from the Swiss Federal graduation exam. At the 5&#x000a0;% frequency of selection level, only 2.8&#x000a0;% of questions in this sample had four functional distractors. The authors also modelled the effect of reducing the number of options from five to three, finding that this change resulted in only a slight increase in mean percentage correct, whilst discrimination was almost unaffected. However, reliability was markedly decreased, which is in contrast to the previous literature. It is important to note, nevertheless, that the assumption was made that candidates who chose the least functional distractors would otherwise have chosen the correct answer, and therefore they reallocated these candidates as such (Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>).</p><p>Despite evidence from the wider educational measurement literature, there is still a lack of high-quality, generalizable and comprehensive research specific to undergraduate medical education to support a reduction in the number of distractors per SBA item. Furthermore, the evidence which has been produced in this area in inconsistent with the previous literature (Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>). There is therefore a clear need for elucidation and clarification in this field. This paucity of evidence may explain the reluctance of medical schools to make changes to the current SBA model (Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>). This is likely further compounded by the fears of assessment writers, who may believe that three-option questions will increase successful guessing by non-competent candidates (Schneid et al. <xref ref-type="bibr" rid="CR32">2014</xref>). The overall objective of this research study was therefore to investigate SBA distractor functionality from an undergraduate medical assessment perspective.</p></sec><sec id="Sec2"><title>Methodology</title><p>We used a mixed-methods approach for this study, involving both statistical analysis of exam data, as well as administering a survey containing a mixture of Likert and free-text responses. Ethical approval was granted for this research by the <italic>Cardiff University School of Medicine Research Ethics Committee</italic> in December 2014.</p><sec id="Sec3"><title>Sample</title><p>Four past examination papers from the 2013/2014 Cardiff University undergraduate medical programme were scrutinised for this study, which provided a sample of 480 five-option SBA questions. A mean of 269 students sat each of these exam papers, which hence provided a sufficiently large evidence base for psychometric modelling (Schuwirth and van der Vleuten <xref ref-type="bibr" rid="CR33">2011</xref>). These papers were chosen due to their high reliability, which indicated an existing high level of quality, in addition to their wide-ranging coverage of general medicine and medical and surgical specialities, including sociology, ethics, pharmacology and epidemiology.</p><p>The survey component of this study used a sample of 327 students from Cardiff University during the 2014/2015 academic year, including 252 fourth year students, and 75 students undertaking an intercalated degree after the third or fourth year. Students for this component of the study were recruited through generic emails sent to all students in the target cohort, and through social media (i.e. Facebook). No financial or other incentives were offered to students to encourage participation.</p></sec><sec id="Sec4"><title>Measures</title><p>Several measures of exam quality were used in the modelling aspect of this study. These measures were reliability, difficulty, mean item discrimination and pass rates. Exam reliability, or internal consistency (Wells and Wollack <xref ref-type="bibr" rid="CR50">2003</xref>), is one of the most important psychometric properties of an exam paper, as it reflects the precision and consistency of the measurement made by an exam, and hence the reproducibility of its outcome (van der Vleuten <xref ref-type="bibr" rid="CR46">2000</xref>). In this study, we measured reliability using Cronbach&#x02019;s Alpha, as it only requires data from one sitting of the exam paper under evaluation (Tavakol and Dennick <xref ref-type="bibr" rid="CR41">2011</xref>). In this study, we also measured exam difficulty, which is equivalent to the exam&#x02019;s mean percentage mark, and item discrimination, using the mean of each item&#x02019;s point-biserial correlation coefficient (Rpbis). In addition, we also sought to measure the student acceptability and educational impact of a reduction in the number of options per question, which is essential to consider (van der Vleuten and Schuwirth <xref ref-type="bibr" rid="CR47">2005</xref>).</p></sec><sec id="Sec5"><title>Procedure</title><p>The study followed a two-step procedure, where the first step was to determine the frequency of non-functioning distractors across the sample of exam papers. This was achieved by analysing the frequency of selection at the below 5&#x000a0;% level. This data was calculated using the ITEMAN psychometric software (Assessment Systems Corporation <xref ref-type="bibr" rid="CR3">2013</xref>).</p><p>The second step was to model the effects of the reduced-option models on the psychometric measures of each exam paper. The impact on each measure was assessed twice; firstly, for a reduction from five options per question to four, and secondly, for a reduction from five options to three. This analysis was firstly achieved by using the frequency data calculated by ITEMAN to eliminate the least and second least functional distractor for each question. In cases where there was a tie in selection frequency between multiple distractors, elimination was determined by comparing the Rpbis; a more-negative point-biserial was considered as indicating a higher level of functionality. The frequency of students who had selected one of the eliminated distractors was then randomly reassigned to one of the remaining options. The rationale for the random redistribution of these students is based on the assumption that they were most likely guessing the answer, having chosen the least plausible response to the question. Random redistribution can hence be considered a legitimate simulation (Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>).</p><p>Once elimination of non-functioning options had occurred, various data sets were then created using IBM SPSS Statistics (IBM Corporation <xref ref-type="bibr" rid="CR19">2014</xref>), to investigate the impact of the reduction in the number of distractors on the various exam attributes. The statistical significance of any changes in mean values were determined by computing relevant tests of significance. Furthermore, one-way analysis of variance (ANOVA) tests were carried out to compare the differences in mean scores between ability groups (low, average and high performance) for each year group and for each of the three exam paper models.</p><p>The questionnaires for the survey component of this study were distributed online through the SurveyMonkey&#x02122; platform (SurveyMonkey Inc. <xref ref-type="bibr" rid="CR38">2014</xref>) (see &#x0201c;<xref rid="Sec15" ref-type="sec">Appendix</xref>&#x0201d; section). Participation was voluntary and anonymous. Once all survey responses were collected, analysis was then carried out for quantitative data using Microsoft Excel (Microsoft Corporation <xref ref-type="bibr" rid="CR25">2011</xref>), and for qualitative data using the ATLAS.ti software (Scientific Software Development GmbH <xref ref-type="bibr" rid="CR34">2012</xref>). This is a specialist qualitative research program, which facilitates the aggregation and comparison of data from across responses through the use of a coding hierarchy, which was developed through an inductive, thematic approach (Bradley et al. <xref ref-type="bibr" rid="CR5">2007</xref>).</p></sec></sec><sec id="Sec6" sec-type="results"><title>Results</title><p>An analysis of the performance of the examination papers included in this study is shown below (Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>). Overall, all of the papers demonstrated reasonable attributes in terms of reliability and individual question performance.<table-wrap id="Tab1"><label>Table&#x000a0;1</label><caption><p>Analysis of overall examination performance for all 3&#x000a0;years</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Statistic</th><th align="left">Y03</th><th align="left">Y04</th><th align="left">Y05</th></tr></thead><tbody><tr><td align="left">Alpha reliability<sup>a</sup>
</td><td align="left">0.81</td><td align="left">0.82</td><td align="left">0.79</td></tr><tr><td align="left">Average score</td><td align="left">95.14</td><td align="left">144.98</td><td align="left">87.94</td></tr><tr><td align="left">Maximum possible score</td><td align="left">140</td><td align="left">200</td><td align="left">140</td></tr><tr><td align="left">Standard deviation (SD)</td><td align="left">11.02</td><td align="left">12.88</td><td align="left">11.01</td></tr><tr><td align="left">Range of scores</td><td align="left">61&#x02013;126</td><td align="left">108&#x02013;172</td><td align="left">63&#x02013;125</td></tr><tr><td align="left">Number of items</td><td align="left">140</td><td align="left">200</td><td align="left">140</td></tr><tr><td align="left">Average percentage correct</td><td align="left">68&#x000a0;%</td><td align="left">72&#x000a0;%</td><td align="left">63&#x000a0;%</td></tr><tr><td align="left">Average item discrimination (Rpbis)<sup>b</sup>
</td><td align="left">0.16</td><td align="left">0.14</td><td align="left">0.14</td></tr><tr><td align="left">Number of candidates</td><td align="left">262</td><td align="left">278</td><td align="left">273</td></tr></tbody></table><table-wrap-foot><p>
<sup>a</sup>Alpha reliability ranges between 0 and 1 (i.e. no consistency to perfect internal consistency). The desirable range for high stake assessments is 0.8&#x02013;0.89. The higher the stakes of the examination, the higher the value of the alpha is required to be in order to ensure a high degree of confidence in pass/fail decisions</p><p>
<sup>b</sup>Rpbis ranges between &#x02212;1 and 1 (i.e. negatively discriminatory to perfectly discriminatory). In high stakes examinations, it is desirable to have an Rpbis approaching 0.20, as this indicates a high level of discrimination between competent and non-competent candidates</p></table-wrap-foot></table-wrap></p><sec id="Sec7"><title>Analysis of question performance</title><p>In total, 480 question items and 1920 distractors were examined in this study. The analysis of question performance (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>) reveals that across the papers included in this study, the average number of functional distractors per question (those which were chosen with a frequency equal to or &#x0003e;5&#x000a0;% of the cohort) was 1.82.<table-wrap id="Tab2"><label>Table&#x000a0;2</label><caption><p>
<bold>Number of</bold> functional distractors per item</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Number of functional distractors per question</th><th align="left">Y03</th><th align="left">Y04</th><th align="left">Y05</th><th align="left">Overall</th></tr></thead><tbody><tr><td align="left">Zero</td><td align="left">18 (12.9&#x000a0;%)</td><td align="left">39 (19.5&#x000a0;%)</td><td align="left">11 (7.9&#x000a0;%)</td><td align="left">68 (14.2&#x000a0;%)</td></tr><tr><td align="left">One</td><td align="left">35 (25.0&#x000a0;%)</td><td align="left">59 (29.5&#x000a0;%)</td><td align="left">33 (23.6&#x000a0;%)</td><td align="left">127 (26.5&#x000a0;%)</td></tr><tr><td align="left">Two</td><td align="left">43 (30.7&#x000a0;%)</td><td align="left">61 (30.5&#x000a0;%)</td><td align="left">55 (39.3&#x000a0;%</td><td align="left">159 (33.1&#x000a0;%)</td></tr><tr><td align="left">Three</td><td align="left">32 (22.9&#x000a0;%)</td><td align="left">30 (15.0&#x000a0;%)</td><td align="left">30 (21.4&#x000a0;%)</td><td align="left">92 (19.2&#x000a0;%)</td></tr><tr><td align="left">Four</td><td align="left">12 (8.6&#x000a0;%)</td><td align="left">11 (5.5&#x000a0;%)</td><td align="left">11 (7.9&#x000a0;%)</td><td align="left">34 (7.1&#x000a0;%)</td></tr><tr><td align="left">Average</td><td align="left">1.89</td><td align="left">1.58</td><td align="left">1.98</td><td align="left">1.82</td></tr></tbody></table></table-wrap></p><p>Overall, only 34 questions (7.1&#x000a0;%) of the 480 included in this study contained four functional distractors, whilst 92 (19.2&#x000a0;%) contained three. The greatest proportion of questions, 159 (33.1&#x000a0;%), had two functional distractors, whilst many questions contained only one (127, 26.5&#x000a0;%). Finally, 68 questions (14.2&#x000a0;%) contained no functional distractors, and were therefore completely non-discriminatory.</p></sec><sec id="Sec8"><title>Analysis of distractor performance</title><p>Analysis of the performance of the 1920 distractors included in this study reveal that 1062 (55.3&#x000a0;%) of the distractors were non-functional, with 341 of the distractors (17.8&#x000a0;%) being so implausible that they were never chosen. Of the 858 (44.6&#x000a0;%) functional distractors analysed, only 206 (10.7&#x000a0;%) were chosen by more than 20&#x000a0;% of the examinee cohort (Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>).<table-wrap id="Tab3"><label>Table&#x000a0;3</label><caption><p>Breakdown of individual distractor performance grouped into categories by frequency of selection (all years combined)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Distractor functionality by frequency of selection (%)</th><th align="left">Number (%)</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">341 (17.8)</td></tr><tr><td align="left">&#x0003c;5</td><td align="left">721 (37.6)</td></tr><tr><td align="left">5&#x02013;10</td><td align="left">374 (19.5)</td></tr><tr><td align="left">11&#x02013;20</td><td align="left">278 (14.5)</td></tr><tr><td align="left">&#x0003e;20</td><td align="left">206 (10.7)</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec9"><title>Modelling the effects of the question models on exam attributes</title><p>The results of the simulation of the different exam paper models are presented in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>, showing the calculated effect on the selected psychometric measures. These results reveal that the mean percentage mark for each paper would have increased by 1&#x000a0;% following a change from the five-option paper to the four-option version, and would have increased by a total of 3&#x000a0;% if the three-option model had been used. In order to test the statistical significance of these changes, a series of paired samples <italic>t</italic> tests were carried out, using the original five-option model as a baseline. For all years, the changes in difficulty between the five-option version of each paper and the four-option version, and the four-option version and the three-option version were all statistically significant, with <italic>t</italic> values &#x0003e;10 points, narrow confidence intervals (CI) and significance in the expected direction (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001).<table-wrap id="Tab4"><label>Table&#x000a0;4</label><caption><p>Effect of reducing the number of options per item on important exam attributes (five, four and three option models)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Year</th><th align="left">Psychometric attribute</th><th align="left">Five-option model</th><th align="left">Four-option model</th><th align="left">Three-option model</th></tr></thead><tbody><tr><td align="left" rowspan="4">Y03</td><td align="left">Mean % correct</td><td align="left">68&#x000a0;%</td><td align="left">69&#x000a0;%</td><td align="left">71&#x000a0;%</td></tr><tr><td align="left">Mean Rpbis</td><td align="left">0.16</td><td align="left">0.16</td><td align="left">0.15</td></tr><tr><td align="left">Alpha reliability</td><td align="left">0.81</td><td align="left">0.81</td><td align="left">0.82</td></tr><tr><td align="left">Number of fails</td><td align="left">2</td><td align="left">2</td><td align="left">2</td></tr><tr><td align="left" rowspan="4">Y04</td><td align="left">Mean % correct</td><td align="left">72&#x000a0;%</td><td align="left">73&#x000a0;%</td><td align="left">75&#x000a0;%</td></tr><tr><td align="left">Mean Rpbis</td><td align="left">0.14</td><td align="left">0.17</td><td align="left">0.16</td></tr><tr><td align="left">Alpha reliability</td><td align="left">0.82</td><td align="left">0.82</td><td align="left">0.82</td></tr><tr><td align="left">Number of fails</td><td align="left">1</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left" rowspan="4">Y05</td><td align="left">Mean % correct</td><td align="left">63&#x000a0;%</td><td align="left">64&#x000a0;%</td><td align="left">66&#x000a0;%</td></tr><tr><td align="left">Mean Rpbis</td><td align="left">0.14</td><td align="left">0.14</td><td align="left">0.13</td></tr><tr><td align="left">Alpha reliability</td><td align="left">0.79</td><td align="left">0.80</td><td align="left">0.80</td></tr><tr><td align="left">Number of fails</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left" rowspan="3">Average</td><td align="left">Mean % correct</td><td align="left">68&#x000a0;%</td><td align="left">69&#x000a0;%</td><td align="left">71&#x000a0;%</td></tr><tr><td align="left">Mean Rpbis</td><td align="left">0.15</td><td align="left">0.16</td><td align="left">0.15</td></tr><tr><td align="left">Alpha reliability</td><td align="left">0.81</td><td align="left">0.81</td><td align="left">0.81</td></tr></tbody></table></table-wrap></p><p>Furthermore, ANOVA testing demonstrated that the difference in mean total score between different ability groups, for each paper, remained statistically significant (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (with substantially large f values and 95&#x000a0;% CI), regardless of which exam model was used.</p><p>Mean item discrimination (Rpbis) changed in a less linear fashion than exam difficulty. For the year three paper, discrimination was equivalent across the five-option and four-option exam models at 0.16, but it decreased by a statistically significant degree to 0.15 for the three-option model (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.017 five-option to three, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.004 four-option to three; upper 95&#x000a0;% CI of the difference for both &#x02212;0.01). For the two papers in year 4, discrimination increased when the four-option model was employed, with the point-biserial correlation significantly increasing from 0.14 to 0.17 (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001; upper 95&#x000a0;% CI 0.03). Discrimination then significantly decreased when the three-option model was employed, to 0.16 (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001; upper 95&#x000a0;% CI &#x02212;0.01), although discrimination for the three-option model remained significantly higher than for the original five-option version (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001; upper 95&#x000a0;% CI 0.01). Finally, for the year 5 paper, discrimination remained constant between the five-option and four-option models (0.14), but decreased (to 0.13) for the three-option model. No statistical testing was performed for this change however, as the standard error of the difference was zero.</p><p>The mean changes in exam reliability and fail rates were also modelled for the three exam formats. Overall, reliability was not significantly affected by the change in the number of options per item, although some slight increases (as the number of options decreased) were observed (year 3 and year 5). The fail rate of each exam also remained constant regardless of the number of options, as determined by the panel-agreed standard set mark for the five-option paper.</p></sec><sec id="Sec10"><title>Survey responses</title><sec id="Sec11"><title>Quantitative data</title><p>There were 108 responses to the survey questionnaire, equalling a response rate of 33.0&#x000a0;%. The first quantitative question (Q3; 103 responses), asked students to assess how fair they believed SBA exams in medical school to be, using a five-point Likert scale (with options ranging from &#x02018;not at all&#x02019; to &#x02018;completely&#x02019;). A high proportion of respondents (83; 80.6&#x000a0;%) indicated high agreement (&#x02018;completely&#x02019; or &#x02018;to a fair extent&#x02019;) with the statement that SBA exams are a fair method of knowledge assessment, whilst 18.4&#x000a0;% (19) of respondents indicated low agreement (&#x02018;average&#x02019; or &#x02018;to some extent&#x02019;), and just 1&#x000a0;% (1) of students indicated disagreement (&#x02018;not at all&#x02019;).</p><p>The next question (Q4; Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>) was split into five sub-questions, with the purpose of examining student perceptions of the functionality of SBA distractors, in terms of their self-perceived ability to eliminate them (104 responses). The results show a positive trend towards students believing themselves able to successfully eliminate down to a single distractor, or to answer the question without consideration of the distractors at all, a high proportion of the time. Only 3.8&#x000a0;% (4) of respondents stated that they often randomly guess the correct answer from the five options available (&#x02018;often&#x02019; or &#x02018;almost always&#x02019;), whilst 59.6&#x000a0;% (62) of respondents stated by the same measure that they often randomly guess the correct answer from just two remaining options. Likewise, 52.4&#x000a0;% (54) of respondents answered that they are often able to answer the question immediately, without consideration of the other options at all.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>Student self-perceived ability to eliminate distractors (N&#x000a0;=&#x000a0;104)</p></caption><graphic xlink:href="10459_2015_9652_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec12"><title>Qualitative data</title><p>Question five of the survey explored the strategies used by students to respond to summative SBA examinations (88 responses). The responses reveal that the majority of students felt that they often use rote memorisation when preparing for these type of assessments, with 52 (59.1&#x000a0;%) responses indicating rote memorisation. The use of practice questions and past papers was another often highlighted surface-learning revision strategy, with 37 (42&#x000a0;%) responses indicating their use, whilst other unspecified surface-learning approaches were referred to in 16 (18.2&#x000a0;%) responses. In contrast to this, some survey respondents also mentioned that they used deep-learning strategies for SBA exam revision. This included 26 (29.5&#x000a0;%) responses referring to unspecified deep-learning approaches, whilst specified approaches included writing notes (12; 13.6&#x000a0;%), group discussion (2; 2.3&#x000a0;%), and use of explanatory videos (2; 2.3&#x000a0;%). Many respondents indicated the use of both surface-learning and deep-learning approaches (44; 50&#x000a0;%).</p><p>Question six of the questionnaire investigated the strategies employed by medical students to respond to five-option SBA assessments (86 responses). Responses were broadly categorised into five response strategies, with the most often cited strategy being the use of the &#x02018;process of elimination&#x02019; (63; 73.3&#x000a0;%).</p><p>The final question (Q7) of the survey related to the students&#x02019; appraisal of a possible change from five-options to four or three (87 responses). Of these responses, 43 (49.4&#x000a0;%) contained elements of positivity towards the proposed change, including twelve referring to increased efficiency (13.8&#x000a0;%), nine relating to the change being a more effective use of time (10.3&#x000a0;%) and six relating to the positive effect on cost effectiveness (5.7&#x000a0;%). One respondent summarised these ideas in their response:<disp-quote><p>As most of the time I am able to narrow it down between two answers and usually one or two of the five options are quite obviously wrong then reducing it to 3 or 4 would seem reasonable (Respondent 75)</p></disp-quote></p><p>Other positive responses also cited a positive effect on learning (2), an increase in assessment quality (2), an increase in fairness (2), a reduction in stress (1), suitability for dyslexics (1) and finally that a reduced option model would be a better reflection of real-life clinical practice (1).</p><p>Just over three-quarters of the responses received from students contained elements of negativity towards the proposed change in SBA format (68; 78.2&#x000a0;%). Many of these responses indicated a belief that the five-option format was the best (11; 12.6&#x000a0;%), whilst some students concluded that whilst they believed that a four-option model would be adequate, three-options per question would be too few (10; 11.5&#x000a0;%). Many students also felt that using a reduced option model would lead to unfairness, as the resulting exam would be &#x02018;easier&#x02019; (13; 15&#x000a0;%) whilst a high number cited the theoretical increase in probability of successful guessing as a source of unfairness and reduced examination rigour (19; 21.8&#x000a0;%). Other negative responses expressed by students included that reducing the number of options would increase the exam&#x02019;s difficulty (3), would have a negative effect on student learning (2), would require adjustment (i.e. &#x02018;getting used to&#x02019;) (2), may be more stressful (2), and may prepare students less for clinical practice (4).</p></sec></sec></sec><sec id="Sec13" sec-type="discussion"><title>Discussion</title><p>This study set out to investigate the optimal number of distractors in SBA assessments. The results of this study reveal that non-functioning distractors may be commonplace in high-stakes, summative, medical school assessments. Across the sample of four papers that we analysed, only 7.1&#x000a0;% of questions contained four functional distractors, whilst 73.8&#x000a0;% of questions contained two or fewer. These results are in line with what was expected, supporting the theory of Haladyna and Downing that for most topics, there is a natural limit to the possible number of plausible distractors (Haladyna and Downing <xref ref-type="bibr" rid="CR17">1993</xref>; Haladyna et al. <xref ref-type="bibr" rid="CR18">2002</xref>). These findings also match those reported by other studies in the healthcare education literature (Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>; Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>). As non-functioning distractors do not add to the discrimination power of the question, it is hence likely that in most circumstances, three-option questions should be sufficient (Rodriguez <xref ref-type="bibr" rid="CR28">2005</xref>; Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>). Overall, as has been previously suggested, it is the quality, not the quantity, of distractors which determines assessment quality (Haladyna and Downing <xref ref-type="bibr" rid="CR16">1989</xref>). It is concerning however that 14.2&#x000a0;% of the questions in our study were found to not contain any functional distractors, and were therefore not discriminatory. A similar finding was also reported in a study of nursing assessments (Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>), suggesting that this may be a widespread problem. It is therefore important that question writers continually refine and improve poorly functioning questions.</p><p>Our simulation of the various exam models has furthermore demonstrated evidence to support that a three-option format may be optimal. Consistent with previous healthcare research, exam reliability across all four papers investigated did not change (Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>; Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>), whilst any changes in mean item discrimination were statistically significant yet negligible (Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>; Swanson et al. <xref ref-type="bibr" rid="CR39">2008</xref>). Contrary to some previous research however, reporting a fail-to-pass reclassification rate of 1.9&#x000a0;% (Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>), in our study, pass rates also remained constant. It should be noted however that the aforementioned study involved low stakes classroom tests, and this may explain the difference. The only psychometric property that we found changed to an important degree between item models was the average percentage mark, which increased consistently by 1&#x000a0;% when a four-option model was employed, and by 3&#x000a0;% when a three-option model was used. These findings are again consistent with the previous healthcare literature (Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>; Swanson et al. <xref ref-type="bibr" rid="CR39">2008</xref>; Tarrant et al. <xref ref-type="bibr" rid="CR40">2009</xref>), reflecting a trend that fewer options per question is associated with a small yet significant increase in percentage correct, resulting in marginally easier exams. This decrease in difficulty could easily be accounted for however during the standard setting process, by adopting an increased pass threshold (Rogausch et al. <xref ref-type="bibr" rid="CR29">2010</xref>).</p><p>To further probe the impact of various distractor models on performance across ability groups, we carried out a series of comparative ANOVA tests across each assessment. Our findings indicated that despite the reduction in difficulty, the differences in average total score between performance groups remained statistically significant, indicating that performance remained discriminative.</p><p>The findings of this new study offer clear support to the notion that a three-option SBA model may be optimal for summative knowledge assessment in undergraduate medical education, maximising efficiency whilst maintaining examination quality and rigour. Fundamentally, the three-option model would allow for an increase in the number of questions per unit time, as has been reported by other studies (Aamodt and McShane <xref ref-type="bibr" rid="CR1">1992</xref>; Owen and Froman <xref ref-type="bibr" rid="CR26">1987</xref>; Rodriguez <xref ref-type="bibr" rid="CR28">2005</xref>; Schneid et al. <xref ref-type="bibr" rid="CR32">2014</xref>; Swanson et al. <xref ref-type="bibr" rid="CR39">2008</xref>; Vyas and Supe <xref ref-type="bibr" rid="CR48">2008</xref>). One such study involving medical and pharmacy students in fact suggested a saving of 8&#x000a0;s per question for three-option MCQs over their five-option counterparts (Schneid et al. <xref ref-type="bibr" rid="CR32">2014</xref>). This is significant, as increasing the number of questions per exam paper would facilitate broader content coverage, enhancing reliability, and potentially validity (Schneid et al. <xref ref-type="bibr" rid="CR32">2014</xref>; Wells and Wollack <xref ref-type="bibr" rid="CR50">2003</xref>). This increase may compensate for any initial reduction in reliability caused by the use of a reduced-option format (Schneid et al. <xref ref-type="bibr" rid="CR32">2014</xref>; Wells and Wollack <xref ref-type="bibr" rid="CR50">2003</xref>), and may in fact actually lead to an improvement in examination quality over the status quo. Furthermore, the burden to question writers would be reduced, allowing them to focus their time on selecting a smaller number of more plausible distractors, consequently increasing quality whilst maximising cost and time efficiency (Delgado and Prieto <xref ref-type="bibr" rid="CR12">1998</xref>).</p><p>Analysis of the data collected from the survey questionnaire additionally offers further evidence to support our conclusions. Notably, 59.6&#x000a0;% of respondents perceived themselves as often needing to guess the correct answer at random from two remaining options, whilst by the same measure just 3.8&#x000a0;% felt that they often had to guess the answer from between four or five options. This data trend reveals that student experience closely corresponds with our statistical data; it is clear that the majority of the time, students are able to use the process of elimination to rule out at least two options. We also collected data on the strategies that students use to learn and revise for SBA assessments, which indicated a preference for rote memorisation (59.1&#x000a0;%). If a three-option model were to be adopted, it is unlikely that student-learning styles would significantly change, most likely remaining highly dominated by surface-learning approaches.</p><p>Student appraisal of the change in SBA format was mixed; 49.4&#x000a0;% of responses contained elements of positivity towards the proposed new format, whilst 78.2&#x000a0;% of responses contained negativity. A significant proportion of students (21.8&#x000a0;%) expressed fear that a reduced number of options per question would increase the probability of successful guessing, hence leading to unmerited passes. This attitude towards the three-option format is surprising, given that the previous literature had suggested strong student support (Owen and Froman <xref ref-type="bibr" rid="CR26">1987</xref>), although similar fears have been discussed in association with educators and question writers (Schneid et al. <xref ref-type="bibr" rid="CR32">2014</xref>). If the three-option format were to be implemented, it would be important to adequately address these fears, in order to mitigate stakeholder resistance.</p><sec id="Sec14"><title>Strengths and limitations</title><p>The primary limitation of this study is that it only analysed the performance of a sample of 480 SBA questions from a single UK medical school. Despite the high concordance of this study&#x02019;s results with the findings of other key papers in the literature, generalisability to all medical assessments cannot be assumed. However, as at least a proportion of the exam questions in this sample came from the UK-wide <italic>Medical Schools Council Assessment Alliance</italic> (Medical Schools Council <xref ref-type="bibr" rid="CR24">2015</xref>), and the university&#x02019;s assessments are subject to regulatory validation, the case for generalisability, at least within the UK, can be argued. It would still be valuable however for future research to be carried out to confirm these findings in other schools internationally, before a change in examination format could be reliably implemented. This is particularly pertinent given that our study was based on theoretical simulation, as opposed to experimental investigation.</p><p>The findings of the questionnaire survey are also subject to some limitations, including the low response rate of 33&#x000a0;% and the recruitment of students from a single year group, at a single medical school. Together, these factors mean that it is hard to gauge the generalisability of the survey&#x02019;s findings to other populations.</p><p>Despite these limitations, this is the first comprehensive and generalizable study to investigate distractor performance in SBA examinations in the context of undergraduate medical education. Our findings have important implications for test development, including question writing training, time and resource use, and for examination quality, with our findings potentially leading to an economical improvement over the status quo. The growth in the use of SBAs as part of progress testing and computerised marking, and the corresponding need for medical schools to invest in the infrastructure to support this, indicate that further research is urgently needed. This research should aim to confirm our findings in other assessment data sets, and use more rigorous item response theory models. Finally, it would be invaluable to qualitatively explore the perceptions of test developers, question writers and assessment groups towards a reduced-option SBA model as part of a pragmatic approach to enhancing medical school assessment programmes.</p></sec></sec></body><back><app-group><app id="App1"><sec id="Sec15"><title>Appendix: Survey questionnaire</title><p><inline-graphic xlink:href="10459_2015_9652_Figa_HTML.gif" id="d30e1356"/><inline-graphic xlink:href="10459_2015_9652_Figb_HTML.gif" id="d30e1358"/></p></sec></app></app-group><ack><p>The authors would like to acknowledge and thank the medical students at Cardiff University who participated in this research. They also wish to thank Julie Bligh (Cardiff University) for completing a pre-submission peer-review of this work.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aamodt</surname><given-names>MG</given-names></name><name><surname>McShane</surname><given-names>T</given-names></name></person-group><article-title>A meta-analytic investigation of the effect of various test item characteristics on test scores</article-title><source>Public Personnel Management</source><year>1992</year><volume>21</volume><issue>2</issue><fpage>151</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1177/009102609202100203</pub-id></element-citation></ref><ref id="CR2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Al-Rukban</surname><given-names>MO</given-names></name></person-group><article-title>Guidelines for the construction of multiple choice questions tests</article-title><source>Journal of Family Community Medicine</source><year>2006</year><volume>13</volume><issue>3</issue><fpage>125</fpage><lpage>133</lpage><pub-id pub-id-type="pmid">23012132</pub-id></element-citation></ref><ref id="CR3"><mixed-citation publication-type="other">Assessment Systems Corporation. (2013). <italic>Iteman (version 4.3)</italic>. Woodbury, MN: Assessment Systems Corporation.</mixed-citation></ref><ref id="CR4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baghaei</surname><given-names>P</given-names></name><name><surname>Amrahi</surname><given-names>N</given-names></name></person-group><article-title>The effects of the number of options on the psychometric characteristics of multiple choice items</article-title><source>Psychological Test and Assessment Modelling</source><year>2011</year><volume>53</volume><issue>2</issue><fpage>192</fpage><lpage>211</lpage></element-citation></ref><ref id="CR5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>EH</given-names></name><name><surname>Curry</surname><given-names>LA</given-names></name><name><surname>Devers</surname><given-names>KJ</given-names></name></person-group><article-title>Qualitative data analysis for health services research: Developing taxonomy, themes, and theory</article-title><source>Health Services Research</source><year>2007</year><volume>42</volume><issue>4</issue><fpage>1758</fpage><lpage>1772</lpage><pub-id pub-id-type="doi">10.1111/j.1475-6773.2006.00684.x</pub-id><pub-id pub-id-type="pmid">17286625</pub-id></element-citation></ref><ref id="CR6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruno</surname><given-names>JE</given-names></name><name><surname>Dirkzwager</surname><given-names>A</given-names></name></person-group><article-title>Determining the optimal number of alternatives to a multiple-choice test item: An information theoretic perspective</article-title><source>Educational and Psychological Measurement</source><year>1995</year><volume>55</volume><issue>6</issue><fpage>959</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1177/0013164495055006004</pub-id></element-citation></ref><ref id="CR7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>SJ</given-names></name><name><surname>Sudweeks</surname><given-names>RR</given-names></name><name><surname>Merrill</surname><given-names>PF</given-names></name><name><surname>Wood</surname><given-names>B</given-names></name></person-group><source>How to prepare better multiple choice test items: Guidelines for university faculty</source><year>1991</year><publisher-loc>Utah</publisher-loc><publisher-name>Brigham Young University Testing Services</publisher-name></element-citation></ref><ref id="CR8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cizek</surname><given-names>GJ</given-names></name><name><surname>O&#x02019;Day</surname><given-names>DM</given-names></name></person-group><article-title>Further investigation of nonfunctioning options in multiple-choice test items</article-title><source>Educational and Psychological Measurement</source><year>1994</year><volume>54</volume><issue>4</issue><fpage>861</fpage><lpage>872</lpage><pub-id pub-id-type="doi">10.1177/0013164494054004002</pub-id></element-citation></ref><ref id="CR9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cizek</surname><given-names>GJ</given-names></name><name><surname>Robinson</surname><given-names>KL</given-names></name><name><surname>O&#x02019;Day</surname><given-names>DM</given-names></name></person-group><article-title>Nonfunctioning options: A closer look</article-title><source>Educational and Psychological Measurement</source><year>1998</year><volume>58</volume><issue>4</issue><fpage>605</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1177/0013164498058004004</pub-id></element-citation></ref><ref id="CR10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costin</surname><given-names>F</given-names></name></person-group><article-title>Three-choice versus four-choice items: Implications for reliability and validity of objective achievement tests</article-title><source>Educational and Psychological Measurement</source><year>1972</year><volume>32</volume><issue>4</issue><fpage>1035</fpage><lpage>1038</lpage></element-citation></ref><ref id="CR11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crehan</surname><given-names>KD</given-names></name><name><surname>Haladyna</surname><given-names>TM</given-names></name><name><surname>Brewer</surname><given-names>BW</given-names></name></person-group><article-title>Use of an inclusive option and the optimal number of options for multiple-choice items</article-title><source>Educational and Psychological Measurement</source><year>1993</year><volume>53</volume><issue>1</issue><fpage>241</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1177/0013164493053001027</pub-id></element-citation></ref><ref id="CR12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delgado</surname><given-names>A</given-names></name><name><surname>Prieto</surname><given-names>G</given-names></name></person-group><article-title>Further evidence favoring three-option items in multiple-choice tests</article-title><source>European Journal of Psychological Assessment</source><year>1998</year><volume>14</volume><issue>3</issue><fpage>197</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1027/1015-5759.14.3.197</pub-id></element-citation></ref><ref id="CR13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>RM</given-names></name></person-group><article-title>Assessment in medical education</article-title><source>New England Journal of Medicine</source><year>2007</year><volume>356</volume><issue>4</issue><fpage>387</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1056/NEJMra054784</pub-id><pub-id pub-id-type="pmid">17251535</pub-id></element-citation></ref><ref id="CR14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grier</surname><given-names>JB</given-names></name></person-group><article-title>The number of alternatives for optimum test reliability</article-title><source>Journal of Educational Measurement</source><year>1975</year><volume>12</volume><issue>2</issue><fpage>109</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1111/j.1745-3984.1975.tb01013.x</pub-id></element-citation></ref><ref id="CR15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haladyna</surname><given-names>TM</given-names></name><name><surname>Downing</surname><given-names>S</given-names></name></person-group><source>Functional distractors: Implications for test-item writing and test design</source><year>1988</year><publisher-loc>New Orleans</publisher-loc><publisher-name>American Educational Research Association</publisher-name></element-citation></ref><ref id="CR16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haladyna</surname><given-names>T</given-names></name><name><surname>Downing</surname><given-names>S</given-names></name></person-group><article-title>Validity of a taxonomy of multiple-choice item-writing rules</article-title><source>Applied Measurement in Education</source><year>1989</year><volume>2</volume><issue>1</issue><fpage>51</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1207/s15324818ame0201_4</pub-id></element-citation></ref><ref id="CR17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haladyna</surname><given-names>TM</given-names></name><name><surname>Downing</surname><given-names>S</given-names></name></person-group><article-title>How many options is enough for a multiple-choice test item?</article-title><source>Educational and Psychological Measurement</source><year>1993</year><volume>53</volume><issue>4</issue><fpage>999</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1177/0013164493053004013</pub-id></element-citation></ref><ref id="CR18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haladyna</surname><given-names>TM</given-names></name><name><surname>Downing</surname><given-names>SM</given-names></name><name><surname>Rodriguez</surname><given-names>MC</given-names></name></person-group><article-title>A review of multiple-choice item-writing guidelines for classroom assessment</article-title><source>Applied Measurement in Education</source><year>2002</year><volume>15</volume><issue>3</issue><fpage>309</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1207/S15324818AME1503_5</pub-id></element-citation></ref><ref id="CR19"><mixed-citation publication-type="other">IBM Corporation. (2014). <italic>SPSS Statistics (version 22)</italic>. New York: IBM Corporation.</mixed-citation></ref><ref id="CR20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolstad</surname><given-names>R</given-names></name><name><surname>Briggs</surname><given-names>L</given-names></name><name><surname>Kolstad</surname><given-names>R</given-names></name></person-group><article-title>Multiple-choice classroom achievement tests: Performance on items with five vs three choices</article-title><source>College Student Journal</source><year>1985</year><volume>19</volume><issue>4</issue><fpage>427</fpage><lpage>431</lpage></element-citation></ref><ref id="CR21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landrum</surname><given-names>RE</given-names></name><name><surname>Cashin</surname><given-names>JR</given-names></name><name><surname>Theis</surname><given-names>KS</given-names></name></person-group><article-title>More evidence in favor of three-option multiple-choice tests</article-title><source>Educational and Psychological Measurement</source><year>1993</year><volume>53</volume><issue>3</issue><fpage>771</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1177/0013164493053003021</pub-id></element-citation></ref><ref id="CR22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lord</surname><given-names>FM</given-names></name></person-group><article-title>OPTIMAL number of choices per item&#x02014;A comparison of four approaches*</article-title><source>Journal of Educational Measurement</source><year>1977</year><volume>14</volume><issue>1</issue><fpage>33</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1111/j.1745-3984.1977.tb00026.x</pub-id></element-citation></ref><ref id="CR23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>D</given-names></name></person-group><article-title>Set a multiple choice question (MCQ) examination</article-title><source>BMJ</source><year>1991</year><volume>302</volume><issue>6779</issue><fpage>780</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1136/bmj.302.6779.780</pub-id><pub-id pub-id-type="pmid">2021771</pub-id></element-citation></ref><ref id="CR24"><mixed-citation publication-type="other">Medical Schools Council (2015). <italic>Medical Schools Council Assessment Alliance</italic>. Retrieved July 22nd, 2015, from <ext-link ext-link-type="uri" xlink:href="http://www.medschools.ac.uk/MSCAA/Pages/default.aspx">http://www.medschools.ac.uk/MSCAA/Pages/default.aspx</ext-link>.</mixed-citation></ref><ref id="CR25"><mixed-citation publication-type="other">Microsoft Corporation. (2011). <italic>Microsoft Excel for Mac 2011 (version 14.4.7)</italic>. Redmond, WA: Microsoft Corporation.</mixed-citation></ref><ref id="CR26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owen</surname><given-names>SV</given-names></name><name><surname>Froman</surname><given-names>RD</given-names></name></person-group><article-title>what&#x02019;s wrong with three-option multiple choice items?</article-title><source>Educational and Psychological Measurement</source><year>1987</year><volume>47</volume><issue>2</issue><fpage>513</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1177/0013164487472027</pub-id></element-citation></ref><ref id="CR27"><mixed-citation publication-type="other">Measured Progress. (2014). <italic>Discovering the point biserial</italic>. Retrieved November 15, 2014, from <ext-link ext-link-type="uri" xlink:href="http://www.measuredprogress.org/learning-tools-statistical-analysis-the-point-biserial">http://www.measuredprogress.org/learning-tools-statistical-analysis-the-point-biserial</ext-link>.</mixed-citation></ref><ref id="CR28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname><given-names>MC</given-names></name></person-group><article-title>Three options are optimal for multiple-choice items: A meta-analysis of 80&#x000a0;years of research</article-title><source>Educational Measurement: Issues and Practice</source><year>2005</year><volume>24</volume><issue>2</issue><fpage>3</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1111/j.1745-3992.2005.00006.x</pub-id></element-citation></ref><ref id="CR29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogausch</surname><given-names>A</given-names></name><name><surname>Hofer</surname><given-names>R</given-names></name><name><surname>Krebs</surname><given-names>R</given-names></name></person-group><article-title>Rarely selected distractors in high stakes medical multiple-choice examinations and their recognition by item authors: A simulation and survey</article-title><source>BMC Medical Education</source><year>2010</year><volume>10</volume><fpage>85</fpage><pub-id pub-id-type="doi">10.1186/1472-6920-10-85</pub-id><pub-id pub-id-type="pmid">21106066</pub-id></element-citation></ref><ref id="CR30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>WT</given-names></name><name><surname>Harley</surname><given-names>D</given-names></name></person-group><article-title>An empirical comparison of three-and four-choice items and tests: Susceptibility to testwiseness and internal consistency reliability</article-title><source>Educational and Psychological Measurement</source><year>1999</year><volume>59</volume><issue>2</issue><fpage>234</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1177/00131649921969820</pub-id></element-citation></ref><ref id="CR31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruch</surname><given-names>GM</given-names></name><name><surname>Charles</surname><given-names>JW</given-names></name></person-group><article-title>A comparison of five types of objective tests in elementary psychology</article-title><source>Journal of Applied Psychology</source><year>1928</year><volume>12</volume><issue>4</issue><fpage>398</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1037/h0075108</pub-id></element-citation></ref><ref id="CR32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneid</surname><given-names>SD</given-names></name><name><surname>Armour</surname><given-names>C</given-names></name><name><surname>Park</surname><given-names>YS</given-names></name><name><surname>Yudkowsky</surname><given-names>R</given-names></name><name><surname>Bordage</surname><given-names>G</given-names></name></person-group><article-title>Reducing the number of options on multiple-choice questions: Response time, psychometrics and standard setting</article-title><source>Medical Education</source><year>2014</year><volume>48</volume><issue>10</issue><fpage>1020</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1111/medu.12525</pub-id><pub-id pub-id-type="pmid">25200022</pub-id></element-citation></ref><ref id="CR33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuwirth</surname><given-names>LW</given-names></name><name><surname>van der Vleuten</surname><given-names>CP</given-names></name></person-group><article-title>General overview of the theories used in assessment: AMEE Guide No. 57</article-title><source>Medical Teacher</source><year>2011</year><volume>33</volume><issue>10</issue><fpage>783</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.3109/0142159X.2011.611022</pub-id><pub-id pub-id-type="pmid">21942477</pub-id></element-citation></ref><ref id="CR34"><mixed-citation publication-type="other">Scientific Software Development GmbH. (2012). <italic>ATLAS.ti (version 7.0)</italic>. Berlin: Scientific Software Development GmbH.</mixed-citation></ref><ref id="CR35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shizuka</surname><given-names>T</given-names></name><name><surname>Takeuchi</surname><given-names>O</given-names></name><name><surname>Yashima</surname><given-names>T</given-names></name><name><surname>Yoshizawa</surname><given-names>K</given-names></name></person-group><article-title>A comparison of three- and four-option English tests for university entrance selection purposes in Japan</article-title><source>Language Testing</source><year>2006</year><volume>23</volume><issue>1</issue><fpage>35</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1191/0265532206lt319oa</pub-id></element-citation></ref><ref id="CR36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shumway</surname><given-names>JM</given-names></name><name><surname>Harden</surname><given-names>RM</given-names></name></person-group><article-title>AMEE Guide No. 25: The assessment of learning outcomes for the competent and reflective physician</article-title><source>Medical Teacher</source><year>2003</year><volume>25</volume><issue>6</issue><fpage>569</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1080/0142159032000151907</pub-id><pub-id pub-id-type="pmid">15369904</pub-id></element-citation></ref><ref id="CR37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sidick</surname><given-names>JT</given-names></name><name><surname>Barrett</surname><given-names>GV</given-names></name><name><surname>Doverspike</surname><given-names>D</given-names></name></person-group><article-title>THREE-alternative multiple choice tests: An attractive option</article-title><source>Personnel Psychology</source><year>1994</year><volume>47</volume><issue>4</issue><fpage>829</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1111/j.1744-6570.1994.tb01579.x</pub-id></element-citation></ref><ref id="CR38"><mixed-citation publication-type="other">SurveyMonkey Inc. (2014). <italic>SurveyMonkey</italic>. Palo Alto, CA: SurveyMonkey Inc.</mixed-citation></ref><ref id="CR39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swanson</surname><given-names>DB</given-names></name><name><surname>Holtzman</surname><given-names>KZ</given-names></name><name><surname>Allbee</surname><given-names>K</given-names></name></person-group><article-title>Measurement characteristics of content-parallel single-best-answer and extended-matching questions in relation to number and source of options</article-title><source>Academic Medicine</source><year>2008</year><volume>83</volume><issue>10 Suppl</issue><fpage>S21</fpage><lpage>S24</lpage><pub-id pub-id-type="doi">10.1097/ACM.0b013e318183e5bb</pub-id><pub-id pub-id-type="pmid">18820493</pub-id></element-citation></ref><ref id="CR40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarrant</surname><given-names>M</given-names></name><name><surname>Ware</surname><given-names>J</given-names></name><name><surname>Mohammed</surname><given-names>AM</given-names></name></person-group><article-title>An assessment of functioning and non-functioning distractors in multiple-choice questions: A descriptive analysis</article-title><source>BMC Medical Education</source><year>2009</year><volume>9</volume><fpage>40</fpage><pub-id pub-id-type="doi">10.1186/1472-6920-9-40</pub-id><pub-id pub-id-type="pmid">19580681</pub-id></element-citation></ref><ref id="CR41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavakol</surname><given-names>M</given-names></name><name><surname>Dennick</surname><given-names>R</given-names></name></person-group><article-title>Making sense of Cronbach&#x02019;s alpha</article-title><source>International Journal of Medical Education</source><year>2011</year><volume>2</volume><fpage>53</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.5116/ijme.4dfb.8dfd</pub-id></element-citation></ref><ref id="CR42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trevisan</surname><given-names>MS</given-names></name><name><surname>Sax</surname><given-names>G</given-names></name><name><surname>Michael</surname><given-names>WB</given-names></name></person-group><article-title>The effects of the number of options per item and student ability on test validity and reliability</article-title><source>Educational and Psychological Measurement</source><year>1991</year><volume>51</volume><issue>4</issue><fpage>829</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1177/001316449105100404</pub-id></element-citation></ref><ref id="CR43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trevisan</surname><given-names>MS</given-names></name><name><surname>Sax</surname><given-names>G</given-names></name><name><surname>Michael</surname><given-names>WB</given-names></name></person-group><article-title>Estimating the optimum number of options per item using an incremental option paradigm</article-title><source>Educational and Psychological Measurement</source><year>1994</year><volume>54</volume><issue>1</issue><fpage>86</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1177/0013164494054001008</pub-id></element-citation></ref><ref id="CR44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trewin</surname><given-names>S</given-names></name></person-group><article-title>History of psychology: Robert Yerkes&#x02019; multiple-choice apparatus, 1913&#x02013;1939</article-title><source>The American Journal of Psychology</source><year>2007</year><volume>120</volume><fpage>645</fpage><lpage>660</lpage><pub-id pub-id-type="pmid">18277520</pub-id></element-citation></ref><ref id="CR45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><article-title>On the optimal number of alternatives at a choice point</article-title><source>Journal of Mathematical Psychology</source><year>1964</year><volume>1</volume><issue>2</issue><fpage>386</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1016/0022-2496(64)90010-0</pub-id></element-citation></ref><ref id="CR46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Vleuten</surname><given-names>C</given-names></name></person-group><article-title>Validity of final examinations in undergraduate medical training</article-title><source>BMJ</source><year>2000</year><volume>321</volume><issue>7270</issue><fpage>1217</fpage><lpage>1219</lpage><pub-id pub-id-type="doi">10.1136/bmj.321.7270.1217</pub-id><pub-id pub-id-type="pmid">11073517</pub-id></element-citation></ref><ref id="CR47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Vleuten</surname><given-names>CP</given-names></name><name><surname>Schuwirth</surname><given-names>LW</given-names></name></person-group><article-title>Assessing professional competence: From methods to programmes</article-title><source>Medical Education</source><year>2005</year><volume>39</volume><issue>3</issue><fpage>309</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2929.2005.02094.x</pub-id><pub-id pub-id-type="pmid">15733167</pub-id></element-citation></ref><ref id="CR48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyas</surname><given-names>R</given-names></name><name><surname>Supe</surname><given-names>A</given-names></name></person-group><article-title>Multiple choice questions: A literature review on the optimal number of options</article-title><source>National Medical Journal of India</source><year>2008</year><volume>21</volume><issue>3</issue><fpage>130</fpage><lpage>133</lpage><pub-id pub-id-type="pmid">19004145</pub-id></element-citation></ref><ref id="CR49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wakefield</surname><given-names>J</given-names></name></person-group><article-title>Does the fifth choice strengthen a test item?</article-title><source>Public Personnel Review</source><year>1958</year><volume>19</volume><fpage>44</fpage><lpage>48</lpage></element-citation></ref><ref id="CR50"><mixed-citation publication-type="other">Wells, C., &#x00026; Wollack, J. (2003). <italic>An instructor&#x02019;s guide to understanding test reliability</italic>. Retrieved November 7, 2014, from <ext-link ext-link-type="uri" xlink:href="http://testing.wisc.edu/Reliability.pdf">http://testing.wisc.edu/Reliability.pdf</ext-link>.</mixed-citation></ref></ref-list></back></article>