
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23441150</article-id><article-id pub-id-type="pmc">3575437</article-id><article-id pub-id-type="publisher-id">PONE-D-12-10059</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0055215</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Anatomy and Physiology</subject><subj-group><subject>Neurological System</subject></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Central Nervous System</subject></subj-group></subj-group><subj-group><subject>Sensory Systems</subject><subj-group><subject>Auditory System</subject></subj-group></subj-group><subj-group><subject>Behavioral Neuroscience</subject><subject>Molecular Neuroscience</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and Behavioral Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Attention (Behavior)</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Task-Related Suppression of the Brainstem Frequency following Response</article-title><alt-title alt-title-type="running-head">Task-Related Suppression of the Brainstem FFR</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hairston</surname><given-names>W. David</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Letowski</surname><given-names>Tomasz R.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author"><name><surname>McDowell</surname><given-names>Kaleb</given-names></name><xref ref-type="aff" rid="aff1"/></contrib></contrib-group><aff id="aff1">
<addr-line>Human Research and Engineering Directorate, United States Army Research Laboratory, Aberdeen Proving Ground, Maryland, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Nerurkar</surname><given-names>Pratibha V.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>College of Tropical Agriculture and Human Resources, University of Hawaii, United States of America</addr-line>
</aff><author-notes><corresp id="cor1">* E-mail: <email>William.d.hairston4.civ@mail.mil</email></corresp><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p>Conceived and designed the experiments: WDH KM TRL. Performed the experiments: WDH KM. Analyzed the data: WDH KM. Contributed reagents/materials/analysis tools: KM. Wrote the paper: WDH KM.</p></fn></author-notes><pub-date pub-type="collection"><year>2013</year></pub-date><pub-date pub-type="epub"><day>18</day><month>2</month><year>2013</year></pub-date><volume>8</volume><issue>2</issue><elocation-id>e55215</elocation-id><history><date date-type="received"><day>7</day><month>4</month><year>2012</year></date><date date-type="accepted"><day>20</day><month>12</month><year>2012</year></date></history><permissions><copyright-year>2013</copyright-year><license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Public Domain declaration, which stipulates that, once placed in the public domain, this work may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose.</license-p></license></permissions><abstract><p>Recent evidence has shown top-down modulation of the brainstem frequency following response (FFR), generally in the form of signal enhancement from concurrent stimuli or from switching between attention-demanding task stimuli. However, it is also possible that the opposite may be true &#x02013; the addition of a task, instead of a resting, passive state may suppress the FFR. Here we examined the influence of a subsequent task, and the relevance of the task modality, on signal clarity within the FFR. Participants performed visual and auditory discrimination tasks in the presence of an irrelevant background sound, as well as a baseline consisting of the same background stimuli in the absence of a task. FFR pitch strength and amplitude of the primary frequency response were assessed within non-task stimulus periods in order to examine influences due solely to general cognitive state, independent of stimulus-driven effects. Results show decreased signal clarity with the addition of a task, especially within the auditory modality. We additionally found consistent relationships between the extent of this suppressive effect and perceptual measures such as response time and proclivity towards one sensory modality. Together these results suggest that the current focus of attention can have a global, top-down effect on the quality of encoding early in the auditory pathway.</p></abstract><funding-group><funding-statement>No current external funding sources for this study.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>The auditory brainstem response (ABR) represents some of the earliest encoding of acoustic information within the auditory system, arising from the pooled synchronized response of brainstem neurons. The frequency following response (FFR) component of the ABR represents a phasic, sustained response generally assumed to arise from phase-locking cells within the rostral brainstem projecting into the auditory midbrain, characterized by a frequency profile matching that of the incoming sound <xref rid="pone.0055215-Skoe1" ref-type="bibr">[1]</xref>&#x02013;<xref rid="pone.0055215-Galbraith1" ref-type="bibr">[7]</xref>. As a result, the FFR can be used as a metric for encoding quality of sound as represented early in the auditory system. Because the ABR can be recorded passively using scalp electrodes, it has been advocated as an excellent means for assessing low-level auditory function in a non-invasive manner in either a clinical or laboratory setting (see <xref rid="pone.0055215-Skoe1" ref-type="bibr">[1]</xref> for a review).</p><p>Despite its low-level origins, there is evidence that the FFR is malleable based on higher-level cognitive influence. For instance, simultaneous presentation of matching visual information with the ABR eliciting acoustic stimuli leads to an enhanced response <xref rid="pone.0055215-Musacchia1" ref-type="bibr">[8]</xref>&#x02013;<xref rid="pone.0055215-Musacchia3" ref-type="bibr">[10]</xref>, with correlated responses at the cortical level <xref rid="pone.0055215-Musacchia3" ref-type="bibr">[10]</xref>. Meanwhile, within a single recording session, adaptation in response amplitude can occur based on the local acoustic statistics of the stimuli used <xref rid="pone.0055215-Skoe2" ref-type="bibr">[11]</xref>, similar to other examples of experience-dependent modulation <xref rid="pone.0055215-Krishnan2" ref-type="bibr">[12]</xref>&#x02013;<xref rid="pone.0055215-DeBoer1" ref-type="bibr">[16]</xref> but on a more rapid scale, further supporting malleability from top-down factors.</p><p>These types of effects are most likely linked to the direction of attention, which has been extensively shown to modulate cortical responses <xref rid="pone.0055215-Posner1" ref-type="bibr">[17]</xref>. For instance selective attention to the auditory domain leads to increased cortical response amplitude and decreased response latency, occurring even in primary cortices <xref rid="pone.0055215-Hillyard1" ref-type="bibr">[18]</xref>&#x02013;<xref rid="pone.0055215-Fujiwara1" ref-type="bibr">[20]</xref>, functionally increasing the gain of the attended signal. Alternatively, recent evidence has suggested enhanced selectivity within auditory cortex by inhibiting responsiveness to non-relevant stimuli or specific features <xref rid="pone.0055215-Ahveninen1" ref-type="bibr">[21]</xref>&#x02013;<xref rid="pone.0055215-Alain1" ref-type="bibr">[24]</xref>. Similar attention-related filtering mechanisms have been observed when looking across modalities as well <xref rid="pone.0055215-Gondan1" ref-type="bibr">[25]</xref>&#x02013;<xref rid="pone.0055215-Hairston1" ref-type="bibr">[27]</xref>.</p><p>Although most attention-related research focuses on cortical-level interactions, there is evidence of attention-related mediation of the FFR in both amplitude and latency <xref rid="pone.0055215-Galbraith2" ref-type="bibr">[28]</xref>&#x02013;<xref rid="pone.0055215-Hoormann1" ref-type="bibr">[30]</xref>. For example, attending to vowels leads to an amplitude increase of the fundamental frequency <xref rid="pone.0055215-Galbraith3" ref-type="bibr">[29]</xref>. In one particular case <xref rid="pone.0055215-Galbraith2" ref-type="bibr">[28]</xref> it has been argued that using a task to focus attention within the auditory modality provides a stronger response than when the task uses an opposing sense (e.g., vision).</p><p>To date, however, it remains unclear whether this level of subcortical auditory encoding can also be <italic>suppressed</italic> based solely on the current focus of attention. That is, top-down interactions may also be able to operate in an inhibitory manner when the scenario necessitates it, as a mechanism for filtering non-relevant information. Several examples of this have been shown cortically, especially when looking across modalities, such as where presentation of a stimulus in one sensory modality (e.g. visual or somatosensory) can lead to a decrease in regional measures of neuronal activity in other cortices (e.g. auditory or visual, respectively) <xref rid="pone.0055215-Hairston1" ref-type="bibr">[27]</xref>, <xref rid="pone.0055215-Shulman1" ref-type="bibr">[31]</xref>&#x02013;<xref rid="pone.0055215-Bense1" ref-type="bibr">[34]</xref>. This paucity is due in part to the fact that most studies of attention effects on early auditory encoding use the ABR-eliciting stimulus as part of the attention-mediating task, making it difficult to separate attention to the response eliciting stimuli from the task itself. Additionally, a non-task &#x0201c;resting&#x0201d; baseline is needed for assessing potential <italic>decreases</italic> from the additional task load.</p><p>In this study, we sought to investigate this possibility through the use of a paradigm more common for studies of cortical processing of non-task relevant information <xref rid="pone.0055215-Papanicolaou1" ref-type="bibr">[35]</xref>. Specifically, participants perform a task in either the visual or auditory modality, where the stimuli used to assess FFR acuity remain in the background, completely irrelevant to the task at hand. Additionally, we include a non-task &#x0201c;baseline&#x0201d; in order to assess potential changes in FFR clarity related to the addition of the task paradigm. We then examine within- versus cross-modality effects on the suppression of low-level auditory encoding, and follow this with an analysis of some behavioral metrics of sensory bias which help predict the extent to which the FFR signal mediation is observed across subjects.</p></sec><sec sec-type="materials|methods" id="s2"><title>Materials and Methods</title><sec id="s2a"><title>Participants</title><p>Participants were twenty-three adults (6 females) between 22&#x02013;45 years of age, with normal or corrected-to-normal vision and normal hearing as determined by 250 to 8,000 Hz pure tone air conduction audiometry (25dB or less hearing level at each frequency). All were native English speakers, and none reported as being &#x0201c;fluently bilingual&#x0201d;. While three participants reported having &#x0003e;3 years of musical experience, none performed professionally or had formal training beyond high school. All methods were approved by the Institutional Review Board and conformed to the 1964 Declaration of Helsinki. The voluntary, fully informed consent of the persons used in this research was obtained as required by U.S. Army human use regulations (U.S. Department of Defense, 1999; U.S. Department of the Army, 1990). Data from 5 participants were removed from the group analysis due to technical failure (1), excessive motion (1), or poor SNR (3).</p></sec><sec id="s2b"><title>Experimental Paradigm</title><sec id="s2b1"><title>Primary Tasks</title><p>Three different experimental conditions were used; an auditory task, a visual task, and no-task &#x0201c;baseline&#x0201d; condition, each tested in a separate block lasting approximately 7 minutes. At the beginning of each run, the participant was given instructions denoting which task type would be performed. Each paradigm type (visual, auditory, no-task) was performed twice in an order randomized across subjects, with the only caveat that no task type was repeated before all three conditions had been completed at least once.</p><p>Subjects were given sensory tasks with the goal of ensuring that they must focus on one particular sensory modality, while also providing behavioral measures of performance. Both sensory tasks were a temporal discrimination presented in a two-interval, forced choice format, designed to be analogous for both sensory modalities. In the <italic>visual task</italic>, subjects were presented with a light grey square centered on the screen, which appeared and disappeared twice. One presentation always lasted 250 ms, and the other was presented within a range of 255&#x02013;400 ms duration, with order randomized across trials. In the <italic>auditory task</italic>, the target stimulus was a pure tone (587 Hz), also presented once for 250 ms and once for 255&#x02013;400 ms. In both cases, participants were given 2 seconds to answer which of the presentations was shorter (first or second). In the <italic>no-task</italic> condition, subjects simply fixated on a constant, central cross, identical to one used for the two active tasks.</p><p>To ensure that subjects constantly paid attention across the entire block of trials, the ISI between trials varied randomly within a range of 3000&#x02013;12000 ms (mean 6750 ms) so they could not predict the onset of the next stimulus. The length of the longer stimulus was set to a range (+/&#x02212;16.7 ms) centered around each participant's own individually-derived 75% discrimination threshold for the task. Discrimination threshold values were acquired prior to testing using an adaptive staircase procedure <xref rid="pone.0055215-Levitt1" ref-type="bibr">[36]</xref> adapted for the E-Prime programming environment <xref rid="pone.0055215-Hairston2" ref-type="bibr">[37]</xref>. Using the adaptive staircase and individual-specific concomitant values both provided an assessment of sensory temporal acuity for each subject and modality, and ensured approximately equal difficulty across subjects and conditions. Background tones (see below) were also included during the staircase procedure so as to match the primary testing environment.</p></sec><sec id="s2b2"><title>Background Tones</title><p>During all conditions, there was an additional background, ongoing &#x0201c;probe&#x0201d; stimulus (1,200 repetitions) used for eliciting the frequency following response (FFR) component of the auditory brainstem response (ABR). The probe was a 100 ms, 220 Hz pure tone (5 ms linear amplitude ramps at on/offset) presented every 455 ms. Occasionally (3% of presentations) the tone changed in pitch (247 Hz); response to this oddball is not discussed here. Participants were told not to worry about the background tones, and focus only on the primary task. By using this interleaved paradigm of both primary and background tasks, we are able to infer responses to both task relevant and task non-relevant stimuli. <xref ref-type="fig" rid="pone-0055215-g001">Figure 1</xref> shows an example of the stimulus paradigm. Background tones occurred with a constant ISI in order to minimize distraction from the primary task.</p><fig id="pone-0055215-g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0055215.g001</object-id><label>Figure 1</label><caption><title>The experimental paradigm.</title><p>Non-task relevant ABR-eliciting stimuli per presented every 455 ms, with task target stimuli occurring asynchronously, randomly presented (6750 ms mean ISI). Only background tones falling between task stimuli (shaded area) were used in the analysis.</p></caption><graphic xlink:href="pone.0055215.g001"/></fig><p>All sounds were presented via Etymotic Research ER-3A insert tube earphones, presented at 83dB (peak) at the ear. Visual stimuli were presented on a Dell 24&#x0201d; 2408WFP monitor, with central fixation approximately 50 cm from the participant's nose. Data were collected in an acoustically isolated chamber with reflective attenuation. During all 6 testing runs, whole-scalp EEG data were collected using a 64 channel Biosemi active electrode biopotential measurement system, sampled at 8.192 kHz, filtered at 0.1&#x02013;4 kHz with external references set to bilateral earlobes.</p></sec></sec><sec id="s2c"><title>Analysis</title><sec id="s2c1"><title>Brainstem FFR</title><p>Because the brainstem FFR is strongest at scalp location CZ when using bilateral stimuli and linked earlobe reference <xref rid="pone.0055215-Skoe1" ref-type="bibr">[1]</xref>, that served as the primary focus for this initial study. To derive the ABR FFR, EEG data were filtered between 80 and 2,000 Hz using a Butterworth filter, and epoched based on the consistent background tone for the period &#x02212;200 to 225 ms relative to stimulus onset. Artifacts were rejected based on a +/&#x02212;35 &#x000b5;V voltage threshold (typically &#x0223c;3%), and data from similar blocks (e.g. 2 auditory task runs) were combined. Any cases overlapping the primary task stimuli (&#x0223c;250) were also removed, and responses to the oddball background tone (247 Hz) were ignored, ensuring all waveforms represented only the auditory response to the consistent non-relevant probe. For <italic>no-task</italic> data, we matched the number and temporal placement of ABR-eliciting events by removing the same cases which overlapped with task stimuli in the other two conditions. Waveforms (typically about 1800 trials remaining) were averaged to yield a phasic FFR for each subject and task type (<italic>visual, auditory, no-task</italic>).</p><p>Waveform averages were imported into the Brainstem Toolbox from Northwestern University <xref rid="pone.0055215-Skoe1" ref-type="bibr">[1]</xref> and used to compute two primary metrics. First, <italic>pitch strength</italic> was calculated as the mean of results from a running autocorrelelogram (40 ms window, 1 ms steps) across the time-locked average EEG waveform during the central 75 ms of response <xref rid="pone.0055215-Krishnan3" ref-type="bibr">[38]</xref>, <xref rid="pone.0055215-Skoe3" ref-type="bibr">[39]</xref>. Thus, it summarizes how consistent the periodicity of the neural response is (but without specificity to a certain frequency), in this case reflecting the highly phasic characteristics of the external acoustic waveform. Note that this is a characterization only of the encoding characteristics, and it distinct from higher cortical processes such as pitch perception which might depend on changes in temporal envelope or more complex features. Second, <italic>frequency response amplitude</italic> was determined by the amplitude of the average response (normalized to nV) of the Fourier-transformed signal within the target frequency range (210&#x02013;230 Hz), thus reflecting the overall power contained within the target frequency of the phasic response but irrespective of minor temporal fluctuations in that period. Response amplitudes were normalized to the pre-stimulus period to account for small but non-significant differences in baseline. Data from three subjects for whom the SNR (calculated as the level of measured response divided by the pre-stimulus period amplitude) during the <italic>no-task</italic> condition was below 1.5 were removed prior to any analyses.</p><p>A slight harmonic distortion was created by the sound presentation equipment (ER3A transducer and tube inserts), as measured by a KEMAR mannequin (GRAS Sound &#x00026; Vibration) inside the ear when compared to direct recording from the PC sound card. While the FFR response to this harmonic effect is evident in group spectrograms, it is outside of the frequency realm focused on for analyses here.</p></sec><sec id="s2c2"><title>Behavioral Responses</title><p>Two primary behavioral metrics were measured. First, for each sensory modality (vision and audition), we used the resulting mean SOA between primary task stimuli from the staircase threshold procedure as an assessment of temporal processing acuity for that sense. Note that in this case, lower values denote a higher acuity &#x02013; less time necessary to perform the task at equivalent accuracy. From these, a &#x0201c;sensory bias&#x0201d; score was calculated as the relative difference (in ms) when the auditory threshold was subtracted from the visual threshold. This yields positive values indicating higher acuity in audition than vision, while negative scores denote subjects with an acuity bias towards the visual domain.</p><p>Second, reaction time (RT) was calculated as the median time, in ms, to press the button relative to the onset of the second of the two primary task stimuli. RTs from the auditory task were subtracted from visual, so that positive values denote faster auditory responses, while negative values (in ms) represent faster responses with a visual, rather than auditory, task. This RT difference (in ms) represents the performance gain by performing the primary task in an opposite modality (visual task, auditory background) versus the same modality (auditory task, auditory background) as the non-relevant stimuli. Additionally, it normalizes RTs across subjects to account for the large variability in general RT tendencies and places all subjects within a common scale.</p></sec></sec></sec><sec id="s3"><title>Results</title><sec id="s3a"><title>Brainstem Response</title><p>For the group of subjects, pitch strength, a measure of the consistency of frequency encoding, was significantly decreased for both <italic>visual</italic> and <italic>auditory</italic> tasks (<xref ref-type="fig" rid="pone-0055215-g002">Figure 2</xref>, <bold>A</bold>) relative to a resting (no task) baseline [<italic>visual,</italic> t(16)&#x0200a;=&#x0200a;2.12, p&#x0003c;.05, <italic>auditory,</italic> t(16)&#x0200a;=&#x0200a;2.10, p&#x0003c;.05)]. In other words, with the addition of the task, brainstem encoding of background, non-relevant signals was less consistent (relative to the actual acoustic signal) than when there was no task. Meanwhile, average signal amplitude within the target frequency (220 Hz) decreased for auditory, but not visual tasks [<xref ref-type="fig" rid="pone-0055215-g002">Figure 2</xref>, <bold>B</bold>, <italic>visual,</italic> t(16)&#x0200a;=&#x0200a;0.14, p&#x0003e;.05, <italic>auditory,</italic> t(16)&#x0200a;=&#x0200a;2.31, p&#x0003c;.05], suggesting a suppression of background auditory signals that is more specific to also performing an auditory (as opposed to visual) task within this domain.</p><fig id="pone-0055215-g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0055215.g002</object-id><label>Figure 2</label><caption><title>FFR metrics for each task type.</title><p>Pitch strength significantly decreased for both <italic>visual</italic> and <italic>auditory</italic> tasks (<bold>A</bold>) relative to baseline (no task). In addition, average signal amplitude within the target frequency (220 Hz) decreased for auditory, but not visual tasks (<bold>B</bold>). Error bars represent standard error across subjects Asterisks on right denote significant (p&#x0003c;.05) differences. Note that only background stimuli not overlapping with the task and identical in all conditions were used in this analysis, suggesting a tonic suppression of background auditory signals that may be greater when also performing an auditory task.</p></caption><graphic xlink:href="pone.0055215.g002"/></fig><p>Importantly, this suppressive effect appears within only the primary frequency range of the background FFR-eliciting tone. <xref ref-type="fig" rid="pone-0055215-g003">Figure 3</xref> shows Fourier time-frequency plots (using a 40 ms sliding window with 1 ms steps across the range from 100&#x02013;900 Hz in 2 Hz increments) for each of the primary conditions. Note there is a substantial response within the 220 Hz range, with transient broadband responses around the beginning and end of each response, but differences across conditions are limited only to the primary frequency tested here.</p><fig id="pone-0055215-g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0055215.g003</object-id><label>Figure 3</label><caption><title>Fourier time-frequency plots for each of the primary conditions.</title><p>The strongest response occurs primarily within the 220 Hz frequency range, although a short broadboand response is seen at onset and offset in the response spectrogram (<bold>A</bold>). Differences across conditions were limited to only the primary frequency. Dashed lines represent time and frequency periods used for ANOVA; 100&#x02013;200 Hz (yellow), 210&#x02013;230 Hz (target range, orange), 240&#x02013;400 Hz (yellow), and 553&#x02013;613 Hz (green), pre- and post-stimulus onset. (<bold>B</bold>) shows group average autocorrelelograms reflecting the highly consistent response with this stimulus.</p></caption><graphic xlink:href="pone.0055215.g003"/></fig><p>Statistically, we tested the specificity of this effect by computing the Fourier power within several frequency ranges for a 50 ms window, both pre- and post stimulus and illustrated by dashed lines in <xref ref-type="fig" rid="pone-0055215-g003">Figure 3</xref>. These included 100&#x02013;200 Hz (yellow), 210&#x02013;230 Hz (target range, orange), 240&#x02013;400 Hz (yellow), and 553&#x02013;613 Hz (green). This latter range centers around the frequency of the auditory task stimuli (587 Hz). Strength of the response was calculated as the difference between the pre- and post-stimuli 50 ms windows. Despite slightly different pre-stimulus baselines, in all three tasks ANOVAs show a significant response across the entire frequency spectrum, including below the target range (100&#x02013;200 Hz, F(16,1)&#x0200a;=&#x0200a;27.8, p&#x0003c;.001), above (240&#x02013;400 Hz, F(16,1)&#x0200a;=&#x0200a;17.6, p&#x0003c;.001), and the specific frequency of the auditory task stimuli (583 Hz, F(16,1)&#x0200a;=&#x0200a;18.9, p&#x0003c;.001).</p><p>However, only the target frequency (220 Hz) showed a significant interaction (F(15,2)&#x0200a;=&#x0200a;3.72, p&#x0003c;.05) with task modality for the response period, where the amplitude of the response during the auditory task was lower than in the other conditions. None of the non-target frequency sidebands (100&#x02013;200 Hz, 240&#x02013;400 Hz, or 553&#x02013;613 Hz) showed this effect, highlighting the specificity of the effect to the range of the background tones and not as a broad suppression of all acoustic frequencies.</p></sec><sec id="s3b"><title>Behavioral Responses</title><p>Because of cross-subject variance observed in the ABR suppression effect described above (e.g. average standard error size covers over 12% of the response amplitude), we sought to examine behavioral metrics which might similarly vary across individuals and help explain this variance. No significant differences were observed in subjects' overall accuracy between the tasks (77.7%+/&#x02212;2% <italic>auditory</italic>, 80.2%+/&#x02212;2% <italic>visual</italic>), suggesting an equal, moderate level of difficulty for both tasks, and consistent with having thresholds matched according to the staircase procedure. Subjects did respond faster (684+/&#x02212;44 ms vs 755+/&#x02212;56 ms, t(16)&#x0200a;=&#x0200a;2.67,p&#x0003c;.05), <xref ref-type="fig" rid="pone-0055215-g004">Figure 4</xref>, <bold>A</bold>) and more consistently (lower mean within-subject variance) (254.5+/&#x02212;20.3 vs 305.0+/&#x02212;22, t(16)&#x0200a;=&#x0200a;5.84,p&#x0003c;.05) when performing the visual task with auditory background, than when the task and background were both auditory.</p><fig id="pone-0055215-g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0055215.g004</object-id><label>Figure 4</label><caption><title>Response time per sensory modality.</title><p>Subjects responded faster (lower median response times) and more consistently (lower mean within-subject variance) with visual than auditory task stimuli (<bold>A</bold>). Error bars depict standard error across subjects. There was a high correlation across subjects' response times between the two tasks (r&#x0200a;=&#x0200a;0.883, p&#x0003c;.05) such that subjects with longer responses during the visual task also tended to take longer to respond with auditory targets as well (<bold>B</bold>). The shaded area emphases how this relative difference increases consistently across subjects.</p></caption><graphic xlink:href="pone.0055215.g004"/></fig><p>Additionally, when looking across subjects, there was a high correlation in response times between the two specific tasks (r&#x0200a;=&#x0200a;0.883, p&#x0003c;.05). That is, subjects with longer responses during the visual task also tended to take longer to respond with auditory targets as well (<xref ref-type="fig" rid="pone-0055215-g004">Figure 4</xref>, <bold>B</bold>). Note that the relative difference increases consistently across subjects (slope of the regression line is less than 1). Specifically, subjects with the longest auditory RT show the greatest RT decrease (e.g., improvement) in the visual task condition relative to their own auditory performance (<xref ref-type="fig" rid="pone-0055215-g002">Figure 2</xref>, increasing shaded region).</p><p>Prior to ABR testing, we determined the threshold test stimulus length necessary for each subject to perform the tasks just above chance using an adaptive staircase procedure. The resulting values show that, on average, temporal discrimination thresholds were significantly longer for visual than auditory stimuli (t(16)&#x0200a;=&#x0200a;3.23, p&#x0003c;.05), consistent with previous reports <xref rid="pone.0055215-Laurienti2" ref-type="bibr">[40]</xref>, <xref rid="pone.0055215-Diederich1" ref-type="bibr">[41]</xref>, and supporting claims that auditory acuity is superior to visual in the temporal domain <xref rid="pone.0055215-Welch1" ref-type="bibr">[42]</xref>&#x02013;<xref rid="pone.0055215-Hairston3" ref-type="bibr">[44]</xref>. For the group a wide range of temporal thresholds were observed, with no consistent corollary relationship between modalities across subjects (r(16)&#x0200a;=&#x0200a;0.079, p&#x0003e;.05, <xref ref-type="fig" rid="pone-0055215-g005">Figure 5</xref>, <bold>A</bold>).</p><fig id="pone-0055215-g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0055215.g005</object-id><label>Figure 5</label><caption><title>Temporal thresholds and the relation to RT.</title><p>Temporal thresholds for each modality derived prior to the study (via staircase procedure) show no systematic relationship across subjects (<bold>A</bold>). Shaded areas represent whether a subject shows more temporal acuity in the visual (blue) or auditory (green) domain. There is, however, a high correlation between the amount of sensory bias (difference between visual and auditory thresholds) observed and reaction time differences across subjects (<bold>B</bold>).</p></caption><graphic xlink:href="pone.0055215.g005"/></fig><p>It is possible, however, that each subject's <italic>bias</italic> or proclivity towards higher acuity in one modality verses another is directly related to low-level processing. To examine this behaviorally, &#x0201c;sensory bias&#x0201d; was calculated as the difference, in ms, between threshold values for each subject. This is highlighted in the diagonal line in <xref ref-type="fig" rid="pone-0055215-g005">Figure 5</xref>, <bold>B</bold>, where high positive values indicate higher acuity to audition than vision (green shaded region, right side), while negative scores denote subjects with a sensory bias towards the visual domain (blue region, left side).</p><p>
<xref ref-type="fig" rid="pone-0055215-g005">Figure 5</xref> (<bold>B</bold>) shows that the relationship between sensory bias, and the difference in RTs observed between visual and auditory tasks, for each subject. Note that these are highly correlated (r(16)&#x0200a;=&#x0200a;0.54, p&#x0003c;.05), such that the degree of sensory bias is predictive of the difference in reaction times between tasks. Specifically, those subjects with a higher temporal acuity in the auditory domain show the largest decrease in response time (a performance benefit) when switching from multiple stimuli occurring within the same modality (auditory task, auditory background) to being across modalities (visual task, auditory background). Likewise, response times by the few subjects with higher visual acuity were increased when the task changed from visual to auditory in nature.</p></sec><sec id="s3c"><title>Relationship between brainstem FFR and performance</title><p>Given the observed individual variation in differences in within- modal (<italic>auditory task</italic>) versus cross-modal (<italic>visual-task</italic>) performance and group-level FFR suppression, we sought to establish how task performance was related to the primary metrics of pitch strength and frequency response amplitude, which contribute to ABR signal encoding clarity.</p><p>While there was no direct correlation between pitch strength and frequency amplitude with auditory and visual response times by themselves, our primary interest was with the relative difference <italic>between</italic> modalities. Indeed, there was a consistent relationship here &#x02013; as can be seen in <xref ref-type="fig" rid="pone-0055215-g006">Figure 6</xref>, subjects with higher pitch strength (<bold>A</bold>) and higher target frequency response amplitude (<bold>B</bold>) also showed the greatest response time difference when comparing auditory and visual (V&#x02013;A) performance. Dashed lines connect each subject's data for the three conditions. This relationship between RT difference and each FFR metric is consistent regardless of which task (<italic>auditory</italic>, <italic>visual</italic>, or <italic>no-task</italic>, see table in <xref ref-type="fig" rid="pone-0055215-g006">Figure 6</xref>, <bold>C</bold>, which lists correlation coefficients for each pair) subjects were performing at the time (similarity of slopes for dotted line in <xref ref-type="fig" rid="pone-0055215-g006">Figure 6</xref>).</p><fig id="pone-0055215-g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0055215.g006</object-id><label>Figure 6</label><caption><title>RT changes relative to each FFR metric.</title><p>Subjects with higher pitch strength (<bold>A</bold>) and higher target frequency response amplitude (<bold>B</bold>) also showed the greatest response time difference (increasingly negative values) when comparing auditory and visual performance. Dashed lines connect each subject's data for the three conditions. This relationship between RT and each FFR metric is consistent regardless of which task (<italic>auditory</italic>, <italic>visual</italic>, or <italic>no-task</italic>, see table in <xref ref-type="fig" rid="pone-0055215-g006">Figure 6</xref>, <bold>C</bold>) subjects were performing at the time (similarity of dotted line in <xref ref-type="fig" rid="pone-0055215-g006">Figure 6</xref>) Table lists correlation coefficients between response times (columns) and each of the two FFR metrics (rows), as observed within each of the three recording conditions (A&#x0200a;=&#x0200a;Auditory, V&#x0200a;=&#x0200a;Visual, N&#x0200a;=&#x0200a;No task). Highlighted values depict correlations significant at p&#x0003c;.05.</p></caption><graphic xlink:href="pone.0055215.g006"/></fig><p>Above we showed that on average, there was a difference in overall ABR clarity (suppression) with the addition of the task, which is particularly dominant within the auditory modality (observed for both metrics). Meanwhile, participants display a wide range in the degree of sensory bias. <xref ref-type="fig" rid="pone-0055215-g007">Figure 7</xref> shows the relationship between this sensory bias, and the degree of suppression in the amplitude of the FFR primary frequency for the auditory task compared to both <italic>no task</italic> and <italic>visual task</italic> conditions.</p><fig id="pone-0055215-g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0055215.g007</object-id><label>Figure 7</label><caption><title>FFR change related to sensory bias.</title><p>The degree of subjects' sensory bias significantly predicts auditory task-related suppression in primary FFR frequency amplitude (relative to no task), such that those subjects that are more highly acute in the auditory domain show minimal or no suppression. Meanwhile, those whom display less bias or a proclivity more towards vision appear more susceptible to the suppressive effect. A similar trend occurs when comparing auditory against visual tasks, but not for visual above baseline, as well as with FFR pitch strength as the metric.</p></caption><graphic xlink:href="pone.0055215.g007"/></fig><p>Here, the degree of subjects' sensory bias significantly predicts (r(17)&#x0200a;=&#x0200a;.55, p&#x0003c;.05) suppression in amplitude of the primary FFR frequency from the additional auditory task (relative to no task). A similar trend (r(17)&#x0200a;=&#x0200a;.46,p&#x0003c;.07) occurs when comparing auditory against visual tasks, but not for visual above baseline (no task, r(17)&#x0200a;=&#x0200a;.02, p&#x0003e;.05)). That is, subjects with the strongest bias towards vision also showed the largest difference in pitch encoding. A matching, but weaker trend was observed for pitch strength with auditory tasks above baseline (r(17)&#x0200a;=&#x0200a;.47, P&#x0003c;.06) while not consistently while comparing against visual (r(17)&#x0200a;=&#x0200a;.34, p&#x0003e;.05) or with visual verses no task (r(17)&#x0200a;=&#x0200a;.06, p&#x0003e;.05).</p></sec></sec><sec id="s4"><title>Discussion</title><p>Here, we have described a method that allows testing the degree to which background, task irrelevant auditory information is encoded at very early stages of processing, independent of the actual task at hand. With it, we have shown that simply adding a task leads to attenuation of the brainstem frequency following response. Additionally, this low-level modulation is linked to behavioral measures of performance for the primary task and a participant's own perceptual acuity, at least within the temporal domain. Specifically, this study provides three primary results.</p><p>First, relative to a baseline of having no task to perform at all, early auditory encoding (brainstem FFR clarity, assessed using two metrics) of background, non-relevant sounds appears inhibited simply by the addition of a sensory-driven primary task. This effect is most evident when the task is also within the auditory domain. While <italic>pitch strength</italic>, a measure of encoding reliability <xref rid="pone.0055215-Skoe1" ref-type="bibr">[1]</xref>, is decreased for visual tasks (a case of operating across modalities) as well, having subjects work within the same modality as the background tone also decreased the spectral amplitude of the brainstem-encoded signal, yielding lower overall encoding acuity. This suggests that the suppression of non-relevant, background information may be stronger when it is within the same modality than when it is across senses.</p><p>It is not entirely clear why, although the effects for pitch strength were identical, the frequency amplitude results are not the same for the visual and auditory tasks. One possibility is that this effect is most likely due to differences in the relative sensitivities of each of these methods, and reflect a change occurring within the temporal domain relating to the periodic encoding of the primary frequency (relative to no task). That is, addition of the visual task may induce slight alterations in the temporal encoding of the signal (e.g. jitter) that affect the periodicity of the signal, but are not substantial enough to alter the mean power of the frequency range tested across the entire response window. The auditory task, on the other hand, involves greater perceptual competition and likely induces additional inhibition on early encoding, which decreases the frequency power as well, consistent with visual and auditory processes using different physiological networks for initial encoding.</p><p>Second, when looking at subjects' behavioral performance, response times were faster for discriminating visual stimuli than when the same task occurred in the auditory domain, even though the tasks were confirmed to be equally difficult (similar accuracy, set via staircase). While one potential explanation for these differences is that is the neural processes involved in auditory discrimination may simply take more time than visual, which has been espoused previously <xref rid="pone.0055215-Krishnan3" ref-type="bibr">[38]</xref>, <xref rid="pone.0055215-Skoe3" ref-type="bibr">[39]</xref>, another possibility is that the facilitation in visual RT over auditory shown here is afforded from operating with stimuli across modalities, rather than within the same sense. Specifically, the auditory condition in this paradigm requires participants to focus on a specific frequency range (e.g. that of the target stimulus) while inhibiting other frequencies (that of the background tones) in order to perform the task, especially since the task target tones and background tones overlap within different, inconsistent periods. In contrast, the visual task likely does not require the same level of focused attention, wherein subjects can simply attend to the visual stream as a whole, and ignore all information within the auditory channel. Much research has suggested that performance is better when dividing attention across senses rather than within (e.g., <xref rid="pone.0055215-Duncan1" ref-type="bibr">[45]</xref>, <xref rid="pone.0055215-Santangelo1" ref-type="bibr">[46]</xref>). This explanation is also supported by the greater overall suppression of the FFR during the auditory than visual task (e.g. <xref ref-type="fig" rid="pone-0055215-g002">Figure 2</xref>), suggesting that subjects more actively inhibited the background tone. The difference in response time, which may be viewed as a facilitation yielded from working across, rather than within modalities, was also highly correlated with each subject's difference in temporal acuity, or proclivity towards one sense (sensory bias as measured by differences in threshold discrimination times).</p><p>Finally, we have shown that there is a consistent relationship between this cross-modality related facilitation and the measures of low-level auditory acuity utilized here. For both <italic>pitch strength</italic> and <italic>response amplitude</italic> higher values (e.g. better signal consistency or higher SNR) predicted a larger difference in performance time when operating across modalities. That is, subjects who exhibit stronger brainstem encoding also show the greatest improvement in response time when their primary task shifts from operating in the auditory realm to being visual instead. This relationship is fairly robust, occurring regardless of what the subject is actually doing when the FFR is derived.</p><p>Perhaps more striking, however, is that the degree to which attenuation in FFR pitch strength (and to a lesser degree frequency amplitude) will be observed when performing an auditory task is directly related to an individual's proclivity towards that same modality. Specifically, those individuals with a higher temporal perceptual acuity within the auditory modality show virtually no change in brainstem encoding quality (when comparing performance on an auditory task to either a visual or no task at all). Meanwhile, participants who are less naturally biased towards the auditory modality also show the largest decrement in signal encoding. This trend does not occur when the task is visual, allowing attention to be focused away from the potentially distracting background tones.</p><p>While this study cannot differentiate the neurophysiological source of the suppressive effect described here, it is most likely that this is a top-down (e.g. cortically-driven) phenomenon, and not from the bottom-up. Previous studies have documented cortico-fugal projections which could mediate midbrain (and lower) function <xref rid="pone.0055215-Krishnan2" ref-type="bibr">[12]</xref>, <xref rid="pone.0055215-Saldaa1" ref-type="bibr">[47]</xref>&#x02013;<xref rid="pone.0055215-Galbraith4" ref-type="bibr">[49]</xref>. Additionally, inhibitory effects from the efferent medial olivocochlear (MOC) tracts innervating outer hair cells have been implicated in modulating evoked otoacoustic emissions when attention is focused on other modalities <xref rid="pone.0055215-Giard1" ref-type="bibr">[50]</xref>&#x02013;<xref rid="pone.0055215-Delano1" ref-type="bibr">[52]</xref>. This falls in line with the numerous previous reports of auditory <xref rid="pone.0055215-Ahveninen1" ref-type="bibr">[21]</xref>&#x02013;<xref rid="pone.0055215-Alain1" ref-type="bibr">[24]</xref> and other cortical <xref rid="pone.0055215-Macaluso1" ref-type="bibr">[26]</xref>, <xref rid="pone.0055215-Johnson1" ref-type="bibr">[53]</xref>, <xref rid="pone.0055215-Shomstein1" ref-type="bibr">[54]</xref> modulation from selectively attending to specific stimuli. It should be noted however that it remains controversial whether attention-related mediation of directly driven stimulus-locked brainstem auditory evoked potentials (BAEPs) at the initial stages of encoding should be expected as well <xref rid="pone.0055215-Gregory1" ref-type="bibr">[55]</xref>&#x02013;<xref rid="pone.0055215-Hirschhorn1" ref-type="bibr">[58]</xref> but also see <xref rid="pone.0055215-Lukas1" ref-type="bibr">[59]</xref>.</p><p>Mechanistically, we propose that the effects observed here reflect an interplay between brainstem and cortex based on these structural connections. Specifically, the addition of the task increases the total attentional demand within cortex, which leads to a concomitant top-down utilization of the inhibitory cortico-fugal projections, which is turn suppresses the encoding clarity of the FFR. Because the cortical demand is greater in the auditory task (where the background tones are more potentially distracting to the primary task within the same modality), the cortical driver increases the suppression even more, which is reflected is the differential change in frequency amplitude.</p><p>The cross-subject variance observed here, which is common in similar studies, certainly represents some combination of differences in individuals' perceptual acuity and whatever strategies they may employ for performing tasks in the presence of background noise. However, accounting for this variance (regardless of its origin) based on participants' perceptual proclivity aides in illustrating differences in sub-populations which might otherwise not be known.</p><p>These results provide important implications within two avenues. First, we have established the utility of a paradigm which allows the ability to compare brainstem-level effects relative to different types of cortical tasks, while remaining independent of the stimuli used for those tasks. Unlike previous approaches using the response-eliciting stimuli as a primary component of the task, ABR time-locked events in this case are completely non-task relevant and temporally separate from task-related events. In our analyses we included only data from time periods non-overlapping with the primary task stimuli or response periods, so that the EEG-measured activity examined here reflects only time <italic>between</italic> task trials and the response to the background tone, and thus all stimulus features were physically identical across all conditions. This means that any effects observed must be related to chronic, global state-mediated changes rather than stimulus-driven, bottom-up or immediate response influences.</p><p>It should be noted that in this paradigm, we cannot completely separate whether the fluctuations observed here stem solely from the brainstem FFR, or alternatively from a concomitant increase in cortical or other sources related to performing a task which could change the overall background electrical field level assumed to be a baseline. That is, although the target signal discussed here is the brainstem FFR, it is possible that the suppressive effect observed with our metrics appear as a result of increased &#x0201c;noise&#x0201d; (meaning non-brainstem originating fluctuations) adding variance to the overall signal during the active task conditions. For instance, cortical oscillations are known to occur for sustained periods of task engagement <xref rid="pone.0055215-Kaiser1" ref-type="bibr">[60]</xref>, <xref rid="pone.0055215-Womelsdorf1" ref-type="bibr">[61]</xref>, and although oscillations differ across sensory modalities, they are critical for binding across senses <xref rid="pone.0055215-Senkowski1" ref-type="bibr">[62]</xref>.</p><p>However, while it is indeed likely that parallel cortical activity is induced by the task and maintained between trial periods, we do not feel this could fully explain the extent of the effects examined here. The highpass filter (80 Hz) should remove typical cortical activity, although this will not eliminate all neurophysiologically driven signals such as microsaccades. Perhaps more importantly, because the techniques here rely on averaging across hundreds of stimulus epochs, any remaining contaminating effect would require the oscillations to be phase-locked with the stimuli in order to survive combining across so many trials. Additionally, any such sources would have to also fall within the specific frequency range targeted in our analyses (220 Hz). Although we did observe small differences in the pre-stimulus period, these were not statistically significant at the group level or correlated with any of the behavioral measures compared here, such as the observed relationship between each participant's sensory proclivity and the extent of suppression observed in the FFR (<xref ref-type="fig" rid="pone-0055215-g007">Figure 7</xref>).</p><p>Second, we have shown that variance in the quality of sound encoding sub-cortically can be introduced simply by the addition of a task. Not only is the degree to which this occurs dependent on the type of task and modality used, but also appears to vary across individuals based on other intrinsic properties, such as their own discrimination acuity within a particular sense. This suggests that care should be taken in future brainstem FFR-based studies, because the size of the effect observed may depend more on external parameters of the study than originally assumed. Likewise, excess variance due to individual differences in susceptibility to these effects could decrease the overall power of a study to properly identify hypothesized effects.</p></sec></body><back><ack><p>The authors would like to thank Tim Mermagen and the Auditory Research Team, as well as Keith Whitaker and Alfred Yu from the Army Research Laboratory for their help and insight in carrying out this project.</p></ack><ref-list><title>References</title><ref id="pone.0055215-Skoe1"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Skoe</surname><given-names>E</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>Auditory brain stem response to complex sounds: a tutorial</article-title>. <source>Ear Hear</source>
<volume>31</volume>: <fpage>302</fpage>&#x02013;<lpage>324</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/AUD.0b013e3181cdb272">10.1097/AUD.0b013e3181cdb272</ext-link></comment>.<pub-id pub-id-type="pmid">20084007</pub-id></mixed-citation></ref><ref id="pone.0055215-Kraus1"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Kraus</surname><given-names>N</given-names></name>, <name><surname>Skoe</surname><given-names>E</given-names></name>, <name><surname>Parbery-Clark</surname><given-names>A</given-names></name>, <name><surname>Ashley</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Experience-induced malleability in neural encoding of pitch, timbre, and timing</article-title>. <source>Ann N Y Acad Sci</source>
<volume>1169</volume>: <fpage>543</fpage>&#x02013;<lpage>557</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1749&#x02013;6632.2009.04549.x">10.1111/j.1749&#x02013;6632.2009.04549.x</ext-link></comment>.<pub-id pub-id-type="pmid">19673837</pub-id></mixed-citation></ref><ref id="pone.0055215-Bidelman1"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Bidelman</surname><given-names>GM</given-names></name>, <name><surname>Krishnan</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>Neural correlates of consonance, dissonance, and the hierarchy of musical pitch in the human brainstem</article-title>. <source>J Neurosci</source>
<volume>29</volume>: <fpage>13165</fpage>&#x02013;<lpage>13171</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3900&#x02013;09.2009">10.1523/JNEUROSCI.3900&#x02013;09.2009</ext-link></comment>.<pub-id pub-id-type="pmid">19846704</pub-id></mixed-citation></ref><ref id="pone.0055215-Krishnan1"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Krishnan</surname><given-names>A</given-names></name> (<year>2002</year>) <article-title>Human frequency-following responses: representation of steady-state synthetic vowels</article-title>. <source>Hear Res</source>
<volume>166</volume>: <fpage>192</fpage>&#x02013;<lpage>201</lpage>.<pub-id pub-id-type="pmid">12062771</pub-id></mixed-citation></ref><ref id="pone.0055215-Chandrasekaran1"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Chandrasekaran</surname><given-names>B</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>The scalp-recorded brainstem response to speech: Neural origins and plasticity</article-title>. <source>Psychophysiology</source>
<volume>47</volume>: <fpage>236</fpage>&#x02013;<lpage>246</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1469&#x02013;8986.2009.00928.x">10.1111/j.1469&#x02013;8986.2009.00928.x</ext-link></comment>.<pub-id pub-id-type="pmid">19824950</pub-id></mixed-citation></ref><ref id="pone.0055215-Burkard1"><label>6</label><mixed-citation publication-type="other">Burkard RF, Eggermont JJ, Don M (2007) Auditory evoked potentials: basic principles and clinical application. Lippincott Williams &#x00026; Wilkins. 783 p.</mixed-citation></ref><ref id="pone.0055215-Galbraith1"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Galbraith</surname><given-names>GC</given-names></name>, <name><surname>Threadgill</surname><given-names>MR</given-names></name>, <name><surname>Hemsley</surname><given-names>J</given-names></name>, <name><surname>Salour</surname><given-names>K</given-names></name>, <name><surname>Songdej</surname><given-names>N</given-names></name>, <etal>et al</etal> (<year>2000</year>) <article-title>Putative measure of peripheral and brainstem frequency-following in humans</article-title>. <source>Neurosci Lett</source>
<volume>292</volume>: <fpage>123</fpage>&#x02013;<lpage>127</lpage>.<pub-id pub-id-type="pmid">10998564</pub-id></mixed-citation></ref><ref id="pone.0055215-Musacchia1"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Musacchia</surname><given-names>G</given-names></name>, <name><surname>Sams</surname><given-names>M</given-names></name>, <name><surname>Nicol</surname><given-names>T</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2006</year>) <article-title>Seeing speech affects acoustic information processing in the human brainstem</article-title>. <source>Exp Brain Res</source>
<volume>168</volume>: <fpage>1</fpage>&#x02013;<lpage>10</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-005-0071-5">10.1007/s00221-005-0071-5</ext-link></comment>.<pub-id pub-id-type="pmid">16217645</pub-id></mixed-citation></ref><ref id="pone.0055215-Musacchia2"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Musacchia</surname><given-names>G</given-names></name>, <name><surname>Sams</surname><given-names>M</given-names></name>, <name><surname>Skoe</surname><given-names>E</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2007</year>) <article-title>Musicians have enhanced subcortical auditory and audiovisual processing of speech and music</article-title>. <source>Proc Natl Acad Sci USA</source>
<volume>104</volume>: <fpage>15894</fpage>&#x02013;<lpage>15898</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0701498104">10.1073/pnas.0701498104</ext-link></comment>.<pub-id pub-id-type="pmid">17898180</pub-id></mixed-citation></ref><ref id="pone.0055215-Musacchia3"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Musacchia</surname><given-names>G</given-names></name>, <name><surname>Strait</surname><given-names>D</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2008</year>) <article-title>Relationships between behavior, brainstem and cortical encoding of seen and heard speech in musicians and non-musicians</article-title>. <source>Hear Res</source>
<volume>241</volume>: <fpage>34</fpage>&#x02013;<lpage>42</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.heares.2008.04.013">10.1016/j.heares.2008.04.013</ext-link></comment>.<pub-id pub-id-type="pmid">18562137</pub-id></mixed-citation></ref><ref id="pone.0055215-Skoe2"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Skoe</surname><given-names>E</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>Hearing it again and again: on-line subcortical plasticity in humans</article-title>. <source>PLoS ONE</source>
<volume>5</volume>: <fpage>e13645</fpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0013645">10.1371/journal.pone.0013645</ext-link></comment>.<pub-id pub-id-type="pmid">21049035</pub-id></mixed-citation></ref><ref id="pone.0055215-Krishnan2"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Krishnan</surname><given-names>A</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Gandour</surname><given-names>J</given-names></name>, <name><surname>Cariani</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Encoding of pitch in the human brainstem is sensitive to language experience</article-title>. <source>Brain Res Cogn Brain Res</source>
<volume>25</volume>: <fpage>161</fpage>&#x02013;<lpage>168</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cogbrainres.2005.05.004">10.1016/j.cogbrainres.2005.05.004</ext-link></comment>.<pub-id pub-id-type="pmid">15935624</pub-id></mixed-citation></ref><ref id="pone.0055215-Akhoun1"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Akhoun</surname><given-names>I</given-names></name>, <name><surname>Gall&#x000e9;go</surname><given-names>S</given-names></name>, <name><surname>Moulin</surname><given-names>A</given-names></name>, <name><surname>M&#x000e9;nard</surname><given-names>M</given-names></name>, <name><surname>Veuillet</surname><given-names>E</given-names></name>, <etal>et al</etal> (<year>2008</year>) <article-title>The temporal relationship between speech auditory brainstem responses and the acoustic pattern of the phoneme/ba/ in normal-hearing adults</article-title>. <source>Clin Neurophysiol</source>
<volume>119</volume>: <fpage>922</fpage>&#x02013;<lpage>933</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.clinph.2007.12.010">10.1016/j.clinph.2007.12.010</ext-link></comment>.<pub-id pub-id-type="pmid">18291717</pub-id></mixed-citation></ref><ref id="pone.0055215-Wong1"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Wong</surname><given-names>PCM</given-names></name>, <name><surname>Skoe</surname><given-names>E</given-names></name>, <name><surname>Russo</surname><given-names>NM</given-names></name>, <name><surname>Dees</surname><given-names>T</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2007</year>) <article-title>Musical experience shapes human brainstem encoding of linguistic pitch patterns</article-title>. <source>Nat Neurosci</source>
<volume>10</volume>: <fpage>420</fpage>&#x02013;<lpage>422</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1872">10.1038/nn1872</ext-link></comment>.<pub-id pub-id-type="pmid">17351633</pub-id></mixed-citation></ref><ref id="pone.0055215-Strait1"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Strait</surname><given-names>DL</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name>, <name><surname>Skoe</surname><given-names>E</given-names></name>, <name><surname>Ashley</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Musical experience and neural efficiency: effects of training on subcortical processing of vocal expressions of emotion</article-title>. <source>Eur J Neurosci</source>
<volume>29</volume>: <fpage>661</fpage>&#x02013;<lpage>668</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460&#x02013;9568.2009.06617.x">10.1111/j.1460&#x02013;9568.2009.06617.x</ext-link></comment>.<pub-id pub-id-type="pmid">19222564</pub-id></mixed-citation></ref><ref id="pone.0055215-DeBoer1"><label>16</label><mixed-citation publication-type="journal">
<name><surname>De Boer</surname><given-names>J</given-names></name>, <name><surname>Thornton</surname><given-names>ARD</given-names></name> (<year>2008</year>) <article-title>Neural correlates of perceptual learning in the auditory brainstem: efferent activity predicts and reflects improvement at a speech-in-noise discrimination task</article-title>. <source>J Neurosci</source>
<volume>28</volume>: <fpage>4929</fpage>&#x02013;<lpage>4937</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0902&#x02013;08.2008">10.1523/JNEUROSCI.0902&#x02013;08.2008</ext-link></comment>.<pub-id pub-id-type="pmid">18463246</pub-id></mixed-citation></ref><ref id="pone.0055215-Posner1"><label>17</label><mixed-citation publication-type="other">Posner MI (2011) Imaging attention networks. NeuroImage. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/22227132">http://www.ncbi.nlm.nih.gov/pubmed/22227132</ext-link>. Accessed 23 February 2012.</mixed-citation></ref><ref id="pone.0055215-Hillyard1"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Hillyard</surname><given-names>SA</given-names></name>, <name><surname>Hink</surname><given-names>RF</given-names></name>, <name><surname>Schwent</surname><given-names>VL</given-names></name>, <name><surname>Picton</surname><given-names>TW</given-names></name> (<year>1973</year>) <article-title>Electrical signs of selective attention in the human brain</article-title>. <source>Science</source>
<volume>182</volume>: <fpage>177</fpage>&#x02013;<lpage>180</lpage>.<pub-id pub-id-type="pmid">4730062</pub-id></mixed-citation></ref><ref id="pone.0055215-Woldorff1"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Woldorff</surname><given-names>MG</given-names></name>, <name><surname>Gallen</surname><given-names>CC</given-names></name>, <name><surname>Hampson</surname><given-names>SA</given-names></name>, <name><surname>Hillyard</surname><given-names>SA</given-names></name>, <name><surname>Pantev</surname><given-names>C</given-names></name>, <etal>et al</etal> (<year>1993</year>) <article-title>Modulation of early sensory processing in human auditory cortex during auditory selective attention</article-title>. <source>Proc Natl Acad Sci USA</source>
<volume>90</volume>: <fpage>8722</fpage>&#x02013;<lpage>8726</lpage>.<pub-id pub-id-type="pmid">8378354</pub-id></mixed-citation></ref><ref id="pone.0055215-Fujiwara1"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Fujiwara</surname><given-names>N</given-names></name>, <name><surname>Nagamine</surname><given-names>T</given-names></name>, <name><surname>Imai</surname><given-names>M</given-names></name>, <name><surname>Tanaka</surname><given-names>T</given-names></name>, <name><surname>Shibasaki</surname><given-names>H</given-names></name> (<year>1998</year>) <article-title>Role of the primary auditory cortex in auditory selective attention studied by whole-head neuromagnetometer</article-title>. <source>Brain Res Cogn Brain Res</source>
<volume>7</volume>: <fpage>99</fpage>&#x02013;<lpage>109</lpage>.<pub-id pub-id-type="pmid">9774711</pub-id></mixed-citation></ref><ref id="pone.0055215-Ahveninen1"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Ahveninen</surname><given-names>J</given-names></name>, <name><surname>H&#x000e4;m&#x000e4;l&#x000e4;inen</surname><given-names>M</given-names></name>, <name><surname>J&#x000e4;&#x000e4;skel&#x000e4;inen</surname><given-names>IP</given-names></name>, <name><surname>Ahlfors</surname><given-names>SP</given-names></name>, <name><surname>Huang</surname><given-names>S</given-names></name>, <etal>et al</etal> (<year>2011</year>) <article-title>Attention-driven auditory cortex short-term plasticity helps segregate relevant sounds from noise</article-title>. <source>Proc Natl Acad Sci USA</source>
<volume>108</volume>: <fpage>4182</fpage>&#x02013;<lpage>4187</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1016134108">10.1073/pnas.1016134108</ext-link></comment>.<pub-id pub-id-type="pmid">21368107</pub-id></mixed-citation></ref><ref id="pone.0055215-Okamoto1"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Okamoto</surname><given-names>H</given-names></name>, <name><surname>Stracke</surname><given-names>H</given-names></name>, <name><surname>Zwitserlood</surname><given-names>P</given-names></name>, <name><surname>Roberts</surname><given-names>LE</given-names></name>, <name><surname>Pantev</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Frequency-specific modulation of population-level frequency tuning in human auditory cortex</article-title>. <source>BMC Neuroscience</source>
<volume>10</volume>: <fpage>1</fpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2202-10-1">10.1186/1471-2202-10-1</ext-link></comment>.<pub-id pub-id-type="pmid">19126204</pub-id></mixed-citation></ref><ref id="pone.0055215-Kauramki1"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Kauram&#x000e4;ki</surname><given-names>J</given-names></name>, <name><surname>J&#x000e4;&#x000e4;skel&#x000e4;inen</surname><given-names>IP</given-names></name>, <name><surname>Sams</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Selective attention increases both gain and feature selectivity of the human auditory cortex</article-title>. <source>PLoS ONE</source>
<volume>2</volume>: <fpage>e909</fpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0000909">10.1371/journal.pone.0000909</ext-link></comment>.<pub-id pub-id-type="pmid">17878944</pub-id></mixed-citation></ref><ref id="pone.0055215-Alain1"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Alain</surname><given-names>C</given-names></name>, <name><surname>Bernstein</surname><given-names>LJ</given-names></name> (<year>2008</year>) <article-title>From sounds to meaning: the role of attention during auditory scene analysis</article-title>. <source>Current Opinion in Otolaryngology &#x00026; Head and Neck Surgery</source>
<volume>16</volume>: <fpage>485</fpage>&#x02013;<lpage>489</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/MOO.0b013e32830e2096">10.1097/MOO.0b013e32830e2096</ext-link></comment>.</mixed-citation></ref><ref id="pone.0055215-Gondan1"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Gondan</surname><given-names>M</given-names></name>, <name><surname>Blurton</surname><given-names>SP</given-names></name>, <name><surname>Hughes</surname><given-names>F</given-names></name>, <name><surname>Greenlee</surname><given-names>MW</given-names></name> (<year>2011</year>) <article-title>Effects of spatial and selective attention on basic multisensory integration</article-title>. <source>J Exp Psychol Hum Percept Perform</source>
<volume>37</volume>: <fpage>1887</fpage>&#x02013;<lpage>1897</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0025635">10.1037/a0025635</ext-link></comment>.<pub-id pub-id-type="pmid">21967270</pub-id></mixed-citation></ref><ref id="pone.0055215-Macaluso1"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Macaluso</surname><given-names>E</given-names></name>, <name><surname>Frith</surname><given-names>CD</given-names></name>, <name><surname>Driver</surname><given-names>J</given-names></name> (<year>2000</year>) <article-title>Modulation of human visual cortex by crossmodal spatial attention</article-title>. <source>Science</source>
<volume>289</volume>: <fpage>1206</fpage>&#x02013;<lpage>1208</lpage>.<pub-id pub-id-type="pmid">10947990</pub-id></mixed-citation></ref><ref id="pone.0055215-Hairston1"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Hairston</surname><given-names>WD</given-names></name>, <name><surname>Hodges</surname><given-names>DA</given-names></name>, <name><surname>Casanova</surname><given-names>R</given-names></name>, <name><surname>Hayasaka</surname><given-names>S</given-names></name>, <name><surname>Kraft</surname><given-names>R</given-names></name>, <etal>et al</etal> (<year>2008</year>) <article-title>Closing the mind's eye: deactivation of visual cortex related to auditory task difficulty</article-title>. <source>Neuroreport</source>
<volume>19</volume>: <fpage>151</fpage>&#x02013;<lpage>154</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/WNR.0b013e3282f42509">10.1097/WNR.0b013e3282f42509</ext-link></comment>.<pub-id pub-id-type="pmid">18185099</pub-id></mixed-citation></ref><ref id="pone.0055215-Galbraith2"><label>28</label><mixed-citation publication-type="journal">
<name><surname>Galbraith</surname><given-names>GC</given-names></name>, <name><surname>Olfman</surname><given-names>DM</given-names></name>, <name><surname>Huffman</surname><given-names>TM</given-names></name> (<year>2003</year>) <article-title>Selective attention affects human brain stem frequency-following response</article-title>. <source>Neuroreport</source>
<volume>14</volume>: <fpage>735</fpage>&#x02013;<lpage>738</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/01.wnr.0000064983.96259.49">10.1097/01.wnr.0000064983.96259.49</ext-link></comment>.<pub-id pub-id-type="pmid">12692473</pub-id></mixed-citation></ref><ref id="pone.0055215-Galbraith3"><label>29</label><mixed-citation publication-type="journal">
<name><surname>Galbraith</surname><given-names>GC</given-names></name>, <name><surname>Bhuta</surname><given-names>SM</given-names></name>, <name><surname>Choate</surname><given-names>AK</given-names></name>, <name><surname>Kitahara</surname><given-names>JM</given-names></name>, <name><surname>Mullen</surname><given-names>TA</given-names><suffix>Jr</suffix></name> (<year>1998</year>) <article-title>Brain stem frequency-following response to dichotic vowels during attention</article-title>. <source>Neuroreport</source>
<volume>9</volume>: <fpage>1889</fpage>&#x02013;<lpage>1893</lpage>.<pub-id pub-id-type="pmid">9665621</pub-id></mixed-citation></ref><ref id="pone.0055215-Hoormann1"><label>30</label><mixed-citation publication-type="journal">
<name><surname>Hoormann</surname><given-names>J</given-names></name>, <name><surname>Falkenstein</surname><given-names>M</given-names></name>, <name><surname>Hohnsbein</surname><given-names>J</given-names></name> (<year>1994</year>) <article-title>Effect of selective attention on the latency of human frequency-following potentials</article-title>. <source>Neuroreport</source>
<volume>5</volume>: <fpage>1609</fpage>&#x02013;<lpage>1612</lpage>.<pub-id pub-id-type="pmid">7819530</pub-id></mixed-citation></ref><ref id="pone.0055215-Shulman1"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Shulman</surname><given-names>GL</given-names></name>, <name><surname>Corbetta</surname><given-names>M</given-names></name>, <name><surname>Buckner</surname><given-names>RL</given-names></name>, <name><surname>Raichle</surname><given-names>ME</given-names></name>, <name><surname>Fiez</surname><given-names>JA</given-names></name>, <etal>et al</etal> (<year>1997</year>) <article-title>Top-down modulation of early sensory cortex</article-title>. <source>Cereb Cortex</source>
<volume>7</volume>: <fpage>193</fpage>&#x02013;<lpage>206</lpage>.<pub-id pub-id-type="pmid">9143441</pub-id></mixed-citation></ref><ref id="pone.0055215-Laurienti1"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Laurienti</surname><given-names>PJ</given-names></name>, <name><surname>Burdette</surname><given-names>JH</given-names></name>, <name><surname>Wallace</surname><given-names>MT</given-names></name>, <name><surname>Yen</surname><given-names>Y-F</given-names></name>, <name><surname>Field</surname><given-names>AS</given-names></name>, <etal>et al</etal> (<year>2002</year>) <article-title>Deactivation of sensory-specific cortex by cross-modal stimuli</article-title>. <source>J Cogn Neurosci</source>
<volume>14</volume>: <fpage>420</fpage>&#x02013;<lpage>429</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089892902317361930">10.1162/089892902317361930</ext-link></comment>.<pub-id pub-id-type="pmid">11970801</pub-id></mixed-citation></ref><ref id="pone.0055215-Haxby1"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Haxby</surname><given-names>JV</given-names></name>, <name><surname>Horwitz</surname><given-names>B</given-names></name>, <name><surname>Ungerleider</surname><given-names>LG</given-names></name>, <name><surname>Maisog</surname><given-names>JM</given-names></name>, <name><surname>Pietrini</surname><given-names>P</given-names></name>, <etal>et al</etal> (<year>1994</year>) <article-title>The functional organization of human extrastriate cortex: a PET-rCBF study of selective attention to faces and locations</article-title>. <source>J Neurosci</source>
<volume>14</volume>: <fpage>6336</fpage>&#x02013;<lpage>6353</lpage>.<pub-id pub-id-type="pmid">7965040</pub-id></mixed-citation></ref><ref id="pone.0055215-Bense1"><label>34</label><mixed-citation publication-type="journal">
<name><surname>Bense</surname><given-names>S</given-names></name>, <name><surname>Stephan</surname><given-names>T</given-names></name>, <name><surname>Yousry</surname><given-names>TA</given-names></name>, <name><surname>Brandt</surname><given-names>T</given-names></name>, <name><surname>Dieterich</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>Multisensory cortical signal increases and decreases during vestibular galvanic stimulation (fMRI)</article-title>. <source>J Neurophysiol</source>
<volume>85</volume>: <fpage>886</fpage>&#x02013;<lpage>899</lpage>.<pub-id pub-id-type="pmid">11160520</pub-id></mixed-citation></ref><ref id="pone.0055215-Papanicolaou1"><label>35</label><mixed-citation publication-type="journal">
<name><surname>Papanicolaou</surname><given-names>AC</given-names></name>, <name><surname>Johnstone</surname><given-names>J</given-names></name> (<year>1984</year>) <article-title>Probe evoked potentials: theory, method and applications</article-title>. <source>Int J Neurosci</source>
<volume>24</volume>: <fpage>107</fpage>&#x02013;<lpage>131</lpage>.<pub-id pub-id-type="pmid">6209236</pub-id></mixed-citation></ref><ref id="pone.0055215-Levitt1"><label>36</label><mixed-citation publication-type="other">Levitt H (1971) Transformed up-down methods in psychoacoustics. J Acoust Soc Am 49: Suppl 2:467+.</mixed-citation></ref><ref id="pone.0055215-Hairston2"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Hairston</surname><given-names>WD</given-names></name>, <name><surname>Maldjian</surname><given-names>JA</given-names></name> (<year>2009</year>) <article-title>An adaptive staircase procedure for the E-Prime programming environment</article-title>. <source>Comput Methods Programs Biomed</source>
<volume>93</volume>: <fpage>104</fpage>&#x02013;<lpage>108</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cmpb.2008.08.003">10.1016/j.cmpb.2008.08.003</ext-link></comment>.<pub-id pub-id-type="pmid">18838189</pub-id></mixed-citation></ref><ref id="pone.0055215-Krishnan3"><label>38</label><mixed-citation publication-type="journal">
<name><surname>Krishnan</surname><given-names>A</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Gandour</surname><given-names>J</given-names></name>, <name><surname>Cariani</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Encoding of pitch in the human brainstem is sensitive to language experience</article-title>. <source>Brain Res Cogn Brain Res</source>
<volume>25</volume>: <fpage>161</fpage>&#x02013;<lpage>168</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cogbrainres.2005.05.004">10.1016/j.cogbrainres.2005.05.004</ext-link></comment>.<pub-id pub-id-type="pmid">15935624</pub-id></mixed-citation></ref><ref id="pone.0055215-Skoe3"><label>39</label><mixed-citation publication-type="journal">
<name><surname>Skoe</surname><given-names>E</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>Auditory brain stem response to complex sounds: a tutorial</article-title>. <source>Ear Hear</source>
<volume>31</volume>: <fpage>302</fpage>&#x02013;<lpage>324</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/AUD.0b013e3181cdb272">10.1097/AUD.0b013e3181cdb272</ext-link></comment>.<pub-id pub-id-type="pmid">20084007</pub-id></mixed-citation></ref><ref id="pone.0055215-Laurienti2"><label>40</label><mixed-citation publication-type="journal">
<name><surname>Laurienti</surname><given-names>PJ</given-names></name>, <name><surname>Kraft</surname><given-names>RA</given-names></name>, <name><surname>Maldjian</surname><given-names>JA</given-names></name>, <name><surname>Burdette</surname><given-names>JH</given-names></name>, <name><surname>Wallace</surname><given-names>MT</given-names></name> (<year>2004</year>) <article-title>Semantic congruence is a critical factor in multisensory behavioral performance</article-title>. <source>Exp Brain Res</source>
<volume>158</volume>: <fpage>405</fpage>&#x02013;<lpage>414</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-004-1913-2">10.1007/s00221-004-1913-2</ext-link></comment>.<pub-id pub-id-type="pmid">15221173</pub-id></mixed-citation></ref><ref id="pone.0055215-Diederich1"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Diederich</surname><given-names>A</given-names></name>, <name><surname>Colonius</surname><given-names>H</given-names></name> (<year>2009</year>) <article-title>Crossmodal interaction in speeded responses: time window of integration model</article-title>. <source>Prog Brain Res</source>
<volume>174</volume>: <fpage>119</fpage>&#x02013;<lpage>135</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0079&#x02013;6123(09)01311-9">10.1016/S0079&#x02013;6123(09)01311-9</ext-link></comment>.<pub-id pub-id-type="pmid">19477335</pub-id></mixed-citation></ref><ref id="pone.0055215-Welch1"><label>42</label><mixed-citation publication-type="journal">
<name><surname>Welch</surname><given-names>RB</given-names></name>, <name><surname>Warren</surname><given-names>DH</given-names></name> (<year>1980</year>) <article-title>Immediate perceptual response to intersensory discrepancy</article-title>. <source>Psychol Bull</source>
<volume>88</volume>: <fpage>638</fpage>&#x02013;<lpage>667</lpage>.<pub-id pub-id-type="pmid">7003641</pub-id></mixed-citation></ref><ref id="pone.0055215-Hirsch1"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Hirsch</surname><given-names>IJ</given-names></name>, <name><surname>Sherrick</surname><given-names>CE</given-names><suffix>Jr</suffix></name> (<year>1961</year>) <article-title>Perceived order in different sense modalities</article-title>. <source>J Exp Psychol</source>
<volume>62</volume>: <fpage>423</fpage>&#x02013;<lpage>432</lpage>.<pub-id pub-id-type="pmid">13907740</pub-id></mixed-citation></ref><ref id="pone.0055215-Hairston3"><label>44</label><mixed-citation publication-type="journal">
<name><surname>Hairston</surname><given-names>WD</given-names></name>, <name><surname>Hodges</surname><given-names>DA</given-names></name>, <name><surname>Burdette</surname><given-names>JH</given-names></name>, <name><surname>Wallace</surname><given-names>MT</given-names></name> (<year>2006</year>) <article-title>Auditory enhancement of visual temporal order judgment</article-title>. <source>Neuroreport</source>
<volume>17</volume>: <fpage>791</fpage>&#x02013;<lpage>795</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/01.wnr.0000220141.29413.b4">10.1097/01.wnr.0000220141.29413.b4</ext-link></comment>.<pub-id pub-id-type="pmid">16708016</pub-id></mixed-citation></ref><ref id="pone.0055215-Duncan1"><label>45</label><mixed-citation publication-type="journal">
<name><surname>Duncan</surname><given-names>J</given-names></name>, <name><surname>Martens</surname><given-names>S</given-names></name>, <name><surname>Ward</surname><given-names>R</given-names></name> (<year>1997</year>) <article-title>Restricted attentional capacity within but not between sensory modalities</article-title>. <source>Nature</source>
<volume>387</volume>: <fpage>808</fpage>&#x02013;<lpage>810</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/42947">10.1038/42947</ext-link></comment>.<pub-id pub-id-type="pmid">9194561</pub-id></mixed-citation></ref><ref id="pone.0055215-Santangelo1"><label>46</label><mixed-citation publication-type="journal">
<name><surname>Santangelo</surname><given-names>V</given-names></name>, <name><surname>Fagioli</surname><given-names>S</given-names></name>, <name><surname>Macaluso</surname><given-names>E</given-names></name> (<year>2010</year>) <article-title>The costs of monitoring simultaneously two sensory modalities decrease when dividing attention in space</article-title>. <source>Neuroimage</source>
<volume>49</volume>: <fpage>2717</fpage>&#x02013;<lpage>2727</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2009.10.061">10.1016/j.neuroimage.2009.10.061</ext-link></comment>.<pub-id pub-id-type="pmid">19878728</pub-id></mixed-citation></ref><ref id="pone.0055215-Saldaa1"><label>47</label><mixed-citation publication-type="journal">Salda&#x000f1;a E, Feliciano M, Mugnaini E (1996) Distribution of descending projections from primary auditory neocortex to inferior colliculus mimics the topography of intracollicular projections. J Comp Neurol <volume>371</volume>: : 15&#x02013;40. doi:10.1002/(SICI)1096-9861(19960715)371:1&#x0003c;15::AID-CNE2&#x0003e;3.0.CO;2-O.</mixed-citation></ref><ref id="pone.0055215-Huffman1"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Huffman</surname><given-names>RF</given-names></name>, <name><surname>Henson</surname><given-names>OW</given-names><suffix>Jr</suffix></name> (<year>1990</year>) <article-title>The descending auditory pathway and acousticomotor systems: connections with the inferior colliculus</article-title>. <source>Brain Res Brain Res Rev</source>
<volume>15</volume>: <fpage>295</fpage>&#x02013;<lpage>323</lpage>.<pub-id pub-id-type="pmid">2289088</pub-id></mixed-citation></ref><ref id="pone.0055215-Galbraith4"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Galbraith</surname><given-names>GC</given-names></name>, <name><surname>Doan</surname><given-names>BQ</given-names></name> (<year>1995</year>) <article-title>Brainstem frequency-following and behavioral responses during selective attention to pure tone and missing fundamental stimuli</article-title>. <source>Int J Psychophysiol</source>
<volume>19</volume>: <fpage>203</fpage>&#x02013;<lpage>214</lpage>.<pub-id pub-id-type="pmid">7558987</pub-id></mixed-citation></ref><ref id="pone.0055215-Giard1"><label>50</label><mixed-citation publication-type="journal">
<name><surname>Giard</surname><given-names>M-H</given-names></name>, <name><surname>Collet</surname><given-names>L</given-names></name>, <name><surname>Bouchet</surname><given-names>P</given-names></name>, <name><surname>Pernier</surname><given-names>J</given-names></name> (<year>1994</year>) <article-title>Auditory selective attention in the human cochlea</article-title>. <source>Brain Research</source>
<volume>633</volume>: <fpage>353</fpage>&#x02013;<lpage>356</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0006&#x02013;8993(94)91561-X">10.1016/0006&#x02013;8993(94)91561-X</ext-link></comment>.<pub-id pub-id-type="pmid">8137171</pub-id></mixed-citation></ref><ref id="pone.0055215-Maison1"><label>51</label><mixed-citation publication-type="journal">
<name><surname>Maison</surname><given-names>S</given-names></name>, <name><surname>Micheyl</surname><given-names>C</given-names></name>, <name><surname>Collet</surname><given-names>L</given-names></name> (<year>2001</year>) <article-title>Influence of focused auditory attention on cochlear activity in humans</article-title>. <source>Psychophysiology</source>
<volume>38</volume>: <fpage>35</fpage>&#x02013;<lpage>40</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1469&#x02013;8986.3810035">10.1111/1469&#x02013;8986.3810035</ext-link></comment>.<pub-id pub-id-type="pmid">11321619</pub-id></mixed-citation></ref><ref id="pone.0055215-Delano1"><label>52</label><mixed-citation publication-type="journal">
<name><surname>Delano</surname><given-names>PH</given-names></name>, <name><surname>Elgueda</surname><given-names>D</given-names></name>, <name><surname>Hamame</surname><given-names>CM</given-names></name>, <name><surname>Robles</surname><given-names>L</given-names></name> (<year>2007</year>) <article-title>Selective Attention to Visual Stimuli Reduces Cochlear Sensitivity in Chinchillas</article-title>. <source>Journal of Neuroscience</source>
<volume>27</volume>: <fpage>4146</fpage>&#x02013;<lpage>4153</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3702&#x02013;06.2007">10.1523/JNEUROSCI.3702&#x02013;06.2007</ext-link></comment>.<pub-id pub-id-type="pmid">17428992</pub-id></mixed-citation></ref><ref id="pone.0055215-Johnson1"><label>53</label><mixed-citation publication-type="journal">
<name><surname>Johnson</surname><given-names>JA</given-names></name>, <name><surname>Zatorre</surname><given-names>RJ</given-names></name> (<year>2006</year>) <article-title>Neural substrates for dividing and focusing attention between simultaneous auditory and visual events</article-title>. <source>Neuroimage</source>
<volume>31</volume>: <fpage>1673</fpage>&#x02013;<lpage>1681</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2006.02.026">10.1016/j.neuroimage.2006.02.026</ext-link></comment>.<pub-id pub-id-type="pmid">16616520</pub-id></mixed-citation></ref><ref id="pone.0055215-Shomstein1"><label>54</label><mixed-citation publication-type="journal">
<name><surname>Shomstein</surname><given-names>S</given-names></name>, <name><surname>Yantis</surname><given-names>S</given-names></name> (<year>2004</year>) <article-title>Control of attention shifts between vision and audition in human cortex</article-title>. <source>J Neurosci</source>
<volume>24</volume>: <fpage>10702</fpage>&#x02013;<lpage>10706</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2939-04.2004">10.1523/JNEUROSCI.2939-04.2004</ext-link></comment>.<pub-id pub-id-type="pmid">15564587</pub-id></mixed-citation></ref><ref id="pone.0055215-Gregory1"><label>55</label><mixed-citation publication-type="journal">
<name><surname>Gregory</surname><given-names>SD</given-names></name>, <name><surname>Heath</surname><given-names>JA</given-names></name>, <name><surname>Rosenberg</surname><given-names>ME</given-names></name> (<year>1989</year>) <article-title>Does selective attention influence the brain-stem auditory evoked potential?</article-title>
<source>Electroencephalography and Clinical Neurophysiology</source>
<volume>73</volume>: <fpage>557</fpage>&#x02013;<lpage>560</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0013&#x02013;4694(89)90266-6">10.1016/0013&#x02013;4694(89)90266-6</ext-link></comment>.<pub-id pub-id-type="pmid">2480891</pub-id></mixed-citation></ref><ref id="pone.0055215-Connolly1"><label>56</label><mixed-citation publication-type="journal">
<name><surname>Connolly</surname><given-names>JF</given-names></name>, <name><surname>Aubry</surname><given-names>K</given-names></name>, <name><surname>McGillivary</surname><given-names>N</given-names></name>, <name><surname>Scott</surname><given-names>DW</given-names></name> (<year>1989</year>) <article-title>Human Brainstem Auditory Evoked Potentials Fail to Provide Evidence of Efferent Modulation of Auditory Input During Attentional Tasks</article-title>. <source>Psychophysiology</source>
<volume>26</volume>: <fpage>292</fpage>&#x02013;<lpage>303</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1469&#x02013;8986.1989.tb01920.x">10.1111/j.1469&#x02013;8986.1989.tb01920.x</ext-link></comment>.<pub-id pub-id-type="pmid">2756078</pub-id></mixed-citation></ref><ref id="pone.0055215-Hackley1"><label>57</label><mixed-citation publication-type="journal">
<name><surname>Hackley</surname><given-names>SA</given-names></name>, <name><surname>Woldorff</surname><given-names>M</given-names></name>, <name><surname>Hillyard</surname><given-names>SA</given-names></name> (<year>1990</year>) <article-title>Cross-Modal Selective Attention Effects on Retinal, Myogenic, Brainstem, and Cerebral Evoked Potentials</article-title>. <source>Psychophysiology</source>
<volume>27</volume>: <fpage>195</fpage>&#x02013;<lpage>208</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1469&#x02013;8986.1990.tb00370.x">10.1111/j.1469&#x02013;8986.1990.tb00370.x</ext-link></comment>.<pub-id pub-id-type="pmid">2247550</pub-id></mixed-citation></ref><ref id="pone.0055215-Hirschhorn1"><label>58</label><mixed-citation publication-type="journal">
<name><surname>Hirschhorn</surname><given-names>TN</given-names></name>, <name><surname>Michie</surname><given-names>PT</given-names></name> (<year>1990</year>) <article-title>Brainstem Auditory Evoked Potentials (BAEPS) and Selective Attention Revisited</article-title>. <source>Psychophysiology</source>
<volume>27</volume>: <fpage>495</fpage>&#x02013;<lpage>512</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1469&#x02013;8986.1990.tb01964.x">10.1111/j.1469&#x02013;8986.1990.tb01964.x</ext-link></comment>.<pub-id pub-id-type="pmid">2274613</pub-id></mixed-citation></ref><ref id="pone.0055215-Lukas1"><label>59</label><mixed-citation publication-type="journal">
<name><surname>Lukas</surname><given-names>JH</given-names></name> (<year>1981</year>) <article-title>The Role of Efferent Inhibition in Human Auditory Attention: An Examination of the Auditory Brainstem Potentials</article-title>. <source>International Journal of Neuroscience</source>
<volume>12</volume>: <fpage>137</fpage>&#x02013;<lpage>145</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3109/00207458108985796">10.3109/00207458108985796</ext-link></comment>.<pub-id pub-id-type="pmid">7203823</pub-id></mixed-citation></ref><ref id="pone.0055215-Kaiser1"><label>60</label><mixed-citation publication-type="journal">
<name><surname>Kaiser</surname><given-names>J</given-names></name>, <name><surname>Lutzenberger</surname><given-names>W</given-names></name> (<year>2005</year>) <article-title>Human gamma-band activity: a window to cognitive processing</article-title>. <source>Neuroreport</source>
<volume>16</volume>: <fpage>207</fpage>&#x02013;<lpage>211</lpage>.<pub-id pub-id-type="pmid">15706221</pub-id></mixed-citation></ref><ref id="pone.0055215-Womelsdorf1"><label>61</label><mixed-citation publication-type="journal">
<name><surname>Womelsdorf</surname><given-names>T</given-names></name>, <name><surname>Fries</surname><given-names>P</given-names></name> (<year>2007</year>) <article-title>The role of neuronal synchronization in selective attention</article-title>. <source>Curr Opin Neurobiol</source>
<volume>17</volume>: <fpage>154</fpage>&#x02013;<lpage>160</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2007.02.002">10.1016/j.conb.2007.02.002</ext-link></comment>.<pub-id pub-id-type="pmid">17306527</pub-id></mixed-citation></ref><ref id="pone.0055215-Senkowski1"><label>62</label><mixed-citation publication-type="journal">
<name><surname>Senkowski</surname><given-names>D</given-names></name>, <name><surname>Schneider</surname><given-names>TR</given-names></name>, <name><surname>Foxe</surname><given-names>JJ</given-names></name>, <name><surname>Engel</surname><given-names>AK</given-names></name> (<year>2008</year>) <article-title>Crossmodal binding through neural coherence: implications for multisensory processing</article-title>. <source>Trends Neurosci</source>
<volume>31</volume>: <fpage>401</fpage>&#x02013;<lpage>409</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2008.05.002">10.1016/j.tins.2008.05.002</ext-link></comment>.<pub-id pub-id-type="pmid">18602171</pub-id></mixed-citation></ref></ref-list></back></article>