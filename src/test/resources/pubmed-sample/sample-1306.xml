
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31767911</article-id><article-id pub-id-type="pmc">6877555</article-id><article-id pub-id-type="publisher-id">54018</article-id><article-id pub-id-type="doi">10.1038/s41598-019-54018-z</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Evidence for confounding eye movements under attempted fixation and active viewing in cognitive neuroscience</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6264-0367</contrib-id><name><surname>Thielen</surname><given-names>Jordy</given-names></name><address><email>jordy.thielen@donders.ru.nl</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Bosch</surname><given-names>Sander E.</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>van Leeuwen</surname><given-names>Tessa M.</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>van Gerven</surname><given-names>Marcel A. J.</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>van Lier</surname><given-names>Rob</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000000122931605</institution-id><institution-id institution-id-type="GRID">grid.5590.9</institution-id><institution>Radboud University, Donders Centre for Cognition, </institution></institution-wrap>Nijmegen, 6525 HR The Netherlands </aff></contrib-group><pub-date pub-type="epub"><day>25</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>25</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>9</volume><elocation-id>17456</elocation-id><history><date date-type="received"><day>28</day><month>6</month><year>2019</year></date><date date-type="accepted"><day>8</day><month>11</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2019</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Eye movements can have serious confounding effects in cognitive neuroscience experiments. Therefore, participants are commonly asked to fixate. Regardless, participants will make so-called fixational eye movements under attempted fixation, which are thought to be necessary to prevent perceptual fading. Neural changes related to these eye movements could potentially explain previously reported neural decoding and neuroimaging results under attempted fixation. In previous work, under attempted fixation and passive viewing, we found no evidence for systematic eye movements. Here, however, we show that participants&#x02019; eye movements are systematic under attempted fixation when active viewing is demanded by the task. Since eye movements directly affect early visual cortex activity, commonly used for neural decoding, our findings imply alternative explanations for previously reported results in neural decoding.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Perception</kwd><kwd>Human behaviour</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Multivariate pattern analysis (MVPA) has become a standard tool for cognitive neuroscience to investigate the representation of cognitive states in the human brain. Compared to its univariate counterpart, MVPA uses distributed patterns of activity to predict regressors of interest (&#x02018;decoding&#x02019;). As a result, it is more sensitive to weak within-subject trial-by-trial variance<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. From functional magnetic resonance imaging (fMRI) responses, passively perceived and actively attended stimulus orientation can be decoded<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, as well as unconsciously perceived stimulus orientations<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and those actively held in visual working memory<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Similarly, passively perceived and actively attended motion direction can be decoded from fMRI responses<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, as well as actively imagined motion direction<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Finally, passively perceived objects can be decoded from fMRI responses<sup><xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref></sup>, but also objects<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> and scenes<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> that are actively held in working memory. From electrophysiological measures like magnetoencephalography (MEG) and electroencephalography (EEG) perceived orientation<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and perceived motion direction have also been classified<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>.</p><p id="Par3">In all of these studies, and as common practice in cognitive neuroscience in general, participants are asked to fixate on a fixation dot during the experiments. This is considered necessary to reduce any potential confounding role of eye movements. Eye movements cause changes at the neuronal level, since they have a direct effect on early visual cortex due to its retinotopic organization and typically small receptive field sizes. Therefore, a shift in the image caused by an eye movement will be reflected as retinal slip, and subsequently as a shift in the location of activity at the neuronal level. Evidence supporting this direct link between eye movements and neural activity changes comes from a study that showed similar neural changes to artificial image shifts, small voluntary eye movements, and microsaccades<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Additionally, low-level visual areas, frequently used in decoding studies, were sensitive to eye movements, while high-level visual areas remained eye movement invariant<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. On the contrary, in the absence of visual stimuli, stimulus-specific gaze patterns were shown to reactivate representations in higher-level areas<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Apart from these effects, eye movements may also affect brain activity indirectly, as motor plans are prepared for saccades.</p><p id="Par4">Importantly, participants make eye movements even under attempted fixation. These so-called fixational eye movements are small jerk-like movements around the point of fixation and comprise of microsaccades, tremor, and drift<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The presumed role of fixational eye movements is to prevent perceptual fading - under perfect fixation retinal input will remain perfectly stable, causing neural adaptation and hence a fading percept. Hence, fixation during neuroimaging experiments may not exclude confounding effects of systematic eye movements on neural activity.</p><p id="Par5">Such fixational eye movements contaminate neural data in two ways. Firstly, when the eye movements are random by nature, they add additional noise to the data reducing the signal to noise ratio and increasing the false negative rate (type II error). In this case, the right choice of fixation target can improve fixation stability<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Secondly, when the eye movements are systematic (i.e., they covary with the experimental conditions), they cause false positives (type I error). This poses a more serious problem, as one might interpret an effect as brain-related, while it was actually caused by artefacts induced by eye movements, or at least eye movements contributed partially to the effect. It is this second type of eye movements that we investigate in the current study.</p><p id="Par6">Confounding fixational eye movements have already been reported in several MEG decoding studies on visual working memory<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup> and on visual imagery<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. A common aspect of those studies was that participants were required to actively attend to the stimuli. This active involvement might have induced systematic eye movements, but this relation to the type of task was not directly tested in those studies.</p><p id="Par7">Indeed, effects of attention have been reported on both involuntary fixational eye movements and voluntary goal-oriented eye movements. For instance, the direction of spatial attention<sup><xref ref-type="bibr" rid="CR22">22</xref>&#x02013;<xref ref-type="bibr" rid="CR24">24</xref></sup> as well as the direction of perceived apparent motion<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> could be predicted from the direction of microsaccades. Additionally, stronger feature-based<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup> and spatial<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> attention decreased the microsaccade rate, but at the same time the microsaccades became more informative about the locus of attention. It should be noted here that biases in such small amplitude eye movements did not only arise in a stimulus-driven fashion, but also arose when the stimulus was not physically presented but was held in visual working memory<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Furthermore, gaze patterns for the same stimulus varied substantially under varying cognitive goals<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. In line with this finding, eye movements were shown to be directed to the salient parts of the visual scene<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. In sum, these studies show that active task involvement could cause systematic eye movements.</p><p id="Par8">In our previous work, we investigated the potential confounding role of eye movements in a passive task, but we found no evidence for any systematic eye movements under passive viewing<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. The passive fixation task means that participants could have potentially ignored the stimuli. In the current study, we investigate whether active versus passive viewing of stimuli could be an explanation for the occurrence of systematic eye movements. Since many paradigms as used throughout neuroimaging studies rely on active paradigms, this study forms an important follow-up on our previous work, of significance to investigate the potential confounding role under an active task.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><p id="Par9">We conducted an experiment using a within-subject design to investigate within-subject effects of a subtle task manipulation and to exclude the potential role of differences between individual participants. We investigated the role of eye movements in orientation decoding, as this is a pioneering framework in neural decoding and is still used for vision sciences in fMRI, EEG, and MEG research. Importantly, task effects on orientation decoding have not yet been demonstrated.</p><p id="Par10">In the pioneering study on orientation decoding, participants passively perceived eight differently oriented square-wave gratings while they fixated on a fixation dot. The study showed that passively perceived stimulus orientation can be decoded from early visual cortex as measured by fMRI<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p><p id="Par11">In the current study, participants also perceived eight differently oriented square-wave gratings (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>). In contrast to the previous studies<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>, participants performed both a passive session (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>) as well as an active session (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1c</xref>) on separate days in counterbalanced order. In both sessions, participants were instructed to fixate on a fixation dot while their fixational eye movements were recorded. The passive session is a replication of the pioneering work on orientation decoding<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and our previous work<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. The active session was similar to the passive one, except for a subtle task manipulation that forced participants to actively attend to the presented stimulus in the active session, while participants passively viewed the stimuli in the passive session and could thus ignore them. Specifically, in the active condition, the orientation of the presented stimulus was slightly perturbed on 12.5 percent of the trials, upon which participants had to make a button press.<fig id="Fig1"><label>Figure 1</label><caption><p>Eye movements were recorded while participants passively or actively perceived differently oriented square-wave gratings while fixating at a fixation dot. (<bold>a</bold>) The experiment presented eight differently oriented (0 to 180 degrees in steps of 22.5 degrees) square-wave (spatial frequency of 1.5 cycles per visual degree, 100% contrast) gratings (outer radius of 10.0 and inner radius of 1.5 visual degrees) presented on a mean-luminance grey background. Throughout the entire experiment, participants were instructed to maintain fixation at a fixation dot (outer radius of 0.1 visual degrees). (<bold>b</bold>) Each of twelve runs of the passive session started with 8&#x02009;seconds of fixation and finished with 8&#x02009;seconds of fixation. In between, sixteen 7-second trials were presented with 1&#x02009;second inter-trial interval. A trial presented one oriented grating at 2 Hertz (i.e., 250&#x02009;ms &#x02018;on&#x02019;, 250&#x02009;ms &#x02018;off&#x02019;), where each &#x02018;on&#x02019; period presented the grating with a uniform random phase. (<bold>c</bold>) The active session was similar to the passive session, but on 12.5 percent of trials one &#x02018;on&#x02019; period within a trial presented the oriented grating with a 10 degree perturbation (clockwise or counter-clockwise). Participants were instructed to detect these perturbed trials upon which they had to press a button.</p></caption><graphic xlink:href="41598_2019_54018_Fig1_HTML" id="d29e425"/></fig></p><p id="Par12">We analyzed the recorded eye-tracking data and attempted to decode the orientation of the presented stimulus given the eye movement time series (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). The classification accuracies were significantly larger in the active (median&#x02009;=&#x02009;19.2%) than in the passive (median&#x02009;=&#x02009;12.1%) session (<italic>t</italic>&#x02009;=&#x02009;4.0, <italic>n</italic>&#x02009;=&#x02009;28, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, Wilcoxon signed-rank test for two related paired samples). Moreover, the classification accuracies in the passive session (minimum 8.1%, maximum 17.1%) were not significantly different from chance level (<italic>t</italic>&#x02009;=&#x02009;164.0, <italic>n</italic>&#x02009;=&#x02009;28, <italic>p</italic>&#x02009;=&#x02009;0.37, Wilcoxon singed-rank test), while the decoding in the active session (minimum 9.4%, maximum 48.5%) was significantly different from chance level (<italic>t</italic>&#x02009;=&#x02009;4.0, <italic>n</italic>&#x02009;=&#x02009;28, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, Wilcoxon signed-rank test). At the level of the individual participants the differences between the active and passive session were equally clear: in the passive session none of the participants&#x02019; eye movements allowed orientation decoding accuracy higher than chance level (<italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05, permutation test with 1000 permutations), while in the active session this was achieved for sixteen out of twenty-eight participants (<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, permutation test with 1000 permutations). It should be noted here that decoding of the active session was performed without the task trials (i.e., the 12.5 percent of trials containing a perturbed orientation), as these could contain eye movements caused by the orientation perturbation. Finally, there was no effect of session order (<italic>t</italic>&#x02009;=&#x02009;43.0, <italic>n</italic>&#x02009;=&#x02009;14, <italic>p</italic>&#x02009;=&#x02009;0.55, Wilcoxon signed-rank test), so the difference in decoding accuracy between the active and passive session was not different between the participants performing the active session first and subsequently the passive session, and those who did the sessions in reversed order.<fig id="Fig2"><label>Figure 2</label><caption><p>Eye movements are systematic in the active but not the passive session. A linear support vector machine was trained to classify stimulus orientation from eye movements. The classification accuracies (i.e., percentage of correctly classified trials) are shown as distributions (top), box plot (middle), and individual data points (bottom) for both active (green) as well as passive (orange) session. The classification accuracies were significantly larger in the active (median&#x02009;=&#x02009;19.2%) than in the passive (median&#x02009;=&#x02009;12.1%) session (<italic>t</italic>&#x02009;=&#x02009;4.0, <italic>n</italic>&#x02009;=&#x02009;28, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, Wilcoxon signed-rank test for two related paired samples). The gray dashed line illustrates chance level (12.5 percent). For detailed results see main text.</p></caption><graphic xlink:href="41598_2019_54018_Fig2_HTML" id="d29e492"/></fig></p><p id="Par13">Considering that the task manipulation drives the difference between the active and passive session, one might expect that those participants who more actively monitor the stimulus will show more systematic eye movements, which in turn will increase the classification accuracy. In general, participants performed the task well in the active condition (average 93.5%, minimum 83.9%, maximum 98.4%). To test the effect of task performance on systematic eye movements, we computed the Spearman correlation coefficient between the task accuracies and classification accuracies. There was no significant correlation between the task performance and decoding performance (<italic>&#x003c1;</italic>(26)&#x02009;=&#x02009;0.08, <italic>p</italic>&#x02009;=&#x02009;0.70, 95% CI [&#x02212;0.32, 0.45]). Since the task trials were infrequent, we also correlated the sensitivity (<italic>&#x003c1;</italic>(26)&#x02009;=&#x02009;0.13, <italic>p</italic>&#x02009;=&#x02009;0.51, 95% CI [&#x02212;0.27, 0.49])) and specificity (<italic>&#x003c1;</italic>(26)&#x02009;=&#x02009;&#x02212;0.26, <italic>p</italic>&#x02009;=&#x02009;0.57, 95% CI [&#x02212;0.48, 0.29]), but both resulted in non-significant correlations.</p><p id="Par14">As an additional analysis, we inspected the average eye movement amplitude. First, the eye movement amplitude in the passive session (median 0.2, minimum 0.1, maximum 0.9 visual degrees) was significantly different (<italic>t</italic>&#x02009;=&#x02009;11.0, <italic>n</italic>&#x02009;=&#x02009;28, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, Wilcoxon signed-rank test) than the amplitude in the active session (median 0.3, minimum 0.1, maximum 2.9 visual degrees). The eye movement amplitude did not correlate with task accuracy (<italic>&#x003c1;</italic>(26)&#x02009;=&#x02009;&#x02212;0.13, <italic>p</italic>&#x02009;=&#x02009;0.51, 95% CI [&#x02212;0.49, 0.27]), sensitivity (<italic>&#x003c1;</italic>(26)&#x02009;=&#x02009;&#x02212;0.17, <italic>p</italic>&#x02009;=&#x02009;0.38, 95% CI [&#x02212;0.52, 0.23]), or specificity (<italic>&#x003c1;</italic>(26)&#x02009;=&#x02009;0.13, <italic>p</italic>&#x02009;=&#x02009;0.52, 95% CI [&#x02212;0.27, 0.49]). However, eye movement amplitude did significantly correlate with classification accuracy (<italic>&#x003c1;</italic>(26)&#x02009;=&#x02009;0.51, <italic>p</italic>&#x02009;=&#x02009;0.01, 95% CI [0.15, 0.75]), so that larger eye movements resulted in higher classification accuracies.</p><p id="Par15">In line with the positive correlation of eye movement amplitude with classification accuracy, we conducted a decoding analysis while removing trials that were contaminated with saccades. The detection of saccades was performed in two ways. In the first analysis, we removed those trials in which the eye position was further away from fixation than a certain threshold (i.e., a circle around fixation beyond which trials would be removed). Note that, with this approach participants could still make saccades within the boundaries. In the second analysis, we removed those trials in which the eye movement velocity exceeded a certain threshold (i.e., a saccade was made). The results are shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> and clearly show that for certain thresholds it is possible to remove trials in which participants did not fixate well, which in turn makes the decoding effect disappear in the active session. Note that for some participants most if not all trials had to be removed for certain thresholds, which makes decoding impossible at all because of the few remaining datapoints.<fig id="Fig3"><label>Figure 3</label><caption><p>Removing trials with large saccades reduces the confounding effect of eye movements. Two ways of detecting deviating eye movements are shown. In the first method (<bold>a</bold>,<bold>b</bold>) those trials are removed in which the eye was a certain degrees of visual angle (DVA) away from the fixation point. In the second method (<bold>c</bold>,<bold>d</bold>) those trials are removed in which at least one saccade is made. Saccades are detected by thresholding the eye movement velocity. By increasing the thresholds, more trials stay in the analysis that might contain systematic eye movements (<bold>a</bold>,<bold>c</bold>), which in turn increases classification accuracy in the active condition (<bold>b</bold>,<bold>d</bold>). An asterisk denotes a significant difference between the active session and the passive session (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.05, Bonferroni corrected). Solid lines show the session-specific median values and dots represent individual datapoints. The gray dashed line represents chance level decoding (i.e., 12.5 percent). Note, a classification accuracy of &#x02212;5% denotes that too many trials were removed to be able to perform decoding. Additionally, the untreshholded results are shown at the end marked as &#x02018;none&#x02019; in all figures, which are the results from the original analysis as in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.</p></caption><graphic xlink:href="41598_2019_54018_Fig3_HTML" id="d29e594"/></fig></p></sec><sec id="Sec3" sec-type="discussion"><title>Discussion</title><p id="Par16">Thus, stimulus-orientation could not be decoded from eye movements in the passive session, but importantly, they could be decoded from eye movements in the active session for the same participants. This implies that stimulus-dependent eye movements arise when participants are actively involved in monitoring the stimuli as compared to a passive session. The absence of orientation decoding in the passive session is in line with our previous study<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Thus, under active viewing but not under passive viewing, orientation decoding is contaminated by eye movements.</p><p id="Par17">We suggest several hypotheses as to why such stimulus-dependent eye movements might emerge. First, the active task might be best solvable at certain locations of the stimulus. For oriented gratings the difference in orientations is the largest at the outer edges of the stimulus along the grating direction. Therefore, participants might have made goal-oriented voluntary saccades along the stimulus orientation to optimize task performance even though they were instructed to fixate throughout the experiment. Second, participants might make subtle eye movements like microsaccades in the direction that changes the retinal input the most to optimize prevention of neuronal adaptation. In the case of gratings, this is orthogonal to the orientation of the grating. It should be noted however, that such microsaccades would happen in both passive as well as active session. Still, the magnitude of these might interact with session type. Third, the spatial phase of the oriented grating is randomized over individual presentations during a trial. This is done to prevent individual pixels of being descriptive for orientations, but may introduce an illusory percept of the grating moving over the screen orthogonal to its orientation. This percept of motion might in turn cause systematic eye movements orthogonal to the orientation of the grating, since the direction of perceived apparent motion is related to the direction of microsaccades<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Also here, one should note that these type of eye movements would emerge in both sessions, but might again interact with session type. Finally, so far we have discussed eye movements as the cause for brain activity, but it might as well be the other way around. Specifically, neural activity might be elicited by the low-level stimulus properties &#x02013; like orientation that can be decoded &#x02013; and might in turn be the cause of stimulus-dependent eye movements rather than its consequence.</p><p id="Par18">We inspected the eye movement patterns and found that those participants that showed largely decodable eye movements (i.e., a classification accuracy higher than 30%) showed large eye movements along the direction of the presented stimulus (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). Those with moderately decodable eye movements showed smaller biases, but still in the direction of the orientation. This might suggest that participants adopted an optimal strategy to solve the task in the active session, even though they were told to fixate. However, as mentioned before, there was a correlation between classification accuracy and eye movement amplitude, but no correlation between classification accuracy and task accuracy. This observation makes it less likely that the strategy of solving the task had an influence on the systematicity of the eye movements. Hence, future research is needed to further investigate how and why these eye movements emerge.<fig id="Fig4"><label>Figure 4</label><caption><p>Eye movements show clear systematic patterns only for the active condition in some participants. (<bold>a</bold>) The top row shows the eye position over the course of a trial in the active session. (<bold>b</bold>) The bottom row shows the eye position over the course of a trial in the passive session. Colours represent trials belonging to one of eight orientations, and are those as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>. The solid lines represent averages over repeated presentations of the same orientation. The dashed line represents the inner annulus of the square-wave gratings, which had a radius of 1.5 visual degrees. Additionally, averages are computed for those participants with higher than 30% classification accuracy in the active session (N&#x02009;=&#x02009;4) shown in the first column, those with lower than 30% classification accuracy but still significant decoding in the active session (N&#x02009;=&#x02009;12) in the middle column, and those that were not decodable in the active session (N&#x02009;=&#x02009;12) in the right column. Note that the passive session was never decodable, but for comparison the same groups are shown. This figure illustrates that for some participants there is a clear pattern in their eye movements, while for others this pattern is less clear. Additionally, these patterns remain completely absent in the passive sessions, even for those showing large patterns in the active session.</p></caption><graphic xlink:href="41598_2019_54018_Fig4_HTML" id="d29e632"/></fig></p><p id="Par19">Given the large eye movements as shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> and the fact that it was possible to reduce the confounding effect by removing trials that contain saccades, we believe large (maybe voluntary) eye movements under attempted fixation drive the reported effect. Still, we cannot exclude the possibility that microsaccades also contribute to the classification accuracy. We detected microsaccades using the methodology as described by Engbert<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, but found no evidence for direction or amplitude of microsaccades to covary with stimulus orientation.</p><p id="Par20">Our results show that only a subset of participants showed eye movements that covary with stimulus orientation. This raises an important question about what could explain the variability in classification accuracy across participants. Again, task performance did not explain these differences. Still, the strategy used to solve the task and the (in)ability to fixate sufficiently, as well as other individual differences might explain the differences. Unfortunately, the present experiments preclude further investigation of these factors. Additionally, although we have shown the emergence of confounding eye movements using only a subtle task manipulation, future research is needed to test other stimulus types (e.g., kinematograms or natural scenes), stimulus characteristics (e.g., duration or contrast), or paradigms (e.g., a working memory paradigm).</p><p id="Par21">We propose several ways to deal with confounding effects of eye movements in neuroimaging studies. We showed that the ability to decode stimulus orientation decreased when removing trials with fixation away from the fixation point or removing trials that contained saccades. First, one could detect such trials in real-time as a measure of how well participants fixate and have them redo a trial when fixation was not stable enough. Second, one could simply remove these trials from post-hoc analysis, but this might result in losing large parts of the data. Do note that for these two options to work, a suitable detection threshold needs to be found, which might be subject and task dependent. Third, one could use the eye movement patterns as nuisance regressors in the analysis. However, one should be careful still, as such regression would only remove the linear effects of eye movements, and would ignore other processes involved in for instance saccade planning. Finally, in reporting results, one could measure the mutual information in the brain activity over eye movements, as was carried out by Quax and colleagues<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>.</p><p id="Par22">In conclusion, based on the current study, we advise researchers to monitor eye movements always by recording and extensively analyzing eye tracking data alongside the brain activity. Within EEG and MEG studies, it is shown that recording the electrooculogram (EOG) is not sufficient to deal with confounding eye movements<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Fixational eye movements might be easily missed and should be analyzed extensively (post-hoc) to investigate the potential confounding effects of eye movements. In general, studies in neural decoding should be aware of the potential confounding role of eye movements under active viewing conditions. Our results suggest that these eye movements are uncontrollable in the sense that they play a functional role in task execution. To what extent specific stimulus paradigms and tasks used throughout cognitive neuroscience are affected and to what degree this affects previously drawn conclusions remains to be analyzed in future research.</p></sec><sec id="Sec4"><title>Methods</title><sec id="Sec5"><title>Participants</title><p id="Par23">Twenty-nine university students (aged 19&#x02013;31; 22 females) from the Radboud University participated in the experiment. Inclusion criteria were age (18&#x02013;31), handedness (right), and vision capabilities (uncorrected, normal). Exclusion criteria were any history with epilepsy or claustrophobia. All participants gave written informed consent prior to the experiment and received payment or course credit after the experiment. The experimental procedure and methods were approved by and performed in accordance with the guidelines of the local ethical committee of the Faculty of Social Sciences of the Radboud University. Participant 15 did not complete the full experimental design and was therefore left out of the analysis.</p></sec><sec id="Sec6"><title>Materials</title><p id="Par24">The stimuli were full contrast (100 percent) black-white square-wave gratings of eight possible orientations ranging from 0 to 180 degrees in steps of 22.5 degrees. The spatial frequency was 1.5 cycles per degree of visual angle and the spatial phase was randomized every presentation. The stimuli had an outer radius of 10.0 degrees and an inner radius of 1.5 degrees of visual angle and were presented at the center of a mean-luminance grey background. A fixation dot was presented at the center of the screen with an outer radius of 0.1 degree of visual angle and an inner radius of 0.075 degree of visual angle.</p><p id="Par25">The experiment was run on a Windows 10 desktop PC running Python version 2.7.14 and the PsychoPy library, version 1.90.2. The stimuli were presented on a 24-inch BenQ XL2420Z monitor with a 60 Hertz frame rate and 1920&#x02009;&#x000d7;&#x02009;1080 pixel resolution. The monitor subtended 39.1 degrees of visual angle horizontally and 22.0 degrees of visual angle vertically. An EyeLink 1000 Plus (SR Research, Ltd.) desktop-mounted eye tracker was used to record binocular eye positions and pupil dilation with the 35-millimeter lens at a sample rate of 1000 Hertz. The eye tracker was positioned just below and in front of the monitor at a distance of 55 centimeters from the participant&#x02019;s eyes. The participant&#x02019;s head position and viewing distance were fixed at 65 centimeters from the monitor with a chin and forehead rest.</p><p id="Par26">Preprocessing and data analyses were performed using Python 3.7.1. Custom analysis pipelines were made using the NumPy library (version 1.15.4) and SciPy library (version 1.1.0) for scientific computing, and the Scikit-Learn library (version 0.20.1) for machine learning.</p></sec><sec id="Sec7"><title>Procedure</title><p id="Par27">The experimental paradigm was similar to that of Kamitani and Tong<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and Thielen and colleagues<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, though some adaptations were made to incorporate the active session. The experiment contained two sessions, which were run on separate days. Participants with an odd participant number completed the active session in the first session and the passive session in the second session, while participants with an even participant number completed the two sessions in reverse order.</p><p id="Par28">Both sessions contained twelve runs, each presenting all eight orientations twice for a total of sixteen trials per run. A run was initiated by a button press and started as well as ended with 8&#x02009;seconds of fixation during which only the fixation dot was presented at the center of the screen. In between these fixation periods, the sixteen trials were presented sequentially in random order with an inter-trial interval of 1&#x02009;second. Throughout the inter-trial interval, only the fixation dot was presented. In each trial, one of eight stimuli was presented at 2 Hertz (on/off for 250 milliseconds) for 7&#x02009;seconds. During the&#x02019;on&#x02019; period the stimulus was presented with a random phase together with the fixation dot, while during the&#x02019;off&#x02019; period only the fixation dot was presented.</p><p id="Par29">The active session differed slightly from the passive session. In the active session, 12.5 percent of the trials were used as task trials. In a task trial the orientation of the stimulus was perturbed for one &#x02018;on&#x02019; period (250 milliseconds) in between 1 to 6&#x02009;seconds of the trial. The perturbation was always 10 degrees and could be clock-wise or counter clock-wise with equal probability. Task trials occurred pseudo-randomly so that there were three task trials for each orientation in the entire session, randomly allocated to runs. In this way, both the timing during a trial, as well as the number of task trials within runs seemed random to the participant. Participants had to push a button with their right index finger upon detection of a deviant stimulus.</p><p id="Par30">Prior to a session, participants were told to fixate throughout runs regardless of the session type. Participants who performed the passive session first, were prior to the active session told that now there was a second task on top of fixation, namely detecting the deviant stimuli. Participants who performed the active session first, were told prior to the first session that there are two tasks being fixation and detection of the deviants, and were told prior to the passive session that the deviant stimuli will not happen anymore so that only the fixation task was left. In both sessions, participants were shown an example run, which included a task trial in the active session, so that they knew what to expect.</p></sec><sec id="Sec8"><title>Preprocessing</title><p id="Par31">The binocular eye positions and pupil dilation recordings were acquired at a sampling rate of 1000 Hertz and saved for offline analysis. Missing values caused by eye blinks were replaced by median values (i.e., the fixation point). Subsequently, the data were low-pass filtered using a Butterworth filter of order 5 and a cutoff frequency at 100 Hertz. The data were then down sampled to 256 Hertz. Finally, the data were sliced to individual trials of 7&#x02009;seconds locked to the onset of the first stimulus presentation in a trial. Medians were subtracted from individual runs to center the data around the fixation point and remove any biases within runs.</p></sec><sec id="Sec9"><title>Analysis</title><p id="Par32">We carried out a decoding analysis with the attempt to identify the presented stimulus from the eye movement data from the active and passive session separately. For this, we computed the averages and standard deviations of the recorded binocular Cartesian eye positions and pupil dilation, yielding 12 features for each trial. We removed the task trials in the active session for a total of 168 remaining trials in the active session (i.e., 21 trials per orientation) and 192 trials in the passive session (i.e., 24 trials per orientation).</p><p id="Par33">We used 10-fold stratified cross-validation to estimate a generalization performance. For each fold, training and validation data were normalized by removing the median and by scaling according to the inter-quartile range fitted on the training data. Subsequently, a linear support vector machine (SVM) with regularization parameter <italic>C</italic>&#x02009;=&#x02009;0.1 was trained on the training data and applied to the validation data to estimate the classification accuracy. We computed the generalization performance as the average classification accuracy over folds.</p><p id="Par34">Statistical significance within participants was estimated using a permutation test. For this, the distribution under the null was estimated by running the decoding pipeline 1000 times with permuted labels. Classification was considered significantly above chance level if it was above 95% of the null distribution.</p><p id="Par35">Statistical significance between the two sessions (active versus passive) was estimated using the Wilcoxon signed-rank test for two related paired samples. Statistical significance within a session (active or passive) was assessed using the Wilcoxon signed-rank test with difference scores with chance level (12.5%). We used an alpha threshold of 5% to consider statistical significance.</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We gratefully acknowledge the help of Tom Willems in the pilot version of this project.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.T., S.B., T.L., M.G. and R.L. conceived the project, designed the experiments, and wrote the paper. J.T. performed the experiment and collected and analyzed the data.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All raw data and analysis scripts are made publicly available at the Donders Data Repository (<ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11633/aacubhf3">http://hdl.handle.net/11633/aacubhf3</ext-link>).</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par36">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Polyn</surname><given-names>SM</given-names></name><name><surname>Detre</surname><given-names>GJ</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title><source>Trends Cogn. Sci.</source><year>2006</year><volume>10</volume><fpage>424</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id><pub-id pub-id-type="pmid">16899397</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Decoding the visual and subjective contents of the human brain</article-title><source>Nat. Neurosci.</source><year>2005</year><volume>8</volume><fpage>679</fpage><pub-id pub-id-type="doi">10.1038/nn1444</pub-id><pub-id pub-id-type="pmid">15852014</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>J-D</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><article-title>Predicting the orientation of invisible stimuli from activity in human primary visual cortex</article-title><source>Nat. Neurosci.</source><year>2005</year><volume>8</volume><fpage>686</fpage><pub-id pub-id-type="doi">10.1038/nn1445</pub-id><pub-id pub-id-type="pmid">15852013</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nat</source><year>2009</year><volume>458</volume><fpage>632</fpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Decoding seen and attended motion directions from activity in the human visual cortex</article-title><source>Curr. Biol.</source><year>2006</year><volume>16</volume><fpage>1096</fpage><lpage>1102</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.04.003</pub-id><pub-id pub-id-type="pmid">16753563</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emmerling</surname><given-names>TC</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Sorger</surname><given-names>B</given-names></name><name><surname>Frost</surname><given-names>MA</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><article-title>Decoding the direction of imagined visual motion using 7 T ultra-high field fMRI</article-title><source>NeuroImage</source><year>2016</year><volume>125</volume><fpage>61</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.022</pub-id><pub-id pub-id-type="pmid">26481673</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><etal/></person-group><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Sci</source><year>2001</year><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>Savoy</surname><given-names>RL</given-names></name></person-group><article-title>Functional magnetic resonance imaging (fMRI)&#x0201c;brain reading&#x0201d;: detecting and classifying distributed patterns of fMRI activity in human visual cortex</article-title><source>NeuroImage</source><year>2003</year><volume>19</volume><fpage>261</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00049-1</pub-id><pub-id pub-id-type="pmid">12814577</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eger</surname><given-names>E</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name><name><surname>Haynes</surname><given-names>J</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><article-title>Functional magnetic resonance imaging activity patterns in human lateral occipital complex carry information about object exemplars within category</article-title><source>J. Cogn. Neurosci.</source><year>2008</year><volume>20</volume><fpage>356</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20019</pub-id><pub-id pub-id-type="pmid">18275340</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bettencourt</surname><given-names>KC</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title><source>Nat. Neurosci.</source><year>2016</year><volume>19</volume><fpage>150</fpage><pub-id pub-id-type="doi">10.1038/nn.4174</pub-id><pub-id pub-id-type="pmid">26595654</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Stansbury</surname><given-names>DE</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</article-title><source>NeuroImage</source><year>2015</year><volume>105</volume><fpage>215</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.018</pub-id><pub-id pub-id-type="pmid">25451480</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Ramirez</surname><given-names>FM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><article-title>Can visual information encoded in cortical columns be decoded from magnetoencephalography data in humans?</article-title><source>NeuroImage</source><year>2015</year><volume>121</volume><fpage>193</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.07.011</pub-id><pub-id pub-id-type="pmid">26162550</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>G-Y</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><article-title>Decoding motion direction using the topography of sustained ERPs and alpha oscillations</article-title><source>NeuroImage</source><year>2019</year><volume>184</volume><fpage>242</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.029</pub-id><pub-id pub-id-type="pmid">30223063</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tse</surname><given-names>PU</given-names></name><name><surname>Baumgartner</surname><given-names>FJ</given-names></name><name><surname>Greenlee</surname><given-names>MW</given-names></name></person-group><article-title>Event-related functional MRI of cortical activity evoked by microsaccades, small visually-guided saccades, and eyeblinks in human visual cortex</article-title><source>NeuroImage</source><year>2010</year><volume>49</volume><fpage>805</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.07.052</pub-id><pub-id pub-id-type="pmid">19646539</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Bilenko</surname><given-names>NY</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Eye movement-invariant representations in the human visual system</article-title><source>J. Vis.</source><year>2017</year><volume>17</volume><fpage>11</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1167/17.1.11</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Wang, L., Baumgartner, F., Kaule, F. R., Hanke, M. &#x00026; Pollmann, S. Individual face and house-related eye movement patterns distinctively activate FFA and PPA in the absence of faces and houses (2019).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Conde</surname><given-names>S</given-names></name><name><surname>Macknik</surname><given-names>SL</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><article-title>The role of fixational eye movements in visual perception</article-title><source>Nat. Rev. Neurosci.</source><year>2004</year><volume>5</volume><fpage>229</fpage><pub-id pub-id-type="doi">10.1038/nrn1348</pub-id><pub-id pub-id-type="pmid">14976522</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thaler</surname><given-names>L</given-names></name><name><surname>Sch&#x000fc;tz</surname><given-names>AC</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>What is the best fixation target? The effect of target shape on stability of fixational eye movements</article-title><source>Vis. Res.</source><year>2013</year><volume>76</volume><fpage>31</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.10.012</pub-id><pub-id pub-id-type="pmid">23099046</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Mostert, P. <italic>et al</italic>. Eye movement-related confounds in neural decoding of visual working memory representations. <italic>eNeuro</italic><bold>5</bold> (2018).</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quax</surname><given-names>SC</given-names></name><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>van Staveren</surname><given-names>MJ</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name></person-group><article-title>Eye movements explain decodability during perception and cued attention in MEG</article-title><source>NeuroImage</source><year>2019</year><volume>195</volume><fpage>444</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.069</pub-id><pub-id pub-id-type="pmid">30951848</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Bosch</surname><given-names>S</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name></person-group><article-title>Differential temporal dynamics during visual imagery and perception</article-title><source>eLife</source><year>2018</year><volume>7</volume><fpage>e33904</fpage><pub-id pub-id-type="doi">10.7554/eLife.33904</pub-id><pub-id pub-id-type="pmid">29807570</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafed</surname><given-names>ZM</given-names></name><name><surname>Clark</surname><given-names>JJ</given-names></name></person-group><article-title>Microsaccades as an overt measure of covert attention shifts</article-title><source>Vis. Res.</source><year>2002</year><volume>42</volume><fpage>2533</fpage><lpage>2545</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(02)00263-8</pub-id><pub-id pub-id-type="pmid">12445847</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><article-title>Microsaccades uncover the orientation of covert attention</article-title><source>Vis. Res.</source><year>2003</year><volume>43</volume><fpage>1035</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(03)00084-1</pub-id><pub-id pub-id-type="pmid">12676246</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pastukhov</surname><given-names>A</given-names></name><name><surname>Braun</surname><given-names>J</given-names></name></person-group><article-title>Rare but precious: microsaccades are highly informative about attentional allocation</article-title><source>Vis. Res.</source><year>2010</year><volume>50</volume><fpage>1173</fpage><lpage>1184</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.04.007</pub-id><pub-id pub-id-type="pmid">20382176</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laubrock</surname><given-names>J</given-names></name><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><article-title>Fixational eye movements predict the perceived direction of ambiguous apparent motion</article-title><source>J. Vis.</source><year>2008</year><volume>8</volume><fpage>13</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1167/8.14.13</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Olmos-Solis, K., van Loon, A. M., Los, S. A. &#x00026; Olivers, C. N. Oculomotor measures reveal the temporal dynamics of preparing for search. In <italic>Progress in Brain Research</italic>, vol. 236, 1&#x02013;23 (Elsevier, 2017).</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Loon</surname><given-names>AM</given-names></name><name><surname>Olmos-Solis</surname><given-names>K</given-names></name><name><surname>Olivers</surname><given-names>CN</given-names></name></person-group><article-title>Subtle eye movement metrics reveal task-relevant representations prior to visual search</article-title><source>J. Vis.</source><year>2017</year><volume>17</volume><fpage>13</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1167/17.6.13</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">van Ede, F., Chekroud, S. R. &#x00026; Nobre, A. C. Human gaze tracks attentional focusing in memorized visual space. <italic>Nat. Hum. Behav</italic>. <bold>1</bold> (2019).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Yarbus, A. L. <italic>Eye movements and vision</italic> (Springer, 2013).</mixed-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><article-title>A saliency-based search mechanism for overt and covert shifts of visual attention</article-title><source>Vis. Res.</source><year>2000</year><volume>40</volume><fpage>1489</fpage><lpage>1506</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(99)00163-7</pub-id><pub-id pub-id-type="pmid">10788654</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tatler</surname><given-names>BW</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name><name><surname>Land</surname><given-names>MF</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><article-title>Eye guidance in natural vision: Reinterpreting salience</article-title><source>J. Vis.</source><year>2011</year><volume>11</volume><fpage>5</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1167/11.5.5</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thielen</surname><given-names>J</given-names></name><name><surname>van Lier</surname><given-names>R</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name></person-group><article-title>No evidence for confounding orientation-dependent fixational eye movements under baseline conditions</article-title><source>Sci. Reports</source><year>2018</year><volume>8</volume><fpage>11644</fpage><pub-id pub-id-type="doi">10.1038/s41598-018-30221-2</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name></person-group><article-title>Microsaccades: A microcosm for research on oculomotor control, attention, and visual perception</article-title><source>Prog. brain research</source><year>2006</year><volume>154</volume><fpage>177</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)54009-9</pub-id><pub-id pub-id-type="pmid">17010710</pub-id></element-citation></ref></ref-list></back></article>