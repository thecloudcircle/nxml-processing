
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28559576</article-id><article-id pub-id-type="pmc">5449410</article-id><article-id pub-id-type="publisher-id">2471</article-id><article-id pub-id-type="doi">10.1038/s41598-017-02471-z</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Working Memory Requires a Combination of Transient and Attractor-Dominated Dynamics to Process Unreliably Timed Inputs</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4369-2305</contrib-id><name><surname>Nachstedt</surname><given-names>Timo</given-names></name><address><email>timo.nachstedt@phys.uni-goettingen.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Tetzlaff</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2364 4210</institution-id><institution-id institution-id-type="GRID">grid.7450.6</institution-id><institution>Third Institute of Physics, </institution><institution>Universit&#x000e4;t G&#x000f6;ttingen, </institution></institution-wrap>37077, G&#x000f6;ttingen Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.455091.c</institution-id><institution/><institution>Bernstein Center for Computational Neuroscience, </institution></institution-wrap>37077 G&#x000f6;ttingen, Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0491 5187</institution-id><institution-id institution-id-type="GRID">grid.419514.c</institution-id><institution/><institution>Max Planck Institute for Dynamics and Self-Organization, </institution></institution-wrap>37077 G&#x000f6;ttingen, Germany </aff></contrib-group><pub-date pub-type="epub"><day>30</day><month>5</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>30</day><month>5</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>7</volume><elocation-id>2473</elocation-id><history><date date-type="received"><day>29</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>11</day><month>4</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p>Working memory stores and processes information received as a stream of continuously incoming stimuli. This requires accurate sequencing and it remains puzzling how this can be reliably achieved by the neuronal system as our perceptual inputs show a high degree of temporal variability. One hypothesis is that accurate timing is achieved by purely transient neuronal dynamics; by contrast a second hypothesis states that the underlying network dynamics are dominated by attractor states. In this study, we resolve this contradiction by theoretically investigating the performance of the system using stimuli with differently accurate timing. Interestingly, only the combination of attractor and transient dynamics enables the network to perform with a low error rate. Further analysis reveals that the transient dynamics of the system are used to process information, while the attractor states store it. The interaction between both types of dynamics yields experimentally testable predictions and we show that this way the system can reliably interact with a timing-unreliable Hebbian-network representing long-term memory. Thus, this study provides a potential solution to the long-standing problem of the basic neuronal dynamics underlying working memory.</p></abstract><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2017</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p>Humans and animals continuously receive information conveyed by stimuli from the environment. To survive, the brain has to store and process this stream of information which is mainly attributed to the processes of working memory (WM<sup><xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref></sup>). These two distinct abilities of WM, to store and to process information, yield a debate about the underlying neuronal network dynamics<sup><xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref></sup>: the network dynamics might either follow (i) attractor or (ii) transient dynamics.</p><p>
<italic>Attractor dynamics</italic> denotes neuronal network dynamics which is dominated by groups of neurons being persistently active. In general, such a persistent activation is related to an attractor state of the dynamics, with each attractor associated to a specific information content<sup><xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR6">6</xref>&#x02013;<xref ref-type="bibr" rid="CR8">8</xref></sup>. Several experimental and theoretical studies hypothesize that the dynamics underlying WM are dominated by such persistent dynamics<sup><xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR8">8</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref></sup>. In contrast to attractor dynamics, neuronal networks with <italic>transient dynamics</italic> are dominated by an attractor-less continuous flow of neuronal activity across a possibly large neuronal population<sup><xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup>. This type of dynamics implies a high diversity and complexity which is linked by theoretical studies with a large computational capacity required to process information<sup><xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref></sup>. These theoretical studies as well as several pieces of experimental evidence<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup> yield the hypothesis that the dynamics underlying WM are dominated by transient dynamics<sup><xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref></sup>. Thus, although the two hypotheses &#x02013; attractor or transient dynamics &#x02013; seem to contradict each other, experimental and theoretical evidence supports both yielding a debate about the neuronal network dynamics underlying WM<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>.</p><p>To resolve this contradiction, in this study, we consider the fact that the timing of stimuli received by the WM is highly unreliable. In other words, when interacting with the environment, the WM of humans and animals evidently cannot rely on receiving precisely timed stimuli. For instance, listening to spoken language requires the ability to deal with different and irregular speech rates. The influence of such variance in the stimuli timing on the WM operation has been mainly analyzed on the psychological level<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> using, amongst others, the so-called <italic>N</italic>-back task. In this task a subject is exposed to a stream of different stimuli<sup><xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref></sup>. Whenever a new stimulus is presented, the subject has to execute an action which depends on the stimulus presented <italic>N</italic> stimuli before. Therefore, in order to succeed in this task, the subject has to store the information of the last <italic>N</italic> stimuli in its WM. Dependent on the timing of the stimuli, this information has to be continuously updated. Interestingly, whether the stimuli are presented with exact inter-stimulus timing or with unreliable timing <italic>does not</italic> influence the subject&#x02019;s performance of solving the <italic>N</italic>-back task<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. This result indicates that the mechanisms implementing WM are robust against variance in the timing of the input stimuli. Based on this experimentally found property of WM, in this study, we investigate under which conditions the dynamics of the neuronal networks underlying WM is able to perform an <italic>N</italic>-back task with the same robustness with respect to variances in the stimuli timing.</p><p>First, we investigate a theoretical neuronal network model of WM showing purely transient dynamics &#x02013; a so called reservoir network<sup><xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref></sup> &#x02013; and test its performance on the <italic>N</italic>-back task. Interestingly, with small variations of the timing of the inputs, such a purely transient system exhibits a very poor performance (Figs&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>). In the next step, we show that the performance of the network increases significantly if the system is directly trained in a supervised manner to maintain the relevant information (Figs&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>). A further analysis reveals that the underlying neuronal dynamics of the trained system are dominated by attractor states which are interlinked by regions of transient dynamics. By comparing these combined dynamics with the dynamics of the purely transient system during performing the <italic>N</italic>-back task, we demonstrate that only this combination of attractor and transient dynamics allows the execution of the task robust against variances in stimuli timings (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>). In addition, we show that, in general, the attractor states store the task-relevant information while the transient dynamics processes the information (Figs&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig6" ref-type="fig">6</xref>). This yields the prediction that a drop in performance resulting from an additional delay between the current stimulus and the execution of the action can be avoided by introducing another stimulus pushing the system into a transient state (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>Setup of the benchmark <italic>N</italic>-back task to test the capability of transient networks to cope with variances in the input timings. The input signal is composed of smooth either positive or negative pulses separated by time intervals &#x00394;<italic>t</italic>
<sub><italic>i</italic></sub> drawn from a normal distribution with mean <italic>&#x003bc;</italic>
<sub>&#x00394;<italic>t</italic></sub> and variance <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{{\rm{\Delta }}t}^{2}$$\end{document}</tex-math><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq1.gif"/></alternatives></inline-formula>. It is projected into the generator network via a synaptic weight matrix <italic>W</italic>
<sup>GI</sup> with elements <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ik}^{GI}$$\end{document}</tex-math><mml:math id="M4"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq2.gif"/></alternatives></inline-formula> drawn from a normal distribution with zero mean and variance <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GI}}}^{2}$$\end{document}</tex-math><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GI</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq3.gif"/></alternatives></inline-formula>. The task is to produce an output pulse of defined shape (at the readout neurons) when a new input pulse is presented. The sign of the output pulse depends on the second last input pulse (compare arrows). The readout weight matrix <italic>W</italic>
<sup>RG</sup> is adapted during learning (red). The resulting readout signal is fed back into the network with a weight matrix <italic>W</italic>
<sup>GR</sup> with elements <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{il}^{GR}$$\end{document}</tex-math><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq4.gif"/></alternatives></inline-formula> drawn from a normal distribution with zero mean and variance <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GR}}}^{2}$$\end{document}</tex-math><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GR</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq5.gif"/></alternatives></inline-formula>.</p></caption><graphic xlink:href="41598_2017_2471_Fig1_HTML" id="d29e507"/></fig>
<fig id="Fig2"><label>Figure 2</label><caption><p>Influence of variances in input timings on the performance of the transient network. The mean normalized readout error <italic>E</italic> (see Methods) for the benchmark task depicted in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> increases with larger standard deviation <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub> of the interstimulus intervals of the input stream independent of the used parameters. In (<bold>a</bold>,<bold>c</bold>,<bold>e</bold>), the network is trained using the echo state network approach (ESN). In (<bold>b</bold>,<bold>d</bold>,<bold>f</bold>), the FORCE-learning method is employed. Every data point represents the mean of 1000 network instantiations. The shaded area indicates the standard deviation of the respective error distribution. The error bars show the standard error of the mean. If for one instantiation the error after training is larger than 1.5, we consider the respective training procedure as not converged and exclude it from the mean. (<bold>a</bold>,<bold>b</bold>) The network is trained with three different values of the standard deviation <italic>g</italic>
<sub>GR</sub> of the feedback-weights from the readout neurons to the generator network. For both training methods, increasing <italic>g</italic>
<sub>GR</sub> also increases the error <italic>E</italic> for a given value of <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>. The constant parameters are <italic>N</italic>
<sub><italic>G</italic></sub>&#x02009;=&#x02009;100 and <italic>g</italic>
<sub>gg</sub>&#x02009;=&#x02009;1.0. (<bold>c</bold>,<bold>d</bold>) Networks of different sizes, i.e. different values of <italic>N</italic>
<sub><italic>G</italic></sub>, are trained to perform the benchmark task. While larger networks perform better for a given value <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>, they qualitatively show the same strong sensitivity to variances in input timings. The constant parameters are <italic>g</italic>
<sub>GR</sub>&#x02009;=&#x02009;0 and <italic>g</italic>
<sub>gg</sub>&#x02009;=&#x02009;1.0. (<bold>e</bold>,<bold>f</bold>) The influence of different values <italic>g</italic>
<sub>GG</sub> of the internal weights of the generator network is investigated. Neither increasing nor decreasing of the critical value <italic>g</italic>
<sub>GG</sub> reduces the error significantly. The constant parameters are <italic>g</italic>
<sub>GR</sub>&#x02009;=&#x02009;0 and <italic>N</italic>
<sub><italic>G</italic></sub>&#x02009;=&#x02009;100.</p></caption><graphic xlink:href="41598_2017_2471_Fig2_HTML" id="d29e662"/></fig>
<fig id="Fig3"><label>Figure 3</label><caption><p>Setup of the benchmark <italic>N</italic>-back task to test the influence of additional, specially-trained readout neurons to cope with variances in the input timings. The input signal as well as the target signal for the readout neuron are the same as before (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). Additional neurons, which are treated similar to readout units, are introduced in order to allow for storing task-relevant information. These additional neurons (ad. readouts) have to store the sign of the last and second last received input pulse as indicated by the arrows. The activities from the additional neurons are fed back into the network with weights <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{im}^{{\rm{GA}}}$$\end{document}</tex-math><mml:math id="M12"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GA</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq6.gif"/></alternatives></inline-formula> drawn from a normal distribution with zero mean and variance <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GA}}}^{2}$$\end{document}</tex-math><mml:math id="M14"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GA</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq7.gif"/></alternatives></inline-formula> basically extending the network. Synaptic weights adapted by the training algorithm are shown in red. The feedback from the readout neurons to the generator network is set to be zero (<italic>g</italic>
<sub>GR</sub>&#x02009;=&#x02009;0).</p></caption><graphic xlink:href="41598_2017_2471_Fig3_HTML" id="d29e723"/></fig>
<fig id="Fig4"><label>Figure 4</label><caption><p>Influence of variances in input timings on the performance of the network with specially-trained neurons. The normalized readout error <italic>E</italic> of a network with specially-trained neurons decreases with larger values of the standard deviation <italic>g</italic>
<sub>GA</sub> determining the feedback between specially-trained neurons and network. If this standard deviation equals 1, the error stays low and becomes basically independent from the standard deviation <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub> of the inter-pulse intervals of the input signal. (<bold>a</bold>) ESN approach; (<bold>b</bold>) FORCE-method.</p></caption><graphic xlink:href="41598_2017_2471_Fig4_HTML" id="d29e757"/></fig>
<fig id="Fig5"><label>Figure 5</label><caption><p>Neural network dynamics during performing the benchmark task projected onto the first two principal components. Trajectory sections which should trigger a positive pulse at the readout units are drawn in red while those which should trigger a negative response are shown in blue. The small arrows indicate the direction in which the system flows along the trajectory. The small pictograms indicate the recent history of the input pulses along the time axis. Green dots indicate attractor states (manually added). (<bold>a</bold>) The network without additional readouts (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>) stores the history of stimuli on transients. (<bold>b</bold>) By introducing variances in input timings, these transients smear impeding a proper readout. (<bold>c</bold>) The additional readouts (or specially-trained neurons; Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>) &#x0201c;structure&#x0201d; the dynamics of the system by introducing several attractor states each storing the history of the last two stimuli. (<bold>d</bold>) Even in the presence of timing variances the attractor-dominated structure in phase space is preserved enabling a proper readout. Parameters: mean inter-pulse interval <italic>&#x003bc;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;100&#x02009;ms; (<bold>a</bold>) <italic>g</italic>
<sub><italic>GR</italic></sub>&#x02009;=&#x02009;0, <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;0&#x02009;ms; (<bold>b</bold>) <italic>g</italic>
<sub><italic>GR</italic></sub>&#x02009;=&#x02009;0, <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;50&#x02009;ms; (<bold>c</bold>) <italic>g</italic>
<sub><italic>GA</italic></sub>&#x02009;=&#x02009;1, <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;0&#x02009;ms; (<bold>d</bold>) <italic>g</italic>
<sub><italic>GA</italic></sub>&#x02009;=&#x02009;1, <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;50&#x02009;ms. Details see Supplementary Discussion&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>.</p></caption><graphic xlink:href="41598_2017_2471_Fig5_HTML" id="d29e872"/></fig>
<fig id="Fig6"><label>Figure 6</label><caption><p>Prediction of the influence of an additional recall stimulus. (<bold>a</bold>) An additional temporal shift is introduced between input and output pulse. In the second setup (lower row) a recall stimulus is applied to the network to trigger the output. This recall stimulus is not relevant for the storage of the task-relevant sign. (<bold>b</bold>) In general the temporal shift increases the error of the system (gray dots; each data point indicates the average over 20 trials) as the system has already reached an attractor state. Introducing a recall stimulus (orange dots) decreases the error for all negative shifts as the system is pushed out of the attractor and the task-relevant information can be read out. This effect diminishes for positive temporal shifts as the system has already forgotten the corresponding information.</p></caption><graphic xlink:href="41598_2017_2471_Fig6_HTML" id="d29e888"/></fig>
</p><p>Furthermore, besides stimuli from the environment, also stimuli from other brain mechanisms, as long-term memory (LTM), are characterized by unreliable timing. We show that in established theoretical neuronal network models of LTM<sup><xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref></sup> the time needed for a cue-triggered recall of stored information varies dependent on the initial conditions of the recall-triggering cue and the neuronal network (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). Due to the continuous coupling between WM and LTM, which is fundamental in order to solve complex tasks<sup><xref ref-type="bibr" rid="CR29">29</xref>&#x02013;<xref ref-type="bibr" rid="CR32">32</xref></sup>, this variance in recall timings has to be reflected in the dynamics of the WM. Thus, we show that, similar to the <italic>N</italic>-back task, only a neuronal network with a combination of attractor and transient dynamics enables a continuous and reliable coupling between WM and LTM which can be used to solve a complex multi-phase task (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>). This describes, to our knowledge, the first theoretical model of the functional, dynamic interaction between a WM- and a LTM-network.<fig id="Fig7"><label>Figure 7</label><caption><p>Network models of long-term memory show variances in recall timings. (<bold>a</bold>) As WM and LTM interact continuously, inherent properties of the LTM network influence the function of WM. (<bold>b</bold>) In standard network models of LTM, the recall of a memory representation (e.g., the letter &#x0201c;A&#x0201d;) corresponds to the convergence of the neuronal system dynamics to a previously learned attractor (green dots). The time span required until convergence (&#x0201c;convergence time&#x0201d;; here <italic>t</italic>
<sub>1</sub> and <italic>t</italic>
<sub>2</sub>) depends on the initial state of the system or the recall stimulus (differently altered &#x0201c;A&#x0201d;s; orange dots). <bold>(c)</bold> 100 random patterns are stored into a standard Hopfield network<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> with <italic>N</italic>
<sub>Hopf</sub>&#x02009;=&#x02009;1000 neurons. Patterns with differences in <italic>d</italic> neurons to one of the stored patterns, corresponding to a pattern overlap of 1&#x02009;&#x02212;&#x02009;<italic>d</italic>/<italic>N</italic>
<sub>Hopf</sub>, are used as initial states for the system. The number of simulation steps to reach the stored pattern represents the convergence time (dots mark average value over 10000 trials with standard deviation indicated by blue shading). The histogram on the right-hand side illustrates the resulting distribution of convergence times for a uniform distribution of initial pattern overlaps (red line shows mean <italic>&#x003bc;</italic>
<sub>Hopf</sub> and yellow shading the corresponding standard deviation <italic>&#x003c3;</italic>
<sub>Hopf</sub>). <bold>(d)</bold> The recall convergence times analyzed similar to panel (C) for a self-organizing cell-assembly network<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. A cell assembly is trained to represent a given input pattern. After training, a similar pattern with an overlap of 1&#x02009;&#x02212;&#x02009;<italic>d</italic>/<italic>N</italic>
<sub>aff</sub>, is presented to the network (i.e. <italic>d</italic> neurons have a different activity compared to the learned pattern). The time span required to activate 90% of the cell assembly represents the convergence time.</p></caption><graphic xlink:href="41598_2017_2471_Fig7_HTML" id="d29e1002"/></fig>
<fig id="Fig8"><label>Figure 8</label><caption><p>A multi-phase task requires the continuous interaction between WM and LTM. A WM network consisting of attractor states and transient dynamics enables the reliable interaction with an LTM network to solve a complex multi-phase task. Details see main text. (<bold>a</bold>) The external input is projected into WM and LTM. For simplicity, the LTM area is separated into two compartments: the first stores abstract symbols and the second forms input-dependent associations between these symbols. (<bold>b</bold>) Multi-phase task and information flow between input, WM, and LTM. (<bold>c</bold>) External inputs, activities of 10% of the <italic>N</italic>
<sub><italic>G</italic></sub>&#x02009;=&#x02009;500&#x02009;WM-neurons, activities of the neurons within the LTM symbol area and activities within the LTM association area during the three phases. For the activities in the LTM symbol area, we indicate whether this activation is evoked by the external input, the WM, or from the LTM association area. (<bold>d</bold>) Distribution of the time intervals needed for the recall of the information stored in cell assemblies in the LTM-association area for equally distributed trials with 75% to 100% complete context signals. Here, the recall interval is defined as the time span between the onset of the external context stimulus and the point of time at which more than 50% of the neurons in the LTM-symbol area, representing the recalled number, fire at rates higher than 90% of the maximum firing rate. The standard deviation of this distribution is approximately 28&#x02009;ms. (<bold>e</bold>) Error of a purely transient network with <italic>N</italic>
<sub><italic>G</italic></sub>&#x02009;=&#x02009;500 neurons performing the summation task (third phase) with only variances in input timings from the LTM recall alone. According to (<bold>d</bold>), this source of unreliability alone already doubles the readout error (dashed vertical line).</p></caption><graphic xlink:href="41598_2017_2471_Fig8_HTML" id="d29e1045"/></fig>
</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Reservoir networks are vulnerable to variances in stimuli timings</title><p>Stimuli received by the working memory (WM), coming from the environment as well as from the long-term memory (LTM), are characterized by an unreliable timing of their occurrence. Thus, to function in a reliable manner, the WM has to reduce the influence of these timing variances. Within the last years, neuronal networks with purely transient dynamics &#x02013;so-called reservoir networks<sup><xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref></sup> &#x02013;have been proposed as a theoretical model of WM<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. A reservoir network consists of a generator network, being composed of <italic>N</italic>
<sub>G</sub> basically randomly connected neurons, which receives temporally varying input stimuli from a set of <italic>N</italic>
<sub>I</sub> input neurons and projects signals to a downstream output layer with <italic>N</italic>
<sub>R</sub> readout neurons. Due to the random connectivity within the generator network, the input stimuli are transformed into a variety of complex traces or, in other words, the inputs are processed in different variants by the network. As a consequence, the readout of a desired processing or target signal requires only the optimization of the weight matrix <italic>W</italic>
<sup>RG</sup> of the synapses transmitting signals from the generator neurons to the readout neurons. In the following, to ensure generality of our results, the readout weight matrix <italic>W</italic>
<sup>RG</sup> is optimized by two alternative strategies (for details see Methods section): On the one hand, we use the offline ESN-approach<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> and, on the other hand, we apply the online FORCE-algorithm<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Thus, after optimization, the readout neurons optimally &#x0201c;combine&#x0201d; the signals naturally present in the generator network. Reservoir networks have been shown to posses a high computational capacity as well as a short-term memory capacity<sup><xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref></sup> which are the two main components of WM.</p><p>We investigate the capability of a standard reservoir network to cope with the described timing variances in a setting similar to the <italic>N</italic>-back task. The neuronal network receives a stream of input stimuli, each of them is a pulse and has either a positive or negative sign (blue line in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). At every occurrence of an input stimulus, the network has to produce a non-zero output signal at the readout neuron (green line) of predefined temporal shape (target shape) with a sign equaling the sign of the stimulus received <italic>N</italic> stimuli before (here, <italic>N</italic>&#x02009;=&#x02009;2; indicated by arrows). Note that even though here the target shape for the output has the same pulsed shape as the input stimuli, the computational capacity of the network allows that it could be of arbitrary shape (see Supplementary Figure&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref> for an example with sine-shaped output signals). In general, to solve this <italic>N</italic>-back task, the network has to fulfill two sub-tasks: It has to store the sign of the last two input stimuli (storage of information) and, given the next input pulse, it has to produce an output signal of target shape with the sign equaling the pulse presented <italic>N</italic> stimuli before. The latter depicts the processing of information as the network has to &#x0201c;combine&#x0201d; the stored information (sign) with the transient network dynamics to produce a complex temporal output signal (target shape). Note that the used target shape also implies that the output signal is zero if no input is present. Variances in the timing of occurrence of the input stimuli are introduced by randomly drawing the interstimulus intervals &#x00394;<italic>t</italic>
<sub><italic>i</italic></sub> from a normal distribution with mean <italic>&#x003bc;</italic>
<sub>&#x00394;<italic>t</italic></sub> and variance <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{{\rm{\Delta }}t}^{2}$$\end{document}</tex-math><mml:math id="M16"><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq8.gif"/></alternatives></inline-formula>.</p><p>The performance of a reservoir network instantiation on the <italic>N</italic>-back task is evaluated after the training of its readout weight matrix by calculating the root mean square error <italic>E</italic> determining the difference between target and actual output signal (see Methods). We systematically investigate the influence of the variance in the timing of the input stimuli by varying the standard deviation <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub> of the interstimulus intervals &#x00394;<italic>t</italic> while keeping the mean <italic>&#x003bc;</italic>
<sub>&#x00394;<italic>t</italic></sub> constant. For each value of the standard deviation, we average the performance over 1000 different (random) network instantiations. Overall, independent of the training method (ESN as well as FORCE) used for the readout weights, the averaged error &#x02329;<italic>E</italic>&#x0232a; increases significantly with increasing values of <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub> until it converges to its theoretical maximum at 1 at about <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;&#x02248;&#x02009;100&#x02009;ms (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). Note that errors larger than 1 are artifacts of the used training method. The increase of the error (or decrease of the performance) with larger variances in the stimuli timings is independent of the parameters of the reservoir network. For instance, we tested the influence of different values of the variance <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GR}}}^{2}$$\end{document}</tex-math><mml:math id="M18"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GR</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq9.gif"/></alternatives></inline-formula> of the feedback weight matrix <italic>W</italic>
<sup>GR</sup> from the readout neurons to the generator network (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref> for ESN and b for FORCE). For the present <italic>N</italic>-back task, feedback of this kind does not improve the performance, although several theoretical studies show<sup><xref ref-type="bibr" rid="CR33">33</xref>&#x02013;<xref ref-type="bibr" rid="CR35">35</xref></sup> that feedback enhances the performance of reservoir networks in other tasks. In contrast, we find that increasing the number of generator neurons <italic>N</italic>
<sub>G</sub> reduces the error for a broad regime of the standard deviation <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub> (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c and d</xref>). Nevertheless, the qualitative relationship is unchanged and the improvement is weak implying a need for large numbers of neurons to solve this rather simple task for medium values of the standard deviation. Another relevant parameter of reservoir networks is the standard deviation <italic>g</italic>
<sub>GG</sub> of the distribution of the synaptic weights within the generator network determining the spectral radius of the weight matrix<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. In general, the spectral radius determines whether the network operates in a sub-critical, critical or chaotic regime and also influences the time scale of the reservoir dynamics<sup><xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR36">36</xref></sup>. Here, we find that both an increase as well as a decrease of <italic>g</italic>
<sub>GG</sub> of about 10% decrease the performance of the system (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2e,f</xref>). Additionally, it turns out that all findings remain valid also when the performance of the network is evaluated in a less restrictive manner by only distinguishing three discrete states of the readout and target signals (Supplementary Figure&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref>).</p><p>In summary, independent of the used parameter values, we find that if the input stimuli occur in an unreliable manner, a reservoir network with purely transient dynamics has a low performance in solving the <italic>N</italic>-back task. This raises doubts about its applicability as a plausible theoretical model of the dynamics underlying WM.</p></sec><sec id="Sec4"><title>Specially-trained neurons improve the performance</title><p>To obtain a neuronal network which is robust against variances in the timing of the input stimuli, we modify the reservoir network to allow for more stable memory storage. For this, we add (here, two) further neurons to the system and treat them as additional readout neurons by training (ESN as well as FORCE) the weight matrix <italic>W</italic>
<sup>AG</sup> between generator network and added neurons (similar to the readout matrix <italic>W</italic>
<sup>RG</sup>). Different to the readout neurons, the target signals of the added neurons are defined such that, after training, the neurons produce a constant positive or negative activity depending on the sign of the last or second last input stimuli, respectively (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). The activities of the additional neurons are fed back into the reservoir network via the weight matrix <italic>W</italic>
<sup>&#x02009;GA</sup> (elements drawn from a normal distribution with zero mean and variance <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GA}}}^{2}$$\end{document}</tex-math><mml:math id="M20"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GA</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq10.gif"/></alternatives></inline-formula>) basically extending the generator network. This procedure ensures that the network memorizes the later-on required information<sup><xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR37">37</xref></sup>. Note that the feedback from the readout neurons to the generator network is neglected (<italic>g</italic>
<sub>GR</sub>&#x02009;=&#x02009;0).</p><p>As above, we evaluate the performance of the extended network while solving the <italic>N</italic>-back task. In general, for a weak feedback from the additional neurons to the generator network (small values of <italic>g</italic>
<sub>GA</sub>), larger standard deviations <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub> of the interstimulus intervals &#x00394;<italic>t</italic> result in larger errors <italic>E</italic> (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4a</xref> for ESN and b for FORCE). However, increasing the standard deviation <italic>g</italic>
<sub>GA</sub> of the synaptic weights from the additional neurons to the generator network decreases the influence of the variances in stimuli timings on the performance of the system. For <italic>g</italic>
<sub>GA</sub>&#x02009;=&#x02009;1.0, the error is only slightly dependent on the standard deviation <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub> of the interstimulus intervals (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). The extension of the network by these specially-trained neurons yields a significant improvement compared to the best setup without these neurons (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). Please note that this finding also holds for a less restrictive performance evaluation (Supplementary Figure&#x000a0;<xref rid="MOESM1" ref-type="media">S3</xref>). Additionally, the same qualitative finding can also be obtained for significantly larger reservoir networks (Supplementary Figure&#x000a0;<xref rid="MOESM1" ref-type="media">S4</xref>). In the following, we investigate the dynamical principles underlying this increase in performance.</p></sec><sec id="Sec5"><title>The combination of attractor and transient dynamics increases performance</title><p>Instead of analyzing the complete high-dimensional activity dynamics of the neuronal network, we project the activity vectors onto its two most significant principal components to understand the basic dynamics<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> underlying the performance changes for the <italic>N</italic>-back task.</p><p>For the purely transient reservoir network (without specially-trained neurons; Figs&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>), we investigate the dynamics of the system with <italic>g</italic>
<sub>GR</sub>&#x02009;=&#x02009;0, <italic>N</italic>
<sub>G</sub>&#x02009;=&#x02009;100, and <italic>g</italic>
<sub>GG</sub>&#x02009;=&#x02009;1 as a representative example in more detail (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a</xref>). The dynamics of the network is dominated by one attractor state at which all neuronal activities equal zero (silent state). However, as the network continuously receives stimuli, it never reaches this state. Instead, dependent on the sign of the input stimulus, the network dynamics runs along specific trajectories (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a</xref>; red trajectories indicate that the second-last stimulus was positive while blue trajectories indicate a negative sign). The marked trajectory (*<sub>1</sub>&#x02009;&#x02192;&#x02009;*<sub>2</sub>&#x02009;&#x02192;&#x02009;*<sub>3</sub>) corresponds to a network having recently received a negative and two positive stimuli which now is exposed to a sequence of two negative stimuli (for details see Supplementary Discussion&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>). The information about the signs of the received stimuli is stored in the trajectory the network takes (transient dynamics). However, the presence of variances in the timing of the stimuli significantly perturbs this storage mechanism of the network. For <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;50&#x02009;ms (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>), the trajectories storing positive and negative signs of the second-last stimulus cannot be separated anymore. As a result, the downstream readout neuron fails to extract the task-relevant information.</p><p>Extending the reservoir network by the specially-trained neurons changes the dynamics of the system significantly (here, <italic>g</italic>
<sub>GA</sub>&#x02009;=&#x02009;1): The network now possesses four distinct attractor states with specific, transient trajectories interlinking them (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5c</xref>). The marked trajectory (&#x025c7;<sub>1</sub>&#x02009;&#x02192;&#x02009;&#x025c7;<sub>21</sub>&#x02009;&#x02192;&#x02009;&#x025c7;<sub>2</sub>&#x02009;&#x02192;&#x02009;&#x025c7;<sub>32</sub>&#x02009;&#x02192;&#x02009;&#x025c7;<sub>3</sub>) corresponds to the same sequence of stimuli as above (for details see Supplementary Discussion&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>). Here, the information about the sign of the two last stimuli is stored in the attractor states while the transients, connecting them, are used to process the information and to produce the complex output signal (target shape). Due to the attractor states, which &#x0201c;structure&#x0201d; the dynamics, variance in the timing of stimuli (here, with a standard deviation of <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;50&#x02009;ms) does not significantly alter the neuronal dynamics (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5d</xref>). The different trajectories and attractor states remain clearly separated. This separation in the presence of timing variance enables the downstream neurons to read out the task-relevant information, which is the dynamical cause of the good performance in the <italic>N</italic>-back task.</p><p>Note that, although an attractor state stores information, this information cannot be read out when the output signal has to be time-dependent (non-constant). This includes tasks during which the information has to be read out at specific periods of time and otherwise not. By introducing a time shift between stimulus and information readout (output) even without variances in input timings (constant &#x00394;<italic>t</italic>; Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>), this yields an interesting prediction. A negative shift means that the information has to be read out before the next stimulus is presented. Interestingly, for large negative shifts (&#x02248;&#x02212;&#x00394;<italic>t</italic>), the error is small as the readout is required briefly after the last stimulus and the system is still in a transient state. With smaller negative shifts, the error increases as the system reaches the next attractor and the information cannot be read out anymore in a time-resolved way. Note, however, that the pure information about the sign can be easily read out from the reservoir as done by one of the two specially - trained readout units. This information alone, however, cannot be used to generate a time-resolved signal. The situation changes when we introduce another stimulus with task-irrelevant sign which triggers the output: now, the error is small for all negative shifts as the stimulus &#x0201c;kicks&#x0201d; the system out of the attractor back in a transient state such that the stored information can be read out in a time-resolved way. This does not work for positive shifts as, here, the relevant information is already lost (overwritten by sign of next stimulus).</p><p>These results demonstrate that the information is reliably stored in the attractor states of the network dynamics while the information processing (temporally specific readout) happens on the transients. This combination of dynamics also enables the reliable interaction of the WM with other brain mechanisms as shown in the following.</p></sec><sec id="Sec6"><title>Attractor and transient dynamics enable continuous interaction with long-term memory</title><p>For solving complex tasks humans and animals incorporate several brain mechanisms which yields, amongst others, to a continuous interaction between working memory and long-term memory (LTM)<sup><xref ref-type="bibr" rid="CR29">29</xref>&#x02013;<xref ref-type="bibr" rid="CR32">32</xref></sup> (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7a</xref>). On the one side, information can be transmitted from the WM to the LTM to be stored and to &#x0201c;free&#x0201d; the computational capacities of the WM for processing further information. On the other side, information stored in the LTM can be transmitted back to the WM to be processed. Such interactions imply that the WM has to deal with the inherent properties of the LTM arising from its underlying dynamics. Several experimental and theoretical studies indicate that the neuronal networks implementing LTM are dominated by attractor dynamics<sup><xref ref-type="bibr" rid="CR39">39</xref>&#x02013;<xref ref-type="bibr" rid="CR42">42</xref></sup>. Thereby, an attractor state corresponds to a long-term memory representation which is recalled if the network dynamics converges to this state. Note that the convergence time &#x02013; the time span the system requires to reach the attractor state &#x02013; is mainly influenced by the recall stimulus and by the initial state of the system (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7b</xref>). As both the recall stimulus and the initial state vary between different recall trials, this variation yields a broad distribution of convergence times (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7c</xref> for the Hopfield model<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> and Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7d</xref> for a Hebbian cell assembly model<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>; please see Methods for more details), which is also found in psychophysical experiments<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. The width of this distribution depends on the parameters of the system, as the network size<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, but it always stays larger than zero implying a variance in the convergence times. In other words, if a complex task requires that information has to be recalled from the LTM and transmitted to the WM, this recalled information reaches the WM with unreliable timings.</p><p>In the following, we will focus on a complex multi-phase task requiring the interaction between WM and LTM (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8a,b</xref>). For simplicity, we assume that the LTM has already formed representations of abstract symbols as numbers and learning context. Now, the LTM has to form associations between these abstract symbols dependent on external inputs and inputs from the WM. Here, we use a self-organizing cell assembly network to form and store these associations (similar to the system used for Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7d</xref>
<sup><xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR28">28</xref></sup>). The WM receives inputs (external and from the LTM), processes them, and provides the system&#x02019;s output. The dynamics of the WM are governed by the combination of attractor and transient dynamics described above. This combination enables the WM to deal with the unreliability of the LTM-network to provide the task-dependent information.</p><p>The task consists of several phases (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8b,c</xref>): In the first phase (context <italic>A</italic>), the WM-network receives and stores two stimuli (<italic>x</italic>
<sub>1</sub> and <italic>x</italic>
<sub>2</sub>) each representing a number, followed by a&#x02009;+&#x02009;-signal instructing the WM to add both numbers (for simplicity we use for all calculations the modulo-three-operator, thus, <italic>z</italic>
<sub>1</sub>&#x02009;=&#x02009;<italic>x</italic>
<sub>1</sub>&#x02009;+&#x02009;<italic>x</italic>
<sub>2</sub> mod 3). The result <italic>z</italic>
<sub>1</sub> is transferred to the LTM which forms a cell assembly storing the association between context <italic>A</italic> and result <italic>z</italic>
<sub>1</sub>. Thus, the WM processes the information and the LTM stores the result of this processing for later reuse. This &#x0201c;frees&#x0201d; the WM making its processing capacities available for other tasks, as needed in the second phase (context <italic>B</italic>) during which these capacities are used for performing a second calculation. Similar to the first phase, two numbers are presented (<italic>x</italic>
<sub>3</sub> and <italic>x</italic>
<sub>4</sub>) and the WM calculates the sum of them. The result is maintained in the LTM by forming a new cell assembly storing the association between context <italic>B</italic> and result <italic>z</italic>
<sub>2</sub>. In the third phase (context <italic>A</italic>&#x02009;+&#x02009;<italic>B</italic>), the information stored in the LTM is transferred back into the WM for further processing. Namely, by externally activating the context signal <italic>A</italic> (<italic>B</italic>), the LTM recalls the stored association and transfers <italic>z</italic>
<sub>1</sub> (<italic>z</italic>
<sub>2</sub>) to the WM. Importantly, due to the variance in convergence times of the LTM (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7d</xref>), the WM receives the information <italic>z</italic>
<sub>1</sub> and <italic>z</italic>
<sub>2</sub> with an unreliable timing. However, as it consists of attractor and transient dynamics (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8c</xref>, WM activity), the WM-network can process the information and sum both numbers up. The result <italic>z</italic>
<sub>3</sub>&#x02009;=&#x02009;<italic>z</italic>
<sub>1</sub>&#x02009;+&#x02009;<italic>z</italic>
<sub>2</sub> mod3 is the final result of the task. Note that, as in each phase the information relevant later on is stored in the LTM, the performance of the system is independent of the time span between the different phases and the WM can also perform other tasks in between. Furthermore, due to the different time scales and phases involved, both networks &#x02013; WM and LTM &#x02013; and their interaction are required to solve this multi-phase task.</p><p>The multi-phase task implies several sources of unreliability of input timings perturbing the proper function of the WM. External inputs can be unreliable (similar to Figs&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>) as well as inputs from the LTM (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). Even when the external signals are accurate in timing and the LTM is at each recall in the same initial state (here in the silent state; see Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8c</xref>, LTM ass. activity), the context cue can induce unreliability. Namely, differences in the context cue triggering the recall of the corresponding association (third phase) compared to the original context signal presented during learning (first and second phase) yield a distribution of LTM recall timings with a significant standard deviation (<italic>&#x003c3;</italic>
<sub>recall</sub>&#x02009;&#x02248;&#x02009;28&#x02009;ms, Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8d</xref>). Already this cue-induced variation alone leads to a doubling of the error when using a purely transient network as WM (dashed line in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8e</xref>). All these different sources of unreliability together impede the proper function of purely transient networks to solve this task. Therefore, all our attempts to solve this task with such a purely transient network failed. This indicates that the dynamics underlying working memory should consist of a combination of transient and attractor dynamics.</p></sec></sec><sec id="Sec7" sec-type="discussion"><title>Discussion</title><p>The neuronal network dynamics underlying the proper function of working memory (WM) is still an unresolved question. Experimental findings are diverse with some studies supporting the view that WM operates mainly by transient dynamics<sup><xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref></sup> while others indicate that persistent activities, i.e. attractor states, suffices to explain WM functions<sup><xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref></sup>. Here, we considered the <italic>N</italic>-back task with variances in the timing of input stimuli<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> to draw conclusions on this dynamics. First, we showed that in purely transient systems the information about the <italic>N</italic> past stimuli is stored, as expected, in distinguishable trajectories. However, if the variance in the input timings increases, the trajectories are disturbed resulting in large overlaps between them which impede the readout of the stored information by downstream neurons. In contrast, introducing attractor states in the dynamics &#x0201c;structures&#x0201d; the phase space of the system: It stores the history of the past stimuli by remaining in the corresponding attractor. Only if a new stimulus is presented, independent of the timing, the system&#x02019;s dynamics traverses by &#x0201c;tubes&#x0201d; of transient dynamics to another, history-dependent attractor. This phase of transient dynamics between the attractor states is sufficient to perform complex temporal computations.</p><p>The most common type of purely transient network models of WM are reservoir networks<sup><xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR46">46</xref></sup>. The robustness of their performance when confronted with noise in the input or within the network has been extensively studied<sup><xref ref-type="bibr" rid="CR34">34</xref>, <xref ref-type="bibr" rid="CR47">47</xref>&#x02013;<xref ref-type="bibr" rid="CR50">50</xref></sup>. However, the susceptibility of such systems to variances in the timing of the input stimuli (Figs&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>) has &#x02013; to the best of our knowledge &#x02013;not been considered and found before. Due to the universality of reservoir networks<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, we expect that the here-presented findings can be generalized to a large class of purely transient systems implying that purely transient dynamics in general are inadequate to describe the dynamics underlying WM. Instead, a combination of transient and attractor dynamics is required (Figs&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>).</p><p>Systems consisting of transient dynamics and attractor states have been proposed and investigated before<sup><xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref></sup>. In general, these attractors are formed by introducing additional readout neurons which feed back into the generator network. In fact, several studies<sup><xref ref-type="bibr" rid="CR33">33</xref>&#x02013;<xref ref-type="bibr" rid="CR35">35</xref></sup> indicate that a feedback from the readout neurons to the generator network enhances the stability and the performance of the network and can even lead to a recurrent neural network with universal computing power<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Additionally, the feedback can also be replaced by adapting the synaptic weights within the network in order to introduce attractor states<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. Note that the attractor states do not have to be stable fixed points of the dynamics. It could be sufficient to introduce slow states<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> or heteroclinic channels<sup><xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR52">52</xref></sup> to make the system robust against variances in input timings. Furthermore, given that neuronal systems have multiple possibilities to store information<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, the attractors could also be realized in the synaptic dynamics<sup><xref ref-type="bibr" rid="CR54">54</xref>, <xref ref-type="bibr" rid="CR55">55</xref></sup> instead of (persistent) neuronal activities.</p><p>The here-derived hypothesis that the combination of transient dynamics and attractor states underlies the dynamics of WM is also supported by several experimental studies: For instance, it was shown that during a monkey WM-task, stimuli trigger the neuronal dynamics of prefrontal cortex neurons to perform fast (transient) transitions through different states until reaching a low-energy attractor state<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Similar dynamics are also found on larger brain scales in a fMRI study<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. Interestingly, in the odor system of the locust, an odor stimulus evokes a succession of states resulting in an odor specific fixed point. However, the separation of different stimuli (a computational task) is optimal during the initial transient dynamics<sup><xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR12">12</xref></sup> supporting our hypothesis.</p><p>To solve more complex tasks, the WM has also to consider information stored in the long-term memory (LTM). The resulting interaction between WM and LTM is extensively investigated in psychological and psychophysical experiments<sup><xref ref-type="bibr" rid="CR29">29</xref>&#x02013;<xref ref-type="bibr" rid="CR32">32</xref></sup> and its neuronal implementation<sup><xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR58">58</xref>&#x02013;<xref ref-type="bibr" rid="CR60">60</xref></sup>. However, from the theoretical side, this interaction has not been studied intensively (but see refs <xref ref-type="bibr" rid="CR61">61</xref> and <xref ref-type="bibr" rid="CR62">62</xref>). Here, we show that LTM-systems are unreliable in recall timings (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>) which implies that the WM-network has to be robust against these unreliabilities to functionally interact with the LTM (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>). This robustness is achieved by the combination of attractor and transient dynamics underlying working memory.</p><p>In summary, in the present study we consider the variance in timings of the inputs to draw conclusions about the underlying basic neuronal dynamics of WM. In particular, we show that a reliable WM-system requires a combination of attractor states and transient dynamics. Furthermore, we argue that these different dynamic regimes have also different functional roles &#x02013; attractors store information while transients process information &#x02013; enabling a continuous interaction with an adaptive LTM-system and yielding experimental verifiable predictions. This provides a further step in understanding the principles generating the functionally important dynamics of working memory.</p></sec><sec id="Sec8" sec-type="materials|methods"><title>Methods</title><sec id="Sec9"><title>Reservoir Network</title><p>The network consist of an input layer, a generator network, and readout neurons. Accordingly, neurons in the actual reservoir are named generator neurons. With <italic>N</italic>
<sub>G</sub> generator neurons, <italic>N</italic>
<sub>I</sub> input signals and <italic>N</italic>
<sub>R</sub> readout neurons, and, if any, <italic>N</italic>
<sub>A</sub> additional specially-trained neurons, the dynamics of the generator neuron <italic>i</italic> is given by a membrane potential <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${u}_{i}^{{\rm{G}}}$$\end{document}</tex-math><mml:math id="M22"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq11.gif"/></alternatives></inline-formula> described by<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\tau \frac{{\rm{d}}{u}_{i}^{{\rm{G}}}}{{\rm{d}}t}=-\,{u}_{i}^{{\rm{G}}}+\sum _{j=1}^{{N}_{{\rm{G}}}}{w}_{ij}^{{\rm{GG}}}{F}_{j}^{{\rm{G}}}+\sum _{k=1}^{{N}_{{\rm{I}}}}{w}_{ik}^{{\rm{GI}}}{I}_{k}+\sum _{l=1}^{{N}_{{\rm{R}}}}{w}_{il}^{{\rm{GR}}}{R}_{l}(+\sum _{m=1}^{{N}_{{\rm{A}}}}{w}_{im}^{{\rm{GA}}}{A}_{m})$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mi>&#x003c4;</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mspace width="-.25em"/><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GG</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GI</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GR</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GA</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>with time constant <italic>&#x003c4;</italic>, firing rates <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{j}^{{\rm{G}}}=\,\tanh ({u}_{j}^{{\rm{G}}})$$\end{document}</tex-math><mml:math id="M26"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mspace width=".25em"/><mml:mi>tanh</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq12.gif"/></alternatives></inline-formula> of generator neurons, input signals <italic>I</italic>
<sub><italic>k</italic></sub>, readout signals <italic>R</italic>
<sub><italic>l</italic></sub> and, if present, specially-trained additional readouts <italic>A</italic>
<sub><italic>m</italic></sub>. The synaptic weights <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ij}^{{\rm{GG}}}$$\end{document}</tex-math><mml:math id="M28"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GG</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq13.gif"/></alternatives></inline-formula> within the generator network are drawn from a normal distribution with zero mean and variance <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GG}}}^{2}/{N}_{{\rm{G}}}$$\end{document}</tex-math><mml:math id="M30"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GG</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq14.gif"/></alternatives></inline-formula>. Similarly, the synaptic weight <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{il}^{{\rm{GR}}}$$\end{document}</tex-math><mml:math id="M32"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GR</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq15.gif"/></alternatives></inline-formula> from the readout neuron <italic>l</italic> back to the generator neuron <italic>i</italic> is drawn from a normal distribution with zero mean and variance <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GR}}}^{2}/{N}_{{\rm{R}}}$$\end{document}</tex-math><mml:math id="M34"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GR</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq16.gif"/></alternatives></inline-formula> and the weight <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{im}^{{\rm{AR}}}$$\end{document}</tex-math><mml:math id="M36"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">AR</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq17.gif"/></alternatives></inline-formula> from specially-trained neuron <italic>m</italic> is drawn from a normal distribution with zero mean and variance <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GA}}}^{2}/{N}_{{\rm{A}}}$$\end{document}</tex-math><mml:math id="M38"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GA</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq18.gif"/></alternatives></inline-formula>. Every generator neuron <italic>i</italic> receives signals from exactly one randomly chosen input signal <italic>k</italic> scaled by a weight <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ik}^{{\rm{GI}}}$$\end{document}</tex-math><mml:math id="M40"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GI</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq19.gif"/></alternatives></inline-formula> drawn from a normal distribution with variance <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{{\rm{GI}}}^{2}$$\end{document}</tex-math><mml:math id="M42"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">GI</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq20.gif"/></alternatives></inline-formula> and zero mean.</p><p>The current activity value <italic>R</italic>
<sub><italic>l</italic></sub> of the linear readout neuron <italic>l</italic>&#x02009;=&#x02009;1, &#x02026;, <italic>N</italic>
<sub>R</sub> is given by<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{l}=\sum _{i=1}^{{N}_{{\rm{G}}}}{w}_{li}^{{\rm{RG}}}{F}_{i}^{{\rm{G}}}\mathrm{.}$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RG</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mn>.</mml:mn></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>
</p><p>The initial values of the weights <inline-formula id="IEq21"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{li}^{{\rm{RG}}}$$\end{document}</tex-math><mml:math id="M46"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RG</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq21.gif"/></alternatives></inline-formula> are drawn from a normal distribution with zero mean and variance <inline-formula id="IEq22"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{{\rm{G}}}^{-1}$$\end{document}</tex-math><mml:math id="M48"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq22.gif"/></alternatives></inline-formula>. These weights are adapted by different supervised algorithms described below. If there are any specially-trained additional readout units in the network, they follow identical dynamics as the default readout neurons.</p><p>If not stated otherwise, the used parameter values are <italic>&#x003c4;</italic>&#x02009;=&#x02009;10&#x02009;ms, <italic>N</italic>
<sub><italic>G</italic></sub>&#x02009;=&#x02009;100, <italic>N</italic>
<sub><italic>I</italic></sub>&#x02009;=&#x02009;1, <italic>N</italic>
<sub><italic>R</italic></sub>&#x02009;=&#x02009;1, <italic>g</italic>
<sub>GG</sub>&#x02009;=&#x02009;1.0, <italic>g</italic>
<sub>GI</sub>&#x02009;=&#x02009;1.0. All equations are solved by using the Euler method with a time step of <inline-formula id="IEq23"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{t}=1\,{\rm{ms}}$$\end{document}</tex-math><mml:math id="M50"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width=".1em"/><mml:mi mathvariant="normal">ms</mml:mi></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq23.gif"/></alternatives></inline-formula>.</p><sec id="Sec10"><title>Echo State Network Approach</title><p>Following the echo state network (ESN) approach<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> to train the weights from the reservoir network to the readouts <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{li}^{{\rm{RG}}}$$\end{document}</tex-math><mml:math id="M52"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RG</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq24.gif"/></alternatives></inline-formula>, the network is sampled for a given number <italic>S</italic> of time steps. The activities of the generator neurons are collected in a state matrix <italic>M</italic> of dimension <italic>N</italic>
<sub>G</sub>&#x02009;&#x000d7;&#x02009;<italic>S</italic> with every row containing the activities at a specific time step. The corresponding target signals of the readout neurons are collected in a teacher matrix <italic>T</italic> of dimension <italic>S</italic>&#x02009;&#x000d7;&#x02009;<italic>N</italic>
<sub>R</sub>. Optimizing the mean squared error of the readout signals is achieved by calculating the pseudoinverse <italic>M</italic>
<sup>&#x02212;1</sup> of <italic>M</italic> and setting the weight matrix accordingly:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}^{{\rm{RG}}}={M}^{-1}\,T\mathrm{.}$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RG</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mspace width="-.25em"/><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width=".25em"/><mml:mi>T</mml:mi><mml:mn>.</mml:mn></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>
</p><p>Note that during the sampling phase, instead of the actual activities of the readout neurons the values of the target signals modified by Gaussian noise with variance <inline-formula id="IEq25"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{{\rm{noise}}}^{2}$$\end{document}</tex-math><mml:math id="M56"><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">noise</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq25.gif"/></alternatives></inline-formula> are fed back to the generator network. We use <italic>&#x003c3;</italic>
<sub>noise</sub>&#x02009;=&#x02009;0.1.</p></sec><sec id="Sec11"><title>FORCE Approach</title><p>In contrast to the ESN approach, FORCE learning<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> is an online-learning method. As originally proposed, we utilize the recursive least-squares (RLS) algorithm<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> to adapt the readout weights fast enough to keep the actual activities of the readout neurons close to the target values from the very beginning. During learning, in every simulation step, the readout weight vector for readout neuron <italic>l</italic> at time <italic>t</italic> is adapted according to<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{l}^{{\rm{RG}}}(t)={w}_{l}^{{\rm{RG}}}(t-\hat{t})-{e}_{l}(t)(P(t){F}^{{\rm{G}}}(t{))}^{T}\mathrm{.}$$\end{document}</tex-math><mml:math id="M58" display="block"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RG</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RG</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mn>.</mml:mn></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>
</p><p>Here, <inline-formula id="IEq26"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{t}$$\end{document}</tex-math><mml:math id="M60"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq26.gif"/></alternatives></inline-formula> denotes the step width of the simulation and<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${e}_{l}(t)={({w}_{l}^{{\rm{RG}}}(t-\hat{t}))}^{T}{F}^{{\rm{G}}}(t)-{f}_{l}(t)$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RG</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>is the difference between the readout signal and the target function <italic>f</italic>
<sub><italic>l</italic></sub>(<italic>t</italic>). The matrix <italic>P</italic>(<italic>t</italic>) has the dimension <italic>N</italic>
<sub>G</sub>&#x02009;&#x000d7;&#x02009;<italic>N</italic>
<sub>G</sub> and is updated according to<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P(t)=P(t-{\rm{\Delta }}t)-\frac{P(t-\hat{t}){F}^{{\rm{G}}}(t){F}^{{\rm{G}}}{(t)}^{T}P(t-\hat{t})}{1+{F}^{{\rm{G}}}{(t)}^{T}P(t-\hat{t}){F}^{{\rm{G}}}(t)}$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>with an initial value of <italic>P</italic>(0)&#x02009;=&#x02009;<italic>&#x003b1;</italic>
<sup>&#x02212;1</sup>&#x1d7d9; where &#x1d7d9; is the identity matrix. We set <italic>&#x003b1;</italic>&#x02009;=&#x02009;100.</p></sec></sec><sec id="Sec12"><title>Benchmark Task</title><p>For the benchmark task (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>), input pulses are generated at random time intervals drawn from a normal distribution with mean <italic>&#x003bc;</italic>
<sub>&#x00394;<italic>t</italic></sub> and variance <italic>&#x003c3;</italic>
<sub>&#x00394;<italic>t</italic></sub>. Every pulse is modeled as a convolution of a constant signal with length <italic>t</italic>
<sub>pulse</sub>, unit magnitude and random sign and a Gaussian window with variance <inline-formula id="IEq27"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{{\rm{smooth}}}^{2}$$\end{document}</tex-math><mml:math id="M66"><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">smooth</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq27.gif"/></alternatives></inline-formula>. To avoid overlaps between pulses, we restrict the time interval between two pulses to a minimum of 2 &#x022c5; <italic>t</italic>
<sub>pulse</sub>. The target readout signal consists of pulses of identical shape whose signs depend on the sign of the respective second-last input pulse. To account for the delays within the generator network, each target pulse is generated with a short delay <italic>t</italic>
<sub>delay</sub> after the corresponding input pulse.</p><p>In the case with specially-trained neurons (or additional readout units) (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>), we add two further target signals. These signals have a value of either +1 or &#x02212;1 with the sign depending on the sign of the last or second-last input pulse, respectively. Also here, the pulses start with a short delay <italic>t</italic>
<sub>delay</sub> with respect to the input pulses and they are smoothed by a convolution with a Gaussian window with variance <inline-formula id="IEq28"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{{\rm{smooth}}}^{2}$$\end{document}</tex-math><mml:math id="M68"><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">smooth</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq28.gif"/></alternatives></inline-formula>.</p><p>To evaluate the performance of the network in generating the desired target signal <italic>f</italic>
<sub><italic>l</italic></sub>(<italic>t</italic>) at the read out neuron <italic>l</italic>, we compare the actual activity <italic>R</italic>
<sub><italic>l</italic></sub>(<italic>t</italic>) with the desired target signal <italic>f</italic>
<sub><italic>l</italic></sub>(<italic>t</italic>) and calculate the root mean square error (RMS). In order to normalize this value, it is divided by the RMS of the target signal:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{l}=\frac{\sqrt{{\sum }_{t}{({R}_{l}(t)-{f}_{l}(t))}^{2}}}{\sqrt{{\sum }_{t}\,{f}_{l}{(t)}^{2}}},\,l=1,\ldots ,{N}_{{\rm{R}}}\mathrm{.}$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mspace width=".15em"/><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:msub><mml:mn>.</mml:mn></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>
</p><p>For the benchmark task, there is only one regular readout unit (<italic>l</italic>&#x02009;=&#x02009;0). We therefore omit the index such that <italic>E</italic>&#x02009;=&#x02009;<italic>E</italic>
<sub>0</sub>. Every parameter configuration is evaluated in 1000 independent network instantiations.</p><p>When adding the recall stimulus in the benchmark task (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>), it is applied 10&#x02009;ms before the onset of the target pulse via an additional input neuron (<italic>N</italic>
<sub><italic>I</italic></sub>&#x02009;=&#x02009;2) and given by a short positive unit pulse of length <italic>t</italic>
<sub>pulse</sub> smoothed by a convolution with a Gaussian window with variance <inline-formula id="IEq29"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{{\rm{smooth}}}^{2}$$\end{document}</tex-math><mml:math id="M72"><mml:msubsup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">smooth</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq29.gif"/></alternatives></inline-formula>.</p><p>The used values are <italic>&#x003bc;</italic>
<sub>&#x00394;<italic>t</italic></sub>&#x02009;=&#x02009;200&#x02009;ms, <italic>t</italic>
<sub>pulse</sub>&#x02009;=&#x02009;10&#x02009;ms, <italic>t</italic>
<sub>delay</sub>&#x02009;=&#x02009;10&#x02009;ms, and <italic>&#x003c3;</italic>
<sub>smooth</sub>&#x02009;=&#x02009;2&#x02009;ms.</p></sec><sec id="Sec13"><title>Hopfield Network</title><p>We use a standard implementation of the Hopfield network<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. It consists of <italic>N</italic>
<sub>Hopf</sub>&#x02009;=&#x02009;1000 binary neurons. In every time step, every neuron <italic>i</italic> is in one of two possible states characterized by a firing rate of either <italic>F</italic>
<sub><italic>i</italic></sub>&#x02009;=&#x02009;1 or <italic>F</italic>
<sub><italic>i</italic></sub>&#x02009;=&#x02009;&#x02212;1. The activity states of all neurons are updated according to<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{i}(t+\mathrm{1)}=(\begin{array}{cc}\mathrm{1,} &#x00026; {\rm{if}}\sum _{j=1}^{{N}_{{\rm{Hopf}}}}{w}_{ij}^{{\rm{Hopf}}}{F}_{j}(t)\ge 0\\ -\,\mathrm{1,} &#x00026; {\rm{if}}\sum _{j=1}^{{N}_{{\rm{Hopf}}}}{w}_{ij}^{{\rm{Hopf}}}{F}_{j}(t) &#x0003c; \mathrm{0,}\end{array}$$\end{document}</tex-math><mml:math id="M74" display="block"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1)</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1,</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">if</mml:mi><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Hopf</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Hopf</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mspace width="-.25em"/><mml:mn>1,</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">if</mml:mi><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Hopf</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Hopf</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:mn>0,</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq30"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ij}^{{\rm{Hopf}}}$$\end{document}</tex-math><mml:math id="M76"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Hopf</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq30.gif"/></alternatives></inline-formula> is the synaptic weight projecting from neuron <italic>j</italic> to neuron <italic>i</italic> (w.l.o.g., <inline-formula id="IEq31"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ii}^{{\rm{Hopf}}}=0$$\end{document}</tex-math><mml:math id="M78"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Hopf</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq31.gif"/></alternatives></inline-formula>). We store <italic>K</italic>&#x02009;=&#x02009;100 random patterns <inline-formula id="IEq32"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{F}}_{j}$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq32.gif"/></alternatives></inline-formula> into the network by choosing<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ij}^{{\rm{Hopf}}}=\frac{1}{K}\sum _{\mu }^{K}{\hat{F}}_{i}^{\mu }{\hat{F}}_{j}^{\mu }\mathrm{.}$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Hopf</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:msubsup><mml:mn>.</mml:mn></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>
</p><p>The network is initialized with a binary vector <italic>F</italic>
<sup>0</sup> whose entries differ in <italic>d</italic> positions from <inline-formula id="IEq33"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{F}}^{1}$$\end{document}</tex-math><mml:math id="M84"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq33.gif"/></alternatives></inline-formula> corresponding to an overlap of 1&#x02009;&#x02212;&#x02009;<italic>d</italic>/<italic>N</italic>
<sub>Hopf</sub>. We simulate the development of the network until a stable pattern is reached. Hence, the number of simulated time steps corresponds to the convergence time. In a few cases, the system does not a reach a stable pattern. The respective trials are excluded from the analysis. For every distance <italic>d</italic> with 0&#x02009;&#x02264;&#x02009;<italic>d</italic>&#x02009;&#x02264;&#x02009;<italic>N</italic>, we repeated the simulation 10,000 times.</p></sec><sec id="Sec14"><title>Self-Organized Cell Assemblies</title><p>The here-used LTM-model is based on Hebbian cell assembly models<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. We introduce some modifications to allow for a self-organized allocation of memory representations. The main part of the model is an <italic>n</italic>&#x02009;&#x000d7;&#x02009;<italic>n</italic>-grid of excitatory neurons, each described by a membrane potential <italic>u</italic>
<sub><italic>i</italic></sub>, <italic>i</italic>&#x02009;=&#x02009;1, &#x02026;, <italic>N</italic>
<sub>CA</sub> (<italic>N</italic>
<sub>CA</sub>&#x02009;=&#x02009;<italic>n</italic>
<sup>2</sup>), according to<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\tau \frac{{\rm{d}}{u}_{i}}{{\rm{d}}t}=-\,{u}_{i}+\sum _{j\mathrm{=1}}^{{N}_{{\rm{CA}}}}{w}_{ij}^{{\rm{lat}}}{F}_{j}+{w}_{{\rm{exc}},{\rm{inh}}}{F}^{{\rm{inh}}}+\sum _{k\mathrm{=1}}^{{N}_{{\rm{aff}}}}{w}_{ik}^{{\rm{aff}}}{F}_{k}^{{\rm{aff}}}$$\end{document}</tex-math><mml:math id="M86" display="block"><mml:mi>&#x003c4;</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mspace width="-.25em"/><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mn>=1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">CA</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">lat</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">exc</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mn>=1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">aff</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">aff</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">aff</mml:mi></mml:mrow></mml:msubsup></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>with time constant <italic>&#x003c4;</italic> (the same value as for the time constant of the reservoir neurons). <italic>F</italic>
<sub><italic>i</italic></sub> represents the firing rate of neuron <italic>i</italic> given by<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{i}=\frac{1}{1+\exp (\beta (\varepsilon -{u}_{i}))}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b5;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>with parameters <italic>&#x003b2;</italic> and <italic>&#x003b5;</italic>. Every neuron <italic>i</italic> on the grid receives lateral inputs with synaptic weights <inline-formula id="IEq34"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ij}^{{\rm{lat}}}$$\end{document}</tex-math><mml:math id="M90"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">lat</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq34.gif"/></alternatives></inline-formula> from all grid neurons <italic>j</italic> whose euclidean distance from neuron <italic>i</italic> (measured in grid units) is smaller than the excitatory interaction radius <italic>r</italic>
<sub>exc</sub> (otherwise <inline-formula id="IEq35"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ij}^{{\rm{lat}}}=0$$\end{document}</tex-math><mml:math id="M92"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">lat</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq35.gif"/></alternatives></inline-formula>). In order to avoid boundary effects, periodic boundary conditions are applied.</p><p>In addition to the excitatory neurons, we consider an inhibitory population which receives signals from all excitatory grid neurons and projects signals back to all of them. The mean-field dynamics of the membrane potential <italic>u</italic>
<sup>inh</sup> and the firing rate <italic>F</italic>
<sup>inh</sup> of this population is given by<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{{\rm{inh}}}\frac{{\rm{d}}{u}^{{\rm{inh}}}}{{\rm{d}}t}=-\,{u}^{{\rm{inh}}}+\sum _{i=1}^{{N}_{{\rm{CA}}}}{w}_{{\rm{inh}},{\rm{exc}}}{F}_{i}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mspace width="-.25em"/><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">CA</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">exc</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>and<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}^{{\rm{inh}}}=\frac{1}{1+\exp ({\beta }_{{\rm{inh}}}({\varepsilon }_{{\rm{inh}}}-{u}^{{\rm{inh}}}))}\mathrm{.}$$\end{document}</tex-math><mml:math id="M96" display="block"><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b5;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">inh</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mn>.</mml:mn></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>
</p><p>Here, <italic>w</italic>
<sub>inh,exc</sub> and <italic>w</italic>
<sub>exc,inh</sub> designate the average synaptic weight from excitatory neurons to the inhibitory population and vice versa.</p><p>A second layer of <italic>N</italic>
<sub>aff</sub> afferent neurons projects signals onto the grid layer via afferent synapses with weights <inline-formula id="IEq36"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{ik}^{{\rm{aff}}}$$\end{document}</tex-math><mml:math id="M98"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">aff</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq36.gif"/></alternatives></inline-formula>. The firing rates <inline-formula id="IEq37"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{k}^{{\rm{aff}}}$$\end{document}</tex-math><mml:math id="M100"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">aff</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2017_2471_Article_IEq37.gif"/></alternatives></inline-formula> of these neurons are externally controlled and represent the input pattern presented to the network. Every grid neuron <italic>i</italic> receives synapses from exactly <italic>n</italic>
<sub>aff</sub> randomly chosen input neurons.</p><p>Nonzero lateral synapses as well as the nonzero afferent synapses are governed by a synaptic plasticity rule composed of a Hebbian term, correlating the postsynaptic activity <italic>F</italic>
<sub><italic>i</italic></sub> and the presynaptic activity <italic>F</italic>
<sub><italic>j</italic></sub>, and a synaptic scaling term, driving the postsynaptic activity towards a target value <italic>F</italic>
<sub><italic>T</italic></sub>
<sup><xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR64">64</xref></sup>:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tau }_{w}\frac{{\rm{d}}{w}_{ij}}{{\rm{d}}t}={F}_{i}{F}_{j}+({F}_{T}-{F}_{i}){w}_{ij}^{2}\mathrm{.}$$\end{document}</tex-math><mml:math id="M102" display="block"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mn>.</mml:mn></mml:math><graphic xlink:href="41598_2017_2471_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>
</p><p>To obtain the memory convergence times for this model (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7d</xref>), we consider several phases: In the first phase, the learning phase, we fully activate half of the afferent neurons (representing an input pattern) and simulate the network long enough to obtain a stable cell assembly (memory representation) in the grid layer. All neurons with a firing rate <italic>F</italic>
<sub><italic>i</italic></sub> larger than 0.5 are considered to be part of the assembly. In the next phase, we deactivate the input to assure that the resulting network structures do not imply persistent activities. In the recall phase, a similar but slightly different input pattern where <italic>d</italic>/2 previously active afferent neurons are now inactive and <italic>d</italic>/2 originally inactive afferent neurons are now active is presented to the network. This corresponds to a total pattern difference of <italic>d</italic> and therefore to an overlap of 1&#x02009;&#x02212;&#x02009;<italic>d</italic>/<italic>N</italic>
<sub>aff</sub>. Applying the modified afferent pattern, we measure the time the network requires until 90% of the neurons which have been already active during learning (representing the stored pattern) get active. The respective time interval is considered as the convergence time of the stored memory. For every difference <italic>d</italic> with 0&#x02009;&#x0003c;&#x02009;<italic>d</italic>&#x02009;&#x0003c;&#x02009;<italic>N</italic>/2 this procedure is repeated for 1000 times. Trials during which 90% activation is not reached are not considered for further analyses.</p><p>The used parameters values are <italic>n</italic>&#x02009;=&#x02009;30 (<italic>N</italic>
<sub>CA</sub>&#x02009;=&#x02009;900), <italic>&#x003c4;</italic>&#x02009;=&#x02009;0.01&#x02009;s, <italic>w</italic>
<sub>exc,inh</sub>&#x02009;=&#x02009;&#x02212;20.0, <italic>&#x003b2;</italic>&#x02009;=&#x02009;1, <italic>&#x003b5;</italic>&#x02009;=&#x02009;12, <italic>r</italic>
<sub>exc</sub>&#x02009;=&#x02009;3, <italic>&#x003c4;</italic>
<sub>inh</sub>&#x02009;=&#x02009;0.02&#x02009;s, <italic>w</italic>
<sub>inh,exc</sub>&#x02009;=&#x02009;1.0, <italic>&#x003b2;</italic>
<sub>inh</sub>&#x02009;=&#x02009;0.1, <italic>&#x003b5;</italic>
<sub>inh</sub>&#x02009;=&#x02009;100, <italic>N</italic>
<sub>aff</sub>&#x02009;=&#x02009;50, <italic>n</italic>
<sub>aff</sub>&#x02009;=&#x02009;20, <italic>&#x003c4;</italic>
<sub><italic>w</italic></sub>&#x02009;=&#x02009;10&#x02009;s, and <italic>F</italic>
<sub><italic>T</italic></sub>&#x02009;=&#x02009;0.</p></sec><sec id="Sec15"><title>WM-LTM interaction</title><p>For the interaction of WM with LTM, as sketched in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, we trained a reservoir network (<italic>N</italic>
<sub><italic>G</italic></sub>&#x02009;=&#x02009;500) with <italic>N</italic>
<sub><italic>I</italic></sub>&#x02009;=&#x02009;4 input signals (&#x0201c;0&#x0201d;, &#x0201c;1&#x0201d;, &#x0201c;2&#x0201d;, &#x0201c;+&#x0201d;) and <italic>N</italic>
<sub><italic>R</italic></sub>&#x02009;=&#x02009;9 readouts. The first three of these readouts represent the actual output of the WM (&#x0201c;0&#x0201d;, &#x0201c;1&#x0201d;, &#x0201c;2&#x0201d;). The second group of three holds additional assisting outputs which encode the last received number input. Accordingly, the third group encodes the second-last received number input. Already during the reservoir training, the actual readouts of the reservoir are connected to the LTM-symbol area. For simplicity, this area is modeled as a collection of single excitatory neurons with dynamics and parameters as described above. Every number is represented by a group of ten neurons, the two context signals by twenty neurons each. The reservoir output signals are fed into the respective symbol neurons with strong synaptic weights <italic>w</italic>
<sup>SR</sup>&#x02009;=&#x02009;20. The activities of the symbol neurons, in turn, are directly taken as input signals for the reservoir.</p><p>The reservoir network is trained using the FORCE approach as described above. All inputs are smoothed by convolution with a Gaussian kernel with a standard deviation of <italic>&#x003c3;</italic>
<sub>smooth</sub>&#x02009;=&#x02009;5&#x02009;ms. The pulses representing numbers have a length drawn from a uniform distribution with a minimum of <italic>t</italic>
<sub>num,min</sub>&#x02009;=&#x02009;50&#x02009;ms and a maximum of <italic>t</italic>
<sub>num,max</sub>&#x02009;=&#x02009;100&#x02009;ms. The pulses representing the &#x0201c;+&#x0201d;-operation have a length uniformly drawn between <italic>t</italic>
<sub>+,min</sub>&#x02009;=&#x02009;100&#x02009;ms and <italic>t</italic>
<sub>+,max</sub>&#x02009;=&#x02009;400&#x02009;ms. The distances between the individual pulses vary between &#x00394;<italic>t</italic>
<sub>min</sub>&#x02009;=&#x02009;100&#x02009;ms and &#x00394;<italic>t</italic>
<sub>max</sub>&#x02009;=&#x02009;400&#x02009;ms. The reservoir network is trained for 100000 time steps and, afterwards, its performance is evaluated for the same number of time steps. This procedure is repeated until the normalized error during evaluation drops below 0.05.</p><p>After training the reservoir network, the LTM-symbol area is taken as an input to the LTM-association area which is modeled by the system of self-organized cell assemblies described above. Apart from the higher number of afferent synapses (<italic>n</italic>
<sub>aff</sub>&#x02009;=&#x02009;30) per excitatory grid neuron, we use the same parameters as before. However, in order to be able to recall actual symbols from the cell assembly activity, additional feedback synapses from the LTM-association area back to the symbol area are introduced. These are governed by the same synaptic plasticity processes as the lateral and afferent synapses. Every neuron in the symbol area receives synapses from <italic>n</italic>
<sub>fb</sub>&#x02009;=&#x02009;250 randomly chosen LTM-association neurons. The feedback synapses are initialized with a weight which equals zero.</p></sec></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec16"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2017_2471_MOESM1_ESM.pdf"><caption><p>Supplementary Material</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Electronic supplementary material</bold></p><p>
<bold>Supplementary information</bold> accompanies this paper at doi:10.1038/s41598-017-02471-z
</p></fn><fn><p>
<bold>Publisher's note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We thank Prof. Dr. Florentin W&#x000f6;rg&#x000f6;tter and Dr. Michael Fauth for fruitful discussions and critics on the manuscript. The research leading to these results has received funding from the Federal Ministry of Education and Research (BMBF) Germany to the G&#x000f6;ttingen Bernstein Center for Computational Neuroscience under grant numbers 01GQ1005A [TN] and 01GQ1005B [CT] and from the International Max Planck Research School for Physics of Biological and Complex Systems (IMPRS-PBCS) by stipends of the country of Lower Saxony with funds from the initiative &#x0201c;Nieders&#x000e4;chsisches Vorab&#x0201d; and of the University of G&#x000f6;ttingen [TN]. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.We acknowledge support by the German Research Foundation and the Open Access Publication Funds of the G&#x000f6;ttingen University.</p></ack><notes notes-type="author-contribution"><title>Author Contributions</title><p>T.N. and C.T. planned the presented analyses and experiments. Implementation and analysis of the data was done by T.N. T.N. and C.T. wrote the manuscript.</p></notes><notes notes-type="COI-statement"><sec id="FPar1"><title>Competing Interests</title><p>The authors declare no competing interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Unsworth</surname><given-names>N</given-names></name></person-group><article-title>On the division of working memory and long-term memory and their relation to intelligence: A latent variable approach</article-title><source>Acta Psychol. (Amst.)</source><year>2010</year><volume>134</volume><fpage>16</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2009.11.010</pub-id><pub-id pub-id-type="pmid">20022311</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>AD</given-names></name></person-group><article-title>Working memory: theories, models, and controversies</article-title><source>Annu. Rev. Psychol.</source><year>2012</year><volume>63</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100422</pub-id><pub-id pub-id-type="pmid">21961947</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>Neurocomputational models of working memory</article-title><source>Nat. Neurosci.</source><year>2000</year><volume>3</volume><issue>Suppl</issue><fpage>1184</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1038/81460</pub-id><pub-id pub-id-type="pmid">11127836</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinovich</surname><given-names>MI</given-names></name><name><surname>Huerta</surname><given-names>R</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><article-title>Transient dynamics for neural processing</article-title><source>Science</source><year>2008</year><volume>321</volume><fpage>48</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1126/science.1155564</pub-id><pub-id pub-id-type="pmid">18599763</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><article-title>Working models of working memory</article-title><source>Curr. Opin. Neurobiol.</source><year>2014</year><volume>25</volume><fpage>20</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.10.008</pub-id><pub-id pub-id-type="pmid">24709596</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname><given-names>JM</given-names></name><name><surname>Alexander</surname><given-names>GE</given-names></name></person-group><article-title>Neuron activity related to short-term memory</article-title><source>Science</source><year>1971</year><volume>173</volume><fpage>652</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1126/science.173.3997.652</pub-id><pub-id pub-id-type="pmid">4998337</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>D&#x02019;Esposito</surname><given-names>M</given-names></name></person-group><article-title>Persistent activity in the prefrontal cortex during working memory</article-title><source>Trends Cogn. Sci.</source><year>2003</year><volume>7</volume><fpage>415</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00197-9</pub-id><pub-id pub-id-type="pmid">12963473</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><article-title>Attention and working memory: A dynamical model of neuronal activity in the prefrontal cortex</article-title><source>Eur. J. Neurosci.</source><year>2003</year><volume>18</volume><fpage>2374</fpage><lpage>2390</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2003.02956.x</pub-id><pub-id pub-id-type="pmid">14622200</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>K</given-names></name><name><surname>Nykamp</surname><given-names>DQ</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title><source>Nat. Neurosci.</source><year>2014</year><volume>17</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1038/nn.3645</pub-id><pub-id pub-id-type="pmid">24487232</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riley</surname><given-names>MR</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name></person-group><article-title>Role of prefrontal persistent activity in working memory</article-title><source>Front. Syst. Neurosci.</source><year>2016</year><volume>9</volume><fpage>181</fpage><pub-id pub-id-type="doi">10.3389/fnsys.2015.00181</pub-id><pub-id pub-id-type="pmid">26778980</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><article-title>Timecourse of object-related neural activity in the primate prefrontal cortex during a short-term memory task</article-title><source>Eur. J. Neurosci.</source><year>2002</year><volume>15</volume><fpage>1244</fpage><lpage>1254</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.01958.x</pub-id><pub-id pub-id-type="pmid">11982635</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazor</surname><given-names>O</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><article-title>Transient dynamics versus fixed points in odor representations by locust antennal lobe projection neurons</article-title><source>Neuron</source><year>2005</year><volume>48</volume><fpage>661</fpage><lpage>673</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.09.032</pub-id><pub-id pub-id-type="pmid">16301181</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Jaeger, H. &#x00026; Eck, D. Can&#x02019;t get you out of my head: A connectionist model of cyclic rehearsal. In Wachsmutch, I. &#x00026; Knoblich, G. (eds.) <italic>Modeling Communication with Robots and Virtual Humans: Second ZiF Research Group International Workshop on Embodied Communication in Humans and Machines, Bielefeld, Germany, April 5</italic>&#x02013;<italic>8, 2006, Revised Selected Papers</italic> 310&#x02013;335 (Springer Berlin Heidelberg, Berlin, Heidelberg, 2008).</mixed-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><article-title>A neurodynamical model for working memory</article-title><source>Neural Netw.</source><year>2011</year><volume>24</volume><fpage>199</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2010.10.003</pub-id><pub-id pub-id-type="pmid">21036537</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname><given-names>DV</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title><source>Nat. Rev. Neurosci.</source><year>2009</year><volume>10</volume><fpage>113</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nrn2558</pub-id><pub-id pub-id-type="pmid">19145235</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Maass, W. Liquid state machines: Motivation, theory, and applications. In Cooper, S. B. &#x00026; Sorbi, A. (eds.) <italic>Computability in Context</italic> 275&#x02013;296 (Imperial College Press, London, 2011).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>C</given-names></name><name><surname>Dasgupta</surname><given-names>S</given-names></name><name><surname>Kulvicius</surname><given-names>T</given-names></name><name><surname>W&#x000f6;rg&#x000f6;tter</surname><given-names>F</given-names></name></person-group><article-title>The use of hebbian cell assemblies for nonlinear computation</article-title><source>Sci. Rep.</source><year>2015</year><volume>5</volume><fpage>12866</fpage><pub-id pub-id-type="doi">10.1038/srep12866</pub-id><pub-id pub-id-type="pmid">26249242</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JK</given-names></name><etal/></person-group><article-title>Heterogenous population coding of a short-term memory and decision task</article-title><source>J. Neurosci.</source><year>2010</year><volume>30</volume><fpage>916</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2062-09.2010</pub-id><pub-id pub-id-type="pmid">20089900</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussar</surname><given-names>CR</given-names></name><name><surname>Pasternak</surname><given-names>T</given-names></name></person-group><article-title>Memory-guided sensory comparisons in the prefrontal cortex: contribution of putative pyramidal cells and interneurons</article-title><source>J. Neurosci.</source><year>2012</year><volume>32</volume><fpage>2747</fpage><lpage>2761</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5135-11.2012</pub-id><pub-id pub-id-type="pmid">22357858</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Yi</surname><given-names>H</given-names></name><name><surname>Bai</surname><given-names>W</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name></person-group><article-title>Dynamic trajectory of multiple single-unit activity during working memory task in rats</article-title><source>Front. Comput. Neurosci.</source><year>2015</year><volume>9</volume><fpage>117</fpage><pub-id pub-id-type="pmid">26441626</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><article-title>From fixed points to chaos: three models of delayed discrimination</article-title><source>Prog. Neurobiol.</source><year>2013</year><volume>103</volume><fpage>214</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2013.02.002</pub-id><pub-id pub-id-type="pmid">23438479</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koppe</surname><given-names>G</given-names></name><etal/></person-group><article-title>Temporal unpredictability of a stimulus sequence affects brain activation differently depending on cognitive task demands</article-title><source>Neuroimage</source><year>2014</year><volume>101</volume><fpage>236</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.008</pub-id><pub-id pub-id-type="pmid">25019681</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>AD</given-names></name></person-group><article-title>Working memory: looking back and looking forward</article-title><source>Nat. Rev. Neurosci.</source><year>2003</year><volume>4</volume><fpage>829</fpage><lpage>839</lpage><pub-id pub-id-type="doi">10.1038/nrn1201</pub-id><pub-id pub-id-type="pmid">14523382</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owen</surname><given-names>AM</given-names></name><name><surname>McMillan</surname><given-names>KM</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Bullmore</surname><given-names>E</given-names></name></person-group><article-title>N-back working memory paradigm: A meta-analysis of normative functional neuroimaging studies</article-title><source>Hum. Brain Mapp.</source><year>2005</year><volume>25</volume><fpage>46</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1002/hbm.20131</pub-id><pub-id pub-id-type="pmid">15846822</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Jaeger, H. The &#x0201c;echo state&#x0201d; approach to analysing and training recurrent neural networks. Tech. Rep., GMD - German National Research Institute for Computer Science (2001).</mixed-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschl&#x000e4;ger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Comput.</source><year>2002</year><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>1984</year><volume>81</volume><fpage>3088</fpage><lpage>3092</lpage><pub-id pub-id-type="doi">10.1073/pnas.81.10.3088</pub-id><pub-id pub-id-type="pmid">6587342</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>C</given-names></name><name><surname>Kolodziejski</surname><given-names>C</given-names></name><name><surname>Timme</surname><given-names>M</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>W&#x000f6;rg&#x000f6;tter</surname><given-names>F</given-names></name></person-group><article-title>Synaptic scaling enables dynamically distinct short- and long-term memory formation</article-title><source>PLoS Comput. Biol.</source><year>2013</year><volume>9</volume><fpage>e1003307</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003307</pub-id><pub-id pub-id-type="pmid">24204240</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>AD</given-names></name><name><surname>Papagno</surname><given-names>C</given-names></name><name><surname>Vallar</surname><given-names>G</given-names></name></person-group><article-title>When long-term learning depends on short-term storage</article-title><source>J Mem. Lang.</source><year>1988</year><volume>27</volume><fpage>586</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1016/0749-596X(88)90028-9</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulme</surname><given-names>C</given-names></name><name><surname>Maughan</surname><given-names>S</given-names></name><name><surname>Brown</surname><given-names>GDA</given-names></name></person-group><article-title>Memory for familiar and unfamiliar words: Evidence for a long-term memory contribution to short-term memory span</article-title><source>J Mem. Lang.</source><year>1991</year><volume>30</volume><fpage>685</fpage><lpage>701</lpage><pub-id pub-id-type="doi">10.1016/0749-596X(91)90032-F</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Poirier, M., Dhir, P., Saint-Aubin, J., Tehan, G. &#x00026; Hampton, J. The influence of semantic memory on verbal short-term memory. In <italic>European Perspectives on Cognitive Science</italic> (New Bulgarian University Press, Sofia, 2011).</mixed-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marton</surname><given-names>K</given-names></name><name><surname>Eichorn</surname><given-names>N</given-names></name></person-group><article-title>Interaction between working memory and long-term memory</article-title><source>Z. Psychol</source><year>2014</year><volume>222</volume><fpage>90</fpage><lpage>99</lpage></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><year>2009</year><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Joshi</surname><given-names>P</given-names></name><name><surname>Sontag</surname><given-names>ED</given-names></name></person-group><article-title>Computational aspects of feedback in neural circuits</article-title><source>PLoS Comput. Biol.</source><year>2007</year><volume>3</volume><fpage>e165</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020165</pub-id><pub-id pub-id-type="pmid">17238280</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gros</surname><given-names>C</given-names></name></person-group><article-title>Cognitive computation with autonomously active neural networks: An emerging field</article-title><source>Cognit. Comput.</source><year>2009</year><volume>1</volume><fpage>77</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1007/s12559-008-9000-9</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Crisanti</surname><given-names>A</given-names></name></person-group><article-title>Chaos in random neural networks</article-title><source>Phys. Rev. Lett.</source><year>1988</year><volume>61</volume><fpage>259</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.61.259</pub-id><pub-id pub-id-type="pmid">10039285</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enel</surname><given-names>P</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Quilodran</surname><given-names>R</given-names></name><name><surname>Dominey</surname><given-names>PF</given-names></name></person-group><article-title>Reservoir computing properties of neural dynamics in prefrontal cortex</article-title><source>PLoS Comput. Biol.</source><year>2016</year><volume>12</volume><fpage>e1004967</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004967</pub-id><pub-id pub-id-type="pmid">27286251</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Comput.</source><year>2013</year><volume>25</volume><fpage>626</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miyashita</surname><given-names>Y</given-names></name></person-group><article-title>Neuronal correlate of visual associative long-term memory in the primate temporal cortex</article-title><source>Nature</source><year>1988</year><volume>335</volume><fpage>817</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1038/335817a0</pub-id><pub-id pub-id-type="pmid">3185711</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname><given-names>DJ</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><article-title>Correlations of cortical hebbian reverberations: theory versus experiment</article-title><source>J. Neurosci.</source><year>1994</year><volume>14</volume><fpage>6435</fpage><lpage>6445</lpage><pub-id pub-id-type="pmid">7965048</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>R</given-names></name><name><surname>Baxter</surname><given-names>P</given-names></name><name><surname>Belpaeme</surname><given-names>T</given-names></name></person-group><article-title>A review of long-term memory in natural and synthetic systems</article-title><source>Adapt. Behav.</source><year>2011</year><volume>20</volume><fpage>81</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1177/1059712311421219</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><article-title>Is cortical connectivity optimized for storing information?</article-title><source>Nat. Neurosci.</source><year>2016</year><volume>19</volume><fpage>749</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1038/nn.4286</pub-id><pub-id pub-id-type="pmid">27065365</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>1982</year><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juola</surname><given-names>JF</given-names></name><name><surname>Fischler</surname><given-names>I</given-names></name><name><surname>Wood</surname><given-names>CT</given-names></name><name><surname>Atkinson</surname><given-names>RC</given-names></name></person-group><article-title>Recognition time for information stored in long-term memory</article-title><source>Percept. Psychophys.</source><year>1971</year><volume>10</volume><fpage>8</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.3758/BF03205757</pub-id></element-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohring</surname><given-names>GA</given-names></name></person-group><article-title>Convergence time and finite size effects in neural networks</article-title><source>J. Phys. A Math. Gen.</source><year>1990</year><volume>23</volume><fpage>2237</fpage><lpage>2241</lpage><pub-id pub-id-type="doi">10.1088/0305-4470/23/11/047</pub-id></element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luko&#x00161;evi&#x0010d;ius</surname><given-names>M</given-names></name><name><surname>Jaeger</surname><given-names>H</given-names></name><name><surname>Schrauwen</surname><given-names>B</given-names></name></person-group><article-title>Reservoir computing trends</article-title><source>K&#x000fc;nstl. Intell.</source><year>2012</year><volume>26</volume><fpage>365</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1007/s13218-012-0204-5</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Sontag</surname><given-names>ED</given-names></name></person-group><article-title>Analog neural nets with gaussian or other common noise distributions cannot recognize arbitrary regular languages</article-title><source>Neural Comput.</source><year>1999</year><volume>11</volume><fpage>771</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1162/089976699300016656</pub-id><pub-id pub-id-type="pmid">10085429</pub-id></element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinovich</surname><given-names>MI</given-names></name><name><surname>Varona</surname><given-names>P</given-names></name><name><surname>Selverston</surname><given-names>AI</given-names></name><name><surname>Abarbanel</surname><given-names>HDI</given-names></name></person-group><article-title>Dynamical principles in neuroscience</article-title><source>Rev. Mod. Phys.</source><year>2006</year><volume>78</volume><fpage>1213</fpage><lpage>1265</lpage><pub-id pub-id-type="doi">10.1103/RevModPhys.78.1213</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luko&#x00161;evi&#x0010d;ius</surname><given-names>M</given-names></name><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><article-title>Reservoir computing approaches to recurrent neural network training</article-title><source>Comp. Sci. Rev.</source><year>2009</year><volume>3</volume><fpage>127</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.cosrev.2009.03.005</pub-id></element-citation></ref><ref id="CR50"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laje</surname><given-names>R</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title><source>Nat. Neurosci.</source><year>2013</year><volume>16</volume><fpage>925</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nn.3405</pub-id><pub-id pub-id-type="pmid">23708144</pub-id></element-citation></ref><ref id="CR51"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><article-title>Transferring learning from external to internal weights in echo-state networks with sparse connectivity</article-title><source>PLoS One</source><year>2012</year><volume>7</volume><fpage>e37372</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0037372</pub-id><pub-id pub-id-type="pmid">22655041</pub-id></element-citation></ref><ref id="CR52"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bick</surname><given-names>C</given-names></name><name><surname>Rabinovich</surname><given-names>MI</given-names></name></person-group><article-title>Dynamical origin of the effective storage capacity in the brain&#x02019;s working memory</article-title><source>Phys. Rev. Lett.</source><year>2009</year><volume>103</volume><fpage>218101</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.103.218101</pub-id><pub-id pub-id-type="pmid">20366069</pub-id></element-citation></ref><ref id="CR53"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>C</given-names></name><name><surname>Kolodziejski</surname><given-names>C</given-names></name><name><surname>Markelic</surname><given-names>I</given-names></name><name><surname>W&#x000f6;rg&#x000f6;tter</surname><given-names>F</given-names></name></person-group><article-title>Time scales of memory, learning, and plasticity</article-title><source>Biol. Cybern.</source><year>2012</year><volume>106</volume><fpage>715</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1007/s00422-012-0529-z</pub-id><pub-id pub-id-type="pmid">23160712</pub-id></element-citation></ref><ref id="CR54"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><article-title>Synaptic theory of working memory</article-title><source>Science</source><year>2008</year><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><pub-id pub-id-type="pmid">18339943</pub-id></element-citation></ref><ref id="CR55"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rose</surname><given-names>NS</given-names></name><etal/></person-group><article-title>Reactivation of latent working memories with transcranial magnetic stimulation</article-title><source>Science</source><year>2016</year><volume>354</volume><fpage>1136</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1126/science.aah7011</pub-id><pub-id pub-id-type="pmid">27934762</pub-id></element-citation></ref><ref id="CR56"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><etal/></person-group><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><year>2013</year><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="CR57"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Courtney</surname><given-names>SN</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Keil</surname><given-names>K</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><article-title>Transient and sustained activity in a distributed neural system for human working memory</article-title><source>Nature</source><year>1997</year><volume>386</volume><fpage>608</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1038/386608a0</pub-id><pub-id pub-id-type="pmid">9121584</pub-id></element-citation></ref><ref id="CR58"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donkin</surname><given-names>C</given-names></name><name><surname>Nosofsky</surname><given-names>RM</given-names></name><name><surname>Gold</surname><given-names>JM</given-names></name><name><surname>Shiffrin</surname><given-names>RM</given-names></name></person-group><article-title>Discrete-slots models of visual working-memory response times</article-title><source>Psychol. Rev.</source><year>2013</year><volume>120</volume><fpage>873</fpage><lpage>902</lpage><pub-id pub-id-type="doi">10.1037/a0034247</pub-id><pub-id pub-id-type="pmid">24015956</pub-id></element-citation></ref><ref id="CR59"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drover</surname><given-names>JD</given-names></name></person-group><article-title>Timing over tuning: Overcoming the shortcomings of a line attractor during a working memory task</article-title><source>PLoS Comput. Biol.</source><year>2014</year><volume>10</volume><fpage>e1003437</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003437</pub-id><pub-id pub-id-type="pmid">24499929</pub-id></element-citation></ref><ref id="CR60"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eriksson</surname><given-names>J</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Lansner</surname><given-names>A</given-names></name><name><surname>Bergstr&#x000f6;m</surname><given-names>F</given-names></name><name><surname>Nyberg</surname><given-names>L</given-names></name></person-group><article-title>Neurocognitive architecture of working memory</article-title><source>Neuron</source><year>2015</year><volume>88</volume><fpage>33</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.020</pub-id><pub-id pub-id-type="pmid">26447571</pub-id></element-citation></ref><ref id="CR61"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><article-title>Long short-term memory</article-title><source>Neural Comput.</source><year>1997</year><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="CR62"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Hitch</surname><given-names>G</given-names></name></person-group><article-title>Computational models of working memory: Putting long-term memory into context</article-title><source>Trends Cogn. Sci.</source><year>2005</year><volume>9</volume><fpage>535</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.09.011</pub-id><pub-id pub-id-type="pmid">16213782</pub-id></element-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Haykin, S. S. <italic>Adaptive Filter Theory</italic> 5th edn. (Prentice Hall, Upper Saddle River, NJ, 2002).</mixed-citation></ref><ref id="CR64"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>C</given-names></name><name><surname>Kolodziejski</surname><given-names>C</given-names></name><name><surname>Timme</surname><given-names>M</given-names></name><name><surname>W&#x000f6;rg&#x000f6;tter</surname><given-names>F</given-names></name></person-group><article-title>Synaptic scaling in combination with many generic plasticity mechanisms stabilizes circuit connectivity</article-title><source>Front. Comput. Neurosci.</source><year>2011</year><volume>5</volume><fpage>47</fpage><pub-id pub-id-type="doi">10.3389/fncom.2011.00047</pub-id><pub-id pub-id-type="pmid">22203799</pub-id></element-citation></ref></ref-list></back></article>