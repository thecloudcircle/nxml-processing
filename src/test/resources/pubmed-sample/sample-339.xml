
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">33352714</article-id><article-id pub-id-type="pmc">7766128</article-id><article-id pub-id-type="doi">10.3390/s20247309</article-id><article-id pub-id-type="publisher-id">sensors-20-07309</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Developing a Motor Imagery-Based Real-Time Asynchronous Hybrid BCI Controller for a Lower-Limb Exoskeleton</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Choi</surname><given-names>Junhyuk</given-names></name><xref ref-type="aff" rid="af1-sensors-20-07309">1</xref><xref ref-type="aff" rid="af2-sensors-20-07309">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2731-3915</contrib-id><name><surname>Kim</surname><given-names>Keun Tae</given-names></name><xref ref-type="aff" rid="af2-sensors-20-07309">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4200-0650</contrib-id><name><surname>Jeong</surname><given-names>Ji Hyeok</given-names></name><xref ref-type="aff" rid="af2-sensors-20-07309">2</xref><xref ref-type="aff" rid="af3-sensors-20-07309">3</xref></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Laehyun</given-names></name><xref ref-type="aff" rid="af2-sensors-20-07309">2</xref></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Song Joo</given-names></name><xref ref-type="aff" rid="af1-sensors-20-07309">1</xref><xref ref-type="aff" rid="af2-sensors-20-07309">2</xref><xref rid="c1-sensors-20-07309" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9527-0609</contrib-id><name><surname>Kim</surname><given-names>Hyungmin</given-names></name><xref ref-type="aff" rid="af1-sensors-20-07309">1</xref><xref ref-type="aff" rid="af2-sensors-20-07309">2</xref><xref rid="c1-sensors-20-07309" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-20-07309"><label>1</label>Division of Bio-Medical Science &#x00026; Technology, KIST School, Korea University of Science and Technology, Seoul 02792, Korea; <email>h14505@kist.re.kr</email></aff><aff id="af2-sensors-20-07309"><label>2</label>Center for Bionics, Biomedical Research Institute, Korea Institute of Science and Technology, Seoul 02792, Korea; <email>ktkim@kist.re.kr</email> (K.T.K.); <email>t20193@kist.re.kr</email> (J.H.J.); <email>laehyunk@kist.re.kr</email> (L.K.)</aff><aff id="af3-sensors-20-07309"><label>3</label>Department of Brain and Cognitive Engineering, Korea University, Seoul 02841, Korea</aff><author-notes><corresp id="c1-sensors-20-07309"><label>*</label>Correspondence: <email>songjoolee@kist.re.kr</email> (S.J.L.); <email>hk@kist.re.kr</email> (H.K.)</corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>12</month><year>2020</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2020</year></pub-date><volume>20</volume><issue>24</issue><elocation-id>7309</elocation-id><history><date date-type="received"><day>05</day><month>11</month><year>2020</year></date><date date-type="accepted"><day>14</day><month>12</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; 2020 by the authors.</copyright-statement><copyright-year>2020</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This study aimed to develop an intuitive gait-related motor imagery (MI)-based hybrid brain-computer interface (BCI) controller for a lower-limb exoskeleton and investigate the feasibility of the controller under a practical scenario including stand-up, gait-forward, and sit-down. A filter bank common spatial pattern (FBCSP) and mutual information-based best individual feature (MIBIF) selection were used in the study to decode MI electroencephalogram (EEG) signals and extract a feature matrix as an input to the support vector machine (SVM) classifier. A successive eye-blink switch was sequentially combined with the EEG decoder in operating the lower-limb exoskeleton. Ten subjects demonstrated more than 80% accuracy in both offline (training) and online. All subjects successfully completed a gait task by wearing the lower-limb exoskeleton through the developed real-time BCI controller. The BCI controller achieved a time ratio of 1.45 compared with a manual smartwatch controller. The developed system can potentially be benefit people with neurological disorders who may have difficulties operating manual control.</p></abstract><kwd-group><kwd>hybrid BCI</kwd><kwd>EEG</kwd><kwd>motor imagery</kwd><kwd>FBCSP</kwd><kwd>lower-limb exoskeleton</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-20-07309"><title>1. Introduction</title><p>Brain&#x02013;computer interface (BCI) technology benefits people suffering from neurological disorders on account of its characteristics of various computer-controlled applications using brain signals [<xref rid="B1-sensors-20-07309" ref-type="bibr">1</xref>,<xref rid="B2-sensors-20-07309" ref-type="bibr">2</xref>]. The recent development of a lower-limb exoskeleton is significant, considering the fact it effectively bridges between brain signals and a motor output of extremities to improve the quality of life of the gait disabilities [<xref rid="B3-sensors-20-07309" ref-type="bibr">3</xref>,<xref rid="B4-sensors-20-07309" ref-type="bibr">4</xref>,<xref rid="B5-sensors-20-07309" ref-type="bibr">5</xref>]. Among the various electroencephalogram (EEG) neural features, three distinguishable ones have been adopted notably for decoding lower-limb movement intentions, namely movement-related cortical potential (MRCP), steady-state visual evoked potential (SSVEP), and event-related desynchronization (ERD). However, utilizing the MRCP for the exoskeleton control requires the BCI system to discern a movement onset time [<xref rid="B6-sensors-20-07309" ref-type="bibr">6</xref>]. In the case of the SSVEP [<xref rid="B7-sensors-20-07309" ref-type="bibr">7</xref>], subjects have to continuously focus on a flickering light until the evoked potential exceeds a threshold. Thereby, it is difficult for the exoskeleton drivers to deal with an unexpected outer situation. Fundamentally, the ERD is another representative EEG neural feature for the exoskeleton BCI controller, usually induced by motor imagery (MI). An asynchronous MI-based ERD indicates both spectral and spatial features. Hence, the BCI controller can match various commands related to distinctive MI strategies with separable scalp topographic patterns [<xref rid="B8-sensors-20-07309" ref-type="bibr">8</xref>].</p><p>In the very beginning, project DARPA tried to move prosthetics based on the sensorimotor signals of the cortical activity [<xref rid="B9-sensors-20-07309" ref-type="bibr">9</xref>,<xref rid="B10-sensors-20-07309" ref-type="bibr">10</xref>]. Additionally, the former EU project named MINDWALKER proceeded lower-limb exoskeleton for clinical use with EEG and various biological and kinematic control signals through advanced algorithms [<xref rid="B11-sensors-20-07309" ref-type="bibr">11</xref>,<xref rid="B12-sensors-20-07309" ref-type="bibr">12</xref>]. The underlying studies adopted MRCP, SSVEP, and evoked potential (EP) to control robotic devices. Lately, several research groups have reported tenable results in operating an overground lower-limb exoskeleton with the MI-based BCI [<xref rid="B13-sensors-20-07309" ref-type="bibr">13</xref>,<xref rid="B14-sensors-20-07309" ref-type="bibr">14</xref>,<xref rid="B15-sensors-20-07309" ref-type="bibr">15</xref>,<xref rid="B16-sensors-20-07309" ref-type="bibr">16</xref>] Gordleeva et al. developed an exoskeleton control system utilizing three MI tasks (left, right hand MI, and rest) and subsequently captured the ERD of sensorimotor rhythms (SMR) for 14 subjects [<xref rid="B13-sensors-20-07309" ref-type="bibr">13</xref>]. Lee et al. captured an EEG power spectral density during the hand MI and rest and performed exoskeleton mounted navigation tasks with five subjects [<xref rid="B14-sensors-20-07309" ref-type="bibr">14</xref>]. Wang et al. compared an SSVEP and an MI-based BCI controller to move the lower-limb exoskeleton with four subjects and revealed that both controllers achieved about 80% accuracy [<xref rid="B15-sensors-20-07309" ref-type="bibr">15</xref>]. Yu et al. developed an MI-based ERD decoder that could control the walking speed of a rehabilitation exoskeleton on the treadmill [<xref rid="B16-sensors-20-07309" ref-type="bibr">16</xref>]. However, the aforementioned studies still adopted the left and right (or both) hand MI to generate a corresponding command output for controlling the lower-limb exoskeleton. To our knowledge, there were a few pieces of research inducing a gait-related MI [<xref rid="B17-sensors-20-07309" ref-type="bibr">17</xref>,<xref rid="B18-sensors-20-07309" ref-type="bibr">18</xref>,<xref rid="B19-sensors-20-07309" ref-type="bibr">19</xref>]. Firstly, Do et al. adopted a kinesthetic MI (KMI) to refine motor skills in sports science and cognitive neurophysiology [<xref rid="B17-sensors-20-07309" ref-type="bibr">17</xref>]. Lopez et al. considered it as a motor-attempt to move subjects&#x02019; right leg as if they have started walking [<xref rid="B18-sensors-20-07309" ref-type="bibr">18</xref>]. Finally, Donati et al. trained spinal cord injury (SCI) patients with kick imagery during a rehabilitation program [<xref rid="B19-sensors-20-07309" ref-type="bibr">19</xref>]. Notably, it is still considered that previously mentioned MI protocols focused on the fragments of gait motions. Hence, presenting a limited correlation between the imagery and the execution, and only utilized a neural mechanism that is discriminative at a cortical level. Therefore, MIs for operating the overground lower-limb exoskeleton throughout an entire &#x02018;sit-to-sit&#x02019; scenario should be more intuitive and associated with stand-up, gait-forward, and sit-down, which may reduce a cognitive load and increase decoding accuracies [<xref rid="B20-sensors-20-07309" ref-type="bibr">20</xref>].</p><p>A real-life MI-based BCI controller for the lower-limb exoskeleton should maintain a low false activation rate in order to ensure the reliability of a control system. A &#x02018;brain switch&#x02019; is a representative concept necessary for the asynchronous BCI to determine whether an ongoing continuous EEG signal implies the user&#x02019;s intention or not [<xref rid="B21-sensors-20-07309" ref-type="bibr">21</xref>,<xref rid="B22-sensors-20-07309" ref-type="bibr">22</xref>,<xref rid="B23-sensors-20-07309" ref-type="bibr">23</xref>,<xref rid="B24-sensors-20-07309" ref-type="bibr">24</xref>,<xref rid="B25-sensors-20-07309" ref-type="bibr">25</xref>]. Pfurtscheller et al. demonstrated that the on/off switch utilizing a foot MI-induced beta Event-related Synchronization (ERS) rebound measured from a single vertex channel prevents the false activation of an SSVEP interface [<xref rid="B26-sensors-20-07309" ref-type="bibr">26</xref>]. Yu et al. extracted a subject&#x02019;s voluntary successive eye-blink signal from an ongoing EEG signal from two prefrontal channels to activate/deactivate a P300-based speller [<xref rid="B24-sensors-20-07309" ref-type="bibr">24</xref>]. Notably, Ortiz et al. recently introduced an attention level monitor parallel with an MI gamma-band SMR, which detects a subject&#x02019;s presence or absence of an MI intention [<xref rid="B25-sensors-20-07309" ref-type="bibr">25</xref>]. Based on previous researches, this study monitored EEG artifact from an electrooculogram (EOG) signal to extract a user&#x02019;s intentional triple eye-blink (TEB) signals to turn on and off the MI decoder under a concept of a sequentially processed hybrid BCI for improving reliabilities of the control system [<xref rid="B27-sensors-20-07309" ref-type="bibr">27</xref>].</p><p>Thus, in this study, we developed an MI-based BCI controller for a lower-limb exoskeleton to perform stand-up, gait, and sit-down, sequentially combined with an eye-blink switch considering a real-life scenario. The feasibility of the developed BCI exoskeleton system was tested with ten healthy subjects to explore the potentiality of its application to people with neurological impairments. This study mainly aimed to reduce a variation between the MI manner and motor output of the mounted exoskeleton. To accomplish this, we designed intuitive MI protocols, which correspond with the lower-limb exoskeleton operation.</p></sec><sec sec-type="methods" id="sec2-sensors-20-07309"><title>2. Methods</title><sec id="sec2dot1-sensors-20-07309"><title>2.1. System Overview</title><p>The developed MI-based BCI exoskeleton control system consists of three parts, namely data acquisition, EEG signal processing, and exoskeleton control (<xref ref-type="fig" rid="sensors-20-07309-f001">Figure 1</xref>). While the subject performs MI tasks (i.e., the kinesthetic feeling of gait and sit), a signal processing algorithm extracts features and trains the offline classifier. A decoded control command is sent to the exoskeleton via a real-time online control interface. We employed a lower-limb exoskeleton robot (RoboWear P10, NT Robot, Seoul, Korea) to integrate the developed BCI controller. The exoskeleton robot was primarily designed to assist people with SCI gait impairments (Class III Medical Device Certification, Ministry of Food and Drug Safety of Korea) to stand-up, sit-down, and gait-forward with two crutches on both hands [<xref rid="B28-sensors-20-07309" ref-type="bibr">28</xref>].</p></sec><sec id="sec2dot2-sensors-20-07309"><title>2.2. Data Acquisition</title><sec id="sec2dot2dot1-sensors-20-07309"><title>2.2.1. EEG System</title><p>Throughout the entire experiment, brain activity was monitored by a wireless wet-type 31 electrodes according to the international 10&#x02013;20 system (FP1, FP2, F7, F3, F4, F8, FC5, FC3, FC1, FC2, FC4, FC6, C3, C1, Cz, C2, C4, CP5, CP3, CP1, CP2, CP4, CP6, P3, P1, Pz, P2, P4, O1, Oz, and O2. The reference electrode is FCz and the ground is AFz). Each electrode collected brain signal at a 500 Hz sampling rate through an EEG amplifier (actiCHamp and MOVE, BrainProducts GmbH, Gilching, Germany). The impedance level was set below 20 K&#x003a9;, and a notch filter cleared 60 Hz line noise.</p><p>Ten healthy subjects (age: 26.6 &#x000b1; 3.06 years.) with no history of neurological disorders participated in this study. The subjects were all male and right-handed. All subjects gave written informed consent, which was approved by the Institutional Review Board of Korea Institute of Science and Technology (KIST IRB number 2019-032). Eight out of 10 subjects had no prior experience in BCI or wearing a powered gait assistive device. We allowed the subjects a one-hour adaptation period to familiarize themselves with operating the wearable exoskeleton.</p></sec><sec id="sec2dot2dot2-sensors-20-07309"><title>2.2.2. MI Protocol</title><p>To minimize external interference, the MIs were performed in an isolated room. The subjects are standing with their hands-on crutches without wearing the lower-limb exoskeleton and facing a monitor, which displayed MI procedures (<xref ref-type="fig" rid="sensors-20-07309-f001">Figure 1</xref>). The subjects were to press a hand-held button attached to the crutch when they were ready to begin each trial. Following the notification of a beep sound, the monitor displayed a gray fixation cross and randomly presented a symbol (&#x02018;upward arrow,&#x02019; &#x02018;downward arrow,&#x02019; or &#x02018;box&#x02019;) after 3&#x02013;5 s, which denotes &#x02018;Gait MI,&#x02019; &#x02018;Sit MI,&#x02019; or &#x02018;Do-nothing,&#x02019; respectively. Once the subjects identified the cue, they started the corresponding MI (&#x02018;Gait&#x02019; or &#x02018;Sit&#x02019;) for 8 s or &#x02018;Do-nothing&#x02019; for 4 s. When the subjects heard a second beep sound, they stopped the task and prepared for the subsequent trial. <xref ref-type="fig" rid="sensors-20-07309-f002">Figure 2</xref> shows the MI procedure.</p><p>Each subject executed two types of MI tasks (&#x02018;Gait&#x02019; and &#x02018;Sit&#x02019;) along with a &#x02018;Do-nothing&#x02019; task. In the &#x02018;Do-nothing&#x02019; task, we let subjects rest with their eyes open without performing MI or other mental tasks. The subjects were instructed during the MI tasks to perform a mental rehearsal of gait or sit. The limbs were to remain still and they were to focus on the kinesthetic feelings, including a somatosensory sensation and experience of motor execution with the exoskeleton. Furthermore, we forbade subjects from visualizing themselves from the viewpoint of an external observer to limit stimulating their visual cortex. The details of the comments were listed in <xref rid="sensors-20-07309-t001" ref-type="table">Table 1</xref>.</p><p>The offline MI procedure consisted of randomly mixed 90 trials, which constituted 30 repetitions for three tasks; Gait MI, Sit MI, and Do-nothing. The whole process was organized and presented on the monitor by a managing software (E-prime3, Psychology Software Tools, Sharpsburg, PA, USA) with an event marking module (BBTK USB TTL, The Black Box ToolKit Ltd., Sheffield, UK).</p></sec></sec><sec id="sec2dot3-sensors-20-07309"><title>2.3. EEG Signal Processing</title><p>EEG signal processing was conducted using MATLAB software (2017a, MathWorks, Natick, MA, USA), which received data through a TCP/IP connection from Remote Data Access host (Recorder, BrainProducts, Gilching, Germany). The offline MI data features were extracted through a Filter Bank Common Spatial Pattern (FBCSP) algorithm. Through a mutual information-based best individual feature (MIBIF) selection method, we sort contributing features as training input to a linear support vector machine (SVM) classifier.</p><sec id="sec2dot3dot1-sensors-20-07309"><title>2.3.1. Feature Extraction</title><p>Since we focused on the gait-related SMR feature, we monitored ERD from low mu to high beta EEG frequency bands. EEG signals were passed through the zero-phase Butterworth infinite impulse response (IIR) bandpass filter between high Theta to low Gamma frequency (7&#x02013;34 Hz). The signals were divided into 6 ranges (filter bank; 7&#x02013;9, 10&#x02013;12, 13&#x02013;15, 16&#x02013;20, 21&#x02013;25, and 26&#x02013;34 Hz) considering the subject-dependent dominant frequency features. Next, six bandpass-filtered EEG data were prepared to derive six different CSP transformation matrices.</p><p>The single-trial EEG input signal matrix E (where <italic>N</italic> &#x000d7; <italic>T</italic>; <italic>N</italic> is the number of channels; <italic>T</italic> the number of samples in time per channel) is linearly transformed by projection matrix <italic>W</italic>. The spatially filtered signal <italic>Z</italic> given as
<disp-formula id="FD1-sensors-20-07309"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We have decided to choose the first and last two rows of signal <inline-formula><mml:math id="mm2"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula>, which differentiate the most [<xref rid="B29-sensors-20-07309" ref-type="bibr">29</xref>]. Therefore, the modified transformation matrix has four rows of six frequency bands and channel columns (24 &#x000d7; 31). Finally, the variance difference maximized EEG signals were then log-normalized [<xref rid="B30-sensors-20-07309" ref-type="bibr">30</xref>].</p></sec><sec id="sec2dot3dot2-sensors-20-07309"><title>2.3.2. Feature Selection</title><p>The 24 features then sorted in descending order following the MIBIF method [<xref rid="B30-sensors-20-07309" ref-type="bibr">30</xref>], which determined the priority of the signal contributions of well differentiating the two classes. The mutual information of two random variables defined as,
<disp-formula id="FD2-sensors-20-07309"><label>(2)</label><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a joint probability mass function of <inline-formula><mml:math id="mm5"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are a marginal probability mass function of <inline-formula><mml:math id="mm9"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> respectively. Here, <inline-formula><mml:math id="mm11"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> is each of 24 features, and <inline-formula><mml:math id="mm12"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> is the corresponding classifier label <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>v</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>h</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>v</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The first <inline-formula><mml:math id="mm14"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> features are empirically selected according to each subject (<inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>~</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Finally, the resulting feature matrix was adopted for training the linear SVM classifier.</p></sec><sec id="sec2dot3dot3-sensors-20-07309"><title>2.3.3. Real-Time Decoder</title><p>The online and offline decoders were synced in signal processing steps. The real-time input EEG signals were sent to the online decoder in every single packet of 31 channels by 10 data points (500 Hz sampling rate). The decoding algorithm ran every 250 data points (window shift). The pre-trained linear SVM classifier outputted a single control command every 0.5 s with a signal processing window size of 2 s. Then, the control interface received the commands to control the exoskeleton.</p></sec></sec><sec id="sec2dot4-sensors-20-07309"><title>2.4. BCI Controller</title><p>To describe an online system logic flow, we illustrate a finite state machine (FSM) of the control interface (<xref ref-type="fig" rid="sensors-20-07309-f003">Figure 3</xref>). The system should be started and terminated from the sit state for safety purposes. The state transitions were represented by arrows corresponding to methods (MI, Do-nothing, or TEB), denoted beside the arrow. Notably, a recurrent arrow indicated that the system remains in the current state.</p><p>We designed two binary classifiers. In the state of &#x02018;Decoder On (GvN)&#x02019;, the &#x02018;classifier_GvN&#x02019; decodes Gait MI vs. Do-nothing EEG signal. In the &#x02018;Decoder On (GvS)&#x02019; state, the &#x02018;classifier_GvS&#x02019; separates Gait MI vs. Sit MI.</p><sec id="sec2dot4dot1-sensors-20-07309"><title>2.4.1. Triple Eye Blink</title><p>We utilize TEB (online 97 trials test, a detection rate of 94.7%; online 40.5 min test, FPR of 0.025 times/min; <italic>n</italic> = 1) to activate and terminate the decoder. Notably, a blinking artifact easily influenced two prefrontal channels among the adopted electrode locations in this study. For both FP1 and FP2 electrodes, a 2&#x02013;15 Hz range of IIR bandpass filter was integrated to clear the signals related to the non-eyelid movement. Subsequently, a biorthogonal wavelet function was adopted to enlarge the eye-blink pulse efficiently. Finally, we could count the wave peak, which exceeded a predefined threshold in separating single or double ordinary occasional eye-blinks. A window size of TEB detection was 1.6 s with a window shift of 0.4 s [<xref rid="B31-sensors-20-07309" ref-type="bibr">31</xref>].</p></sec><sec id="sec2dot4dot2-sensors-20-07309"><title>2.4.2. MI Buffer and Visual Feedback</title><p>We adopted command stack buffers to minimize potential risks to safety based on a single false detection of the movement intention, as shown in <xref ref-type="fig" rid="sensors-20-07309-f004">Figure 4</xref>. There were three buffers of Sit-to-Stand, Stand-to-Gait and Stand-to-Sit in each size of 10, which is necessary for subjects to engage MI tasks with the exoskeleton movement. First, in the &#x02018;Decoder On (GvN)&#x02019; state, the robot stands-up only when the repetitive correct Gait MI command fully filled the Sit-to-Stand buffer, while the Do-nothing command emptied the stacked buffer. Second, in the &#x02018;Decoder On (GvS)&#x02019; state, while the Gait MI command filled the Stand-to-Gait buffer, the Stand-to-Sit buffer emptied at the same time, vice versa. The fill/empty ratio of the buffer was set as 1:3 in order to provide reliable state transitions by balancing between the correct and false classification [<xref rid="B32-sensors-20-07309" ref-type="bibr">32</xref>].</p></sec></sec><sec id="sec2dot5-sensors-20-07309"><title>2.5. System Evaluation</title><sec id="sec2dot5dot1-sensors-20-07309"><title>2.5.1. Controller Performance</title><p>The online BCI controller was compared with a ready-made smartwatch controller through a predefined 10 m gait scenario to evaluate the developed exoskeleton BCI controller feasibility (<xref ref-type="fig" rid="sensors-20-07309-f005">Figure 5</xref>). All subjects executed stand-up, start 5 m gait and stop, resume 5 m gait and stop again, and finally sit-down. The wearable smartwatch (Galaxy gear series 1, SAMSUNG, Suwon, Korea) and the application were provided to control the exoskeleton (<xref ref-type="fig" rid="sensors-20-07309-f006">Figure 6</xref>). Three control commands (&#x02018;stand-up/gait-stop&#x02019;, &#x02018;gait&#x02019;, and &#x02018;sit-down&#x02019;) were transmitted through a Bluetooth wireless communication to the exoskeleton control computer. We compared the required time to complete the gait scenario between the BCI controller and the smartwatch controller.</p></sec><sec id="sec2dot5dot2-sensors-20-07309"><title>2.5.2. Classification Accuracy</title><p>To evaluate the performance of two binary decoders in offline, we measured classification accuracy of 100 repetitions with the prepared MI data composed with 7:3 of train-test ratio. Initially, randomly chosen trials constituted 10 test questions, and 10 train guesses were sampled by the Bootstrap restoration method except for the test trials. The total result was averaged and reported with a standard deviation.</p><p>For the online decoder, we recorded the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) of the two classifiers while subjects were executing the gait scenario. The classifier_GvS showed all four occasions hence the accuracy of the decoder could be calculated. On the other hand, the classifier_GvN operated only a single time during the entire gait scenario. Consequently, we chose to use TPR as an online accuracy measurement of the classifier_GvN.
<disp-formula id="FD3-sensors-20-07309"><label>(3)</label><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4-sensors-20-07309"><label>(4)</label><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD5-sensors-20-07309"><label>(5)</label><mml:math id="mm18"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> stands for the numbers of each of the four parameters: <italic>TP</italic>, <italic>TN</italic>, <italic>FP</italic>, and <italic>FN</italic>. The entire performance of the online decoder was determined as a lower number of the accuracy of two classifiers.</p></sec><sec id="sec2dot5dot3-sensors-20-07309"><title>2.5.3. Information Transfer Rate</title><p>An information transfer rate (ITR) in communication per unit time was calculated as follows:<disp-formula id="FD6-sensors-20-07309"><label>(6)</label><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD7-sensors-20-07309"><label>(7)</label><mml:math id="mm21"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the bit rate (bit/trial) and <inline-formula><mml:math id="mm23"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> denotes the number of tasks (in this case, <inline-formula><mml:math id="mm24"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> = 3). <inline-formula><mml:math id="mm25"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> denotes decoding accuracy, and <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the decision rate (trial/min) [<xref rid="B33-sensors-20-07309" ref-type="bibr">33</xref>]. In the offline session, we assumed the theoretical decision rate as the 90 trial repetitions divided by a total accumulated time of engaging MI for each subject (average of 4.60 trial/min). In the online session, we set the decision rate as an accumulated time of the MI during the entire gait scenario (average of 5.97 trial/min).</p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-20-07309"><title>3. Results</title><sec id="sec3dot1-sensors-20-07309"><title>3.1. Feature Selection</title><p>The MI repetition data was processed to reveal discriminant MI features (<xref ref-type="fig" rid="sensors-20-07309-f007">Figure 7</xref>). Through a Fisher&#x02019;s ratio topography, we could estimate electrodes with a high signal-to-noise ratio. Based on those representative electrodes, we examined a trial-averaged event-related spectral perturbation (ERSP) spectrogram (<xref ref-type="fig" rid="sensors-20-07309-f008">Figure 8</xref>) [<xref rid="B34-sensors-20-07309" ref-type="bibr">34</xref>]. The spectrogram reveals that the ERD appeared while subjects are engaging in both Gait MI and Sit MI, whereas less or no ERD was observed during Do-nothing task.</p></sec><sec id="sec3dot2-sensors-20-07309"><title>3.2. System Evaluation</title><sec id="sec3dot2dot1-sensors-20-07309"><title>3.2.1. Control Performance</title><p><xref rid="sensors-20-07309-t002" ref-type="table">Table 2</xref> indicates the time taken to accomplish the 10 m gait scenario of 10 subjects. The hybrid BCI controller showed 144.8 &#x000b1; 15.12% of average performance in terms of operation time compared to the smartwatch controller. <xref ref-type="app" rid="app1-sensors-20-07309">Supplementary Video S1</xref> is provided to compare the consuming time between the smartwatch controller and the hybrid BCI controller.</p></sec><sec id="sec3dot2dot2-sensors-20-07309"><title>3.2.2. Classification Accuracy</title><p>As mentioned in <xref ref-type="sec" rid="sec2dot5dot2-sensors-20-07309">Section 2.5.2</xref>, the accuracy of the 10 decoders for each subject were inspected through 100 train-test repetitions. The classifier_GvN showed 88.4 &#x000b1; 7.48% accuracy, while the classifier_GvS showed 80.3 &#x000b1; 6.79% accuracy (<xref rid="sensors-20-07309-t003" ref-type="table">Table 3</xref>).</p><p>The online decoder accuracy was estimated by a log record following the execution of the real-time 10 m gait scenario (<xref ref-type="fig" rid="sensors-20-07309-f009">Figure 9</xref>). During the operation, each subject engaged MI for at least four times; (1) to stand-up, do Gait MI for the classifier_GvN, (2) to start gait, do Gait MI for the classifier_GvS, after the TEB (3) to gait again, after gait pause, do same as (2), (4) finally to sit-down, do Sit MI for the classifier_GvS. If the subject failed to fill the corresponding buffer, they made subsequent attempts until they succeeded. The online accuracy was around 85% for both classifiers (<xref rid="sensors-20-07309-t003" ref-type="table">Table 3</xref>).</p></sec><sec id="sec3dot2dot3-sensors-20-07309"><title>3.2.3. Information Transfer Rate</title><p><xref rid="sensors-20-07309-t004" ref-type="table">Table 4</xref> shows the ITR for all subjects. By estimating the ITR, we could evaluate the efficiencies of the developed BCI controllers. The offline and online ITR was 3.21 bit/min and 3.13 bit/min on average, respectively.</p></sec></sec></sec><sec sec-type="discussion" id="sec4-sensors-20-07309"><title>4. Discussion</title><p>In this study, we developed an MI-based hybrid BCI controller for the lower-limb exoskeleton operation. The subjects could control the exoskeleton to stand-up, gait start/stop, and sit-down without any steer or button press using the real-time TEB switch and EEG decoder. Ten healthy subjects participated in the offline and online sessions, and the average classification accuracy was more than 80% for both sessions. All subjects completed a 10-m walking scenario with the lower-limb exoskeleton using the MI-based hybrid BCI controller and spent 145% of the control time compared with the conventional smartwatch controller.</p><sec id="sec4dot1-sensors-20-07309"><title>4.1. Characteristics of the EEG Decoder</title><p>As shown in <xref ref-type="fig" rid="sensors-20-07309-f007">Figure 7</xref>, the Gait MI vs. Do-nothing topographic plot appeared relatively consistent through the subjects around a motor and somatosensory area than the Gait MI vs. Sit MI. Following the study of the most prominent electrode channel, we illustrated the MI-related power desynchronization from low Mu (8&#x02013;12 Hz) to around high Beta (13&#x02013;30 Hz) frequency band by trial-averaged time-frequency wavelet analysis (<xref ref-type="fig" rid="sensors-20-07309-f008">Figure 8</xref>). The baseline was mean amplitude through the entire epoch time. Within 1 s after the MI cue disappeared, the ERD was revealed in the 10&#x02013;15 Hz band while few subjects showed EEG signals in the upper bandwidth (21&#x02013;25 Hz band or higher for S6). According to the research of Cebolla et al., significant ERSP appeared between Mu and low Beta frequency (8 ~ 17 Hz) in FCz channel, induced by the context based MI [<xref rid="B35-sensors-20-07309" ref-type="bibr">35</xref>]. Our result also revealed the correlation between MI and spatial-spectral cortical activity on the mu and beta rhythm in the primary motor cortex, consistent with the previous studies [<xref rid="B36-sensors-20-07309" ref-type="bibr">36</xref>,<xref rid="B37-sensors-20-07309" ref-type="bibr">37</xref>,<xref rid="B38-sensors-20-07309" ref-type="bibr">38</xref>]. Additionally, the result demonstrated that the adopted FBCSP algorithm [<xref rid="B30-sensors-20-07309" ref-type="bibr">30</xref>] was suitable for incorporating the difference between the Gait MI vs. Do-nothing and the Gait MI vs. Sit MI in terms of both subject-specific spectral and spatial domain.</p><p>According to <xref ref-type="fig" rid="sensors-20-07309-f009">Figure 9</xref>A, there were continuous misclassifications. Additionally, subjects experienced a delayed movement of buffer during the MI tasks. The repeated false classification attributed mainly to the EEG processing window set as 2 s length with a 0.5-s window shift. Consequently, if there were a dominant false feature inside the window, it required at least four steps to renew the signal processing window. Moreover, the decoder cannot respond to the subjects&#x02019; immediate intention change, consequently allowing a long buffer reaction time. In further research, this problem could be mitigated by shortening the window or reducing the effect of artifacts and noise.</p></sec><sec id="sec4dot2-sensors-20-07309"><title>4.2. Performance of the BCI Controller</title><p>In our study, 10 subjects demonstrated 1.45 of the average time ratio compared with the smartwatch controller. The result suggested that the developed controller could accommodate further improvement. Compared with the existing manual controller, previously developed BCI controllers showed an average time ratio of 2.03 for lower-limb exoskeleton [<xref rid="B7-sensors-20-07309" ref-type="bibr">7</xref>], 1.27&#x02013;1.35 for remote-controlled mobile robots [<xref rid="B39-sensors-20-07309" ref-type="bibr">39</xref>,<xref rid="B40-sensors-20-07309" ref-type="bibr">40</xref>]. According to the aforementioned studies which presented less performance, considering the subjects were in an ambulatory environment instead of sitting still to control the exoskeleton.</p><p>Utilizing the FBCSP algorithm, we could discriminate gait-related SMR with more than 80% accuracy both offline and online. Meanwhile, the classifier_GvN presented an average of 8%-point higher offline accuracy (<italic>t</italic>(18) = 2.6, <italic>p</italic> = 0.018) and 2%-point higher online accuracy (<italic>t</italic>(18) = 0.7, <italic>p</italic> = 0.495) than the classifier_GvS (<xref rid="sensors-20-07309-t002" ref-type="table">Table 2</xref>). Thus, the EEG feature difference between the Gait MI and the Do-nothing appeared to be more discriminative than the two MIs. Based on interviews of the subjects, we could assess that non-repeating single action imagery such as Sit MI may be less effective in causing the EEG signal variations than the Gait MI, which is relatively familiar and straightforward. This variation might be the reason that the Gait MI vs. Sit MI classification results were not as high as that of Gait MI and Do-nothing despite the instructions and guidelines (<xref rid="sensors-20-07309-t001" ref-type="table">Table 1</xref>). Further experiments should consider these concerns about the MI protocol.</p></sec><sec id="sec4dot3-sensors-20-07309"><title>4.3. Limitations and Future Direction</title><p>Notably, we acknowledged the existence of numerous alternative novel algorithms for decoding neural features of the EEG signal [<xref rid="B41-sensors-20-07309" ref-type="bibr">41</xref>,<xref rid="B42-sensors-20-07309" ref-type="bibr">42</xref>,<xref rid="B43-sensors-20-07309" ref-type="bibr">43</xref>,<xref rid="B44-sensors-20-07309" ref-type="bibr">44</xref>,<xref rid="B45-sensors-20-07309" ref-type="bibr">45</xref>]. Among them, deep learning and EEG channel optimization methods are the most relevant methods for this study. Convolutional Neural Network and its applied algorithms are the prominent and spotlighted algorithm for MI signal toward an image domain analysis through the ERSP or short-time Fourier transform (STFT) [<xref rid="B43-sensors-20-07309" ref-type="bibr">43</xref>]. Additionally, the EEG MI signals present prevailing spatial feature via a multi-electrodes channel. Consequently, it is recommended to adopt the channel selection method to enhance the performance of the decoder [<xref rid="B44-sensors-20-07309" ref-type="bibr">44</xref>]. Further research can proceed from the above-mentioned updating algorithms concerning practical BCI application. While competing with the classification accuracies, in this study, for the first time, we tried to focus on demonstrating the feasibility of the real-time operation of the lower-limb exoskeleton with the gait-related MI accompanied by a conventional yet well-settled FBCSP algorithm. Our approach and findings can form a basis for further developing an online BCI controller for aiding gait disabilities.</p><p>Due to the natural and endogenous characteristics of the MI-actuated exoskeleton, it is the most corresponding BCI application to a fundamental property in terms of it&#x02019;s goal-direct and voluntary nature [<xref rid="B3-sensors-20-07309" ref-type="bibr">3</xref>]. Therefore, it is significant that the BCI controlled lower-limb exoskeleton could be advantageous in rehabilitation circumstances [<xref rid="B19-sensors-20-07309" ref-type="bibr">19</xref>,<xref rid="B46-sensors-20-07309" ref-type="bibr">46</xref>,<xref rid="B47-sensors-20-07309" ref-type="bibr">47</xref>,<xref rid="B48-sensors-20-07309" ref-type="bibr">48</xref>]. Patients with lower-limb disabilities following a stroke or SCI devote their efforts to regaining the utility of their limbs. The traditional rehabilitation paradigm has been bottom-up, i.e., physical therapists or treadmill move patients&#x02019; limb repeatedly to trigger neuroplasticity in the brain. Contrarily, a self-paced assistive exoskeleton controller directly decodes the brain signal and bypasses the path to the damaged limb [<xref rid="B49-sensors-20-07309" ref-type="bibr">49</xref>]. Accompanied by this top-down and the classic bottom-up rehabilitation route, a closed-loop feedback interface brings the promising result for the disabilities to regain ambulation ability at will [<xref rid="B50-sensors-20-07309" ref-type="bibr">50</xref>,<xref rid="B51-sensors-20-07309" ref-type="bibr">51</xref>]. Other researches have also demonstrated the effect of MI-based rehabilitation on balancing or ambulatory skills [<xref rid="B19-sensors-20-07309" ref-type="bibr">19</xref>,<xref rid="B52-sensors-20-07309" ref-type="bibr">52</xref>]. While this study presents the feasibility of the real-time intuitive MI-based hybrid BCI controller with a wearable exoskeleton on healthy subjects, testing the system with the patients is our intended future study. Further research will recruit more subjects including a SCI gait impairment for practical real-life BCI applications, accompanied by an advanced display device such as portable augmented reality (AR) glasses with an MI assistive environment [<xref rid="B53-sensors-20-07309" ref-type="bibr">53</xref>]. We expect that the gait rehabilitation with a BCI-controlled exoskeleton can significantly improve the degree of motor recovery.</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><app-group><app id="app1-sensors-20-07309"><title>Supplementary Materials</title><p>The following are available online at <uri xlink:href="https://www.mdpi.com/1424-8220/20/24/7309/s1">https://www.mdpi.com/1424-8220/20/24/7309/s1</uri>, Video S1.</p><supplementary-material content-type="local-data" id="sensors-20-07309-s001"><media xlink:href="sensors-20-07309-s001.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization and methodology, J.C., K.T.K. and H.K.; resources, J.C.; investigation, J.C. and K.T.K.; software, J.C. and J.H.J.; formal analysis, J.C.; data curation, J.C.; validation, J.C. and J.H.J.; visualization, J.C.; writing-original draft, J.C.; writing-review &#x00026; editing, K.T.K., S.J.L. and H.K.; supervision, S.J.L. and H.K.; project administration, L.K., S.J.L. and H.K.; funding acquisition, L.K. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Funding</title><p>This work was supported in part by the Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korean Government (Development of Non-Invasive Integrated BCI SW Platform to Control Home Appliances and External Devices by User&#x02019;s Thought via AR/VR Interface) under Grant 2017-0-00432.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-20-07309"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Neuper</surname><given-names>C.</given-names></name></person-group><article-title>Motor imagery and direct brain-computer communication</article-title><source>Proc. IEEE</source><year>2001</year><volume>89</volume><fpage>1123</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1109/5.939829</pub-id></element-citation></ref><ref id="B2-sensors-20-07309"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpaw</surname><given-names>J.</given-names></name><name><surname>Birbaumer</surname><given-names>N.</given-names></name><name><surname>Heetderks</surname><given-names>W.J.</given-names></name><name><surname>Mcfarland</surname><given-names>D.</given-names></name><name><surname>Peckham</surname><given-names>P.</given-names></name><name><surname>Schalk</surname><given-names>G.</given-names></name><name><surname>Donchin</surname><given-names>E.</given-names></name><name><surname>Quatrano</surname><given-names>L.A.</given-names></name><name><surname>Robinson</surname><given-names>C.</given-names></name><name><surname>Vaughan</surname><given-names>T.</given-names></name></person-group><article-title>Brain-Computer interface technology: A review of the first international meeting</article-title><source>IEEE Trans. Rehabil. Eng.</source><year>2000</year><volume>8</volume><fpage>164</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1109/TRE.2000.847807</pub-id><pub-id pub-id-type="pmid">10896178</pub-id></element-citation></ref><ref id="B3-sensors-20-07309"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>Y.</given-names></name><name><surname>Eguren</surname><given-names>D.</given-names></name><name><surname>Azor&#x000ed;n</surname><given-names>J.M.</given-names></name><name><surname>Grossman</surname><given-names>R.G.</given-names></name><name><surname>Luu</surname><given-names>T.P.</given-names></name><name><surname>Contreras-Vidal</surname><given-names>J.L.</given-names></name></person-group><article-title>Brain-machine interfaces for controlling lower-limb powered robotic systems</article-title><source>J. Neural Eng.</source><year>2018</year><volume>15</volume><fpage>21004</fpage><pub-id pub-id-type="doi">10.1088/1741-2552/aaa8c0</pub-id><pub-id pub-id-type="pmid">29345632</pub-id></element-citation></ref><ref id="B4-sensors-20-07309"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tariq</surname><given-names>M.</given-names></name><name><surname>Trivailo</surname><given-names>P.M.</given-names></name><name><surname>Simic</surname><given-names>M.</given-names></name></person-group><article-title>EEG-Based BCI Control Schemes for Lower-Limb Assistive-Robots</article-title><source>Front. Hum. Neurosci.</source><year>2018</year><volume>12</volume><fpage>312</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2018.00312</pub-id><pub-id pub-id-type="pmid">30127730</pub-id></element-citation></ref><ref id="B5-sensors-20-07309"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaughan</surname><given-names>T.M.</given-names></name><name><surname>McFarland</surname><given-names>D.J.</given-names></name><name><surname>Schalk</surname><given-names>G.</given-names></name><name><surname>Sarnacki</surname><given-names>W.A.</given-names></name><name><surname>Krusienski</surname><given-names>D.J.</given-names></name><name><surname>Sellers</surname><given-names>E.W.</given-names></name><name><surname>Wolpaw</surname><given-names>J.R.</given-names></name></person-group><article-title>The wadsworth BCI research and development program: At home with BCI</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2006</year><volume>14</volume><fpage>229</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2006.875577</pub-id><pub-id pub-id-type="pmid">16792301</pub-id></element-citation></ref><ref id="B6-sensors-20-07309"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>J.-H.</given-names></name><name><surname>Kwak</surname><given-names>N.-S.</given-names></name><name><surname>Lee</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>S.</given-names></name></person-group><article-title>Decoding of walking Intention under Lower limb exoskeleton Environment using MRCP Feature</article-title><source>Proceedings of the GBCIC</source><conf-loc>Graz, Austria</conf-loc><conf-date>18&#x02013;22 September 2017</conf-date></element-citation></ref><ref id="B7-sensors-20-07309"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwak</surname><given-names>N.-S.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K.-R.</given-names></name><name><surname>Lee</surname><given-names>S.-W.</given-names></name></person-group><article-title>A lower limb exoskeleton control system based on steady state visual evoked potentials</article-title><source>J. Neural Eng.</source><year>2015</year><volume>12</volume><fpage>56009</fpage><pub-id pub-id-type="doi">10.1088/1741-2560/12/5/056009</pub-id></element-citation></ref><ref id="B8-sensors-20-07309"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Lopes da Silva</surname><given-names>F.H.</given-names></name></person-group><article-title>Event-related EEG/MEG synchronization and desynchronization: Basic principles</article-title><source>Clin. Neurophysiol. Off. J. Int. Fed. Clin. Neurophysiol.</source><year>1999</year><volume>110</volume><fpage>1842</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1016/S1388-2457(99)00141-8</pub-id></element-citation></ref><ref id="B9-sensors-20-07309"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miranda</surname><given-names>R.A.</given-names></name><name><surname>Casebeer</surname><given-names>W.D.</given-names></name><name><surname>Hein</surname><given-names>A.M.</given-names></name><name><surname>Judy</surname><given-names>J.W.</given-names></name><name><surname>Krotkov</surname><given-names>E.P.</given-names></name><name><surname>Laabs</surname><given-names>T.L.</given-names></name><name><surname>Manzo</surname><given-names>J.E.</given-names></name><name><surname>Pankratz</surname><given-names>K.G.</given-names></name><name><surname>Pratt</surname><given-names>G.A.</given-names></name><name><surname>Sanchez</surname><given-names>J.C.</given-names></name><etal/></person-group><article-title>DARPA-funded efforts in the development of novel brain-computer interface technologies</article-title><source>J. Neurosci. Methods</source><year>2015</year><volume>244</volume><fpage>52</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.07.019</pub-id><pub-id pub-id-type="pmid">25107852</pub-id></element-citation></ref><ref id="B10-sensors-20-07309"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname><given-names>J.J.</given-names></name></person-group><article-title>Toward Direct Brain-Computer Communication</article-title><source>Annu. Rev. Biophys. Bioeng.</source><year>1973</year><volume>2</volume><fpage>157</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1146/annurev.bb.02.060173.001105</pub-id><pub-id pub-id-type="pmid">4583653</pub-id></element-citation></ref><ref id="B11-sensors-20-07309"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheron</surname><given-names>G.</given-names></name><name><surname>Duvinage</surname><given-names>M.</given-names></name><name><surname>De Saedeleer</surname><given-names>C.</given-names></name><name><surname>Castermans</surname><given-names>T.</given-names></name><name><surname>Bengoetxea</surname><given-names>A.</given-names></name><name><surname>Petieau</surname><given-names>M.</given-names></name><name><surname>Seetharaman</surname><given-names>K.</given-names></name><name><surname>Hoellinger</surname><given-names>T.</given-names></name><name><surname>Dan</surname><given-names>B.</given-names></name><name><surname>Dutoit</surname><given-names>T.</given-names></name><etal/></person-group><article-title>From spinal central pattern generators to cortical network: Integrated BCI for walking rehabilitation</article-title><source>Neural Plast.</source><year>2012</year><volume>2012</volume><pub-id pub-id-type="doi">10.1155/2012/375148</pub-id></element-citation></ref><ref id="B12-sensors-20-07309"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Meijneke</surname><given-names>C.</given-names></name><name><surname>Van Asseldonk</surname><given-names>E.</given-names></name><name><surname>Hoellinger</surname><given-names>T.</given-names></name><name><surname>Cheron</surname><given-names>G.</given-names></name><name><surname>Ivanenko</surname><given-names>Y.</given-names></name><name><surname>La Scaleia</surname><given-names>V.</given-names></name><name><surname>Sylos-Labini</surname><given-names>F.</given-names></name><name><surname>Molinari</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Design and Control of the MINDWALKER Exoskeleton</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2015</year><volume>23</volume><fpage>277</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2014.2365697</pub-id><pub-id pub-id-type="pmid">25373109</pub-id></element-citation></ref><ref id="B13-sensors-20-07309"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordleeva</surname><given-names>S.</given-names></name><name><surname>Lukoyanov</surname><given-names>M.V.</given-names></name><name><surname>Mineev</surname><given-names>S.</given-names></name><name><surname>Khoruzhko</surname><given-names>M.A.</given-names></name><name><surname>Mironov</surname><given-names>V.</given-names></name><name><surname>Kaplan</surname><given-names>A.</given-names></name><name><surname>Kazantsev</surname><given-names>V.</given-names></name></person-group><article-title>Exoskeleton Control System Based on Motor-Imaginary Brain&#x02013;Computer Interface</article-title><source>Sovrem. Tehnol. Med.</source><year>2017</year><volume>9</volume><fpage>31</fpage><pub-id pub-id-type="doi">10.17691/stm2017.9.3.04</pub-id></element-citation></ref><ref id="B14-sensors-20-07309"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Liu</surname><given-names>D.</given-names></name><name><surname>Perroud</surname><given-names>L.</given-names></name><name><surname>Chavarriaga</surname><given-names>R.</given-names></name><name><surname>del Mill&#x000e1;n</surname><given-names>J.R.</given-names></name></person-group><article-title>Endogenous Control of Powered Lower-Limb Exoskeleton</article-title><source>Proceedings of the Wearable Robotics: Challenges and Trends</source><conf-loc>Segovia, Spain</conf-loc><conf-date>18&#x02013;21 October 2016</conf-date><person-group person-group-type="editor"><name><surname>Gonz&#x000e1;lez-Vargas</surname><given-names>J.</given-names></name><name><surname>Ib&#x000e1;&#x000f1;ez</surname><given-names>J.</given-names></name><name><surname>Contreras-Vidal</surname><given-names>J.L.</given-names></name><name><surname>van der Kooij</surname><given-names>H.</given-names></name><name><surname>Pons</surname><given-names>J.L.</given-names></name></person-group><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2017</year><fpage>115</fpage><lpage>119</lpage></element-citation></ref><ref id="B15-sensors-20-07309"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Wu</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Ma</surname><given-names>Y.</given-names></name></person-group><article-title>Implementation of a Brain-Computer Interface on a Lower-Limb Exoskeleton</article-title><source>IEEE Access</source><year>2018</year><volume>6</volume><fpage>38524</fpage><lpage>38534</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2853628</pub-id></element-citation></ref><ref id="B16-sensors-20-07309"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>EEG-based brain-controlled lower extremity exoskeleton rehabilitation robot</article-title><source>Proceedings of the 2017 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)</source><conf-loc>Ningbo, China</conf-loc><conf-date>19&#x02013;21 November 2017</conf-date><fpage>763</fpage><lpage>767</lpage></element-citation></ref><ref id="B17-sensors-20-07309"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Do</surname><given-names>A.H.</given-names></name><name><surname>Wang</surname><given-names>P.T.</given-names></name><name><surname>King</surname><given-names>C.E.</given-names></name><name><surname>Chun</surname><given-names>S.N.</given-names></name><name><surname>Nenadic</surname><given-names>Z.</given-names></name></person-group><article-title>Brain-computer interface controlled robotic gait orthosis</article-title><source>J. Neuroeng. Rehabil.</source><year>2013</year><volume>10</volume><fpage>111</fpage><pub-id pub-id-type="doi">10.1186/1743-0003-10-111</pub-id><pub-id pub-id-type="pmid">24321081</pub-id></element-citation></ref><ref id="B18-sensors-20-07309"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>L&#x000f3;pez-Larraz</surname><given-names>E.</given-names></name><name><surname>Trincado-Alonso</surname><given-names>F.</given-names></name><name><surname>Rajasekaran</surname><given-names>V.</given-names></name><name><surname>P&#x000e9;rez-Nombela</surname><given-names>S.</given-names></name><name><surname>del-Ama</surname><given-names>A.J.</given-names></name><name><surname>Aranda</surname><given-names>J.</given-names></name><name><surname>Minguez</surname><given-names>J.</given-names></name><name><surname>Gil-Agudo</surname><given-names>A.</given-names></name><name><surname>Montesano</surname><given-names>L.</given-names></name></person-group><article-title>Control of an Ambulatory Exoskeleton with a Brain&#x02013;Machine Interface for Spinal Cord Injury Gait Rehabilitation</article-title><source>Front. Neurosci.</source><year>2016</year><volume>10</volume><fpage>359</fpage><pub-id pub-id-type="doi">10.3389/fnins.2016.00359</pub-id><pub-id pub-id-type="pmid">27536214</pub-id></element-citation></ref><ref id="B19-sensors-20-07309"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donati</surname><given-names>A.R.C.</given-names></name><name><surname>Shokur</surname><given-names>S.</given-names></name><name><surname>Morya</surname><given-names>E.</given-names></name><name><surname>Campos</surname><given-names>D.S.F.</given-names></name><name><surname>Moioli</surname><given-names>R.C.</given-names></name><name><surname>Gitti</surname><given-names>C.M.</given-names></name><name><surname>Augusto</surname><given-names>P.B.</given-names></name><name><surname>Tripodi</surname><given-names>S.</given-names></name><name><surname>Pires</surname><given-names>C.G.</given-names></name><name><surname>Pereira</surname><given-names>G.A.</given-names></name><etal/></person-group><article-title>Long-Term Training with a Brain-Machine Interface-Based Gait Protocol Induces Partial Neurological Recovery in Paraplegic Patients</article-title><source>Sci. Rep.</source><year>2016</year><volume>6</volume><fpage>30383</fpage><pub-id pub-id-type="doi">10.1038/srep30383</pub-id><pub-id pub-id-type="pmid">27513629</pub-id></element-citation></ref><ref id="B20-sensors-20-07309"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talukdar</surname><given-names>U.</given-names></name><name><surname>Hazarika</surname><given-names>S.M.</given-names></name><name><surname>Gan</surname><given-names>J.Q.</given-names></name></person-group><article-title>Motor imagery and mental fatigue: Inter-relationship and EEG based estimation</article-title><source>J. Comput. Neurosci.</source><year>2019</year><volume>46</volume><fpage>55</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1007/s10827-018-0701-0</pub-id><pub-id pub-id-type="pmid">30488148</pub-id></element-citation></ref><ref id="B21-sensors-20-07309"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Townsend</surname><given-names>G.</given-names></name><name><surname>Graimann</surname><given-names>B.</given-names></name><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name></person-group><article-title>Continuous EEG classification during motor imagery-simulation of an asynchronous BCI</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2004</year><volume>12</volume><fpage>258</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2004.827220</pub-id><pub-id pub-id-type="pmid">15218939</pub-id></element-citation></ref><ref id="B22-sensors-20-07309"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>C.-H.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K.-R.</given-names></name><name><surname>Hwang</surname><given-names>H.-J.</given-names></name></person-group><article-title>Brain-Switches for Asynchronous Brain&#x02013;Computer Interfaces: A Systematic Review</article-title><source>Electronics</source><year>2020</year><volume>9</volume><elocation-id>422</elocation-id><pub-id pub-id-type="doi">10.3390/electronics9030422</pub-id></element-citation></ref><ref id="B23-sensors-20-07309"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>C.-H.</given-names></name><name><surname>Kim</surname><given-names>E.</given-names></name><name><surname>Im</surname><given-names>C.-H.</given-names></name></person-group><article-title>Development of a Brain-Computer Interface Toggle Switch with Low False-Positive Rate Using Respiration-Modulated Photoplethysmography</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>348</elocation-id><pub-id pub-id-type="doi">10.3390/s20020348</pub-id></element-citation></ref><ref id="B24-sensors-20-07309"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Yin</surname><given-names>E.</given-names></name><name><surname>Jiang</surname><given-names>J.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Hu</surname><given-names>D.</given-names></name></person-group><article-title>An Asynchronous Hybrid Spelling Approach Based on EEG&#x02013;EOG Signals for Chinese Character Input</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2019</year><volume>27</volume><fpage>1292</fpage><lpage>1302</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2019.2914916</pub-id><pub-id pub-id-type="pmid">31071045</pub-id></element-citation></ref><ref id="B25-sensors-20-07309"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ortiz</surname><given-names>M.</given-names></name><name><surname>Ferrero</surname><given-names>L.</given-names></name><name><surname>I&#x000e1;&#x000f1;ez</surname><given-names>E.</given-names></name><name><surname>Azor&#x000ed;n</surname><given-names>J.M.</given-names></name><name><surname>Contreras-Vidal</surname><given-names>J.L.</given-names></name></person-group><article-title>Sensory Integration in Human Movement: A New Brain-Machine Interface Based on Gamma Band and Attention Level for Controlling a Lower-Limb Exoskeleton</article-title><source>Front. Bioeng. Biotechnol.</source><year>2020</year><volume>8</volume><pub-id pub-id-type="doi">10.3389/fbioe.2020.00735</pub-id><pub-id pub-id-type="pmid">33014987</pub-id></element-citation></ref><ref id="B26-sensors-20-07309"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Solis-Escalante</surname><given-names>T.</given-names></name><name><surname>Ortner</surname><given-names>R.</given-names></name><name><surname>Linortner</surname><given-names>P.</given-names></name><name><surname>M&#x000fc;ller-Putz</surname><given-names>G.R.</given-names></name></person-group><article-title>Self-paced operation of an SSVEP-Based orthosis with and without an imagery-based &#x0201c;brain switch:&#x0201d; A feasibility study towards a hybrid BCI</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng. Publ. IEEE Eng. Med. Biol. Soc.</source><year>2010</year><volume>18</volume><fpage>409</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2010.2040837</pub-id><pub-id pub-id-type="pmid">20144923</pub-id></element-citation></ref><ref id="B27-sensors-20-07309"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Allison</surname><given-names>B.Z.</given-names></name><name><surname>Bauernfeind</surname><given-names>G.</given-names></name><name><surname>Brunner</surname><given-names>C.</given-names></name><name><surname>Solis Escalante</surname><given-names>T.</given-names></name><name><surname>Scherer</surname><given-names>R.</given-names></name><name><surname>Zander</surname><given-names>T.O.</given-names></name><name><surname>Mueller-Putz</surname><given-names>G.</given-names></name><name><surname>Neuper</surname><given-names>C.</given-names></name><name><surname>Birbaumer</surname><given-names>N.</given-names></name></person-group><article-title>The hybrid BCI</article-title><source>Front. Neurosci.</source><year>2010</year><volume>4</volume><fpage>3</fpage><pub-id pub-id-type="doi">10.3389/fnpro.2010.00003</pub-id><pub-id pub-id-type="pmid">20582257</pub-id></element-citation></ref><ref id="B28-sensors-20-07309"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y.</given-names></name><name><surname>Song</surname><given-names>C.</given-names></name><name><surname>Park</surname><given-names>J.</given-names></name></person-group><article-title>Development of actuation system for wearable robots using spiral spring</article-title><source>Proceedings of the 2012 12th International Conference on Control, Automation and Systems</source><conf-loc>Jeju Island, Korea</conf-loc><conf-date>17&#x02013;21 October 2012</conf-date><fpage>1863</fpage><lpage>1868</lpage></element-citation></ref><ref id="B29-sensors-20-07309"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramoser</surname><given-names>H.</given-names></name><name><surname>Muller-Gerking</surname><given-names>J.</given-names></name><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name></person-group><article-title>Optimal spatial filtering of single trial EEG during imagined hand movement</article-title><source>IEEE Trans. Rehabil. Eng.</source><year>2000</year><volume>8</volume><fpage>441</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1109/86.895946</pub-id><pub-id pub-id-type="pmid">11204034</pub-id></element-citation></ref><ref id="B30-sensors-20-07309"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ang</surname><given-names>K.K.</given-names></name><name><surname>Chin</surname><given-names>Z.Y.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Guan</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Filter Bank Common Spatial Pattern Algorithm on BCI Competition IV Datasets 2a and 2b</article-title><source>Front. Neurosci.</source><year>2012</year><volume>6</volume><fpage>39</fpage><pub-id pub-id-type="doi">10.3389/fnins.2012.00039</pub-id><pub-id pub-id-type="pmid">22479236</pub-id></element-citation></ref><ref id="B31-sensors-20-07309"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>R.</given-names></name><name><surname>Schachter</surname><given-names>E.</given-names></name><name><surname>Miranda</surname><given-names>M.</given-names></name></person-group><article-title>Recognition and Real-Time Detection of Blinking Eyes on Electroencephalographic Signals Using Wavelet Transform</article-title><source>Proceedings of the Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</source><conf-loc>Buenos Aires, Argentina</conf-loc><conf-date>3&#x02013;6 September 2012</conf-date><person-group person-group-type="editor"><name><surname>Alvarez</surname><given-names>L.</given-names></name><name><surname>Mejail</surname><given-names>M.</given-names></name><name><surname>Gomez</surname><given-names>L.</given-names></name><name><surname>Jacobo</surname><given-names>J.</given-names></name></person-group><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2012</year><fpage>682</fpage><lpage>690</lpage></element-citation></ref><ref id="B32-sensors-20-07309"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>J.</given-names></name><name><surname>Kim</surname><given-names>K.</given-names></name><name><surname>Lee</surname><given-names>J.</given-names></name><name><surname>Lee</surname><given-names>S.J.</given-names></name><name><surname>Kim</surname><given-names>H.</given-names></name></person-group><article-title>Robust Semi-synchronous BCI Controller for Brain-Actuated Exoskeleton System</article-title><source>Proceedings of the 2020 8th International Winter Conference on Brain-Computer Interface (BCI)</source><conf-loc>High1 Resort, Korea</conf-loc><conf-date>18&#x02013;20 February 2020</conf-date><fpage>1</fpage><lpage>3</lpage></element-citation></ref><ref id="B33-sensors-20-07309"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mcfarland</surname><given-names>D.</given-names></name><name><surname>Sarnacki</surname><given-names>W.</given-names></name><name><surname>Wolpaw</surname><given-names>J.</given-names></name></person-group><article-title>Brain-computer interface (BCI) operation: Optimizing information transfer rates</article-title><source>Biol. Psychol.</source><year>2003</year><volume>63</volume><fpage>237</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1016/S0301-0511(03)00073-5</pub-id><pub-id pub-id-type="pmid">12853169</pub-id></element-citation></ref><ref id="B34-sensors-20-07309"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A.</given-names></name><name><surname>Makeig</surname><given-names>S.</given-names></name></person-group><article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>J. Neurosci. Methods</source><year>2004</year><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="B35-sensors-20-07309"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cebolla</surname><given-names>A.M.</given-names></name><name><surname>Petieau</surname><given-names>M.</given-names></name><name><surname>Cevallos</surname><given-names>C.</given-names></name><name><surname>Leroy</surname><given-names>A.</given-names></name><name><surname>Dan</surname><given-names>B.</given-names></name><name><surname>Cheron</surname><given-names>G.</given-names></name></person-group><article-title>Long-lasting cortical reorganization as the result of motor imagery of throwing a ball in a virtual tennis court</article-title><source>Front. Psychol.</source><year>2015</year><volume>6</volume><fpage>1869</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01869</pub-id><pub-id pub-id-type="pmid">26648903</pub-id></element-citation></ref><ref id="B36-sensors-20-07309"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabate</surname><given-names>M.</given-names></name><name><surname>Llanos</surname><given-names>C.</given-names></name><name><surname>Enriquez</surname><given-names>E.</given-names></name><name><surname>D&#x000ed;az</surname><given-names>M.</given-names></name></person-group><article-title>Mu rhythm, visual processing and motor control</article-title><source>Clin. Neurophysiol.</source><year>2011</year><volume>123</volume><fpage>550</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2011.07.034</pub-id><pub-id pub-id-type="pmid">21840253</pub-id></element-citation></ref><ref id="B37-sensors-20-07309"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stinear</surname><given-names>C.M.</given-names></name><name><surname>Byblow</surname><given-names>W.D.</given-names></name><name><surname>Steyvers</surname><given-names>M.</given-names></name><name><surname>Levin</surname><given-names>O.</given-names></name><name><surname>Swinnen</surname><given-names>S.P.</given-names></name></person-group><article-title>Kinesthetic, but not visual, motor imagery modulates corticomotor excitability</article-title><source>Exp. Brain Res.</source><year>2006</year><volume>168</volume><fpage>157</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-0078-y</pub-id><pub-id pub-id-type="pmid">16078024</pub-id></element-citation></ref><ref id="B38-sensors-20-07309"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tariq</surname><given-names>M.</given-names></name><name><surname>Trivailo</surname><given-names>P.M.</given-names></name><name><surname>Simic</surname><given-names>M.</given-names></name></person-group><article-title>Mu-Beta event-related (de)synchronization and EEG classification of left-right foot dorsiflexion kinaesthetic motor imagery for BCI</article-title><source>PLoS ONE</source><year>2020</year><volume>15</volume><elocation-id>e0230184</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0230184</pub-id><pub-id pub-id-type="pmid">32182270</pub-id></element-citation></ref><ref id="B39-sensors-20-07309"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Millan</surname><given-names>J.R.</given-names></name><name><surname>Renkens</surname><given-names>F.</given-names></name><name><surname>Mourino</surname><given-names>J.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group><article-title>Noninvasive brain-actuated control of a mobile robot by human EEG</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2004</year><volume>51</volume><fpage>1026</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1109/TBME.2004.827086</pub-id><pub-id pub-id-type="pmid">15188874</pub-id></element-citation></ref><ref id="B40-sensors-20-07309"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chae</surname><given-names>Y.</given-names></name><name><surname>Jeong</surname><given-names>J.</given-names></name><name><surname>Jo</surname><given-names>S.</given-names></name></person-group><article-title>Toward Brain-Actuated Humanoid Robots: Asynchronous Direct Control Using an EEG-Based BCI</article-title><source>IEEE Trans. Robot.</source><year>2012</year><volume>28</volume><fpage>1131</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1109/TRO.2012.2201310</pub-id></element-citation></ref><ref id="B41-sensors-20-07309"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Sun</surname><given-names>S.</given-names></name></person-group><article-title>Single-trial EEG classification of motor imagery using deep convolutional neural networks</article-title><source>Optik</source><year>2017</year><volume>130</volume><fpage>11</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.ijleo.2016.10.117</pub-id></element-citation></ref><ref id="B42-sensors-20-07309"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>Y.</given-names></name><name><surname>Banville</surname><given-names>H.</given-names></name><name><surname>Albuquerque</surname><given-names>I.</given-names></name><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Falk</surname><given-names>T.H.</given-names></name><name><surname>Faubert</surname><given-names>J.</given-names></name></person-group><article-title>Deep learning-based electroencephalography analysis: A systematic review</article-title><source>J. Neural Eng.</source><year>2019</year><volume>16</volume><fpage>51001</fpage><pub-id pub-id-type="doi">10.1088/1741-2552/ab260c</pub-id></element-citation></ref><ref id="B43-sensors-20-07309"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ha</surname><given-names>K.-W.</given-names></name><name><surname>Jeong</surname><given-names>J.-W.</given-names></name></person-group><article-title>Motor Imagery EEG Classification Using Capsule Networks</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>2854</elocation-id><pub-id pub-id-type="doi">10.3390/s19132854</pub-id></element-citation></ref><ref id="B44-sensors-20-07309"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>J.</given-names></name><name><surname>Xiao</surname><given-names>R.</given-names></name><name><surname>Daly</surname><given-names>I.</given-names></name><name><surname>Miao</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Cichocki</surname><given-names>A.</given-names></name></person-group><article-title>Internal Feature Selection Method of CSP Based on L1-Norm and Dempster-Shafer Theory</article-title><source>IEEE Trans. Neural Networks Learn. Syst.</source><year>2020</year><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2020.3015505</pub-id></element-citation></ref><ref id="B45-sensors-20-07309"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>C.</given-names></name><name><surname>Daly</surname><given-names>I.</given-names></name><name><surname>Miao</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Cichocki</surname><given-names>A.</given-names></name></person-group><article-title>Bispectrum-Based Channel Selection for Motor Imagery Based Brain-Computer Interfacing</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2020</year><volume>28</volume><fpage>2153</fpage><lpage>2163</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2020.3020975</pub-id><pub-id pub-id-type="pmid">32870796</pub-id></element-citation></ref><ref id="B46-sensors-20-07309"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebedev</surname><given-names>M.A.</given-names></name><name><surname>Nicolelis</surname><given-names>M.A.L.</given-names></name></person-group><article-title>Brain-Machine Interfaces: From Basic Science to Neuroprostheses and Neurorehabilitation</article-title><source>Physiol. Rev.</source><year>2017</year><volume>97</volume><fpage>767</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1152/physrev.00027.2016</pub-id><pub-id pub-id-type="pmid">28275048</pub-id></element-citation></ref><ref id="B47-sensors-20-07309"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bockbrader</surname><given-names>M.A.</given-names></name><name><surname>Francisco</surname><given-names>G.</given-names></name><name><surname>Lee</surname><given-names>R.</given-names></name><name><surname>Olson</surname><given-names>J.</given-names></name><name><surname>Solinsky</surname><given-names>R.</given-names></name><name><surname>Boninger</surname><given-names>M.L.</given-names></name></person-group><article-title>Brain Computer Interfaces in Rehabilitation Medicine</article-title><source>PM R</source><year>2018</year><volume>10</volume><fpage>S233</fpage><lpage>S243</lpage><pub-id pub-id-type="doi">10.1016/j.pmrj.2018.05.028</pub-id><pub-id pub-id-type="pmid">30269808</pub-id></element-citation></ref><ref id="B48-sensors-20-07309"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazarou</surname><given-names>I.</given-names></name><name><surname>Nikolopoulos</surname><given-names>S.</given-names></name><name><surname>Petrantonakis</surname><given-names>P.C.</given-names></name><name><surname>Kompatsiaris</surname><given-names>I.</given-names></name><name><surname>Tsolaki</surname><given-names>M.</given-names></name></person-group><article-title>EEG-Based Brain&#x02013;Computer Interfaces for Communication and Rehabilitation of People with Motor Impairment: A Novel Approach of the 21st Century</article-title><source>Front. Hum. Neurosci.</source><year>2018</year><volume>12</volume><fpage>14</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2018.00014</pub-id><pub-id pub-id-type="pmid">29472849</pub-id></element-citation></ref><ref id="B49-sensors-20-07309"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Neuper</surname><given-names>C.</given-names></name><name><surname>Muller</surname><given-names>G.R.</given-names></name><name><surname>Obermaier</surname><given-names>B.</given-names></name><name><surname>Krausz</surname><given-names>G.</given-names></name><name><surname>Schlogl</surname><given-names>A.</given-names></name><name><surname>Scherer</surname><given-names>R.</given-names></name><name><surname>Graimann</surname><given-names>B.</given-names></name><name><surname>Keinrath</surname><given-names>C.</given-names></name><name><surname>Skliris</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Graz-BCI: State of the art and clinical applications</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2003</year><volume>11</volume><fpage>177</fpage><pub-id pub-id-type="doi">10.1109/TNSRE.2003.814454</pub-id><pub-id pub-id-type="pmid">12899267</pub-id></element-citation></ref><ref id="B50-sensors-20-07309"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sitaram</surname><given-names>R.</given-names></name><name><surname>Ros</surname><given-names>T.</given-names></name><name><surname>Stoeckel</surname><given-names>L.</given-names></name><name><surname>Haller</surname><given-names>S.</given-names></name><name><surname>Scharnowski</surname><given-names>F.</given-names></name><name><surname>Lewis-Peacock</surname><given-names>J.</given-names></name><name><surname>Weiskopf</surname><given-names>N.</given-names></name><name><surname>Blefari</surname><given-names>M.L.</given-names></name><name><surname>Rana</surname><given-names>M.</given-names></name><name><surname>Oblak</surname><given-names>E.</given-names></name><etal/></person-group><article-title>Closed-loop brain training: The science of neurofeedback</article-title><source>Nat. Rev. Neurosci.</source><year>2017</year><volume>18</volume><fpage>86</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.164</pub-id><pub-id pub-id-type="pmid">28003656</pub-id></element-citation></ref><ref id="B51-sensors-20-07309"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morone</surname><given-names>G.</given-names></name><name><surname>Spitoni</surname><given-names>G.F.</given-names></name><name><surname>De Bartolo</surname><given-names>D.</given-names></name><name><surname>Ghanbari Ghooshchy</surname><given-names>S.</given-names></name><name><surname>Di Iulio</surname><given-names>F.</given-names></name><name><surname>Paolucci</surname><given-names>S.</given-names></name><name><surname>Zoccolotti</surname><given-names>P.</given-names></name><name><surname>Iosa</surname><given-names>M.</given-names></name></person-group><article-title>Rehabilitative devices for a top-down approach</article-title><source>Expert Rev. Med. Devices</source><year>2019</year><volume>16</volume><fpage>187</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1080/17434440.2019.1574567</pub-id><pub-id pub-id-type="pmid">30677307</pub-id></element-citation></ref><ref id="B52-sensors-20-07309"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>H.-Y.</given-names></name><name><surname>Kim</surname><given-names>J.-S.</given-names></name><name><surname>Lee</surname><given-names>G.-C.</given-names></name></person-group><article-title>Effects of motor imagery training on balance and gait abilities in post-stroke patients: A randomized controlled trial</article-title><source>Clin. Rehabil.</source><year>2012</year><volume>27</volume><pub-id pub-id-type="doi">10.1177/0269215512464702</pub-id><pub-id pub-id-type="pmid">23129815</pub-id></element-citation></ref><ref id="B53-sensors-20-07309"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cevallos</surname><given-names>C.</given-names></name><name><surname>Zarka</surname><given-names>D.</given-names></name><name><surname>Hoellinger</surname><given-names>T.</given-names></name><name><surname>Leroy</surname><given-names>A.</given-names></name><name><surname>Dan</surname><given-names>B.</given-names></name><name><surname>Cheron</surname><given-names>G.</given-names></name></person-group><article-title>Oscillations in the human brain during walking execution, imagination and observation</article-title><source>Neuropsychologia</source><year>2015</year><volume>79</volume><fpage>223</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.06.039</pub-id><pub-id pub-id-type="pmid">26164473</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-20-07309-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>A diagram of the motor imagery (MI)-based brain-computer interface (BCI) exoskeleton control system.</p></caption><graphic xlink:href="sensors-20-07309-g001"/></fig><fig id="sensors-20-07309-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The offline MI procedure.</p></caption><graphic xlink:href="sensors-20-07309-g002"/></fig><fig id="sensors-20-07309-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The finite state machine is illustrated in the diagram. The transition between states is indicated in the manner of triple eye-blink (TEB) or MI, respectively. Subjects engage MI in &#x02018;Decoder On&#x02019; shown in double circle.</p></caption><graphic xlink:href="sensors-20-07309-g003"/></fig><fig id="sensors-20-07309-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The illustration of the feedback buffer which represented the size of 10 of the exoskeleton BCI controller. (<bold>A</bold>) MI buffer is linked to the classifier_GvN (Gait MI vs. Do-nothing). (<bold>B</bold>) MI buffer is linked to the classifier_GvS (Gait MI vs. Sit MI).</p></caption><graphic xlink:href="sensors-20-07309-g004"/></fig><fig id="sensors-20-07309-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>The illustration of the online exoskeleton operation plan. All subjects drove exoskeleton with the scenario of stand-up, gait, pause, resume gait, stop, and sit-down. The subjects completed the procedure two times (one with the BCI controller and the other with the smartwatch controller) to compare the performance of the developed BCI controller and the smartwatch controller.</p></caption><graphic xlink:href="sensors-20-07309-g005"/></fig><fig id="sensors-20-07309-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>The wearable smartwatch for exoskeleton manual controller (<bold>A</bold>) and the application which was replaced by the BCI controller in this study (<bold>B</bold>).</p></caption><graphic xlink:href="sensors-20-07309-g006"/></fig><fig id="sensors-20-07309-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>A topography of normalized Fisher ratio between Gait MI(Gait) vs. Do-nothing (Dnth) and Gait MI vs. Sit MI(Sit). Repeated trials of signal power in each frequency band were averaged to calculate the fisher ratio. The most dominant frequency band and electrode channels were visually illustrated and highlighted in yellow color. Three out of ten subjects&#x02019; topography were representatively showed to demonstrate a distinct desynchronization area.</p></caption><graphic xlink:href="sensors-20-07309-g007"/></fig><fig id="sensors-20-07309-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>The event-related spectral perturbation (ERSP) of trial averaged power spectrogram plot; (<bold>A</bold>) Gait MI, (<bold>B</bold>) Sit MI, and (<bold>C</bold>) Do-nothing from top to bottom. The blue vertical line (time 0) represents cue onset, and the red line depicts offset. Subjects engage MIs at time 1 to 9 s and Do-nothing at 1 to 5 s.</p></caption><graphic xlink:href="sensors-20-07309-g008"/></fig><fig id="sensors-20-07309-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>A representative example of the fill/empty log plot of MI buffers (<bold>A</bold>) and MI class discrimination plot (<bold>B</bold>) shares the timeline (subject no. 2). Three kinds of buffers (stand, gait, and sit, size of 10) were illustrated in light-gray, mid-gray, and dark-gray box. Stair shaped line depicts the fill/empty of each corresponding buffer. The false classification was marked as an arrow beneath the timeline. The deviation from true classification (solid red line) was shown in a square plot below.</p></caption><graphic xlink:href="sensors-20-07309-g009"/></fig><table-wrap id="sensors-20-07309-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-20-07309-t001_Table 1</object-id><label>Table 1</label><caption><p>Detail of motor imagery (MI) instructions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Operator&#x02019;s Instructions</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Before MI</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0201c;Be familiar with consistent locomotion of the robot trajectory with your pair of crutches.&#x0201d;<break/>&#x0201c;While practicing &#x02018;sit&#x02019;, please pay attention to your upper limb movement which plays an important role in lowering the body down to the chair with the exoskeleton.&#x0201d;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">During MI</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0201c;Pay attention to the kinesthetic sensation that just before your limb about to execute the movement.&#x0201d;<break/> &#x0201c;Do mental rehearsal in a slow movement phase, for example, heel strike, weight shift, and toe-off.&#x0201d;<break/>&#x0201c;We also recommend you to perceive the input sensation of foot sole and hand grip.&#x0201d;<break/>&#x0201c;For &#x02018;Do-nothing&#x02019;, please ignore the somatosensory or visual input sensation, rather stay unfocused eyes with an absent-minded.&#x0201d;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prohibited</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0201c;Do not picture the scene of observing yourselves or other person&#x02019;s movement execution.&#x0201d;</td></tr></tbody></table></table-wrap><table-wrap id="sensors-20-07309-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-20-07309-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of operating time between the development hybrid BCI controller and the smartwatch controller.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Subject</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BCI Controller (s)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Smartwatch Controller (s)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Time Ratio (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">S1</td><td align="center" valign="middle" rowspan="1" colspan="1">170.0</td><td align="center" valign="middle" rowspan="1" colspan="1">118.6</td><td align="center" valign="middle" rowspan="1" colspan="1">143.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S2</td><td align="center" valign="middle" rowspan="1" colspan="1">125.4</td><td align="center" valign="middle" rowspan="1" colspan="1">93.7</td><td align="center" valign="middle" rowspan="1" colspan="1">133.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S3</td><td align="center" valign="middle" rowspan="1" colspan="1">145.4</td><td align="center" valign="middle" rowspan="1" colspan="1">103.2</td><td align="center" valign="middle" rowspan="1" colspan="1">140.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S4</td><td align="center" valign="middle" rowspan="1" colspan="1">159.6</td><td align="center" valign="middle" rowspan="1" colspan="1">97.1</td><td align="center" valign="middle" rowspan="1" colspan="1">164.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S5</td><td align="center" valign="middle" rowspan="1" colspan="1">144.3</td><td align="center" valign="middle" rowspan="1" colspan="1">94.2</td><td align="center" valign="middle" rowspan="1" colspan="1">153.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S6</td><td align="center" valign="middle" rowspan="1" colspan="1">157.1</td><td align="center" valign="middle" rowspan="1" colspan="1">123.7</td><td align="center" valign="middle" rowspan="1" colspan="1">127.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S7</td><td align="center" valign="middle" rowspan="1" colspan="1">153.2</td><td align="center" valign="middle" rowspan="1" colspan="1">121.9</td><td align="center" valign="middle" rowspan="1" colspan="1">125.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S8</td><td align="center" valign="middle" rowspan="1" colspan="1">138.1</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td><td align="center" valign="middle" rowspan="1" colspan="1">154.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S9</td><td align="center" valign="middle" rowspan="1" colspan="1">180.7</td><td align="center" valign="middle" rowspan="1" colspan="1">106.6</td><td align="center" valign="middle" rowspan="1" colspan="1">169.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">S10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">158.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">116.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">135.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mean &#x000b1; std.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">153.2 &#x000b1; 15.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">106.5 &#x000b1; 12.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">144.8 &#x000b1; 15.12</td></tr></tbody></table></table-wrap><table-wrap id="sensors-20-07309-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-20-07309-t003_Table 3</object-id><label>Table 3</label><caption><p>Offline and online classification accuracy (%).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Subject</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Offline</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Online</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GvN</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GvS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GvN</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GvS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">S1</td><td align="center" valign="middle" rowspan="1" colspan="1">83.3</td><td align="center" valign="middle" rowspan="1" colspan="1">75.7</td><td align="center" valign="middle" rowspan="1" colspan="1">94.2</td><td align="center" valign="middle" rowspan="1" colspan="1">85.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S2</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">77.4</td><td align="center" valign="middle" rowspan="1" colspan="1">81.3</td><td align="center" valign="middle" rowspan="1" colspan="1">77.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S3</td><td align="center" valign="middle" rowspan="1" colspan="1">80.0 </td><td align="center" valign="middle" rowspan="1" colspan="1">78.4 </td><td align="center" valign="middle" rowspan="1" colspan="1">85.5</td><td align="center" valign="middle" rowspan="1" colspan="1">85.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S4</td><td align="center" valign="middle" rowspan="1" colspan="1">94.0 </td><td align="center" valign="middle" rowspan="1" colspan="1">83.9 </td><td align="center" valign="middle" rowspan="1" colspan="1">81.0</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S5</td><td align="center" valign="middle" rowspan="1" colspan="1">95.1 </td><td align="center" valign="middle" rowspan="1" colspan="1">74.3</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">86.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.0 </td><td align="center" valign="middle" rowspan="1" colspan="1">71.9</td><td align="center" valign="middle" rowspan="1" colspan="1">88.0</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S7</td><td align="center" valign="middle" rowspan="1" colspan="1">93.4</td><td align="center" valign="middle" rowspan="1" colspan="1">79.4 </td><td align="center" valign="middle" rowspan="1" colspan="1">91.7</td><td align="center" valign="middle" rowspan="1" colspan="1">83.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S8</td><td align="center" valign="middle" rowspan="1" colspan="1">98.1 </td><td align="center" valign="middle" rowspan="1" colspan="1">94.4 </td><td align="center" valign="middle" rowspan="1" colspan="1">94.5</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S9</td><td align="center" valign="middle" rowspan="1" colspan="1">95.1 </td><td align="center" valign="middle" rowspan="1" colspan="1">87.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.2</td><td align="center" valign="middle" rowspan="1" colspan="1">85.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">S10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean &#x000b1; std.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.4 &#x000b1; 7.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.3 &#x000b1; 6.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.7 &#x000b1; 8.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.4 &#x000b1; 5.43</td></tr></tbody></table></table-wrap><table-wrap id="sensors-20-07309-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-20-07309-t004_Table 4</object-id><label>Table 4</label><caption><p>Offline and online information transfer rate (ITR) (bits/min).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Subject</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Offline</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Online</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">S1</td><td align="center" valign="middle" rowspan="1" colspan="1">1.86</td><td align="center" valign="middle" rowspan="1" colspan="1">2.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.51</td><td align="center" valign="middle" rowspan="1" colspan="1">2.59</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S3</td><td align="center" valign="middle" rowspan="1" colspan="1">2.71</td><td align="center" valign="middle" rowspan="1" colspan="1">3.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S4</td><td align="center" valign="middle" rowspan="1" colspan="1">3.80</td><td align="center" valign="middle" rowspan="1" colspan="1">3.31</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.39</td><td align="center" valign="middle" rowspan="1" colspan="1">3.54</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S6</td><td align="center" valign="middle" rowspan="1" colspan="1">2.24</td><td align="center" valign="middle" rowspan="1" colspan="1">2.23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S7</td><td align="center" valign="middle" rowspan="1" colspan="1">2.31</td><td align="center" valign="middle" rowspan="1" colspan="1">3.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S8</td><td align="center" valign="middle" rowspan="1" colspan="1">6.37</td><td align="center" valign="middle" rowspan="1" colspan="1">3.16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S9</td><td align="center" valign="middle" rowspan="1" colspan="1">4.80</td><td align="center" valign="middle" rowspan="1" colspan="1">3.96</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">S10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.80</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mean &#x000b1; std.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.21 &#x000b1; 1.442</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.13 &#x000b1; 0.514</td></tr></tbody></table></table-wrap></floats-group></article>