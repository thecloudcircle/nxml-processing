
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med Imaging</journal-id><journal-id journal-id-type="iso-abbrev">BMC Med Imaging</journal-id><journal-title-group><journal-title>BMC Medical Imaging</journal-title></journal-title-group><issn pub-type="epub">1471-2342</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">32698839</article-id><article-id pub-id-type="pmc">7374886</article-id><article-id pub-id-type="publisher-id">482</article-id><article-id pub-id-type="doi">10.1186/s12880-020-00482-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Technical Advance</subject></subj-group></article-categories><title-group><article-title>An improved deep learning approach and its applications on colonic polyp images detection</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2298-3429</contrib-id><name><surname>Wang</surname><given-names>Wei</given-names></name><address><email>wangwei@csust.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Tian</surname><given-names>Jinge</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Chengwen</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Luo</surname><given-names>Yanhong</given-names></name><address><email>mfxgz123@163.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Wang</surname><given-names>Xin</given-names></name><address><email>wangxin@csust.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Ji</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.440669.9</institution-id><institution-id institution-id-type="ISNI">0000 0001 0703 2206</institution-id><institution>School of Computer and Communication Engineering, </institution><institution>Changsha University of Science and Technology, </institution></institution-wrap>Changsha, 410114 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.440223.3</institution-id><institution>Hunan Children&#x02019;s Hospital, </institution></institution-wrap>Changsha, 410000 China </aff></contrib-group><pub-date pub-type="epub"><day>22</day><month>7</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>22</day><month>7</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>20</volume><elocation-id>83</elocation-id><history><date date-type="received"><day>7</day><month>1</month><year>2020</year></date><date date-type="accepted"><day>8</day><month>7</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">Colonic polyps are more likely to be cancerous, especially those with large diameter, large number and atypical hyperplasia. If colonic polyps cannot be treated in early stage, they are likely to develop into colon cancer. Colonoscopy is easily limited by the operator&#x02019;s experience, and factors such as inexperience and visual fatigue will directly affect the accuracy of diagnosis. Cooperating with Hunan children&#x02019;s hospital, we proposed and improved a deep learning approach with global average pooling (GAP) in colonoscopy for assisted diagnosis. Our approach for assisted diagnosis in colonoscopy can prompt endoscopists to pay attention to polyps that may be ignored in real time, improve the detection rate, reduce missed diagnosis, and improve the efficiency of medical diagnosis.</p></sec><sec><title>Methods</title><p id="Par2">We selected colonoscopy images from the gastrointestinal endoscopy room of Hunan children&#x02019;s hospital to form the colonic polyp datasets. And we applied the image classification method based on Deep Learning to the classification of Colonic Polyps. The classic networks we used are VGGNets and ResNets. By using global average pooling, we proposed the improved approaches: VGGNets-GAP and ResNets-GAP.</p></sec><sec><title>Results</title><p id="Par3">The accuracies of all models in datasets exceed 98%. The TPR and TNR are above 96 and 98% respectively. In addition, VGGNets-GAP networks not only have high classification accuracies, but also have much fewer parameters than those of VGGNets.</p></sec><sec><title>Conclusions</title><p id="Par4">The experimental results show that the proposed approach has good effect on the automatic detection of colonic polyps. The innovations of our method are in two aspects: (1) the detection accuracy of colonic polyps has been improved. (2) our approach reduces the memory consumption and makes the model lightweight. Compared with the original VGG networks, the parameters of our VGG19-GAP networks are greatly reduced.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Colonic polyps</kwd><kwd>Deep learning</kwd><kwd>Convolutional neural networks</kwd><kwd>Global average pooling</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009000</institution-id><institution>National Defense Pre-Research Foundation of China</institution></institution-wrap></funding-source><award-id>7301506</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>61070040</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014472</institution-id><institution>Scientific Research Foundation of Hunan Provincial Education Department</institution></institution-wrap></funding-source><award-id>17C0043</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Xin</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Natural Science Foundation of&#x000a0;Hunan Province (CN)</institution></funding-source><award-id>2019JJ80105</award-id><principal-award-recipient><name><surname>Luo</surname><given-names>Yanhong</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Clinical Medical technology Innovation and Guidance Project of Hunan Province</institution></funding-source><award-id>2018SK5040</award-id><principal-award-recipient><name><surname>Luo</surname><given-names>Yanhong</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par11">Colonic polyp is a common disease in children, which seriously affects the normal growth and development of children. Colonic polyps are more likely to be cancerous, especially those with large diameter, large number and atypical hyperplasia. If colonic polyps cannot be treated at an early stage, they are prone to develop into colon cancer. Therefore, early detection and treatment can effectively reduce the incidence of colon cancer, which is of great significance to patients [<xref ref-type="bibr" rid="CR1">1</xref>].</p><p id="Par12">Computer-aided diagnosis (CAD) of colonoscopy has been a hot spot in artificial intelligence research [<xref ref-type="bibr" rid="CR2">2</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref>]. In the aspect of polyp detection CAD, in 2003, wavelet transform was used as an image classifier in the preliminary study of detecting polyps in white light colonoscopy images [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]. After that, there were more CAD applications based on specific databases [<xref ref-type="bibr" rid="CR8">8</xref>&#x02013;<xref ref-type="bibr" rid="CR11">11</xref>], but the number of images in these databases was limited, mostly less than 20. In 2015, Tyler Berzin&#x02019;s team put forward the product idea of AI assisted endoscopy diagnosis [<xref ref-type="bibr" rid="CR12">12</xref>].</p><p id="Par13">In computer-aided polyp detection, feature extraction is especially important. At present, the feature extraction methods for polyp detection mainly include hand-crafted, end-to-end learning and hybrid approaches [<xref ref-type="bibr" rid="CR13">13</xref>]. The hand-crafted feature approaches mainly use low-level image processing methods to obtain candidate boundaries of polyps, and then define the special boundary feature of each polyp with this information. Zhu et al. [<xref ref-type="bibr" rid="CR14">14</xref>] analyzed the curvature of the detected boundary. Kang et al. [<xref ref-type="bibr" rid="CR15">15</xref>] searched for oval shapes related to polyps. Hwang et al. [<xref ref-type="bibr" rid="CR16">16</xref>] combined curvature analysis and shape fitting in the above two strategies. In end-to-end approaches, texture and color information are used as descriptors. Gross et al. [<xref ref-type="bibr" rid="CR17">17</xref>] used local binary patterns (LBP) features. Ribeiro et al. [<xref ref-type="bibr" rid="CR18">18</xref>] used deep learning approach to assist the detection of polyps. After that, there were hybrid approaches of combining hand-crafted and end-to-end learning. Tajbakhsh et al. [<xref ref-type="bibr" rid="CR11">11</xref>] combined edge detection and feature extraction to improve the accuracy of detection. The above approaches usually use low-level simple features to detect polyps. The acquisition of these polyp features mainly extracts information such as the boundary (shape), texture, intensity, color and spatiotemporal features through artificial design program. However, only the complete and accurate extraction of colonoscopy image information can reduce the omission of polyp characteristics, which is difficult to ensure the high accuracy of polyp intelligent recognition [<xref ref-type="bibr" rid="CR13">13</xref>].</p><p id="Par14">Due to the limitation of computer algorithm and ability, the computer-aided diagnosis of colonoscopy has been limited to the basic research in the field of engineering for a long time. With the emergence of deep learning algorithm and the significant improvement of computer&#x02019;s computing ability, the CAD of colonoscopy is becoming a reality [<xref ref-type="bibr" rid="CR19">19</xref>].</p><p id="Par15">In recent years, deep learning (DL) approaches, such as Convolutional Neural Networks (CNNs) [<xref ref-type="bibr" rid="CR20">20</xref>], have achieved great success in image recognition [<xref ref-type="bibr" rid="CR21">21</xref>], image segmentation [<xref ref-type="bibr" rid="CR22">22</xref>], language understanding and other fields. Esteva et al. [<xref ref-type="bibr" rid="CR23">23</xref>] used CNN model to classify skin cancer images, and the overall accuracy was higher than that of dermatologists. Gulshan et al. [<xref ref-type="bibr" rid="CR24">24</xref>] established deep learning algorithm to recognize diabetic retinopathy, and its sensitivity and specificity both exceeded 87%. The convolution neural network algorithm was established to determine the depth of gastric cancer infiltration, and its abstract recognition ability was better than that of endoscopists [<xref ref-type="bibr" rid="CR25">25</xref>]. Park et al. [<xref ref-type="bibr" rid="CR26">26</xref>] used CNN to automatically extract diagnostic features from colonoscopy images. Tajbakhsh et al. [<xref ref-type="bibr" rid="CR27">27</xref>] proposed a new polyp detection method based on convolutional neural network, enabling more accurate polyp localization. Urban et al. [<xref ref-type="bibr" rid="CR28">28</xref>] applied CNN system to colonoscopy images, and the CNN identified polyps with a cross-validation accuracy of 96.4%.</p><p id="Par16">Colonoscopy is easily limited by the operator&#x02019;s experience, and factors such as inexperience limitation and visual fatigue will directly affect the accuracy of diagnosis. The CAD application of DL in colonoscopy can effectively simplify complicated diagnosis steps and improve the efficiency of medical diagnosis. The DL-based CAD can prompt endoscopists to pay attention to polyps that may be ignored in real time, improve the detection rate, and reduce missed diagnosis. In addition, the results of CAD can be used to expand the dataset so that we can conduct a convenient review of the patient&#x02019;s colonoscopy images and further train the network to improve the classification performance. Based on this, we cooperate with Hunan children&#x02019;s hospital to carry out the research on the assisted diagnosis of colonic polyps in children, so as to improve the detection level of intestinal polyps in children.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par17">Different from the traditional detection method, we apply the image classification method of DL network to the colonic polyp detection. Due to the variety of colonic polyps in morphology and the complexity of intestinal environment, it is necessary to choose the CNN model with a high degree of non-linear. The classical VGGNets and ResNets show excellent recognition ability on the labeled colonic polyp datasets. Based on the above networks, we propose two new network structures by using global average pooling, which are VGGNets-GAP and ResNets-GAP. The improved networks have outstanding classification performance, and also greatly reduce the number of parameters compared with the original networks.</p><p id="Par18">In the task of colonic polyp recognition based on DL network, the medical images are directly fed into the trained network model, and the diagnosis results are given by the system, which can give doctors a clear prompt. Doctors can judge more carefully according to the results. Such assistance can effectively help endoscopists reduce missed diagnosis and improve the accuracy of colonic polyp diagnosis. The processing framework is shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Diagnosis processing framework</p></caption><graphic xlink:href="12880_2020_482_Fig1_HTML" id="MO1"/></fig></p><sec id="Sec3"><title>Datasets</title><p id="Par19">At present, there are very few public datasets related to colonic polyps and few images in the available datasets of colonic polyps. Compared with adults, children&#x02019;s colonic polyps are easier to treat. Therefore, cooperating with Hunan children&#x02019;s hospital, we collected colonoscopy images of 1600 children from Hunan children&#x02019;s hospital. The age range of the patients is from 0 to 18&#x02009;years old, and the average age is 3.5&#x02009;years old. With the patient&#x02019;s knowledge, we used these data to label two colonic polyp datasets, i.e., CP-CHILD-A dataset and CP-CHILD-B dataset. We selected 10,000 colonoscopy RGB images taken by Olympus PCF-H290DI from March 2018 to April 2019 for further processing. There are black edges around the pictures caused by the device, the pixel values of these black edges are close to 0, which are useless information features. So we first cut them before labeling, and unified the images&#x02019; size to 256&#x02009;&#x000d7;&#x02009;256. The unified images were then handed over to 4 endoscopists of Hunan children&#x02019;s hospital. The endoscopists compared pathology to determine whether the image should be classified, and deleted some blurred and basically completely immersed in intestinal fluid, completely covered by feces or food debris. After labeling, we get the CP-CHILD-A dataset, which contains 1000 colonic polyp images, and 7000 normal or other pathological images, as shown in Figs.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig3" ref-type="fig">3</xref>, respectively.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Colonic polyp images in CP-CHILD-A</p></caption><graphic xlink:href="12880_2020_482_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Non-polyp images in CP-CHILD-A</p></caption><graphic xlink:href="12880_2020_482_Fig3_HTML" id="MO3"/></fig></p><p id="Par20">Colonic polyp detection is difficult for the following reasons: (1) Other colonic lesions, such as inflammatory bowel disease, ulcerative colitis pictures, etc., may cause bleeding, follicles, etc. their pictures look like polyp pictures. (2) Many of the polyps in the picture do not appear in the field completely, and some of them even only appear in the corner of the picture. (3) Light and shooting angle also affect the imaging quality.</p><p id="Par21">The method we used to divide the training set and test set is the hold-out method. We randomly selected 6200 images from 7000 non-polyp images and 800 images from 1000 colonic polyp images to form a training set. The remaining 800 non-polyp images and 200 polyp images are used as test set. Because data enhancement is used in training and testing, including random horizontal rotation, random vertical rotation, random rotation of a certain angle between +&#x02009;90&#x000b0; and&#x02009;&#x02212;&#x02009;90&#x000b0;, brightness and contrast change, which greatly increase the amount of data. Therefore, the total number of image samples in the experiments is 40,000, which is 5 times of the original data.</p><p id="Par22">In order to verify the generalization ability of the models, we also selected the colonic images taken by FUJIFLIM EC-530wm to form the CP-CHILD-B dataset. After the same processing as the images in CP-CHILD-A dataset, we obtained the small CP-CHILD-B dataset containing 1500 images, which consist of 400 colonic polyp images, 1100 normal or other pathological images. The training set includes 800 non-polyp images, 300 polyp images, and the test set consists of 300 non-polyp images and 100 polyp images. The polyp and non-polyp images are shown in Figs.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref>. Compared with CP-CHILD-A, the images in CP-CHILD-B are darker and there is a large difference in color between the images.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Colonic polyp images in CP-CHILD-B</p></caption><graphic xlink:href="12880_2020_482_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Non-polyp images in CP-CHILD-B</p></caption><graphic xlink:href="12880_2020_482_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec4"><title>VGGNets</title><p id="Par23">VGGNet is a deep convolutional neural network model developed by Simonyan and Zisserman [<xref ref-type="bibr" rid="CR29">29</xref>]. It explores the relationship between the depth of convolutional neural network and its performance. By repeatedly stacking 3&#x02009;&#x000d7;&#x02009;3 small convolution kernel and 2&#x02009;&#x000d7;&#x02009;2 maximum pooling layer, the convolutional neural networks with 16&#x02009;~&#x02009;19 layers of depth are successfully constructed.</p><p id="Par24">In VGGNets, the parameters are mainly concentrated in the last three fully connected layers, so we can improve the performance by deepening the network. In particular, when polyps appear in the endoscopic field of vision, many of them are not complete and have no clear edges. Some polyps are covered by plica, immersed in intestinal fluid, covered by feces or food debris, and out of the camera&#x02019;s view, etc. So it is exceedingly difficult to extract the edge information and texture features of the target by using the large convolution kernel, while the 3&#x02009;&#x000d7;&#x02009;3 convolution kernel can extract the details more effectively.</p><p id="Par25">The VGGNets used in our experiments are VGG16 and VGG19, with 16 and 19 layers respectively, as shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>. Each VGGNet is divided into five segments. In each segment, several 3&#x02009;&#x000d7;&#x02009;3 convolution kernels are connected in series. There is a maximum pooling layer following each segment of convolution, and 3 fully connected (FC) layers and a softmax layer are added at last. In the networks, all hidden layers use ReLU as the activation function. For simplicity, the activation function is not shown in the Fig. <xref rid="Fig6" ref-type="fig">6</xref>.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Network structures of VGG16 (top) and VGG19 (bottom)</p></caption><graphic xlink:href="12880_2020_482_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec5"><title>ResNets</title><p id="Par26">Deeper networks often have better feature extraction ability and performance. But with the increase of network depth, the degradation problem will appear, i.e., with the increase of training times, the accuracy will reach saturation or even decline. The residual network model proposed by He et al. [<xref ref-type="bibr" rid="CR30">30</xref>] effectively solves this problem.</p><p id="Par27">In ResNets, identity mapping is applied to optimize the residual part in order to better highlight the changes, as shown in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>a. Compared with the general convolution, the residual block uses skip connection to realize identity mapping and make it bypass the nonlinear transformation, as the curved line in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. ResNets are stacked by residual blocks, so the network is easier to optimize and its depth can be greatly increased, both of which can improve the recognition accuracy [<xref ref-type="bibr" rid="CR31">31</xref>].
<fig id="Fig7"><label>Fig. 7</label><caption><p>Residual Blocks</p></caption><graphic xlink:href="12880_2020_482_Fig7_HTML" id="MO7"/></fig></p><p id="Par28">The addition skip connection of the identity mapping can effectively increase the training speed of the model and improve the training effect. The residual networks used in our experiments are ResNet101 and ResNet152. The residual blocks used in these two ResNets are shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>b, which greatly reduce the number of parameters. The concrete network structures of ResNet101 and ResNet152 are shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>.
<fig id="Fig8"><label>Fig. 8</label><caption><p>Network structures of ResNet101 (top) and ResNet152 (bottom)</p></caption><graphic xlink:href="12880_2020_482_Fig8_HTML" id="MO8"/></fig></p></sec><sec id="Sec6"><title>Net-GAP</title><p id="Par29">For image classification, the feature maps generated by the last convolutional layer in CNN are usually fed into a fully connected layer, and finally connected with soft-max logistic regression layer [<xref ref-type="bibr" rid="CR32">32</xref>]. However, the fully connected layer not only brings a huge number of parameters, but also makes the network easily fall into over-fitting, which leads to the weak generalization ability of the network. Lin et al. [<xref ref-type="bibr" rid="CR33">33</xref>] proposed the global average pooling (GAP) method for the first time. Different from the traditional FC layer, GAP layer applies global average pooling to the whole feature map, so that each feature map only gets one output. GAP greatly reduces the number of parameters, thus greatly simplifying the network and avoiding over fitting. By using GAP, each feature map has only one output feature. This one-to-one correspondence mode of feature map and category strengthens the relationship between the credibility of feature map and concept (category), and makes the classification task highly understandable.</p><p id="Par30">Based on the above idea, we combine VGG16, VGG19, ResNet101 and ResNet152 with GAP in this paper. By replacing the FC layers with GAP, four new network structures, named Net-GAP, where the &#x0201c;Net&#x0201d; is the original neural networks&#x02019; model before replacement, are obtained. The rule of replacement is shown in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>.
<fig id="Fig9"><label>Fig. 9</label><caption><p>Replace FC with GAP</p></caption><graphic xlink:href="12880_2020_482_Fig9_HTML" id="MO9"/></fig></p><p id="Par31">The structures of VGGNets-GAP and ResNets-GAP are shown in Figs.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref> and <xref rid="Fig11" ref-type="fig">11</xref> respectively, in which the red boxes are used to identify the changes relative to the original networks.
<fig id="Fig10"><label>Fig. 10</label><caption><p>Network structures of VGG16-GAP (top) and VGG19-GAP (bottom)</p></caption><graphic xlink:href="12880_2020_482_Fig10_HTML" id="MO10"/></fig><fig id="Fig11"><label>Fig. 11</label><caption><p>Network structures of ResNet101-GAP (top) and ResNet152-GAP (bottom)</p></caption><graphic xlink:href="12880_2020_482_Fig11_HTML" id="MO11"/></fig></p><p id="Par32">After the network structure is improved, the number of parameters is changed. The numbers of parameters of the VGGNets and VGGNets-GAP are shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. By using GAP, the number of parameters of VGGNets-GAP is reduced about 153 million compared with the original network based on the same depth. The parameters of the ResNets and ResNets-GAP are shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. Compared with the same depth of ResNets, the parameters of ResNets-GAP are reduced about 12,300. Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref> show that the GAP-based networks can effectively reduce the complex of networks.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Parameters number of VGGNets and VGGNets-GAP</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Network</th><th>Number of parameters (in million)</th></tr></thead><tbody><tr><td>VGG16</td><td>165.73</td></tr><tr><td><bold>VGG16-GAP</bold></td><td><bold>12.37</bold></td></tr><tr><td>VGG19</td><td>171.05</td></tr><tr><td><bold>VGG19-GAP</bold></td><td><bold>17.68</bold></td></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Parameters number of ResNets and ResNets-GAP</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Network</th><th>Number of parameters (in thousand)</th></tr></thead><tbody><tr><td>ResNet101</td><td>42,516.546</td></tr><tr><td>ResNet101-A</td><td>42,504.260</td></tr><tr><td>ResNet152</td><td>58,160.194</td></tr><tr><td>ResNet152-GAP</td><td>58,147.908</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec7"><title>Implementation details</title><p id="Par33">In our experiments, PyTorch is chosen as the deep learning framework. In order to better analyze and compare the detection results of different network models, all experiments are carried out on the same platform and environment. The experimental platform configuration is shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Experimental environment configuration</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Configuration</th><th>Configuration parameter</th></tr></thead><tbody><tr><td>operating system</td><td>Windows 10</td></tr><tr><td>CPU</td><td>Intel i7 3.30GHz</td></tr><tr><td>GPU</td><td>GTX1080Ti(11G)</td></tr><tr><td>RAM</td><td>16G/DDR3/2.10GHz</td></tr><tr><td>cuDNN</td><td>CuDNN 10.0</td></tr><tr><td>CUDA</td><td>CUDA10.0</td></tr><tr><td>Frame</td><td>PyTorch</td></tr></tbody></table></table-wrap></p><p id="Par34">The pre-training model on ImageNet [<xref ref-type="bibr" rid="CR29">29</xref>] is used to initialize the parameters. In this paper, the stochastic gradient descent (SGD) method is adopted to minimize the loss function, that is, one batch of data is used instead of all data for gradient operation. The batch size of training set is set to 16, and that of test set is set to 4. In the process of training, the method of learning rate decay is introduced, that is, with the increase of the iterations, the learning rate decreases gradually. Learning rate decay can ensure that the model does not fluctuate greatly in the later period of training, so the result is closer to the optimal solution. In the experiments, the initial learning rate of ResNets and VGGNets is set to 0.01 and 0.1, respectively. Every network trains 200 epochs in total, and the learning rate decreases to half of the previous in the 50th epoch and then decays by half every 20 epochs. The average recognition accuracy of the last 100 epochs is taken as the final accuracy, and the average values of sensitivity and specificity of the last 10 epochs are taken as the final sensitivity and specificity.</p></sec><sec id="Sec8"><title>Evaluation criteria</title><p id="Par35">As the evaluation criteria used by most medical image classification models, we use accuracy, sensitivity and specificity as the evaluation criteria.</p><p id="Par36">In the experiments, we set the polyp samples which are few but especially important as positive samples, and non-polyp samples as negative samples. If TP represents the number of samples belonging to polyps that are correctly classified, FP represents the number of samples belonging to non-polyp samples but falsely classified to polyps, FN represents the number of samples belonging to polyps but falsely classified, and TN represents the number of samples belonging to non-poly samples that correctly classified.</p><p id="Par37">Accuracy refers to the ratio of all correct classification results in the classification model to the total input pictures. It is defined as:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Accuracy}=\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{TP}+\mathrm{TN}+\mathrm{FP}+\mathrm{FN}} $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>TN</mml:mi></mml:mrow><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>TN</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12880_2020_482_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>Sensitivity, also known as Recall, is the proportion that the model predicts the polyps correctly to all the results of the polyp samples, which is defined as:
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{TPR}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}} $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mi>TPR</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12880_2020_482_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Specificity is the proportion that the model predicts the non-polyps correctly to all the results of the negative samples:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{TNR}=\frac{\mathrm{TN}}{\mathrm{TN}+\mathrm{FP}} $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mi>TNR</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>TN</mml:mi><mml:mrow><mml:mi>TN</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12880_2020_482_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p></sec></sec><sec id="Sec9"><title>Results</title><p id="Par38">The close connection between the CNN layers and the spatial information make CNN particularly suitable for image processing and understanding. It can automatically extract rich relevant features from images. Figure&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref> is an example of feature maps extracted by VGG19 model. The left is the original image, the upper row shows the features extracted from the second layer, and lower row shows the features extracted from the 4th layer. From Fig.&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref>, it can be found that most texture and detail features are extracted by the shallow layers, while the contour and shape features are extracted by the deeper layers. Relatively speaking, the deeper the layer is, the more representative the extracted features are, but the resolution of the feature maps become lower.
<fig id="Fig12"><label>Fig. 12</label><caption><p>Feature examples extracted by VGG19</p></caption><graphic xlink:href="12880_2020_482_Fig12_HTML" id="MO12"/></fig></p><p id="Par39">We used the network models introduced in &#x0201c;Method&#x0201d; to conduct experiments on CP-CHILD-A. The average accuracies of 10 repeated experiments are shown in Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref>. All the 8 models perform well and the accuracies are over 99%. Our two VGGNets-GAP models not only perform slightly better than VGGNets with the same depth, but also reduce the number of parameters by about 153 million. With same depth, ResNets-GAP and ResNets have similar recognition Accuracies, but the parameters of ResNets-GAP are slightly less than that of the ResNets. Among them, ResNet152 and ResNet152-GAP have the best recognition results, and the accuracy is up to 99.29%.
<fig id="Fig13"><label>Fig. 13</label><caption><p>Recognition accuracy on CP-CHILD-A</p></caption><graphic xlink:href="12880_2020_482_Fig13_HTML" id="MO13"/></fig></p><p id="Par40">The TPR and TNR of different models on CP-CHILD-A are shown in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>. The TPR and TNR of colonic polyp recognition based on CNNs are all over 96 and 99% respectively. Except that the TPR of VGGNets-GAP is 0.2%&#x02009;~&#x02009;0.3% lower than that of VGGNets, the TPR and TNR of all other networks combined with GAP are higher than or equal to those of the original networks. Among all the networks, the TPR of ResNet152-GAP reaches 97.55% and the TNR of VGG19-GAP reaches 99.88%, which are the best scores.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Recognition sensitivity and specificity on CP-CHILD-A</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>TPR</th><th>TNR</th></tr></thead><tbody><tr><td>VGG16</td><td>96.60%</td><td>99.74%</td></tr><tr><td>VGG16-GAP</td><td>96.40%</td><td>99.83%</td></tr><tr><td>VGG19</td><td>96.85%</td><td>99.81%</td></tr><tr><td>VGG19-GAP</td><td>96.55%</td><td>99.88%</td></tr><tr><td>ResNet101</td><td>96.95%</td><td>99.74%</td></tr><tr><td>ResNet101-GAP</td><td>97.50%</td><td>99.85%</td></tr><tr><td>ResNet152</td><td>97.35%</td><td>99.84%</td></tr><tr><td>ResNet152-GAP</td><td>97.55%</td><td>99.80%</td></tr></tbody></table></table-wrap></p><p id="Par41">In the above experiments, the numbers of positive and negative samples in the training set are not equal. In order to further illustrate the influence of the training samples and the effectiveness of the model, we set the ratio of non-polyp images and polyp images in training set of CP-CHILD-A to 1:1. We randomly select 800 samples from 6200 non-polyp samples in the training set, and the other 5400 non-polyp samples in the training set are put into the test set. The training set of the balanced dataset contains 800 non-polyp samples and 800 polyp samples, and the test set contains 6200 non-polyp samples and 200 polyp samples. All experiments were repeated 10 times, and the average value was taken as the final experimental result. CNNs still show good performance in the test set, as shown in Fig.&#x000a0;<xref rid="Fig14" ref-type="fig">14</xref>. The accuracies of the improved networks are over 98%, which are all better than the corresponding original networks. Compared with the results shown in Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref>, the accuracies of the proposed models are only reduced by 0.3%&#x02009;~&#x02009;0.7%, which reflects the applicability of the models.
<fig id="Fig14"><label>Fig. 14</label><caption><p>Recognition accuracy with balanced positive and negative samples</p></caption><graphic xlink:href="12880_2020_482_Fig14_HTML" id="MO14"/></fig></p><p id="Par42">To verify the generalization ability of the models, we also conducted extensive experiments on CP-CHILD-B dataset. The average accuracies of 10 repeated experiments are shown in Fig.&#x000a0;<xref rid="Fig15" ref-type="fig">15</xref>. The CNNs show good performance on the test set, and the accuracies are exceeding 98%. The accuracies of the improved networks are better than those of the original networks. And the accuracies of ResNet101-GAP and ResNet152-GAP are all over 99.3%.
<fig id="Fig15"><label>Fig. 15</label><caption><p>Recognition accuracy on CP-CHILD-B</p></caption><graphic xlink:href="12880_2020_482_Fig15_HTML" id="MO15"/></fig></p><p id="Par43">The sensitivity and specificity of different models on CP-CHILD-B are shown in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>. All the sensitivity and specificity of colonic polyp recognition based on CNNs are over 96 and 98%, respectively. Except that the TPR of ResNet152-GAP is 0.3% lower than that of ResNet152, the TPR and TNR of all other networks combined with GAP are higher than or equal to those of the original networks. Among all the models, TPRs of both ResNet101-GAP and ResNet152 reach 98%, and the TNRs of VGG16-GAP and VGG19-GAP reach 99.97%.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Recognition sensitivity and specificity on CP-CHILD-B</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>TPR</th><th>TNR</th></tr></thead><tbody><tr><td>VGG16</td><td>96.50%</td><td>98.80%</td></tr><tr><td>VGG16-GAP</td><td>96.80%</td><td>99.97%</td></tr><tr><td>VGG19</td><td>96.60%</td><td>99.00%</td></tr><tr><td>VGG19-GAP</td><td>97.60%</td><td>99.97%</td></tr><tr><td>ResNet101</td><td>97.40%</td><td>99.80%</td></tr><tr><td>ResNet101-GAP</td><td>98.00%</td><td>99.83%</td></tr><tr><td>ResNet152</td><td>98.00%</td><td>99.60%</td></tr><tr><td>ResNet152-GAP</td><td>97.70%</td><td>99.93%</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec10"><title>Discussion</title><p id="Par44">From the &#x0201c;results&#x0201d; section, we can see that the recognition results of 8 CNN models in the experiments have little difference between CP-CHILD-A and CP-CHILD-B. Among the four methods (with GAP) we proposed in this paper, the classification accuracies of residual structures are relatively better. Although the accuracies of VGGNets-GAP are a little less than those of the residual structures, but its number of parameters is much less than that of the original network, so it use less memory in the application. Overall, the results of VGGNets-GAP network are the best.</p></sec><sec id="Sec11"><title>Conclusions</title><p id="Par45">The combination of medical image processing and neural network is a new branch and industry hotspot in the field of digital medicine, and its application in the medical field has received extensive attention. Colonoscopy is easily limited by the operator&#x02019;s experience, and the factors such as inexperience and visual fatigue will directly affect the accuracy of diagnosis. Cooperating with Hunan children&#x02019;s hospital, we collected colonoscopy images from the database of the gastrointestinal endoscopy room to label the colonic polyp datasets. We further proposed the application of CNNs in colonoscopy for assisted diagnosis.</p><p id="Par46">In the experiments, we use the classical CNN models VGGNets and ResNets. Combining them with global average pooling, we propose the new network structures VGGNets-GAP and ResNets-GAP. The accuracies of all models in datasets exceed 98%. The TPR and TNR are above 96 and 98% respectively. The experimental results show that the proposed approach has good effect on the automatic detection of colonic polyps. In particular, the two VGGNets-GAP networks not only have high classification accuracies, but also have much fewer parameters than those of VGGNets. The reduction of model&#x02019;s parameters has great benefits for reducing the memory consumption and making the model lightweight.</p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>GAP</term><def><p id="Par5">Global average pooling</p></def></def-item><def-item><term>CAD</term><def><p id="Par6">Computer-aided diagnosis</p></def></def-item><def-item><term>CNN</term><def><p id="Par7">Convolutional Neural Network</p></def></def-item><def-item><term>DL</term><def><p id="Par8">Deep learning</p></def></def-item><def-item><term>FC</term><def><p id="Par9">Fully connected</p></def></def-item><def-item><term>SGD</term><def><p id="Par10">Stochastic gradient descent</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We are grateful to our colleagues for their suggestions.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>W.W. designed the study, wrote the paper, managed the project and was responsible for the final submission and revisions of the manuscript. J.T. wrote the python scripts, trained the CNN models and did the data analysis. C.Z. configured the experimental environment arranged experimental data. Y.L. organized the datasets. X.W. implemented the feature visualization. J.L. revised the paper. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research was supported by National Defense Pre-Research Foundation of China under Grant 7301506, National Natural Science Foundation of China under Grant 61070040, Scientific Research Fund of Hunan Provincial Education Department under Grant 17C0043, Natural Science Foundation of Hunan Province under Grant 2019JJ80105, and Clinical Medical technology Innovation and Guidance Project of Hunan Province under Grant 2018SK5040. The funding bodies played no role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>The datasets used during the current study are available at the website 10.6084/m9.figshare.12554042.</p></notes><notes id="FPar1"><title>Ethics approval and consent to participate</title><p id="Par47">The full name of the ethics committee that approved the study is &#x0201c;medical ethics committee of Hunan children&#x02019;s Hospital&#x0201d;, and the number of the approval document is KS2020&#x02013;21.</p><p id="Par48">Without any personal information, the data was obtained retrospectively, and therefore consent is not applicable.</p></notes><notes id="FPar2"><title>Consent for publication</title><p id="Par49">Not applicable.</p></notes><notes id="FPar3" notes-type="COI-statement"><title>Competing interests</title><p id="Par50">The authors declare that they have no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>CD</given-names></name><name><surname>Dachman</surname><given-names>AH</given-names></name></person-group><article-title>CT Colonography: the next Colon screening examination?</article-title><source>Radiology.</source><year>2000</year><volume>216</volume><issue>2</issue><fpage>331</fpage><lpage>341</lpage><pub-id pub-id-type="pmid">10924550</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mughal</surname><given-names>B</given-names></name><name><surname>Sharif</surname><given-names>M</given-names></name></person-group><article-title>Automated detection of breast tumor in different imaging modalities: a review</article-title><source>Curr Med Imaging Rev</source><year>2017</year><volume>13</volume><issue>2</issue><fpage>121</fpage><lpage>139</lpage></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanesaka</surname><given-names>T</given-names></name><name><surname>Lee</surname><given-names>TC</given-names></name><name><surname>Uedo</surname><given-names>N</given-names></name><name><surname>Lin</surname><given-names>KP</given-names></name><name><surname>Chen</surname><given-names>HZ</given-names></name><name><surname>Lee</surname><given-names>JY</given-names></name><etal/></person-group><article-title>Computer-aided diagnosis for identifying and delineating early gastric cancers in magnifying narrow-band imaging</article-title><source>Gastrointest Endosc</source><year>2018</year><volume>87</volume><issue>5</issue><fpage>1339</fpage><lpage>1344</lpage><pub-id pub-id-type="pmid">29225083</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badgeley</surname><given-names>MA</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Glicksberg</surname><given-names>BS</given-names></name><name><surname>Shervey</surname><given-names>M</given-names></name><name><surname>Zech</surname><given-names>J</given-names></name><name><surname>Shameer</surname><given-names>K</given-names></name><etal/></person-group><article-title>CANDI: an R package and shiny app for annotating radiographs and evaluating computer-aided diagnosis</article-title><source>Bioinformatics</source><year>2018</year><volume>35</volume><issue>9</issue><fpage>1610</fpage><lpage>1612</lpage></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lo</surname><given-names>SB</given-names></name><name><surname>Freedman</surname><given-names>MT</given-names></name><name><surname>Gillis</surname><given-names>LB</given-names></name><name><surname>White</surname><given-names>CS</given-names></name><name><surname>Mun</surname><given-names>SK</given-names></name></person-group><article-title>Journal club: computer-aided detection of lung nodules on CT with a computerized pulmonary vessel suppressed function</article-title><source>Am J Roentgeno</source><year>2018</year><volume>210</volume><issue>3</issue><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karkanis</surname><given-names>SA</given-names></name><name><surname>Iakovidis</surname><given-names>DK</given-names></name><name><surname>Maroulis</surname><given-names>DE</given-names></name><name><surname>Karras</surname><given-names>DA</given-names></name><name><surname>Tzivras</surname><given-names>M</given-names></name></person-group><article-title>Computer-aided tumor detection in endoscopic video using color wavelet features</article-title><source>IEEE Trans Inf Technol Biomed</source><year>2003</year><volume>7</volume><issue>3</issue><fpage>141</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">14518727</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maroulis</surname><given-names>DE</given-names></name><name><surname>Iakovidis</surname><given-names>DK</given-names></name><name><surname>Karkanis</surname><given-names>SA</given-names></name><name><surname>Karras</surname><given-names>DA</given-names></name></person-group><article-title>Cold: a versatile detection system for colorectal lesions in endoscopy video-frames</article-title><source>Comput Meth Prog Bio</source><year>2003</year><volume>70</volume><issue>2</issue><fpage>151</fpage><lpage>166</lpage></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jerebko</surname><given-names>A</given-names></name><name><surname>Lakare</surname><given-names>S</given-names></name><name><surname>Cathier</surname><given-names>P</given-names></name><name><surname>Periaswamy</surname><given-names>S</given-names></name><name><surname>Bogoni</surname><given-names>L</given-names></name></person-group><article-title>Symmetric curvature patterns for colonic polyp detection</article-title><source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source><year>2006</year><publisher-loc>Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>169</fpage><lpage>176</lpage></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Tavanapong</surname><given-names>W</given-names></name><name><surname>Wong</surname><given-names>J</given-names></name><name><surname>Oh</surname><given-names>J</given-names></name><name><surname>De Groen</surname><given-names>PC</given-names></name></person-group><article-title>Part-based multiderivative edge cross-sectional profiles for polyp detection in colonoscopy</article-title><source>IEEE J of Biomed Health</source><year>2013</year><volume>18</volume><issue>4</issue><fpage>1379</fpage><lpage>1389</lpage></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Liang</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>LC</given-names></name><name><surname>Han</surname><given-names>H</given-names></name><name><surname>Song</surname><given-names>B</given-names></name><name><surname>Pickhardt</surname><given-names>PJ</given-names></name><etal/></person-group><article-title>An adaptive paradigm for computer-aided detection of colonic polyps</article-title><source>Phys Med Biol</source><year>2015</year><volume>60</volume><issue>18</issue><fpage>7207</fpage><lpage>7228</lpage><pub-id pub-id-type="pmid">26348125</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajbakhsh</surname><given-names>N</given-names></name><name><surname>Gurudu</surname><given-names>SR</given-names></name><name><surname>Liang</surname><given-names>J</given-names></name></person-group><article-title>Automated polyp detection in colonoscopy videos using shape and context information</article-title><source>IEEE T Med Imaging</source><year>2015</year><volume>35</volume><issue>2</issue><fpage>630</fpage><lpage>644</lpage></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmud</surname><given-names>N</given-names></name><name><surname>Cohen</surname><given-names>J</given-names></name><name><surname>Tsourides</surname><given-names>K</given-names></name><name><surname>Tyler</surname><given-names>MB</given-names></name></person-group><article-title>Computer vision and augmented reality in gastrointestinal endoscopy</article-title><source>Gastroenterol Rep</source><year>2015</year><volume>3</volume><issue>3</issue><fpage>179</fpage><lpage>184</lpage></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernal</surname><given-names>J</given-names></name><name><surname>Tajkbaksh</surname><given-names>N</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>FJ</given-names></name><name><surname>Matuszewski</surname><given-names>BJ</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name><etal/></person-group><article-title>Comparative validation of polyp detection methods in video colonoscopy: results from the MICCAI 2015 endoscopic vision challenge</article-title><source>IEEE T Med Imaging</source><year>2017</year><volume>36</volume><issue>6</issue><fpage>1231</fpage><lpage>1249</lpage></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>H</given-names></name><name><surname>Fan</surname><given-names>Y</given-names></name><name><surname>Liang</surname><given-names>Z</given-names></name></person-group><article-title>Improved Curvature Estimation for Shape Analysis in Computer-Aided Detection of Colonic Polyps</article-title><source>International MICCAI Workshop on Computational Challenges and Clinical Opportunities in Virtual Colonoscopy and Abdominal Imaging</source><year>2010</year><publisher-loc>Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>9</fpage><lpage>14</lpage></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>J</given-names></name><name><surname>Doraiswami</surname><given-names>R</given-names></name></person-group><article-title>Real-time image processing system for endoscopic applications</article-title><source><italic>Electrical and Computer Engineering,</italic> 2003<italic>.</italic> IEEE CCECE 2003<italic>.</italic> Canadian Conference on IEEE</source><year>2003</year></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>S</given-names></name><name><surname>Oh</surname><given-names>J</given-names></name><name><surname>Tavanapong</surname><given-names>W</given-names></name><name><surname>Wong</surname><given-names>J</given-names></name><name><surname>De Groen</surname><given-names>PC</given-names></name></person-group><article-title>Polyp Detection in Colonoscopy Video using Elliptical Shape Feature</article-title><source>Image Processing, 2007. ICIP 2007. IEEE International Conference on IEEE</source><year>2007</year></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Thomas</surname><given-names>S</given-names></name><name><surname>Behrens</surname><given-names>A</given-names></name><name><surname>Auer</surname><given-names>R</given-names></name><name><surname>Tischendorf</surname><given-names>J</given-names></name></person-group><article-title>A comparison of blood vessel features and local binary patterns for colorectal polyp classification</article-title><source>Medical Imaging 2009: Computer-Aided Diagnosis</source><year>2009</year></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ribeiro</surname><given-names>E</given-names></name><name><surname>Uhl</surname><given-names>A</given-names></name><name><surname>H&#x000e4;fner</surname><given-names>M</given-names></name></person-group><article-title>Colonic Polyp Classification with Convolutional Neural Networks</article-title><source>2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)</source><year>2016</year><fpage>253</fpage><lpage>258</lpage></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mori</surname><given-names>Y</given-names></name><name><surname>Kudo</surname><given-names>SE</given-names></name><name><surname>Berzin</surname><given-names>TM</given-names></name><name><surname>Misawa</surname><given-names>M</given-names></name><name><surname>Takeda</surname><given-names>K</given-names></name></person-group><article-title>Computer-aided diagnosis for colonoscopy</article-title><source>Endoscopy</source><year>2017</year><volume>49</volume><issue>08</issue><fpage>813</fpage><lpage>819</lpage><pub-id pub-id-type="pmid">28561195</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Haffner</surname><given-names>P</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc IEEE</source><year>1998</year><volume>86</volume><issue>11</issue><fpage>2278</fpage><lpage>2324</lpage></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Tang</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>YH</surname><given-names>L</given-names></name><name><surname>Hu</surname><given-names>YL</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title>Image Object Recognition via Deep Feature-Based Adaptive Joint Sparse Representation</article-title><source>Computational Intelligence and Neuroscience</source><year>2019</year><fpage>8258275</fpage></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title>The development of convolution neural network and its application in image classification: a survey</article-title><source>Opt Eng</source><year>2019</year><volume>58</volume><issue>4</issue><fpage>040901</fpage></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteva</surname><given-names>A</given-names></name><name><surname>Kuprel</surname><given-names>B</given-names></name><name><surname>Novoa</surname><given-names>RA</given-names></name><name><surname>Ko</surname><given-names>J</given-names></name><name><surname>Swetter</surname><given-names>SM</given-names></name><name><surname>Blau</surname><given-names>HM</given-names></name><name><surname>Thrun</surname><given-names>S</given-names></name></person-group><article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title><source>Nature.</source><year>2017</year><volume>542</volume><issue>7639</issue><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="pmid">28117445</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulshan</surname><given-names>V</given-names></name><name><surname>Peng</surname><given-names>L</given-names></name><name><surname>Coram</surname><given-names>M</given-names></name><name><surname>Stumpe</surname><given-names>MC</given-names></name><name><surname>Wu</surname><given-names>D</given-names></name><name><surname>Narayanaswamy</surname><given-names>A</given-names></name><etal/></person-group><article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title><source>JAMA</source><year>2016</year><volume>316</volume><issue>22</issue><fpage>2402</fpage><lpage>2410</lpage><pub-id pub-id-type="pmid">27898976</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>QC</given-names></name><name><surname>Xu</surname><given-names>MD</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Zhong</surname><given-names>YS</given-names></name><etal/></person-group><article-title>Application of convolutional neural network in the diagnosis of the invasion depth of gastric cancer based on conventional endoscopy</article-title><source>Gastrointest Endosc</source><year>2018</year><volume>89</volume><issue>4</issue><fpage>806</fpage><lpage>815</lpage><pub-id pub-id-type="pmid">30452913</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>SY</given-names></name><name><surname>Sargent</surname><given-names>D</given-names></name></person-group><article-title>Colonoscopic polyp detection using convolutional neural networks [C]// <italic>Medical Imaging 2016: Computer-Aided Diagnosis</italic></article-title><source>Int Soc Optics Photonics</source><year>2016</year><volume>9785</volume><fpage>978528</fpage></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tajbakhsh</surname><given-names>N</given-names></name><name><surname>Gurudu</surname><given-names>SR</given-names></name><name><surname>Liang</surname><given-names>J</given-names></name></person-group><article-title>Automatic polyp detection in colonoscopy videos using an ensemble of convolutional neural networks</article-title><source>2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). IEEE</source><year>2015</year><fpage>79</fpage><lpage>83</lpage></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urban</surname><given-names>G</given-names></name><name><surname>Tripathi</surname><given-names>P</given-names></name><name><surname>Alkayali</surname><given-names>T</given-names></name><name><surname>Mittal</surname><given-names>M</given-names></name><name><surname>Jalali</surname><given-names>F</given-names></name><name><surname>Karnes</surname><given-names>W</given-names></name><name><surname>Baldi</surname><given-names>P</given-names></name></person-group><article-title>Deep learning localizes and identifies polyps in real time with 96% accuracy in screening colonoscopy [J]</article-title><source>Gastroenterology</source><year>2018</year><volume>155</volume><issue>4</issue><fpage>1069</fpage><lpage>1078</lpage><pub-id pub-id-type="pmid">29928897</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><source>Very deep convolutional networks for large-scale image recognition</source><year>2014</year></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Wang W, Jiang YB, Luo YH, Li J, Wang X, Zhang T. An advanced deep residual dense network (DRDN) approach for image super-resolution. Int J Comput Intell Syst. 2019;12(2):1592&#x02013;601. 10.2991/ijcis.d.191209.001.</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Wang W, Li YT, Zou T, Wang X, You JY, Luo YH. A novel image classification approach via dense-MobileNet models. Mob Inf Syst. 2020:1&#x02013;8.</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Yan</surname><given-names>S</given-names></name></person-group><source>Network in network</source><year>2013</year></element-citation></ref></ref-list></back></article>