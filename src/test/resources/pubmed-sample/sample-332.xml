
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28125055</article-id><article-id pub-id-type="pmc">5336000</article-id><article-id pub-id-type="doi">10.3390/s17020228</article-id><article-id pub-id-type="publisher-id">sensors-17-00228</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Fusion of High Resolution Multispectral Imagery in Vulnerable Coastal and Land Ecosystems</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ibarrola-Ulzurrun</surname><given-names>Edurne</given-names></name><xref ref-type="aff" rid="af1-sensors-17-00228">1</xref><xref ref-type="aff" rid="af2-sensors-17-00228">2</xref><xref rid="c1-sensors-17-00228" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Gonzalo-Martin</surname><given-names>Consuelo</given-names></name><xref ref-type="aff" rid="af2-sensors-17-00228">2</xref></contrib><contrib contrib-type="author"><name><surname>Marcello-Ruiz</surname><given-names>Javier</given-names></name><xref ref-type="aff" rid="af1-sensors-17-00228">1</xref></contrib><contrib contrib-type="author"><name><surname>Garcia-Pedrero</surname><given-names>Angel</given-names></name><xref ref-type="aff" rid="af2-sensors-17-00228">2</xref></contrib><contrib contrib-type="author"><name><surname>Rodriguez-Esparragon</surname><given-names>Dionisio</given-names></name><xref ref-type="aff" rid="af1-sensors-17-00228">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Smith</surname><given-names>Alistair M. S.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-17-00228"><label>1</label>Instituto de Oceanograf&#x000ed;a y Cambio Global, IOCAG, Universidad Las Palmas de Gran Canaria (ULPGC), Parque Cient&#x000ed;fico Tecnol&#x000f3;gico Marino de Taliarte , 35214 Telde, Spain; <email>javier.marcello@ulpgc.es</email> (J.M.-R.); <email>dionisio.rodriguez@ulpgc.es</email> (D.R.-E.)</aff><aff id="af2-sensors-17-00228"><label>2</label>Centro de Tecnolog&#x000ed;a Biom&#x000e9;dica, Universidad Polit&#x000e9;cnica de Madrid (UPM), Campus de Montegancedo, 28223 Pozuelo de Alarc&#x000f3;n, Spain; <email>chelo@fi.upm.es</email> (C.G.-M.); <email>am.garcia@alumnos.upm.es</email> (A.G.-P.)</aff><author-notes><corresp id="c1-sensors-17-00228"><label>*</label>Correspondence: <email>edurne.ibarrola101@alu.ulpgc.es</email>; Tel.: +34-928-457-365</corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>1</month><year>2017</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2017</year></pub-date><volume>17</volume><issue>2</issue><elocation-id>228</elocation-id><history><date date-type="received"><day>04</day><month>10</month><year>2016</year></date><date date-type="accepted"><day>17</day><month>1</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2017 by the authors.</copyright-statement><copyright-year>2017</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Ecosystems provide a wide variety of useful resources that enhance human welfare, but these resources are declining due to climate change and anthropogenic pressure. In this work, three vulnerable ecosystems, including shrublands, coastal areas with dunes systems and areas of shallow water, are studied. As far as these resources&#x02019; reduction is concerned, remote sensing and image processing techniques could contribute to the management of these natural resources in a practical and cost-effective way, although some improvements are needed for obtaining a higher quality of the information available. An important quality improvement is the fusion at the pixel level. Hence, the objective of this work is to assess which pansharpening technique provides the best fused image for the different types of ecosystems. After a preliminary evaluation of twelve classic and novel fusion algorithms, a total of four pansharpening algorithms was analyzed using six quality indices. The quality assessment was implemented not only for the whole set of multispectral bands, but also for the subset of spectral bands covered by the wavelength range of the panchromatic image and outside of it. A better quality result is observed in the fused image using only the bands covered by the panchromatic band range. It is important to highlight the use of these techniques not only in land and urban areas, but a novel analysis in areas of shallow water ecosystems. Although the algorithms do not show a high difference in land and coastal areas, coastal ecosystems require simpler algorithms, such as fast intensity hue saturation, whereas more heterogeneous ecosystems need advanced algorithms, as weighted wavelet &#x02018;<italic>&#x000e0; trous</italic>&#x02019; through fractal dimension maps for shrublands and mixed ecosystems. Moreover, quality map analysis was carried out in order to study the fusion result in each band at the local level. Finally, to demonstrate the performance of these pansharpening techniques, advanced Object-Based (OBIA) support vector machine classification was applied, and a thematic map for the shrubland ecosystem was obtained, which corroborates wavelet &#x02018;<italic>&#x000e0; trous</italic>&#x02019; through fractal dimension maps as the best fusion algorithm for this ecosystem.</p></abstract><kwd-group><kwd>pansharpening</kwd><kwd>high resolution satellite images</kwd><kwd>WorldView-2</kwd><kwd>ecosystem management</kwd><kwd>classification</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-17-00228"><title>1. Introduction</title><p>Natural ecosystems provide a wide variety of useful resources that enhance human welfare. However, in recent decades, there has been a decline in these ecosystem services, as well as their biodiversity [<xref rid="B1-sensors-17-00228" ref-type="bibr">1</xref>]. In particular, coastal ecosystems are the most complex, dynamic and productive systems in the world [<xref rid="B2-sensors-17-00228" ref-type="bibr">2</xref>]. This creates a demand to preserve environmental resources; hence, it is of great importance to develop reliable methodologies, applied to new high resolution satellite imagery. Thus, the analysis, conservation and management of these environments could be studied, in a continuous, reliable and economic way, and at the suitable spatial, spectral and temporal resolution. However, some processing tasks need to be improved; for instance, the weaknesses in the classification and analysis of land and coastal ecosystems on the basis of remote sensing data, as well as the lack of reliability of the maps; particularly, the extreme difficulty to monitor the coastal ecosystems from remote sensing imagery due to the low reflectivity of these areas covered by water.</p><p>The framework in which the study has been developed is the analysis of both coastal and land ecosystems through very high resolution (VHR) remote sensing imagery in order to obtain high quality products that will allow the comprehensive analysis of natural resources. In this context, remote sensing imagery offers practical and cost-effective means for a good environmental management, especially when large areas have to be monitored [<xref rid="B3-sensors-17-00228" ref-type="bibr">3</xref>] or periodic information is needed. Most VHR optical sensors provide a multispectral image (MS) and a panchromatic image (PAN), which require a number of corrections and enhancements. Image fusion or pansharpening algorithms are important for improving the spatial quality of information available. The pansharpening data fusion technique could be defined as the process of merging MS and PAN images to create new multispectral images with a high spatial resolution [<xref rid="B4-sensors-17-00228" ref-type="bibr">4</xref>,<xref rid="B5-sensors-17-00228" ref-type="bibr">5</xref>]. This fusion stage is important in the analysis of such vulnerable ecosystems, mainly characterized by heterogeneous and mixed vegetation shrubs, with small shrubs in the case of terrestrial ecosystems and the complexity of seagrass meadows or algae distribution in shallow water ecosystems.</p><p>Image fusion techniques combine sensor data from different sources with the aim of providing more detailed and reliable information. The extensive research into image fusion techniques in remote sensing started in the 1980s [<xref rid="B6-sensors-17-00228" ref-type="bibr">6</xref>,<xref rid="B7-sensors-17-00228" ref-type="bibr">7</xref>]. Generally, image fusion can be categorized into three levels: pixel level, feature level and knowledge or decision level [<xref rid="B8-sensors-17-00228" ref-type="bibr">8</xref>,<xref rid="B9-sensors-17-00228" ref-type="bibr">9</xref>], and pansharpening is performed at the pixel level.</p><p>Many pansharpening techniques have appeared in recent decades, as a consequence of the launch of very high resolution sensors [<xref rid="B10-sensors-17-00228" ref-type="bibr">10</xref>,<xref rid="B11-sensors-17-00228" ref-type="bibr">11</xref>,<xref rid="B12-sensors-17-00228" ref-type="bibr">12</xref>,<xref rid="B13-sensors-17-00228" ref-type="bibr">13</xref>,<xref rid="B14-sensors-17-00228" ref-type="bibr">14</xref>]. An ideal pansharpening algorithm should have two main attributes: (i) enhancing high spatial resolution; and (ii) reducing spectral distortion [<xref rid="B15-sensors-17-00228" ref-type="bibr">15</xref>]. The simplest pansharpening methods, at the conceptual and computational level, are intensity-hue-saturation (IHS), principal component analysis (PCA) and Brovey transforms (BT). However, these techniques have problems because the radiometry on the spectral channels is distorted after fusion. New approaches, such as wavelet transformations and high pass filtering (HPF) [<xref rid="B4-sensors-17-00228" ref-type="bibr">4</xref>,<xref rid="B8-sensors-17-00228" ref-type="bibr">8</xref>,<xref rid="B16-sensors-17-00228" ref-type="bibr">16</xref>,<xref rid="B17-sensors-17-00228" ref-type="bibr">17</xref>,<xref rid="B18-sensors-17-00228" ref-type="bibr">18</xref>], have been proposed to address particular problems with the traditional techniques.</p><p>On the other hand, quality evaluation is a fundamental issue to benchmark and optimize different pansharpening algorithms [<xref rid="B18-sensors-17-00228" ref-type="bibr">18</xref>,<xref rid="B19-sensors-17-00228" ref-type="bibr">19</xref>], as there is the necessity to assess the spectral and spatial quality of the fused images. There are two types of evaluation approaches commonly used: (1) qualitative analysis (visual assessment); and (2) quantitative analysis (quality indices). Visual analysis is a powerful tool for capturing the geometrical aspect [<xref rid="B20-sensors-17-00228" ref-type="bibr">20</xref>] and the main color disturbances. According to [<xref rid="B10-sensors-17-00228" ref-type="bibr">10</xref>], some visual parameters are necessary for testing the properties of the image, such as the spectral preservation of features, multispectral synthesis in fused images and the synthesis of images close to actual images at high resolution. On the other hand, quality indices measure the spectral and the spatial distortion produced due to the pansharpening process by comparing each fused image to the reference MS or PAN image. The work in [<xref rid="B21-sensors-17-00228" ref-type="bibr">21</xref>] categorized them into three main groups: (i) spectral quality indices such as the spectral angle mapper (SAM) [<xref rid="B22-sensors-17-00228" ref-type="bibr">22</xref>] and the spectral relative dimensionless global error, in French &#x02018;<italic>erreur relative globale adimensionnelle de synth&#x000e9;se</italic>&#x02019; (ERGAS) [<xref rid="B23-sensors-17-00228" ref-type="bibr">23</xref>]; (ii) spatial quality indices, i.e., the spatial ERGAS [<xref rid="B24-sensors-17-00228" ref-type="bibr">24</xref>], the frequency comparison index (FC) [<xref rid="B25-sensors-17-00228" ref-type="bibr">25</xref>] and the Zhou index [<xref rid="B26-sensors-17-00228" ref-type="bibr">26</xref>]; and (iii) global quality indices, such as the 8-band image quality index (Q8) [<xref rid="B27-sensors-17-00228" ref-type="bibr">27</xref>]. On the other hand, there are several evaluation techniques with no reference requirement, such as the quality with no reference (QNR) approach [<xref rid="B28-sensors-17-00228" ref-type="bibr">28</xref>].</p><p>In this study, the main goal is to assess which pansharpening technique, using Worldview-2 VHR imagery with eight MS bands, provides a better fused image. Future research will be focused on the classification of the vulnerable ecosystems, in order to obtain specific products for the management of coastal and land resources, in contrast to several studies assessing and reviewing pansharpening techniques [<xref rid="B11-sensors-17-00228" ref-type="bibr">11</xref>,<xref rid="B16-sensors-17-00228" ref-type="bibr">16</xref>,<xref rid="B20-sensors-17-00228" ref-type="bibr">20</xref>,<xref rid="B29-sensors-17-00228" ref-type="bibr">29</xref>,<xref rid="B30-sensors-17-00228" ref-type="bibr">30</xref>,<xref rid="B31-sensors-17-00228" ref-type="bibr">31</xref>,<xref rid="B32-sensors-17-00228" ref-type="bibr">32</xref>,<xref rid="B33-sensors-17-00228" ref-type="bibr">33</xref>,<xref rid="B34-sensors-17-00228" ref-type="bibr">34</xref>]. Further specific goals in this paper are: (i) the study of the spatial and spectral quality of pansharpened bands covered by and outside the PAN wavelength range; (ii) analysis of pansharpening algorithms&#x02019; behavior in vulnerable natural ecosystems, unlike the majority of previous studies, which apply the pansharpening techniques in urban areas or on other land cover types; and (iii) novel assessment in VHR image fusion in shallow coastal waters, whilst being aware of the pansharpening difficulty of these ecosystems. Although other authors apply fusion in water areas, such as [<xref rid="B35-sensors-17-00228" ref-type="bibr">35</xref>,<xref rid="B36-sensors-17-00228" ref-type="bibr">36</xref>], they use Landsat imagery, not VHR imagery. The aim of the last point is to identify which techniques were more suitable for shallow water areas and the improvement achieved. This information can lead to obtaining more accurate seafloor segmentation or mapping of coastal zones [<xref rid="B37-sensors-17-00228" ref-type="bibr">37</xref>]; hence, studies on the state of conservation of natural resources could be conducted.</p><p>Finally, in order to strengthen the study, a final thematic map of the shrubland area was carried out. Thus, it will analyze the influence of the fusion on the classification results which serve to obtain accurate information for the conservation of natural resources. This study can be found in more detail in [<xref rid="B38-sensors-17-00228" ref-type="bibr">38</xref>].</p><p>The paper is structured as follows: <xref ref-type="sec" rid="sec2-sensors-17-00228">Section 2</xref> includes the description of the study area, datasets, the image fusion methods used in the analysis and the evaluation methodology. The visual and quantitative evaluation of the different algorithms, as well as map analysis are presented in <xref ref-type="sec" rid="sec3-sensors-17-00228">Section 3</xref>. <xref ref-type="sec" rid="sec4-sensors-17-00228">Section 4</xref> includes the critical analysis of the results. Finally, <xref ref-type="sec" rid="sec4-sensors-17-00228">Section 4</xref> summarizes the main outcomes and contributions.</p></sec><sec id="sec2-sensors-17-00228"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-17-00228"><title>2.1. Study Area</title><p>This study focuses on three types of vulnerable ecosystems found in different islands of the Macaronesia region. Macaronesia is considered a geological and a biodiversity hotspot due to the volcanic origin and due to the high degree of vulnerability that insular ecosystems are subjected to, mainly as a consequence of climate change and anthropogenic pressure. The ecosystems selected from the Canary Islands were: the shrubland ecosystem, the coastal ecosystem and, finally, a mixed ecosystem surrounded by a touristic area and a lagoon, as a transitional system within the coastal and land ecosystems. The Canary Islands are one of the most remarkable biodiversity areas on the planet [<xref rid="B39-sensors-17-00228" ref-type="bibr">39</xref>], and they are chosen as a representative sample of these ecosystems because of the availability of VHR remote sensing imagery and simultaneous field data. <xref ref-type="fig" rid="sensors-17-00228-f001">Figure 1</xref> displays the 3 protected areas considered in the analysis.</p><p>As regards shrubland ecosystems, it is important to highlight the large concentration of vascular plants, which are highly vulnerable to environmental changes. In coastal areas, an intensive natural fragility also appears due to the interaction of a great variety of environmental factors. Moreover, the coastal occupation of urban areas and the development of tourism increase this frailty and vulnerability. In particular, dune systems are affected by this urban-touristic expansion [<xref rid="B40-sensors-17-00228" ref-type="bibr">40</xref>,<xref rid="B41-sensors-17-00228" ref-type="bibr">41</xref>,<xref rid="B42-sensors-17-00228" ref-type="bibr">42</xref>,<xref rid="B43-sensors-17-00228" ref-type="bibr">43</xref>]. Furthermore, many coastal ecosystems contain seagrass meadows [<xref rid="B44-sensors-17-00228" ref-type="bibr">44</xref>]. The importance of these particular meadows is related to the ocean productivity as they are one of the most valuable ecosystems in the world. In addition, these meadows are part of the solution to climate change, not only producing oxygen, but storing up to twice as much carbon per unit area as the world&#x02019;s temperate and tropical forests [<xref rid="B45-sensors-17-00228" ref-type="bibr">45</xref>].</p><p>As indicated, three sensible and heterogenic protected areas of the Canary Islands have been selected (<xref ref-type="fig" rid="sensors-17-00228-f001">Figure 1</xref>) as representative examples of more general ecosystems: the Teide National Park (Tenerife Island), as an example of shrubland ecosystems, the Corralejo Natural Park and Islote de Lobos (Fuerteventura), representing coastal ecosystems, and the Maspalomas Natural Reserve (Gran Canaria Island), an important coastal-dune ecosystem with significant tourism pressure, called the mixed ecosystem in this paper, not only because it is the transitional region within the coastal and land ecosystem, but also due to the inner water ecosystem, known as the Maspalomas lagoon.</p><p>Other similar shrubland ecosystems can be identified around the world, for example: Pico do Pico in the Azores, Mt. Halla in South Korea and Hawaii or the Galapagos. Corralejo and Maspalomas are important coastal sand dune systems in Europe, as they contain a large degree of biodiversity [<xref rid="B46-sensors-17-00228" ref-type="bibr">46</xref>]. The importance of studying these ecosystems is their similarity with other Mediterranean, temperate or tropical parts of the world: Tuscany (Italy), Do&#x000f1;ana National Park (Spain), as well as other parts of the world, such as Parangtritis (Java, Indonesia), Malaysia, Philippines, Vietnam, NE Queensland, the tropical coast of Brazil, the West African coast, Cuba, the Galapagos islands, the West Indies, Cox&#x02019;s Bazaar (Bangladesh), Hawaii, Ghana, the coast of India or Christmas Island [<xref rid="B47-sensors-17-00228" ref-type="bibr">47</xref>].</p></sec><sec id="sec2dot2-sensors-17-00228"><title>2.2. Datasets</title><p>The WorldView-2 (WV-2) satellite, launched by Digital Globe on 8 October 2009, was the first commercial satellite to provide a VHR sensor with one PAN and eight MS bands (<xref ref-type="table" rid="sensors-17-00228-t001">Table 1</xref>). WV-2 ortho-ready imagery of the three representative ecosystems were used in the study. Images of the Canary Islands and the central locations of the corresponding three study areas are detailed in <xref ref-type="table" rid="sensors-17-00228-t002">Table 2</xref>. In order to reduce the computation times in the multiple analyses, 512 &#x000d7; 512 MS pixel scenes were used. <xref ref-type="fig" rid="sensors-17-00228-f002">Figure 2</xref> displays the PAN band and the corresponding resized MS image (RGB composite). They were selected for their spectral and spatial richness and the content of land and coastal areas. In addition, the scenes include different coverages, basically predominating coastal areas, shallow waters, vegetation and urban regions, and they contain features with different shapes and edges.</p></sec><sec id="sec2dot3-sensors-17-00228" sec-type="methods"><title>2.3. Image Fusion Methodology</title><p>In the flow shown in <xref ref-type="fig" rid="sensors-17-00228-f003">Figure 3</xref>, every step of the methodology for assessing which algorithm gives the best fused image for each area is presented.</p><p>The first four pansharpening techniques were implemented in the three different vulnerable ecosystems; afterwards, a visual and quantitative assessment was undertaken in order to evaluate the pansharpening results in the different fused images. The quality assessment was carried out for the whole set of MS bands, as well as for those MS bands covering the range of the PAN channel (Bands 2&#x02013;6) and those outside this range (Bands 1, 7 and 8). Furthermore, an individual band quality map analysis was carried out in the best fused images according to each ecosystem type. Finally, a classification map is obtained in the different fused images, to analyze the influence of the pansharpening in the classification result.</p><sec id="sec2dot3dot1-sensors-17-00228"><title>2.3.1. Image Fusion Methods</title><p>After a review of the state-of-the-art in pansharpening techniques, at the pixel level, a preliminary assessment was carried out selecting classic and new algorithms that could achieve good performance with WV-2 imagery. Some algorithms specifically adapted to the WorldView-2 sensor have been chosen. An initial visual and quantitative assessment was carried out using a total of 12 different pansharpening techniques in these data, but only the best algorithms were selected for this study. The final election of these four algorithms was carried out with the same methodology explained in this paper. Thus, a visual and quantitative evaluation was performed to assess the spectral and the spatial quality of each algorithm, taking into account the compromise between both qualities in each fused image, depending on each ecosystem type. Thus, after obtaining an objective ranking of the 12 algorithms selected using the Borda count method, explained in <xref ref-type="sec" rid="sec2dot3dot2-sensors-17-00228">Section 2.3.2</xref>, the final best four pansharpening algorithms were selected, in order to obtain the most suitable fused image for each ecosystem. Next, a brief overview of each family of the algorithms selected in the study is presented. Any formula or block diagram is omitted (for detailed information, see the references).
<list list-type="bullet"><list-item><p>Fast intensity hue saturation (FIHS) [<xref rid="B48-sensors-17-00228" ref-type="bibr">48</xref>]: It uses the spectral bands to estimate the new component <italic>I</italic>. The spatial detail is extracted, computing the difference between the PAN band and the new component <italic>I</italic>. The spatial detail is injected into any number of bands.</p></list-item><list-item><p>Hyperspherical color sharpening (HCS): This pansharpening algorithm is designed specifically for WV-2 by [<xref rid="B49-sensors-17-00228" ref-type="bibr">49</xref>] based on the transformation between any native color space and the hyperspherical color space. Once transformed into HCS, the intensity can be scaled without changing the color, essential for the HCS pansharpening algorithm [<xref rid="B15-sensors-17-00228" ref-type="bibr">15</xref>,<xref rid="B50-sensors-17-00228" ref-type="bibr">50</xref>]. The transformation to HCS can be made from any native color space.</p></list-item><list-item><p>Based on modulation transfer function: The modulation transfer function (MTF) is a function of the sensor spatial frequency response, describing the resolution of an imaging system [<xref rid="B28-sensors-17-00228" ref-type="bibr">28</xref>]. Generalized Laplacian pyramid (GLP) is an extension of the Laplacian pyramid where a scale factor different from two is used [<xref rid="B10-sensors-17-00228" ref-type="bibr">10</xref>]. Finally, in high pass modulation (HPM), the PAN image is multiplied by each band of the original MS image and normalized by a low pass filtered version of the PAN image in order to estimate the enhanced MS image bands.</p></list-item><list-item><p>Weighted wavelet &#x02018;<italic>&#x000e0; trous</italic>&#x02019; method through fractal dimension maps (WAT&#x02297;FRAC) [<xref rid="B14-sensors-17-00228" ref-type="bibr">14</xref>]: This method is based on the wavelet &#x02018;<italic>&#x000e0; trous</italic>&#x02019; algorithm. A mechanism that controls the trade-off between the spatial and spectral quality by introducing a weighting factor (&#x003b1;<sub>i</sub>) for the PAN wavelet coefficients is established. However, this factor only discriminates between different spectral bands, but not between different land covers; therefore, the authors proposed a new approach [<xref rid="B51-sensors-17-00228" ref-type="bibr">51</xref>], defining a different weight factor &#x003b1;<sub>i</sub> (x, y) for each point of each band. &#x003b1;<sub>i</sub> (x, y) was defined as a fractal dimension map (FDM) with the same size as the original image. A preliminary analysis was carried out using three different window sizes for the windowing process: 7, 15 and 27.</p></list-item></list></p></sec><sec id="sec2dot3dot2-sensors-17-00228" sec-type="methods"><title>2.3.2. Quality Evaluation Methodology</title><p>(a) Visual quality:</p><p>A visual analysis was the first step in the quality assessment. Through this approach, the main errors on an overall scale were observed, and then, local artefacts where analyzed closely. For the visual spectral assessment, fused true color images were compared to their original MS image, used as the reference, and its spectral features compared with the original MS image. Firstly, several aspects of the image features were taken into account in the spectral assessment, such as the tone, contrast, saturation, sharpness and texture. Furthermore, we paid attention to color disturbances. False color composites were produced in order to evaluate the fused NIR bands, where we focused on the same aspects as mentioned above. Finally, we concentrated on linear features, specific objects, surfaces or edges of buildings in order to observe spatial disturbances using the PAN as a reference.</p><p>(b) Quality indices</p><p>There is no current consensus in the literature on the best quality index for pansharpening images [<xref rid="B52-sensors-17-00228" ref-type="bibr">52</xref>]. The quantitative quality evaluation of fused images is a debated issue since no reference image exists at the pansharpened resolution [<xref rid="B4-sensors-17-00228" ref-type="bibr">4</xref>,<xref rid="B20-sensors-17-00228" ref-type="bibr">20</xref>]. A number of statistical evaluation indices were used to measure the quality of the fused images. Each fused image is compared to the reference MS image.</p><p>The spectral quality assessment measures the spectral distortion brought by the pansharpening process. The metrics considered in the analysis are as follows:
<list list-type="bullet"><list-item><p>The spectral angle mapper (SAM) was designed to determine the spectral similarity in a multidimensional space [<xref rid="B22-sensors-17-00228" ref-type="bibr">22</xref>] (Equation (1) in <xref ref-type="table" rid="sensors-17-00228-t003">Table 3</xref>).</p></list-item><list-item><p>The spectral ERGAS (relative dimensionless global error) is an overall quality index sensitive to mean shifting and dynamic range change (<xref ref-type="table" rid="sensors-17-00228-t002">Table 2</xref>, Equation (3)). The <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (root mean square error) is calculated by its standard definition [<xref rid="B23-sensors-17-00228" ref-type="bibr">23</xref>].</p></list-item></list></p><p>The correlation coefficient was not selected as spectral index due to its low capability in techniques with low quality differences.</p><p>On the other hand, the spatial detail information of each fused band is compared with the spatial information of the reference PAN image. The metrics considered in the analysis are as follows:
<list list-type="bullet"><list-item><p>The spatial ERGAS was proposed by [<xref rid="B24-sensors-17-00228" ref-type="bibr">24</xref>]. It is a new spatial index considering the PAN band as a reference (<xref ref-type="table" rid="sensors-17-00228-t003">Table 3</xref>, Equation (3)).</p></list-item><list-item><p>The frequency comparison index (FC) is proposed by [<xref rid="B25-sensors-17-00228" ref-type="bibr">25</xref>]. It is based on the discrete cosine transform (<italic>dct</italic>) for the spatial assessment (<xref ref-type="table" rid="sensors-17-00228-t003">Table 3</xref>, Equation (4)).</p></list-item><list-item><p>The Zhou index (<xref ref-type="table" rid="sensors-17-00228-t003">Table 3</xref>, Equation (5)) measures the spatial quality computing correlation between the high pass filtered fused image (<inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and PAN (<inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) image for each band.</p></list-item></list></p><p>Finally, as the global quality assessment:
<list list-type="bullet"><list-item><p>The Q8 index is a generalization to eight-band images of the Q index [<xref rid="B27-sensors-17-00228" ref-type="bibr">27</xref>]. It is based on the different statistical properties of the fused and MS images (<xref ref-type="table" rid="sensors-17-00228-t003">Table 3</xref>, Equation (6)).</p></list-item></list></p><p>In order to identify, in an objective way, the best fused image for each ecosystem, the best algorithms at the spectral, spatial and global level for each scene have been established by the Borda count rank aggregation method (Equation (7)) [<xref rid="B53-sensors-17-00228" ref-type="bibr">53</xref>]. Consider <italic>U</italic> a set of items <italic>i</italic>, called the universe, and <italic>R</italic> a set of the rank list, where <inline-formula><mml:math id="mm14"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula> is an item of the rank list. The method is equivalent to: for each item <italic>i</italic>
<inline-formula><mml:math id="mm15"><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>U</italic>, a rank list <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02208;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and considering Borda normalized weight <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003c9;</mml:mi><mml:mi>&#x003c4;</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the fused rank list <inline-formula><mml:math id="mm18"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is ordered with respect to the Borda score <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mover accent="true"><mml:mi mathvariant="sans-serif">&#x003c4;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where the Borda score of an item <italic>i</italic>
<inline-formula><mml:math id="mm20"><mml:mrow><mml:mo>&#x02208;</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>U</italic> in <inline-formula><mml:math id="mm21"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is defined as:
<disp-formula id="FD1-sensors-17-00228"><label>(7)</label><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mover accent="true"><mml:mi mathvariant="sans-serif">&#x003c4;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="sans-serif">&#x003c4;</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>&#x003c9;</mml:mi><mml:mi>&#x003c4;</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec2dot4-sensors-17-00228"><title>2.4. Classification Maps</title><p>A supervised classification technique was applied only in the shrubland ecosystem scene in order to analyze the influence of the different pansharpening techniques in the generation of thematic maps [<xref rid="B38-sensors-17-00228" ref-type="bibr">38</xref>]. The first step was to determine the classes appearing in the image and obtain a sufficient number of training samples. The classes chosen for this ecosystem were selected by experts of the Teide National Park, the vegetation classes being: <italic>Spartocytisus supranubius</italic> (Teide broom), <italic>Pterocephalus lasiospermus</italic> (rosalillo de cumbre), <italic>Descurainia bourgaeana</italic> (hierba pajonera) and <italic>Pinus canariensis</italic> (Canarian pine). Urban, road and bare soil classes were also included. In the second step, the OBIA process starts with a segmentation of the input images into local groups of pixels, i.e., objects, that become spatial units in the later analysis, classifications and accuracy assessment. Object shape, size and spectral properties depend on both the segmentation approach and the research goals. The image was segmented using the multiresolution segmentation followed by the spectral difference segmentation, in order to preserve the small objects of interest to classify. Once the objects are obtained from the segmentation techniques, classification algorithms can be applied. The last step was to determine the classification algorithm; in our case, we applied the novel object-based or OBIA classification approach [<xref rid="B54-sensors-17-00228" ref-type="bibr">54</xref>], using support vector machine (SVM) as the supervised classifier [<xref rid="B55-sensors-17-00228" ref-type="bibr">55</xref>]. SVM is a related supervised learning method that analyzes data and recognizes patterns, used for classification and regression analysis. The standard SVM takes a set of input data and predicts, for each given input, which of the different possible classes the input is a member. Given a set of training examples, each marked as belonging to the categories, an SVM training algorithm builds a model that assigns new examples into one category [<xref rid="B56-sensors-17-00228" ref-type="bibr">56</xref>]. Thematic maps were obtained after implementing the SVM classifier in each fused image. Afterwards, the accuracy of the classification must be measured; in this case, testing samples were collected. The statistical accuracy assessment technique used was the overall accuracy and the kappa coefficient.</p></sec></sec><sec id="sec3-sensors-17-00228"><title>3. Results</title><p>This section is divided into three main blocks: (i) the visual assessment; (ii) the quantitative evaluation based on the quality indexes; and (iii) the thematic maps resulting from the OBIA classification in the shrubland ecosystem.</p><sec id="sec3dot1-sensors-17-00228"><title>3.1. Visual Evaluation</title><p>For the visual analysis, both color and edge preservation are the most important criteria to evaluate the performance of image fusion techniques in order to identify the fusion technique that provides the fused image with the highest spectral and spatial quality. To facilitate the visual inspection and for a more detailed spatial analysis, a zoom of the previous scenes is shown in <xref ref-type="fig" rid="sensors-17-00228-f004">Figure 4</xref>, <xref ref-type="fig" rid="sensors-17-00228-f005">Figure 5</xref> and <xref ref-type="fig" rid="sensors-17-00228-f006">Figure 6</xref>. It is important to highlight that, after the preliminary assessment, robust pansharpening algorithms have been selected, so all fusion results are satisfactory, and the spectral differences are difficult to appreciate visually, except in some specific areas. We want to underline that, to the best of our knowledge, this is the first time pansharpening algorithms have been assessed in coastal ecosystems using VHR imagery. This improvement in the spatial quality due to the application of fusion techniques could be useful to improve seafloor or benthic classification of shallow waters (i.e., coral reefs or seagrass meadows).</p><p>The visual interpretation at the spectral level in the shrubland region (<xref ref-type="fig" rid="sensors-17-00228-f004">Figure 4</xref>) indicates that every algorithm, except for WAT&#x02297;FRAC, produces a slight color distortion over the entire fused image. On the other hand, in the preliminary analysis, WAT&#x02297;FRAC with a window size of seven pixels provides the best fused image from among the WAT&#x02297;FRAC algorithms, although the differences are minimal.</p><p>Observing the coastal region (<xref ref-type="fig" rid="sensors-17-00228-f005">Figure 5</xref>), differences among the techniques are basically undetectable at the spectral level. The WAT&#x02297;FRAC window size, which gives a slightly better result in this image, is 27.</p><p>Visual results similar to those of the shrubland region appear in the mixed ecosystem (<xref ref-type="fig" rid="sensors-17-00228-f006">Figure 6</xref>), where the FIHS, HCS and MTF algorithms show a more perceptible color distortion in the sea than in buildings. The WAT&#x02297;FRAC window size chosen for this scene is 15.</p><p>Spatially, the differences are clearer than spectrally. In the case of the HCS and MTF_GLP_HPM techniques, although they maintain the spectral information well, the spatial details are not satisfactorily injected, thus not achieving a good spatial enhancement. Furthermore, the blurred aspect found in the WAT&#x02297;FRAC algorithm is because the algorithm makes uniform the homogenous areas found in the multispectral image, which appear as heterogeneous areas in the panchromatic image. Thus, the &#x02018;salt and pepper&#x02019; effect is avoided with it, obtaining a better classification and more accurate thematic maps, as demonstrated in <xref ref-type="sec" rid="sec3dot3-sensors-17-00228">Section 3.3</xref>.</p><p>Finally, <xref ref-type="fig" rid="sensors-17-00228-f007">Figure 7</xref> shows an example of the false color composite using bands outside the PAN range (Bands (B) 8, 7 and 1).</p><p>In this case, a region with soil, vegetation and water is chosen in order to analyze the behavior of the different techniques. Spectrally, algorithms show a more significant color distortion in the water area. At the spatial level, pansharpening algorithms demonstrate the same behavior as in the true color composite, with WAT&#x02297;FRAC achieving an optimum result.</p></sec><sec id="sec3dot2-sensors-17-00228"><title>3.2. Quantitative Evaluation of Pansharpened Images</title><p>As was mentioned in <xref ref-type="sec" rid="sec2dot4-sensors-17-00228">Section 2.4</xref>, in order to perform an objective evaluation of the pansharpening techniques, six spectral and spatial quality indices have been computed on the complete scenes of <xref ref-type="fig" rid="sensors-17-00228-f002">Figure 2</xref>. First, quantitative indices were calculated for all of the bands in the MS image in the three ecosystems. Second, the pansharpening performance for bands covered by and outside of the PAN range was assessed. Finally, an individual band quality map analysis was carried out (<xref ref-type="fig" rid="sensors-17-00228-f003">Figure 3</xref>).</p><sec id="sec3dot2dot1-sensors-17-00228"><title>3.2.1. Shrubland Ecosystems</title><p>The quality analysis of all multispectral bands can be appreciated in <xref ref-type="table" rid="sensors-17-00228-t004">Table 4</xref>. SAM, spectral ERGAS and Q8 confirm that the MTF method and the HCS algorithm provide better spectral performance, while FIHS and WAT&#x02297;FRAC get the lowest result, even though there is not a large difference between the highest and the lowest value. As regards the spatial performance, WAT&#x02297;FRAC is confirmed as the best spatial quality method, while HCS shows the worst values. Furthermore, these results confirm the compromise between the spectral and spatial quality, in which the best fused image considered in this study would be the one that provides the best compromise between them. Hence, the Borda count method is used a posteriori to observe this compromise.</p><p>The Borda count method has allowed analyzing the balance between the spectral and spatial quality in the pansharpening algorithms to be distinguished to avoid any bias in the global result (<xref ref-type="table" rid="sensors-17-00228-t004">Table 4</xref>). The results obtained by the Borda count method including the respective spectral and spatial quality indices appear in the &#x02018;spectral&#x02019; and &#x02018;spatial&#x02019; columns. On the other hand, the &#x02018;global&#x02019; column shows the results using the Borda count method in every algorithm considering all of the quality indices.</p><p>Analyzing <xref ref-type="table" rid="sensors-17-00228-t004">Table 4</xref>, WAT&#x02297;FRAC generates the best fused image, not only in the overall evaluation, but also at the spatial level.</p><p>As regards the band analysis based on the PAN range, the behavior of the quality index is analyzed with respect to the MS fused bands. Thus, <xref ref-type="fig" rid="sensors-17-00228-f008">Figure 8</xref> shows the quantitative values obtained for the pansharpened bands within the PAN spectral range (B 2&#x02013;6) and outside of this range (B 1, 7 and 8). The results for the complete set of bands are included as a reference.</p><p>SAM and spectral ERGAS indices provide similar results, where fused bands outside of the PAN range have better quality than the bands within the PAN range, the results for the complete set of bands being an average of them. There is an exception with MTF_GLP_HPM, providing similar results irrespective of the bands that are used for their estimation. While the analysis of the spatial indices, in general, shows that bands inside the PAN range achieve better spatial quality.</p></sec><sec id="sec3dot2dot2-sensors-17-00228"><title>3.2.2. Coastal Ecosystems</title><p><xref ref-type="table" rid="sensors-17-00228-t005">Table 5</xref> includes the values of the quality indices for all bands obtained when applying the different fusion methods in the coastal ecosystem scenario. It is important to point out that most of the scene is covered by shallow water. The SAM index shows as the best fused images the ones obtained by MTF_GLP_HPM and FIHS; however, it does not demonstrate a large variability in the results, just like that of Q8. In the case of spectral ERGAS, MTF_GLP_HPM shows the best fused image. Spatial indices also show some variability, where spatial ERGAS and FC identify FIHS as the best method, while the WAT&#x02297;FRAC algorithm gets the best quality result in Zhou. All spatial indices agree that the lowest quality image is achieved by the MTF_GLP_HPM and HCS methods.</p><p>According to the Borda count method, the best algorithm for getting a good fusion in coastal ecosystems is FIHS, which achieves a good balance between the spectral and the spatial quality.</p><p>With respect to the band analysis based on the PAN range (<xref ref-type="fig" rid="sensors-17-00228-f009">Figure 9</xref>), from the spectral point of view, SAM provides similar values between the fusion algorithms, and the spectral ERGAS shows better quality images for the MS bands covered by the PAN range, as expected, except in FIHS. Concerning the spatial indices, the Spatial ERGAS and FC metrics achieve a superior spatial quality for Bands 2&#x02013;6, while Zhou shows very similar results. The Q8 index also gives higher values to the bands covered by the PAN.</p></sec><sec id="sec3dot2dot3-sensors-17-00228"><title>3.2.3. Mixed Ecosystems</title><p>The quantitative evaluation in all bands for the mixed ecosystem is shown in <xref ref-type="table" rid="sensors-17-00228-t006">Table 6</xref>. According to the spectral and spatial quality indices, the results are similar to those of shrubland ecosystems. The MTF_GLP_HPM technique provides the best result at the spectral level, while the WAT&#x02297;FRAC algorithm does so at the spatial level.</p><p>Looking at the spectral quality indices in the mixed ecosystem scene (<xref ref-type="fig" rid="sensors-17-00228-f010">Figure 10</xref>), the same behavior is found for the Spectral ERGAS, which obtains the worst fused image using MS bands outside of the PAN range for the fusion, while in SAM, the values are, in general, very similar between them. With respect to the spatial quality, the bands inside the PAN range obtain better quality values. Finally, Q8 obtains worse quality results for the MS bands outside of the PAN range, as expected.</p><p>In this case, the WAT&#x02297;FRAC algorithm also achieves the best score in the Borda count rank, making it the most reliable algorithm for this kind of ecosystem.</p></sec><sec id="sec3dot2dot4-sensors-17-00228"><title>3.2.4. Individual Band Quality Map Analysis</title><p>Quality values for each individual band in each scene are included in <xref ref-type="table" rid="sensors-17-00228-t007">Table 7</xref>. The Q8 overall index does not show the variability at the local level; thus, quality map analyses were carried out using the Q8 metric with a block size of 64 in order to examine the quality at local level in each band using the best algorithms for each area. The maximum quality index is achieved if a window is not used. This block size of 64 was chosen because increasing the block size increases the index values, and thus, the values obtained in the quality maps could be comparable to the values obtained for the overall Q8. In each quality map, a blue scale has been used with white indicating a higher similarity (Q8 metric) between the original and the fused MS band.</p><p>As indicated, the WAT&#x02297;FRAC algorithm was selected as the best compromise to fuse images in a shrubland ecosystem scenario. Quality maps of WAT&#x02297;FRAC (window size of seven) are presented in <xref ref-type="fig" rid="sensors-17-00228-f011">Figure 11</xref>. Better results can be appreciated for Bands 3&#x02013;8, with Band 4 achieving the best fusion. On the other hand, Bands 1 and 2 (coastal blue and blue) do not show good quality in some areas, with a greater concentration of dark blue pixels, in accordance with the lowest quality results of <xref ref-type="table" rid="sensors-17-00228-t007">Table 7</xref>.</p><p>As regards the coastal ecosystem, in the individual band quality maps of FIHS (<xref ref-type="table" rid="sensors-17-00228-t007">Table 7</xref> and <xref ref-type="fig" rid="sensors-17-00228-f012">Figure 12</xref>), the higher quality is achieved in Bands 2 and 3, with values over 0.88. For Band 1 (0.764), a light blue aspect (medium quality) in the sea area is clear, whereas, in the land area, both bands get dark blue pixels in the quality maps with this algorithm. From Bands 4&#x02013;8, the quality increases in land areas, showing mostly dark blue pixels; however, the quality in sea areas decreases considerably. Band 8 gets a quality value of 0.239, while Bands 5&#x02013;7 show values around 0.3&#x02013;0.4. Regardless of the fact that the FIHS algorithm gets the best fusion for this scenario, the quality maps are not very satisfactory for longer wavelengths, but this portion of the spectrum is of minimum interest in seafloor mapping applications due to the low capability of light to penetrate the water.</p><p>Concerning the quality maps of the WAT&#x02297;FRAC algorithm in mixed ecosystems (<xref ref-type="fig" rid="sensors-17-00228-f013">Figure 13</xref>), they present a similar behavior to that of shrubland areas. Specifically, Bands 3 and 4 have the highest quality values (0.905 and 0.890, respectively, as presented in <xref ref-type="table" rid="sensors-17-00228-t007">Table 7</xref>), while Bands 1 and 2 show lower quality (0.695 for Band 1 and 0.842 for Band 2). On the other hand, water areas have, in general, worse quality as the band number increases.</p></sec></sec><sec id="sec3dot3-sensors-17-00228"><title>3.3. Thematic Maps of Shrubland Ecosystems</title><p><xref ref-type="table" rid="sensors-17-00228-t008">Table 8</xref> shows the overall accuracy and the kappa coefficient for the SVM object-based classification applied to each fused image. The best result is obtained in the WAT&#x02297;FRAC fused image, corroborating the results achieved in the Quantitative Evaluation section, where WAT&#x02297;FRAC shows the best fusion result.</p><p>Finally, <xref ref-type="fig" rid="sensors-17-00228-f014">Figure 14</xref> presents the thematic map from the shrubland ecosystem obtained in the original MS image and in the WAT&#x02297;FRAC fused image by SVM classification. It is observed how the fusion is a fundamental preprocessing in these complex ecosystems because some classes of interest are not well delimited in the original multispectral thematic maps. On the other hand, buildings (red) are erroneously classified as bare soil (brown) in the original multispectral image, and road limits (grey) and vegetation contours seem to be stepped, due to the pixel size in the original image. The results were analyzed by the experts of the national park, confirming a good agreement with respect the real vegetation of the area.</p></sec></sec><sec id="sec4-sensors-17-00228"><title>4. Conclusions</title><p>As indicated, the main objective of this work was to select the pansharpening algorithm that provides the image with the best spatial and spectral quality for land and coastal ecosystems. Due to this reason, the most efficient pansharpening techniques developed in recent years have been applied, in order to achieve the highest spatial resolution of the MS bands while preserving the original spectral information, and assessed. As not a single pansharpening algorithm has exhibited a superior performance, the best techniques have been evaluated in three different types of ecosystems, i.e., heterogenic shrubland ecosystems, coastal systems and coastal-land transitional zones with inland water and an urban area. Fusion methods have frequently been applied to land and urban areas; however, a novel analysis has been conducted covering their evaluation in areas of shallow water using VHR imagery, as well, as they could be useful for the mapping of seabed species, such as seagrasses and coral reefs.</p><p>After a preliminary assessment of twelve pansharpening techniques, a total of four algorithms was selected for the study. In the literature, four band sensors are mostly selected to carry out this kind of study (Ikonos, GeoEye, QuickBird, etc.); however, we have tried to find the best fused image using an eight-band sensor (WorldView-2).</p><p>Both the visual evaluation and the quantitative analysis were achieved using six quality indices at the overall, spectral and spatial level. The best algorithms at the spectral and spatial levels were obtained for each type of ecosystem. Finally, the best fused technique with a compromise between the spectral and spatial quality was identified following the Borda count method. Thus, we provide guidance to users in order to choose the best algorithm that would be more suitable in accordance with the type of ecosystem and the information to be preserved.</p><p>It is interesting to observe that, for land regions, the MTF algorithm performs better at preserving the spectral quality, while the weighted wavelet &#x02018;<italic>&#x000e0; trous</italic>&#x02019; method through the fractal dimension maps algorithm demonstrates better results considering the spatial detail of the fused imagery. Balancing the spectral and spatial quality, the most appropriate pansharpening algorithm for shrubland and mixed ecosystems is the WAT&#x02297;FRAC technique, while FIHS is selected for the coastal ecosystems. Note that to date, the WAT&#x02297;FRAC algorithm has only been used in agricultural areas; however, we have applied this algorithm in natural vulnerable ecosystems, where a successful result has been obtained. Moreover, we can conclude that the more heterogenic the area to be fused, the smaller the window size in WAT&#x02297;FRAC. FIHS provides the best overall fused image in the simplest scenario. Thus, even though there is no remarkable difference in the way the algorithms perform with respect to land and water areas, we have concluded that images with low variability, such as a coastal scenario, covered mostly by water, require simpler algorithms, rather than more complex and heterogeneous images (i.e., shrubland and mixed ecosystems), which need more advanced algorithms in order to obtain good fused imagery.</p><p>Moreover, we have studied the behavior of each algorithm when applied to the complete set of MS bands and on bands covered by and outside of the PAN range. In general, Bands 2&#x02013;6 mainly have better spatial and spectral quality, but the quality of the remaining bands is acceptable. Analyzing the results, there is a difference in how the same algorithm works on land and coastal areas. The fusion might have higher quality on land, while a lower quality appears in bodies of water.</p><p>Additionally, a local study was carried out to identify the distortion introduced in each single band by the best fused algorithms chosen for each scenario. In general, Bands 3&#x02013;8 attained higher quality for land areas, while in water areas, red and near-infrared bands (5, 7 and 8) experience high spectral distortion. However, these bands are not usually used in seabed mapping applications due to their low penetration capability in water.</p><p>Finally, it is important to recall the need to obtain the best fused image in the analyzed ecosystems, as they are heterogenic regions with sparse vegetation mainly made up of small and mixed shrubs with reduced leaf area in the case of shrubland ecosystems and with low radiance absorption in complex and dynamic coastal ecosystems. In this context, thematic maps were obtained using the SVM classifier in the original MS image and in the WAT&#x02297;FRAC fused image. This corroborates the best performance of the WAT&#x02297;FRAC algorithm to generate accurate thematic maps in the shrubland ecosystem. The excellent results provided by these studies are being applied to the generation of challenging thematic maps of coastal and land protected areas, and studies of the state of conservation of natural resources will be performed.</p></sec></body><back><ack><title>Acknowledgments</title><p>This research has been supported by the ARTEMISAT (<italic>An&#x000e1;lisis de recursos marinos y terrestres mediante el procesado de imagines de alta resoluci&#x000f3;n</italic>) (CGL2013-46674-R) project, funded by the Spanish Ministerio de Econom&#x000ed;a y Competitividad.</p></ack><notes><title>Author Contributions</title><p>All of these authors contributed extensively to the work. Edurne Ibarrola-Ulzurrun, Consuelo Gonzalo-Martin and Javier Marcello-Ruiz developed the methodology and analyzed the results. &#x000c1;ngel Garc&#x000ed;a-Pedrero contributed with the implementation of the pansharpening algorithms, and Dionisio Rodriguez-Esparragon contributed with the quality evaluation indices. Edurne Ibarrola-Ulzurrun processed the WorldView-2 data and carried out the image fusions and their quality evaluation, supervised by Consuelo Gonzalo-Martin and Javier Marcello-Ruiz. Finally, Edurne Ibarrola-Ulzurrun wrote the overall paper under the supervision of Consuelo Gonzalo-Martin and Javier Marcello-Ruiz, who were also involved in the manuscript&#x02019;s discussion and revision.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. The founding sponsors had no role in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript; nor in the decision to publish the results.</p></notes><ref-list><title>References</title><ref id="B1-sensors-17-00228"><label>1.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Pagiola</surname><given-names>S.</given-names></name><name><surname>Von Ritter</surname><given-names>K.</given-names></name><name><surname>Bishop</surname><given-names>J.</given-names></name></person-group>
<source>Assessing the Economic Value of Ecosystem Conservation</source>
<publisher-name>World Bank</publisher-name>
<publisher-loc>Washington, DC, USA</publisher-loc>
<year>2004</year>
</element-citation></ref><ref id="B2-sensors-17-00228"><label>2.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Barange</surname><given-names>M.</given-names></name><name><surname>Harris</surname><given-names>R.P.</given-names></name></person-group>
<source>Marine Ecosystems and Global Change</source>
<publisher-name>Oxford University Press</publisher-name>
<publisher-loc>Oxford, UK</publisher-loc>
<year>2010</year>
</element-citation></ref><ref id="B3-sensors-17-00228"><label>3.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Reinke</surname><given-names>K.</given-names></name><name><surname>Jones</surname><given-names>S.</given-names></name></person-group>
<article-title>Integrating vegetation field surveys with remotely sensed data</article-title>
<source>Ecol. Manag. Restor.</source>
<year>2006</year>
<volume>7</volume>
<fpage>S18</fpage>
<lpage>S23</lpage>
<pub-id pub-id-type="doi">10.1111/j.1442-8903.2006.00287.x</pub-id>
</element-citation></ref><ref id="B4-sensors-17-00228"><label>4.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Kpalma</surname><given-names>K.</given-names></name><name><surname>El-Mezouar</surname><given-names>M.C.</given-names></name><name><surname>Taleb</surname><given-names>N.</given-names></name></person-group>
<article-title>Recent trends in satellite image pan-sharpening techniques</article-title>
<source>Proceedings of the 1st International Conference on Electrical, Electronic and Computing Engineering</source>
<conf-loc>Vrniacka Banja, Serbia</conf-loc>
<conf-date>2&#x02013;5 June 2014</conf-date>
</element-citation></ref><ref id="B5-sensors-17-00228"><label>5.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>He</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name></person-group>
<article-title>Hyperspherical color transform based pansharpening method for worldview-2 satellite images</article-title>
<source>Proceedings of the 2013 IEEE 8th Conference on Industrial Electronics and Applications</source>
<conf-loc>Melbourne, Australia</conf-loc>
<conf-date>19&#x02013;21 June 2013</conf-date>
</element-citation></ref><ref id="B6-sensors-17-00228"><label>6.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Hallanda</surname><given-names>W.A.</given-names></name><name><surname>Cox</surname><given-names>S.</given-names></name></person-group>
<article-title>Image sharpening for mixed spatial and spectral resolution satellite systems</article-title>
<source>Proceedings of the 17th International Symposium of Remote Sensing of Environment</source>
<conf-loc>University of Michigan, Ann Arbor, MI, USA</conf-loc>
<conf-date>9&#x02013;13 May 1983</conf-date>
</element-citation></ref><ref id="B7-sensors-17-00228"><label>7.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Schowengerdt</surname><given-names>R.A.</given-names></name></person-group>
<article-title>Recosntruction of multispatial, multispectral image data using spatial frequency content</article-title>
<source>Photogramm. Eng. Remote Sens.</source>
<year>1980</year>
<volume>46</volume>
<fpage>1325</fpage>
<lpage>1334</lpage>
</element-citation></ref><ref id="B8-sensors-17-00228"><label>8.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pohl</surname><given-names>C.</given-names></name></person-group>
<article-title>Challenges of remote sensing image fusion to optimize earth observation data exploitation</article-title>
<source>Eur. Sci. J.</source>
<year>2014</year>
<volume>9</volume>
<fpage>355</fpage>
<lpage>365</lpage>
</element-citation></ref><ref id="B9-sensors-17-00228"><label>9.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group>
<article-title>Multi-source remote sensing data fusion: Status and trends</article-title>
<source>Int. J. Image Data Fusion</source>
<year>2010</year>
<volume>1</volume>
<fpage>5</fpage>
<lpage>24</lpage>
<pub-id pub-id-type="doi">10.1080/19479830903561035</pub-id>
</element-citation></ref><ref id="B10-sensors-17-00228"><label>10.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Amro</surname><given-names>I.</given-names></name><name><surname>Mateos</surname><given-names>J.</given-names></name><name><surname>Vega</surname><given-names>M.</given-names></name><name><surname>Molina</surname><given-names>R.</given-names></name><name><surname>Katsaggelos</surname><given-names>A.K.</given-names></name></person-group>
<article-title>A survey of classical methods and new trends in pansharpening of multispectral images</article-title>
<source>EURASIP J. Adv. Sig. Proc.</source>
<year>2011</year>
<volume>2011</volume>
<fpage>1</fpage>
<lpage>22</lpage>
<pub-id pub-id-type="doi">10.1186/1687-6180-2011-79</pub-id>
</element-citation></ref><ref id="B11-sensors-17-00228"><label>11.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Fonseca</surname><given-names>L.</given-names></name><name><surname>Namikawa</surname><given-names>L.</given-names></name><name><surname>Castejon</surname><given-names>E.</given-names></name><name><surname>Carvalho</surname><given-names>L.</given-names></name><name><surname>Pinho</surname><given-names>C.</given-names></name><name><surname>Pagamisse</surname><given-names>A.</given-names></name></person-group>
<article-title>Image fusion for remote sensing applications</article-title>
<source>Image Fusion and Its Applications</source>
<publisher-name>InTech</publisher-name>
<publisher-loc>Rijeka, Croatia</publisher-loc>
<year>2011</year>
</element-citation></ref><ref id="B12-sensors-17-00228"><label>12.</label><element-citation publication-type="patent">
<person-group person-group-type="author"><name><surname>Laben</surname><given-names>C.A.</given-names></name><name><surname>Brower</surname><given-names>B.V.</given-names></name></person-group>
<article-title>Process for Enhancing the Spatial Resolution of multispectral Imagery Using Pan-Sharpening</article-title>
<source>U.S. Patent</source>
<patent>6011875 A</patent>
<day>4</day>
<month>1</month>
<year>2000</year>
</element-citation></ref><ref id="B13-sensors-17-00228"><label>13.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Qi</surname><given-names>W.</given-names></name></person-group>
<article-title>An effective pansharpening method for worldview-2 satellite images</article-title>
<source>Proceedings of the International Conference on Estimation, Detection and Information Fusion (ICEDIF)</source>
<conf-loc>Harbin, China</conf-loc>
<conf-date>10&#x02013;11 January 2015</conf-date>
</element-citation></ref><ref id="B14-sensors-17-00228"><label>14.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lillo-Saavedra</surname><given-names>M.</given-names></name><name><surname>Gonzalo</surname><given-names>C.</given-names></name></person-group>
<article-title>Spectral or spatial quality for fused satellite imagery? A trade-off solution using the wavelet &#x000e0; trous algorithm</article-title>
<source>Int. J. Remote Sens.</source>
<year>2006</year>
<volume>27</volume>
<fpage>1453</fpage>
<lpage>1464</lpage>
<pub-id pub-id-type="doi">10.1080/01431160500462188</pub-id>
</element-citation></ref><ref id="B15-sensors-17-00228"><label>15.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>He</surname><given-names>M.</given-names></name></person-group>
<article-title>A novel pansharpening algorithm for worldview-2 satellite images</article-title>
<source>Proceedings of the International Conference on Industrial and Intelligent Information (ICIII 2012)</source>
<conf-loc>Singapore</conf-loc>
<conf-date>17&#x02013;18 March 2012</conf-date>
</element-citation></ref><ref id="B16-sensors-17-00228"><label>16.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Alimuddin</surname><given-names>I.</given-names></name><name><surname>Sumantyo</surname><given-names>J.T.S.</given-names></name><name><surname>Kuze</surname><given-names>H.</given-names></name></person-group>
<article-title>Assessment of pan-sharpening methods applied to image fusion of remotely sensed multi-band data</article-title>
<source>Int. J. Appl. Earth Observ. Geoinf.</source>
<year>2012</year>
<volume>18</volume>
<fpage>165</fpage>
<lpage>175</lpage>
</element-citation></ref><ref id="B17-sensors-17-00228"><label>17.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gonz&#x000e1;lez-Aud&#x000ed;cana</surname><given-names>M.</given-names></name><name><surname>Saleta</surname><given-names>J.L.</given-names></name><name><surname>Catal&#x000e1;n</surname><given-names>R.G.</given-names></name><name><surname>Garc&#x000ed;a</surname><given-names>R.</given-names></name></person-group>
<article-title>Fusion of multispectral and panchromatic images using improved ihs and pca mergers based on wavelet decomposition</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2004</year>
<volume>42</volume>
<fpage>1291</fpage>
<lpage>1299</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2004.825593</pub-id>
</element-citation></ref><ref id="B18-sensors-17-00228"><label>18.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Marcello</surname><given-names>J.</given-names></name><name><surname>Medina</surname><given-names>A.</given-names></name><name><surname>Eugenio</surname><given-names>F.</given-names></name></person-group>
<article-title>Evaluation of spatial and spectral effectiveness of pixel-level fusion techniques</article-title>
<source>IEEE Geosci. Remote Sens. Lett.</source>
<year>2013</year>
<volume>10</volume>
<fpage>432</fpage>
<lpage>436</lpage>
<pub-id pub-id-type="doi">10.1109/LGRS.2012.2207944</pub-id>
</element-citation></ref><ref id="B19-sensors-17-00228"><label>19.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Rodr&#x000ed;guez-Esparrag&#x000f3;n</surname><given-names>D.</given-names></name></person-group>
<source>Evaluaci&#x000f3;n y Desarrollo de M&#x000e9;tricas de Calidad Espacial y Espectral Para Aplicaciones de Fusi&#x000f3;n de Im&#x000e1;genes Multiespectrales de Teledetecci&#x000f3;n de Alta Resoluci&#x000f3;n</source>
<publisher-name>Universidad Las Palmas de Gran Canaria</publisher-name>
<publisher-loc>Palmas, Spain</publisher-loc>
<year>2015</year>
</element-citation></ref><ref id="B20-sensors-17-00228"><label>20.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Alparone</surname><given-names>L.</given-names></name><name><surname>Wald</surname><given-names>L.</given-names></name><name><surname>Chanussot</surname><given-names>J.</given-names></name><name><surname>Thomas</surname><given-names>C.</given-names></name><name><surname>Gamba</surname><given-names>P.</given-names></name><name><surname>Bruce</surname><given-names>L.M.</given-names></name></person-group>
<article-title>Comparison of pansharpening algorithms: Outcome of the 2006 grs-s data-fusion contest</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2007</year>
<volume>45</volume>
<fpage>3012</fpage>
<lpage>3021</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2007.904923</pub-id>
</element-citation></ref><ref id="B21-sensors-17-00228"><label>21.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Alparone</surname><given-names>L.</given-names></name><name><surname>Aiazzi</surname><given-names>B.</given-names></name><name><surname>Baronti</surname><given-names>S.</given-names></name><name><surname>Garzelli</surname><given-names>A.</given-names></name><name><surname>Nencini</surname><given-names>F.</given-names></name><name><surname>Selva</surname><given-names>M.</given-names></name></person-group>
<article-title>Multispectral and panchromatic data fusion assessment without reference</article-title>
<source>Photogramm. Eng. Remote Sens.</source>
<year>2008</year>
<volume>74</volume>
<fpage>193</fpage>
<lpage>200</lpage>
<pub-id pub-id-type="doi">10.14358/PERS.74.2.193</pub-id>
</element-citation></ref><ref id="B22-sensors-17-00228"><label>22.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kruse</surname><given-names>F.</given-names></name><name><surname>Lefkoff</surname><given-names>A.</given-names></name><name><surname>Boardman</surname><given-names>J.</given-names></name><name><surname>Heidebrecht</surname><given-names>K.</given-names></name><name><surname>Shapiro</surname><given-names>A.</given-names></name><name><surname>Barloon</surname><given-names>P.</given-names></name><name><surname>Goetz</surname><given-names>A.</given-names></name></person-group>
<article-title>The spectral image processing system (sips)&#x02014;Interactive visualization and analysis of imaging spectrometer data</article-title>
<source>Remote Sens. Environ.</source>
<year>1993</year>
<volume>44</volume>
<fpage>145</fpage>
<lpage>163</lpage>
<pub-id pub-id-type="doi">10.1016/0034-4257(93)90013-N</pub-id>
</element-citation></ref><ref id="B23-sensors-17-00228"><label>23.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wald</surname><given-names>L.</given-names></name></person-group>
<article-title>Quality of high resolution synthesised images: Is there a simple criterion?</article-title>
<source>Proceedings of the Third Conference: Fusion of Earth Data: Merging Point Measurements, Raster Maps And Remotely Sensed Images</source>
<conf-loc>Sophia Antipolis, France</conf-loc>
<conf-date>26&#x02013;28 January 2000</conf-date>
</element-citation></ref><ref id="B24-sensors-17-00228"><label>24.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lillo-Saavedra</surname><given-names>M.</given-names></name><name><surname>Gonzalo</surname><given-names>C.</given-names></name><name><surname>Arquero</surname><given-names>A.</given-names></name><name><surname>Martinez</surname><given-names>E.</given-names></name></person-group>
<article-title>Fusion of multispectral and panchromatic satellite sensor imagery based on tailored filtering in the fourier domain</article-title>
<source>Int. J. Remote Sens.</source>
<year>2005</year>
<volume>26</volume>
<fpage>1263</fpage>
<lpage>1268</lpage>
<pub-id pub-id-type="doi">10.1080/01431160412331330239</pub-id>
</element-citation></ref><ref id="B25-sensors-17-00228"><label>25.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Rodr&#x000ed;guez-Esparrag&#x000f3;n</surname><given-names>D.</given-names></name><name><surname>Marcello-Ruiz</surname><given-names>J.</given-names></name><name><surname>Medina-Mach&#x000ed;n</surname><given-names>A.</given-names></name><name><surname>Eugenio-Gonz&#x000e1;lez</surname><given-names>F.</given-names></name><name><surname>Gonzalo-Mart&#x000ed;n</surname><given-names>C.</given-names></name><name><surname>Garc&#x000ed;a-Pedrero</surname><given-names>A.</given-names></name></person-group>
<article-title>Evaluation of the performance of the spatial assessment of pansharpened images</article-title>
<source>Proceedings of the 2014 IEEE International Geoscience and Remote Sensing Symposium</source>
<conf-loc>Quebec City, QC, Canada</conf-loc>
<conf-date>13&#x02013;18 July 2014</conf-date>
</element-citation></ref><ref id="B26-sensors-17-00228"><label>26.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Civco</surname><given-names>D.</given-names></name><name><surname>Silander</surname><given-names>J.</given-names></name></person-group>
<article-title>A wavelet transform method to merge landsat TM and SPOT panchromatic data</article-title>
<source>Int. J. Remote Sens.</source>
<year>1998</year>
<volume>19</volume>
<fpage>743</fpage>
<lpage>757</lpage>
<pub-id pub-id-type="doi">10.1080/014311698215973</pub-id>
</element-citation></ref><ref id="B27-sensors-17-00228"><label>27.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Bovik</surname><given-names>A.C.</given-names></name></person-group>
<article-title>A universal image quality index</article-title>
<source>IEEE Signal Lett.</source>
<year>2002</year>
<volume>9</volume>
<fpage>81</fpage>
<lpage>84</lpage>
<pub-id pub-id-type="doi">10.1109/97.995823</pub-id>
</element-citation></ref><ref id="B28-sensors-17-00228"><label>28.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Vivone</surname><given-names>G.</given-names></name><name><surname>Alparone</surname><given-names>L.</given-names></name><name><surname>Chanussot</surname><given-names>J.</given-names></name><name><surname>Dalla Mura</surname><given-names>M.</given-names></name><name><surname>Garzelli</surname><given-names>A.</given-names></name><name><surname>Licciardi</surname><given-names>G.</given-names></name><name><surname>Restaino</surname><given-names>R.</given-names></name><name><surname>Wald</surname><given-names>L.</given-names></name></person-group>
<article-title>A critical comparison among pansharpening algorithms</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2015</year>
<volume>53</volume>
<fpage>2565</fpage>
<lpage>2586</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2014.2361734</pub-id>
</element-citation></ref><ref id="B29-sensors-17-00228"><label>29.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Amolins</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Dare</surname><given-names>P.</given-names></name></person-group>
<article-title>Wavelet based image fusion techniques&#x02014;An introduction, review and comparison</article-title>
<source>ISPRS J. Photogramm. Remote Sens.</source>
<year>2007</year>
<volume>62</volume>
<fpage>249</fpage>
<lpage>263</lpage>
<pub-id pub-id-type="doi">10.1016/j.isprsjprs.2007.05.009</pub-id>
</element-citation></ref><ref id="B30-sensors-17-00228"><label>30.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ehlers</surname><given-names>M.</given-names></name><name><surname>Klonus</surname><given-names>S.</given-names></name><name><surname>Johan &#x000c5;strand</surname><given-names>P.</given-names></name><name><surname>Rosso</surname><given-names>P.</given-names></name></person-group>
<article-title>Multi-sensor image fusion for pansharpening in remote sensing</article-title>
<source>Int. J. Image Data Fusion</source>
<year>2010</year>
<volume>1</volume>
<fpage>25</fpage>
<lpage>45</lpage>
<pub-id pub-id-type="doi">10.1080/19479830903561985</pub-id>
</element-citation></ref><ref id="B31-sensors-17-00228"><label>31.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ozdemir</surname><given-names>I.</given-names></name><name><surname>Karnieli</surname><given-names>A.</given-names></name></person-group>
<article-title>Predicting forest structural parameters using the image texture derived from worldview-2 multispectral imagery in a dryland forest, israel</article-title>
<source>Int. J. Appl. Earth Observ. Geoinf.</source>
<year>2011</year>
<volume>13</volume>
<fpage>701</fpage>
<lpage>710</lpage>
<pub-id pub-id-type="doi">10.1016/j.jag.2011.05.006</pub-id>
</element-citation></ref><ref id="B32-sensors-17-00228"><label>32.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Witharana</surname><given-names>C.</given-names></name><name><surname>Civco</surname><given-names>D.L.</given-names></name><name><surname>Meyer</surname><given-names>T.H.</given-names></name></person-group>
<article-title>Evaluation of pansharpening algorithms in support of earth observation based rapid-mapping workflows</article-title>
<source>Appl. Geogr.</source>
<year>2013</year>
<volume>37</volume>
<fpage>63</fpage>
<lpage>87</lpage>
<pub-id pub-id-type="doi">10.1016/j.apgeog.2012.10.008</pub-id>
</element-citation></ref><ref id="B33-sensors-17-00228"><label>33.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Hasanlou</surname><given-names>M.</given-names></name><name><surname>Saradjian</surname><given-names>M.R.</given-names></name></person-group>
<article-title>Quality assessment of pan-sharpening methods in high-resolution satellite images using radiometric and geometric index</article-title>
<source>Arab. J. Geosci.</source>
<year>2016</year>
<volume>9</volume>
<fpage>1</fpage>
<lpage>10</lpage>
<pub-id pub-id-type="doi">10.1007/s12517-015-2015-0</pub-id>
</element-citation></ref><ref id="B34-sensors-17-00228"><label>34.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Jawak</surname><given-names>S.D.</given-names></name><name><surname>Luis</surname><given-names>A.J.</given-names></name></person-group>
<article-title>A semiautomatic extraction of antarctic lake features using worldview-2 imagery</article-title>
<source>Photogramm. Eng. Remote Sens.</source>
<year>2014</year>
<volume>80</volume>
<fpage>939</fpage>
<lpage>952</lpage>
<pub-id pub-id-type="doi">10.14358/PERS.80.10.939</pub-id>
</element-citation></ref><ref id="B35-sensors-17-00228"><label>35.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Gungor</surname><given-names>O.</given-names></name><name><surname>Boz</surname><given-names>Y.</given-names></name><name><surname>Gokalp</surname><given-names>E.</given-names></name><name><surname>Comert</surname><given-names>C.</given-names></name><name><surname>Akar</surname><given-names>A.</given-names></name></person-group>
<article-title>Fusion of low and high resolution satellite images to monitor changes on costal zones</article-title>
<source>Sci. Res. Essays</source>
<year>2010</year>
<volume>5</volume>
<fpage>654</fpage>
<lpage>662</lpage>
</element-citation></ref><ref id="B36-sensors-17-00228"><label>36.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Embabi</surname><given-names>N.S.</given-names></name><name><surname>Moawad</surname><given-names>M.B.</given-names></name></person-group>
<article-title>A semi-automated approach for mapping geomorphology of el bardawil lake, northern sinai, egypt, using integrated remote sensing and gis techniques</article-title>
<source>Egypt. J. Remote Sens. Space Sci.</source>
<year>2014</year>
<volume>17</volume>
<fpage>41</fpage>
<lpage>60</lpage>
<pub-id pub-id-type="doi">10.1016/j.ejrs.2014.02.002</pub-id>
</element-citation></ref><ref id="B37-sensors-17-00228"><label>37.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Eugenio</surname><given-names>F.</given-names></name><name><surname>Marcello</surname><given-names>J.</given-names></name><name><surname>Martin</surname><given-names>J.</given-names></name></person-group>
<article-title>High-resolution maps of bathymetry and benthic habitats in shallow-water environments using multispectral remote sensing imagery</article-title>
<source>IEEE Trans. Geosci. Remote Sens.</source>
<year>2015</year>
<volume>53</volume>
<fpage>3539</fpage>
<lpage>3549</lpage>
<pub-id pub-id-type="doi">10.1109/TGRS.2014.2377300</pub-id>
</element-citation></ref><ref id="B38-sensors-17-00228"><label>38.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Ibarrola-Ulzurrun</surname><given-names>E.</given-names></name><name><surname>Gonzalo-Mart&#x000ed;n</surname><given-names>C.</given-names></name><name><surname>Marcello-Ruiz</surname><given-names>J.</given-names></name></person-group>
<article-title>Influence of pansharpening techniques in obtaining accurate vegetation thematic maps</article-title>
<source>Proceedings of the Earth Resources and Environmental Remote Sensing/GIS Applications</source>
<conf-loc>Edinburgh, UK</conf-loc>
<conf-date>26&#x02013;29 September 2016</conf-date>
</element-citation></ref><ref id="B39-sensors-17-00228"><label>39.</label><element-citation publication-type="webpage">
<person-group person-group-type="author"><name><surname>Mayer-Su&#x000e1;rez</surname><given-names>P.</given-names></name><name><surname>Romero-Mart&#x000ed;n</surname><given-names>L.E.</given-names></name></person-group>
<article-title>La Naturaleza Des&#x000e9;rtica de Fuerteventura y la Erosionabilidad de sus Precipitaciones</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://catalogo.museosdetenerife.org/cdm/singleitem/collection/Arca/id/4130/rec/14">http://catalogo.museosdetenerife.org/cdm/singleitem/collection/Arca/id/4130/rec/14</ext-link></comment>
<date-in-citation>(accessed on 14 December 2016)</date-in-citation>
</element-citation></ref><ref id="B40-sensors-17-00228"><label>40.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Fern&#x000e1;ndez-Cabrera</surname><given-names>E.</given-names></name><name><surname>P&#x000e9;rez-Chac&#x000f3;n</surname><given-names>E.</given-names></name><name><surname>Cruz Avero</surname><given-names>N.</given-names></name><name><surname>Hern&#x000e1;ndez Cordero</surname><given-names>A.</given-names></name><name><surname>Hern&#x000e1;ndez Calvento</surname><given-names>L.</given-names></name></person-group>
<article-title>Consecuencias ambientales del crecimiento urbano-tur&#x000ed;stico en el sistema de dunas de corralejo (fuerteventura-islas canarias)</article-title>
<source>Urbanismo Expansivo de la Utop&#x000ed;a a la Realidad. Asociaci&#x000f3;n de Ge&#x000f3;grafos Espa&#x000f1;oles</source>
<publisher-name>Colegio de Ge&#x000f3;grafos de Espa&#x000f1;a y Universidad de Alicante</publisher-name>
<publisher-loc>Alicante, Spain</publisher-loc>
<year>2011</year>
<fpage>241</fpage>
<lpage>252</lpage>
</element-citation></ref><ref id="B41-sensors-17-00228"><label>41.</label><element-citation publication-type="webpage">
<person-group person-group-type="author"><name><surname>Fern&#x000e1;ndez Cabrera</surname><given-names>E.</given-names></name><name><surname>Roca Bosch</surname><given-names>E.</given-names></name><name><surname>Cabrera</surname><given-names>L.</given-names></name><name><surname>Hern&#x000e1;ndez-Calvento</surname><given-names>L.</given-names></name><name><surname>P&#x000e9;rez-Chacon</surname><given-names>E.</given-names></name></person-group>
<article-title>Estudio de la Percepci&#x000f3;n Social en el Entorno del Parque Natural de Las Dunas de Corralejo (Fuerteventura, Islas Canarias): Aplicaciones Para la Gesti&#x000f3;n Integrada de Zonas Costeras</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://upcommons.upc.edu/handle/2117/18108">http://upcommons.upc.edu/handle/2117/18108</ext-link></comment>
<date-in-citation>(accessed on 14 December 2016)</date-in-citation>
</element-citation></ref><ref id="B42-sensors-17-00228"><label>42.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Garz&#x000f3;n-Machado</surname><given-names>V.</given-names></name><name><surname>del Arco-Aguilar</surname><given-names>M.J.</given-names></name><name><surname>P&#x000e9;rez-de-Paz</surname><given-names>P.L.</given-names></name></person-group>
<article-title>A tool set for description and mapping vegetation on protected natural areas: An example from the canary islands</article-title>
<source>Biodivers. Conserv.</source>
<year>2011</year>
<volume>20</volume>
<fpage>3605</fpage>
<lpage>3625</lpage>
<pub-id pub-id-type="doi">10.1007/s10531-011-0153-6</pub-id>
</element-citation></ref><ref id="B43-sensors-17-00228"><label>43.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Hern&#x000e1;ndez-Cordero</surname><given-names>A.</given-names></name><name><surname>P&#x000e9;rez-Chac&#x000f3;n</surname><given-names>E.</given-names></name><name><surname>Hern&#x000e1;ndez-Calvento</surname><given-names>L.</given-names></name></person-group>
<article-title>Aplicaci&#x000f3;n de tecnolog&#x000ed;as de la informaci&#x000f3;n geogr&#x000e1;fica al estudio de la vegetaci&#x000f3;n en sistemas de dunas litorales. Resultados preliminares en el campo de dunas de maspalomas (gran canaria, islas canarias)</article-title>
<source>Tecnolog&#x000ed;as de la Informaci&#x000f3;n Geogr&#x000e1;fica para el Desarrollo Territorial</source>
<publisher-name>Servicio de Publicaciones y Difusi&#x000f3;n Cient&#x000ed;fica de la ULPGC</publisher-name>
<publisher-loc>Las Palmas de Gran Canaria, Spain</publisher-loc>
<year>2008</year>
</element-citation></ref><ref id="B44-sensors-17-00228"><label>44.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Gobierno de Canarias</surname><given-names>C.</given-names></name></person-group>
<article-title>Plan director reserva natural especial de las dunas de maspalomas. Gobierno de Canarias. Consejeria de Medio Ambiente y Ordenaci&#x000f3;n Territorial</article-title>
<source>Videconsejer&#x000ed;a de Ordenaci&#x000f3;n Territorial</source>
<publisher-name>Direcci&#x000f3;n General de Ordenaci&#x000f3;n al Territorio</publisher-name>
<publisher-loc>Las Palmas de Gran Canaria, Spain</publisher-loc>
<year>2004</year>
</element-citation></ref><ref id="B45-sensors-17-00228"><label>45.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Fourqurean</surname><given-names>J.W.</given-names></name><name><surname>Duarte</surname><given-names>C.M.</given-names></name><name><surname>Kennedy</surname><given-names>H.</given-names></name><name><surname>Marb&#x000e0;</surname><given-names>N.</given-names></name><name><surname>Holmer</surname><given-names>M.</given-names></name><name><surname>Mateo</surname><given-names>M.A.</given-names></name><name><surname>Apostolaki</surname><given-names>E.T.</given-names></name><name><surname>Kendrick</surname><given-names>G.A.</given-names></name><name><surname>Krause-Jensen</surname><given-names>D.</given-names></name><name><surname>McGlathery</surname><given-names>K.J.</given-names></name></person-group>
<article-title>Seagrass ecosystems as a globally significant carbon stock</article-title>
<source>Nat. Geosci.</source>
<year>2012</year>
<volume>5</volume>
<fpage>505</fpage>
<lpage>509</lpage>
<pub-id pub-id-type="doi">10.1038/ngeo1477</pub-id>
</element-citation></ref><ref id="B46-sensors-17-00228"><label>46.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ruocco</surname><given-names>M.</given-names></name><name><surname>Bertoni</surname><given-names>D.</given-names></name><name><surname>Sarti</surname><given-names>G.</given-names></name><name><surname>Ciccarelli</surname><given-names>D.</given-names></name></person-group>
<article-title>Mediterranean coastal dune systems: Which abiotic factors have the most influence on plant communities?</article-title>
<source>Estuar. Coast. Shelf Sci.</source>
<year>2014</year>
<volume>149</volume>
<fpage>213</fpage>
<lpage>222</lpage>
<pub-id pub-id-type="doi">10.1016/j.ecss.2014.08.019</pub-id>
</element-citation></ref><ref id="B47-sensors-17-00228"><label>47.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Mart&#x000ed;nez</surname><given-names>M.L.</given-names></name><name><surname>Psuty</surname><given-names>N.P.</given-names></name></person-group>
<source>Coastal Dunes</source>
<publisher-name>Springer Science &#x00026; Bussiness Media</publisher-name>
<publisher-loc>Berlin, Germany</publisher-loc>
<year>2004</year>
</element-citation></ref><ref id="B48-sensors-17-00228"><label>48.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tu</surname><given-names>T.M.</given-names></name><name><surname>Su</surname><given-names>S.C.</given-names></name><name><surname>Shyu</surname><given-names>H.C.</given-names></name><name><surname>Huang</surname><given-names>P.S.</given-names></name></person-group>
<article-title>A new look at ihs-like image fusion methods</article-title>
<source>Inf. Fusion</source>
<year>2001</year>
<volume>2</volume>
<fpage>177</fpage>
<lpage>186</lpage>
<pub-id pub-id-type="doi">10.1016/S1566-2535(01)00036-7</pub-id>
</element-citation></ref><ref id="B49-sensors-17-00228"><label>49.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Padwick</surname><given-names>C.</given-names></name><name><surname>Deskevich</surname><given-names>M.</given-names></name><name><surname>Pacifici</surname><given-names>F.</given-names></name><name><surname>Smallwood</surname><given-names>S.</given-names></name></person-group>
<article-title>Worldview-2 Pan-Sharpening</article-title>
<source>Proceedings of the ASPRS 2010 Annual Conference</source>
<conf-loc>San Diego, CA, USA</conf-loc>
<conf-date>26&#x02013;30 April 2010</conf-date>
</element-citation></ref><ref id="B50-sensors-17-00228"><label>50.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wu</surname><given-names>B.</given-names></name><name><surname>Fu</surname><given-names>Q.</given-names></name><name><surname>Sun</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name></person-group>
<article-title>Enhanced hyperspherical color space fusion technique preserving spectral and spatial content</article-title>
<source>J. Appl. Remote Sens.</source>
<year>2015</year>
<volume>9</volume>
<fpage>097291</fpage>
<pub-id pub-id-type="doi">10.1117/1.JRS.9.097291</pub-id>
</element-citation></ref><ref id="B51-sensors-17-00228"><label>51.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lillo-Saavedra</surname><given-names>M.</given-names></name><name><surname>Gonzalo</surname><given-names>C.</given-names></name><name><surname>Lagosa</surname><given-names>O.</given-names></name></person-group>
<article-title>Toward reduction of artifacts in fused images</article-title>
<source>Int. J. Appl. Earth Observ. Geoinf.</source>
<year>2011</year>
<volume>13</volume>
<fpage>368</fpage>
<lpage>375</lpage>
<pub-id pub-id-type="doi">10.1016/j.jag.2011.01.001</pub-id>
</element-citation></ref><ref id="B52-sensors-17-00228"><label>52.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Shridhar</surname><given-names>J.D.</given-names></name><name><surname>Alvarinho</surname><given-names>L.J.</given-names></name></person-group>
<article-title>A spectral index ratio-based antarctic land-cover mapping using hyperspatial 8-band worldview-2 imagery</article-title>
<source>Polar Sci.</source>
<year>2013</year>
<volume>7</volume>
<fpage>18</fpage>
<lpage>38</lpage>
</element-citation></ref><ref id="B53-sensors-17-00228"><label>53.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Renda</surname><given-names>M.E.</given-names></name><name><surname>Straccia</surname><given-names>U.</given-names></name></person-group>
<article-title>Web metasearch: Rank vs. Score based rank aggregation methods</article-title>
<source>Proceedings of the 2003 ACM symposium on Applied computing</source>
<conf-loc>Melbourne, FL, USA</conf-loc>
<conf-date>9&#x02013;12 March 2003</conf-date>
</element-citation></ref><ref id="B54-sensors-17-00228"><label>54.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Blaschke</surname><given-names>T.</given-names></name><name><surname>Lang</surname><given-names>S.</given-names></name><name><surname>Hay</surname><given-names>G.</given-names></name></person-group>
<source>Object-Based Image Analysis: Spatial Concepts for Knowledge-Driven Remote Sensing Applications</source>
<publisher-name>Springer Science &#x00026; Business Media</publisher-name>
<publisher-loc>Berlin, Germany</publisher-loc>
<year>2008</year>
</element-citation></ref><ref id="B55-sensors-17-00228"><label>55.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Mountrakis</surname><given-names>G.</given-names></name><name><surname>Im</surname><given-names>J.</given-names></name><name><surname>Ogole</surname><given-names>C.</given-names></name></person-group>
<article-title>Support vector machines in remote sensing: A review</article-title>
<source>ISPRS J. Photogramm. Remote Sens.</source>
<year>2011</year>
<volume>66</volume>
<fpage>247</fpage>
<lpage>259</lpage>
<pub-id pub-id-type="doi">10.1016/j.isprsjprs.2010.11.001</pub-id>
</element-citation></ref><ref id="B56-sensors-17-00228"><label>56.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Baatz</surname><given-names>M.</given-names></name><name><surname>Benz</surname><given-names>U.</given-names></name><name><surname>Dehghani</surname><given-names>S.</given-names></name><name><surname>Heynen</surname><given-names>M.</given-names></name><name><surname>H&#x000f6;ltje</surname><given-names>A.</given-names></name><name><surname>Hofmann</surname><given-names>P.</given-names></name><name><surname>Lingenfelder</surname><given-names>I.</given-names></name><name><surname>Mimler</surname><given-names>M.</given-names></name><name><surname>Sohlbach</surname><given-names>M.</given-names></name><name><surname>Weber</surname><given-names>M.</given-names></name></person-group>
<source>eCognition User Guide</source>
<publisher-name>Definiens Imaging GmbH</publisher-name>
<publisher-loc>Munich, Germany</publisher-loc>
<year>2004</year>
</element-citation></ref></ref-list></back><floats-group><fig id="sensors-17-00228-f001" position="float"><label>Figure 1</label><caption><p>Study areas from the Canary Islands: (<bold>a</bold>) Teide National Park; (<bold>b</bold>) Corralejo and Islote de Lobos Natural Park; and (<bold>c</bold>) Maspalomas Natural Reserve.</p></caption><graphic xlink:href="sensors-17-00228-g001"/></fig><fig id="sensors-17-00228-f002" position="float"><label>Figure 2</label><caption><p>PAN and MS scenes of WorldView-2 images (512 &#x000d7; 512 pixels for the MS image): (<bold>a</bold>,<bold>d</bold>) shrub land ecosystem; (<bold>b</bold>,<bold>e</bold>) coastal ecosystem; (<bold>c</bold>,<bold>f</bold>) mixed ecosystem with urban area and inner water lagoon.</p></caption><graphic xlink:href="sensors-17-00228-g002"/></fig><fig id="sensors-17-00228-f003" position="float"><label>Figure 3</label><caption><p>The flow diagram followed in the study for each scenario.</p></caption><graphic xlink:href="sensors-17-00228-g003"/></fig><fig id="sensors-17-00228-f004" position="float"><label>Figure 4</label><caption><p>True color fused images of the shrubland region: (<bold>a</bold>) original MS; (<bold>b</bold>) FIHS; (<bold>c</bold>) HCS; (<bold>d</bold>) MTF_GLP_HPM; (<bold>e</bold>) WAT&#x02297;FRAC with a window size of seven.</p></caption><graphic xlink:href="sensors-17-00228-g004"/></fig><fig id="sensors-17-00228-f005" position="float"><label>Figure 5</label><caption><p>True color fused images of the coastal region: (<bold>a</bold>) original MS; (<bold>b</bold>) FIHS; (<bold>c</bold>) HCS; (<bold>d</bold>) MTF_GLP_HPM; (<bold>e</bold>) WAT&#x02297;FRAC with a window size of 27.</p></caption><graphic xlink:href="sensors-17-00228-g005"/></fig><fig id="sensors-17-00228-f006" position="float"><label>Figure 6</label><caption><p>True color fused images of the mixed ecosystem with an urban region: (<bold>a</bold>) original MS; (<bold>b</bold>) FIHS; (<bold>c</bold>) HCS; (<bold>d</bold>) MTF_GLP_HPM; (<bold>e</bold>) WAT&#x02297;FRAC with a window size of 15.</p></caption><graphic xlink:href="sensors-17-00228-g006"/></fig><fig id="sensors-17-00228-f007" position="float"><label>Figure 7</label><caption><p>False color fused images using Bands 8, 7 and 1 color composition (bands outside the PAN range): (<bold>a</bold>) original MS; (<bold>b</bold>) FIHS; (<bold>c</bold>) HCS; (<bold>d</bold>) MTF_GLP_HPM; (<bold>e</bold>) WAT&#x02297;FRAC.</p></caption><graphic xlink:href="sensors-17-00228-g007"/></fig><fig id="sensors-17-00228-f008" position="float"><label>Figure 8</label><caption><p>Results of the quality indices for the shrubland ecosystem fused image considering the three different MS band combinations (blue: total bands; red: Bands 2&#x02013;6; green: Bands 1, 7 and 8). X-axis: panharpening algorithms and Y-axis: range values of each quality indices (SAM: better value closer to 0; Spectral and Spatial ERGAS: better values closer to 0; FC: values between 0 and 1, better closer to 1; Zhou: values between 0 and 1, better closer to 1; Q8: values between 0 and 1, better closer to 1).</p></caption><graphic xlink:href="sensors-17-00228-g008"/></fig><fig id="sensors-17-00228-f009" position="float"><label>Figure 9</label><caption><p>Results of the quality indices for the coastal ecosystem fused image considering the three different MS band combinations (blue: total bands; red: Bands 2&#x02013;6; green: Bands 1, 7 and 8). X-axis: panharpening algorithms and Y-axis: range values of each quality indices (SAM: better value closer to 0; Spectral and Spatial ERGAS: better values closer to 0; FC: values between 0 and 1, better closer to 1; Zhou: values between 0 and 1, better closer to 1; Q8: values between 0 and 1, better closer to 1).</p></caption><graphic xlink:href="sensors-17-00228-g009"/></fig><fig id="sensors-17-00228-f010" position="float"><label>Figure 10</label><caption><p>Results of the quality indices for the mixed ecosystem fused image considering the three different MS band combinations (blue: total bands; red: Bands 2&#x02013;6; green: Bands 1, 7 and 8). X-axis: panharpening algorithms and Y-axis: range values of each quality indices (SAM: better value closer to 0; Spectral and Spatial ERGAS: better values closer to 0; FC: values between 0 and 1, better closer to 1; Zhou: values between 0 and 1, better closer to 1; Q8: values between 0 and 1, better closer to 1).</p></caption><graphic xlink:href="sensors-17-00228-g010"/></fig><fig id="sensors-17-00228-f011" position="float"><label>Figure 11</label><caption><p>Fused image with the WAT&#x02297;FRAC of the shrubland ecosystem and its quality maps for each band (scale from 0&#x02013;1, zero being less fusion quality and one the highest fusion quality).</p></caption><graphic xlink:href="sensors-17-00228-g011"/></fig><fig id="sensors-17-00228-f012" position="float"><label>Figure 12</label><caption><p>Fused image with FIHS of the coastal ecosystem and its quality maps for each band (scale from 0&#x02013;1, zero being less fusion quality and one the higher fusion quality).</p></caption><graphic xlink:href="sensors-17-00228-g012"/></fig><fig id="sensors-17-00228-f013" position="float"><label>Figure 13</label><caption><p>Fused image with WAT&#x02297;FRAC of the mixed ecosystem and its quality maps for each band. Scale from 0&#x02013;1, zero being less fusion quality and one the higher fusion quality.</p></caption><graphic xlink:href="sensors-17-00228-g013"/></fig><fig id="sensors-17-00228-f014" position="float"><label>Figure 14</label><caption><p>Zoom of the original MS (<bold>a</bold>) and WAT&#x02297;FRAC fused image (<bold>b</bold>) of the thematic maps obtained by the OBIA-SVM classifier applied to the multispectral image (<bold>c</bold>) and WAT&#x02297;FRAC fused image (<bold>d</bold>) in the shrubland ecosystem.</p></caption><graphic xlink:href="sensors-17-00228-g014"/></fig><table-wrap id="sensors-17-00228-t001" position="float"><object-id pub-id-type="pii">sensors-17-00228-t001_Table 1</object-id><label>Table 1</label><caption><p>WorldView-2 sensor technical specifications.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Imaging Mode</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Panchromatic</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Multispectral</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spatial Resolution</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.84 m</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Spectral Range</td><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">450&#x02013;800 nm</td><td align="center" valign="middle" rowspan="1" colspan="1">400&#x02013;450 nm (coastal)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">450&#x02013;510 nm (blue)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">510&#x02013;580 nm (green)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">585&#x02013;625 nm (yellow)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">630&#x02013;690 nm (red)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">705&#x02013;745 nm (red edge)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">770&#x02013;895 nm (near IR 1)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">860&#x02013;1040 nm (near IR 2)</td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-00228-t002" position="float"><object-id pub-id-type="pii">sensors-17-00228-t002_Table 2</object-id><label>Table 2</label><caption><p>Location and acquisition date of the three images selected from the Canary Islands.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Worldview-2 Image</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Coordinates</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Acquisition Date</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Teide National Park</td><td align="center" valign="middle" rowspan="1" colspan="1">28&#x000b0;18&#x02032;16&#x02033; N, 16&#x000b0;33&#x02032;50&#x02033; W</td><td align="center" valign="middle" rowspan="1" colspan="1">16 May 2011</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maspalomas Natural Reserve</td><td align="center" valign="middle" rowspan="1" colspan="1">27&#x000b0;44&#x02032;12&#x02033; N, 15&#x000b0;35&#x02032;52&#x02033; W</td><td align="center" valign="middle" rowspan="1" colspan="1">17 January 2013</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Corralejo and Islote de Lobos Natural Park</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28&#x000b0;43&#x02032;52&#x02033; N, 13&#x000b0;50&#x02032;37&#x02033; W</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28 October 2010</td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-00228-t003" position="float"><object-id pub-id-type="pii">sensors-17-00228-t003_Table 3</object-id><label>Table 3</label><caption><p>Indices for the quality assessment of the fused image.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Quality Indices</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Equation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Reference</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Equation</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spectral Angle Mapper</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>cos</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mi>M</mml:mi><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B22-sensors-17-00228" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">(1)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spectral ERGAS</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mfrac><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B23-sensors-17-00228" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">(2)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spatial ERGAS</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mfrac><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B24-sensors-17-00228" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">(3)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Frequency Comparison</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x000a0;</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>c</mml:mi><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>c</mml:mi><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B25-sensors-17-00228" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">(4)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Zhou</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B26-sensors-17-00228" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">(5)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:msub><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B27-sensors-17-00228" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6)</td></tr></tbody></table><table-wrap-foot><fn><p>Note: <italic>nband</italic> is the number of bands; <italic>FUS<sub>i</sub></italic> represents the fused image; <italic>MS<sub>i</sub></italic> is the <italic>i</italic>-th band of the MS image; <italic>PAN<sub>i</sub></italic> is the PAN image; <italic>h</italic> and <italic>l</italic> represent the spatial resolution of the PAN and MS images, respectively; <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>c</mml:mi><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the discrete cosine transform computed in blocks of <italic>nxn</italic> pixels, and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> defines the cross-correlation of the <italic>i</italic>-th band; <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the high pass filtered fused image, and <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the high pass filtered PAN image; &#x003c3; is the variance of the MS and FUS image.</p></fn></table-wrap-foot></table-wrap><table-wrap id="sensors-17-00228-t004" position="float"><object-id pub-id-type="pii">sensors-17-00228-t004_Table 4</object-id><label>Table 4</label><caption><p>Quality results for the complete WV-2 bands and the shrubland ecosystem (best results in bold). SAM: spectral angle mapper; FC: frequency comparison; Spec.: spectral; Spat.: spatial.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Spectral Quality</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Spatial Quality</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Global Quality</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Borda Count Rank</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SAM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral ERGAS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spatial ERGAS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zhou</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q8</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spat.</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>FIHS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">3.78</td><td align="center" valign="middle" rowspan="1" colspan="1">1.68</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90</td><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>HCS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>3.52</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" rowspan="1" colspan="1">0.67</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.93</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">14</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>7</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>MTF_GLP_HPM</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">3.87</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.33</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">0.81</td><td align="center" valign="middle" rowspan="1" colspan="1">0.71</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>WAT&#x02297;FRAC</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.82</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.86</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.89</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>17</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>8</bold></td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-00228-t005" position="float"><object-id pub-id-type="pii">sensors-17-00228-t005_Table 5</object-id><label>Table 5</label><caption><p>Quality results for the complete WV-2 bands and the coastal ecosystem (best results in bold). SAM: spectral angle mapper; FC: frequency comparison; Spec.: spectral; Spat.: spatial.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Spectral Quality</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Spatial Quality</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Global Quality</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Borda Count Rank</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SAM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral ERGAS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spatial ERGAS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zhou</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q8</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spat.</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>FIHS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">1.77</td><td align="center" valign="middle" rowspan="1" colspan="1">2.91</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>2.36</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.85</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.83</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>19</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>7</bold></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>HCS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">1.81</td><td align="center" valign="middle" rowspan="1" colspan="1">1.73</td><td align="center" valign="middle" rowspan="1" colspan="1">2.64</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">0.71</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>MTF_GLP_HPM</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.64</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.22</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">2.61</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98</td><td align="center" valign="middle" rowspan="1" colspan="1">17</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>18</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>WAT&#x02297;FRAC</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.88</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>7</bold></td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-00228-t006" position="float"><object-id pub-id-type="pii">sensors-17-00228-t006_Table 6</object-id><label>Table 6</label><caption><p>Quality results for the complete WV-2 bands and the mixed ecosystem (best results in bold). SAM: spectral angle mapper; FC: frequency comparison; Spec.: spectral; Spat.: spatial.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Spectral Quality</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Spatial Quality</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Global Quality</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Borda Count Rank</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SAM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral ERGAS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spatial ERGAS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zhou</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q8</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spec.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spat.</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>FIHS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">7.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.98</td><td align="center" valign="middle" rowspan="1" colspan="1">2.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>HCS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">5.66</td><td align="center" valign="middle" rowspan="1" colspan="1">1.73</td><td align="center" valign="middle" rowspan="1" colspan="1">2.23</td><td align="center" valign="middle" rowspan="1" colspan="1">0.80</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.96</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>MTF_GLP_HPM</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>5.62</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.72</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">2.23</td><td align="center" valign="middle" rowspan="1" colspan="1">0.81</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.96</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">17</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>8</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>WAT&#x02297;FRAC</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>2.05</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.93</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>0.98</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>18</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>8</bold></td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-00228-t007" position="float"><object-id pub-id-type="pii">sensors-17-00228-t007_Table 7</object-id><label>Table 7</label><caption><p>Quality results for Bands 1&#x02013;8 using the best algorithms for each scene. Best results are in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Q8, Block Size: 64</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Shrubland Ecosystem</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Coastal Ecosystem</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mixed Ecosystem</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q8 Value for WAT&#x02297;FRAC_w7</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q8 Value for FIHS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q8 value for WAT&#x02297;FRAC_15</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>B1 (Coastal Blue)</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.696</td><td align="center" valign="middle" rowspan="1" colspan="1">0.764</td><td align="center" valign="middle" rowspan="1" colspan="1">0.695</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>B2 (Blue)</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.736</td><td align="center" valign="middle" rowspan="1" colspan="1">0.889</td><td align="center" valign="middle" rowspan="1" colspan="1">0.842</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>B3 (Green)</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.878</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.936</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.905</bold></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>B4 (Yellow)</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.904</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.647</td><td align="center" valign="middle" rowspan="1" colspan="1">0.890</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>B5 (Red)</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.872</td><td align="center" valign="middle" rowspan="1" colspan="1">0.410</td><td align="center" valign="middle" rowspan="1" colspan="1">0.851</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>B6 (Red Edge)</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.897</td><td align="center" valign="middle" rowspan="1" colspan="1">0.395</td><td align="center" valign="middle" rowspan="1" colspan="1">0.857</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>B7 (NIR 1)</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">0.841</td><td align="center" valign="middle" rowspan="1" colspan="1">0.318</td><td align="center" valign="middle" rowspan="1" colspan="1">0.845</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>B8 (NIR 2)</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.881</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.239</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.832</td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-00228-t008" position="float"><object-id pub-id-type="pii">sensors-17-00228-t008_Table 8</object-id><label>Table 8</label><caption><p>Segmentation parameters used for the images and classification accuracy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Classification Techniques</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Support Vector Machine</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pansharpening Algorithms</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>MS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">80.61%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>FIHS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">83.72%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>HCS</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">82.72%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>MTF_GLP_HPM</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">83.18%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>WAT&#x02297;FRAC</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.39%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td></tr></tbody></table></table-wrap></floats-group></article>