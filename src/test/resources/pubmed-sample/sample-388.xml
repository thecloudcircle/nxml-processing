
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23840810</article-id><article-id pub-id-type="pmc">3695934</article-id><article-id pub-id-type="publisher-id">PONE-D-13-04135</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0068051</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Neuroscience</subject><subj-group><subject>Cognition</subject></subj-group></subj-group><subj-group><subject>Sensory Perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group><subj-group><subject>Sensory Systems</subject><subj-group><subject>Visual System</subject></subj-group></subj-group><subj-group><subject>Behavioral Neuroscience</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Medicine</subject><subj-group><subject>Mental Health</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group><subj-group><subject>Neurology</subject><subj-group><subject>Cognitive Neurology</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Ultra Rapid Object Categorization: Effects of Level, Animacy and Context</article-title><alt-title alt-title-type="running-head">Object Categorization: Level, Animacy and Context</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pra&#x000df;</surname><given-names>Maren</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Grimsen</surname><given-names>Cathleen</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>K&#x000f6;nig</surname><given-names>Martina</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Fahle</surname><given-names>Manfred</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<addr-line>Center for Cognitive Science, Human Neurobiology, Bremen University, Bremen, Germany</addr-line>
</aff><aff id="aff2">
<label>2</label>
<addr-line>The Henry Wellcome Laboratories for Vision Sciences, City University London, London, United Kingdom</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>National Institute of Mental Health, United States of America</addr-line>
</aff><author-notes><corresp id="cor1">* E-mail: <email>mprass@uni-bremen.de</email></corresp><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p>Conceived and designed the experiments: MP CG MK MF. Performed the experiments: MK. Analyzed the data: MP CG MK. Wrote the paper: MP CG MF.</p></fn></author-notes><pub-date pub-type="collection"><year>2013</year></pub-date><pub-date pub-type="epub"><day>28</day><month>6</month><year>2013</year></pub-date><volume>8</volume><issue>6</issue><elocation-id>e68051</elocation-id><history><date date-type="received"><day>25</day><month>1</month><year>2013</year></date><date date-type="accepted"><day>23</day><month>5</month><year>2013</year></date></history><permissions><copyright-statement>&#x000a9; 2013 Pra&#x000df; et al</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Pra&#x000df; et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>It is widely agreed that in object categorization bottom-up and top-down influences interact. How top-down processes affect categorization has been primarily investigated in isolation, with only one higher level process at a time being manipulated. Here, we investigate the combination of different top-down influences (by varying the level of category, the animacy and the background of the object) and their effect on rapid object categorization. Subjects participated in a two-alternative forced choice rapid categorization task, while we measured accuracy and reaction times. Subjects had to categorize objects on the superordinate, basic or subordinate level. Objects belonged to the category animal or vehicle and each object was presented on a gray, congruent (upright) or incongruent (inverted) background. The results show that each top-down manipulation impacts object categorization and that they interact strongly. The best categorization was achieved on the superordinate level, providing no advantage for basic level in rapid categorization. Categorization between vehicles was faster than between animals on the basic level and vice versa on the subordinate level. Objects in homogenous gray background (context) yielded better overall performance than objects embedded in complex scenes, an effect most prominent on the subordinate level. An inverted background had no negative effect on object categorization compared to upright scenes. These results show how different top-down manipulations, such as category level, category type and background information, are related. We discuss the implications of top-down interactions on the interpretation of categorization results.</p></abstract><funding-group><funding-statement>The authors have no support or funding to report.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>In object categorization objects are classified corresponding to common characteristics, a process which depends both on perceptual and higher cognitive processing stages. The perceptual part of object categorization includes early sensory processing (contrast detection, contour integration, color processing and segmentation from background), and complex visual processing, like object detection and coarse classification to certain categories (e.g. faces, objects, scenes). Cognitive aspects of object processing are associated with semantic interpretation, classification, memorization and lexical processing of an object. Top-down influences, such as expectation, attention, context and expertise, are believed to facilitate object recognition.</p><p>The aim of investigating perceptual object categorization is to understand how objects and scenes are represented in early and higher visual cortices. It is known that the visual system is extremely fast and accurate in recognizing complex natural scenes and objects <xref rid="pone.0068051-Thorpe1" ref-type="bibr">[1]</xref>, <xref rid="pone.0068051-Delorme1" ref-type="bibr">[2]</xref>. It is assumed that a first sweep of feedforward information is sufficient to discriminate whether or not an object is present in a scene <xref rid="pone.0068051-VanRullen1" ref-type="bibr">[3]</xref>, <xref rid="pone.0068051-Serre1" ref-type="bibr">[4]</xref> and that top-down processes can speed up response times <xref rid="pone.0068051-Delorme1" ref-type="bibr">[2]</xref>, <xref rid="pone.0068051-Fenske1" ref-type="bibr">[5]</xref>, <xref rid="pone.0068051-Gazzaley1" ref-type="bibr">[6]</xref>. Key visual features of an object (such as eyes, mouth and limbs of an animal) are crucial, while color information plays a minor role in rapid categorization <xref rid="pone.0068051-Delorme2" ref-type="bibr">[7]</xref>. Studies have claimed that the rapid detection of an object in a scene might be pre-attentive <xref rid="pone.0068051-Li1" ref-type="bibr">[8]</xref>&#x02013;<xref rid="pone.0068051-Peelen1" ref-type="bibr">[10]</xref>, but this view has been challenged recently, showing that scene perception requires attention <xref rid="pone.0068051-Cohen1" ref-type="bibr">[11]</xref>. If demands on object categorization are more complex, such as identification of an object, higher stages of cognitive processing are required <xref rid="pone.0068051-Kiefer1" ref-type="bibr">[12]</xref>&#x02013;<xref rid="pone.0068051-Harel1" ref-type="bibr">[15]</xref>. These processes are beyond pure object detection and include a more detailed analysis of the object and its semantic interpretation. Aspects influencing object categorization in a top-down manner are, among others, spatial and feature-based attention, the likelihood of an object being present, expertise, the level of abstraction and, thus, the amount of information necessary to analyze the object, the category of an object, e.g. living or non-living, and the contextual information an object co-occurs with. This study combines perceptual categorization processes (rapid visual object categorization) and the influence of top-down processes (here: level of abstraction, animacy and contextual background) that potentially impact on object categorization.</p><sec id="s1a"><title>Levels of Abstraction</title><p>The same object can be categorized at different levels of abstraction, for example at a general, superordinate (e.g. animal), a basic (e.g. cat) or a subordinate (e.g. Siamese cat) level <xref rid="pone.0068051-Rosch1" ref-type="bibr">[16]</xref>. As the object itself is always the same, the effect of level is clearly not a perceptual one, but rather a cognitive process where information is evaluated corresponding to task demands. It is a matter of discussion what the entry level of categorization is. Some studies state an advantage for the basic level (e.g. &#x0201c;cat&#x0201d;) over both the more general category &#x0201c;animal&#x0201d; (superordinate level) and the more precise category &#x0201c;Siamese cat&#x0201d; (subordinate level) <xref rid="pone.0068051-Rosch1" ref-type="bibr">[16]</xref>&#x02013;<xref rid="pone.0068051-Rogers1" ref-type="bibr">[18]</xref>. It is assumed that the basic level is processed before the superordinate and subordinate level. Other studies found evidence against the basic level advantage <xref rid="pone.0068051-Mac1" ref-type="bibr">[19]</xref>&#x02013;<xref rid="pone.0068051-Loschky1" ref-type="bibr">[21]</xref>. It has been suggested that the advantage for basic level scenes diminishes if stimulus processing time is limited <xref rid="pone.0068051-Rogers1" ref-type="bibr">[18]</xref>, <xref rid="pone.0068051-Mac1" ref-type="bibr">[19]</xref>, <xref rid="pone.0068051-Loschky1" ref-type="bibr">[21]</xref>. In a go/no-go paradigm subjects responded faster and more accurate to the category &#x0201c;animal&#x0201d; than to the basic level category &#x0201c;bird&#x0201d; or &#x0201c;dog&#x0201d; <xref rid="pone.0068051-Mac1" ref-type="bibr">[19]</xref>. According to a coarse-to-fine account, coarse information is important for global image features and fine information for local image features. Thus, the more specific an object is categorized, the finer grained the perceptual information about that object should be <xref rid="pone.0068051-Collin1" ref-type="bibr">[22]</xref>, <xref rid="pone.0068051-Goffaux1" ref-type="bibr">[23]</xref>. This hypothesis argues against an advantage for the basic level because for basic level categorization finer perceptual information is needed than for superordinate categorization. It remains elusive under what circumstances the basic-level advantage occurs. The question arises whether the loss of the basic level advantage is a consequence of rapid categorization processes and whether it can be replicated in other test situations using rapid categorization tasks. We tested the occurrence of the basic-level advantage in a two alternative forced choice (2-AFC) task. Thus, for each stimulus presentation a response was given which allowed for the categories to be compared directly. Categories were more similar than in the study of Mac&#x000e9; et al. to assess the effect of level under more controlled stimulus conditions. Based on the results of Mac&#x000e9; and colleagues we hypothesized a behavioral advantage for the superordinate over the basic level, probably due to limited processing time in rapid categorization.</p></sec><sec id="s1b"><title>Animate vs. Inanimate categories</title><p>Objects can also be classified into living and non-living categories. Different processing mechanisms for animate and inanimate object categories have been suggested by behavioral, functional and neuropsychological studies <xref rid="pone.0068051-McMullen1" ref-type="bibr">[24]</xref>&#x02013;<xref rid="pone.0068051-Mahon2" ref-type="bibr">[26]</xref>. Animate/inanimate categories were found to engage different neural subsystems in the brain <xref rid="pone.0068051-Chao1" ref-type="bibr">[13]</xref>, <xref rid="pone.0068051-Gerlach1" ref-type="bibr">[27]</xref>, <xref rid="pone.0068051-Martin1" ref-type="bibr">[28]</xref>. This may lead to faster and more accurate responses for either an animate or an inanimate category <xref rid="pone.0068051-New1" ref-type="bibr">[29]</xref>&#x02013;<xref rid="pone.0068051-Crouzet2" ref-type="bibr">[32]</xref>. These animacy effects might be derived from perceptual differences between object categories. It has been shown, for example, that animate categories depend on different spatial frequencies than categorization of non-animals <xref rid="pone.0068051-Viggiano1" ref-type="bibr">[33]</xref>, <xref rid="pone.0068051-Harel2" ref-type="bibr">[34]</xref>. Additionally, cognitive processes, like different functionality and specialization of the object, may account for animacy effects, (e.g. tools automatically recruit action related circuits; <xref rid="pone.0068051-Mahon1" ref-type="bibr">[14]</xref>).</p><p>How animate and inanimate objects affect behavior and whether a behavioral advantage for either category exists is still an open question. Here we use animate and non-manipulable inanimate categories at different levels of abstraction in order to investigate whether animate categories yield better performance than non-manipulable inanimate categories. This suggestion is based on the <italic>sensory/functional account theory</italic>, which assumes that objects are represented according to their information content and functionality <xref rid="pone.0068051-Warrington1" ref-type="bibr">[35]</xref>, <xref rid="pone.0068051-Humphreys1" ref-type="bibr">[36]</xref>. Following this theory, manipulable artifacts, such as tools, have a behavioral benefit over animate categories, because faster, action related neural circuits are engaged during processing. On the other hand, processing of non-manipulable artifacts, such as vehicles, depend more on perceptual than on functional properties. This would lower the behavioral benefit and lead to similar or even worse performance, compared to animate categories.</p><p>It is unknown whether animate or inanimate objects are categorized differently at different levels of abstraction. We therefore explored the effect of animate and inanimate categories at the superordinate, basic and subordinate level. It was hypothesized that a similar animacy effect should occur at each level of abstraction.</p></sec><sec id="s1c"><title>Background</title><p>The context in which an object occurs may be crucial for its recognition. Objects usually co-occur with a contextual frame which is perceptually and semantically congruent with the object. A bird belongs to a tree, a car on a street and a computer on a desk. Semantically congruent contextual associations have been suggested to facilitate both bottom-up and top-down object recognition <xref rid="pone.0068051-Fenske1" ref-type="bibr">[5]</xref>, <xref rid="pone.0068051-Bar1" ref-type="bibr">[37]</xref>&#x02013;<xref rid="pone.0068051-Kveraga1" ref-type="bibr">[39]</xref>. Furthermore, in rapid categorization subjects are better and faster detecting objects in a semantically congruent context <xref rid="pone.0068051-Crouzet2" ref-type="bibr">[32]</xref>, <xref rid="pone.0068051-Joubert1" ref-type="bibr">[40]</xref>. Another factor may be the physical relationship between an object and its background. That is, object-background proportions such as size, position and orientation may influence object categorization <xref rid="pone.0068051-Biederman1" ref-type="bibr">[41]</xref>&#x02013;<xref rid="pone.0068051-Rieger1" ref-type="bibr">[43]</xref>. It remains an open question whether or not such perceptually (physically) incongruent background information affect rapid object categorization processes and whether such physical manipulations differ from semantically manipulations. In this study the background information was varied in terms of orientation (upright vs. inverted context), while keeping the level and category constant. This allows for the investigation of background manipulations while controlling for object types and task demands. A &#x0201c;no background&#x0201d; (gray background) condition was used to compare categorization performance between isolated objects and objects embedded in a complex natural scene background. An inverted scene is believed to hamper processing of the object categorization, thus leading to higher error rates and slower reaction times.</p><p>Several studies have shown that the level, animacy and background of objects influence categorization processes, but the effects and interactions of these higher cognitive processes on perceptual categorization remain unclear. The experiments of the current study address the question of how perceptual and cognitive task manipulations influence rapid categorization. The study was driven by two main motivations: investigating (1) aspects of perceptual stages of object processing (bottom-up mechanisms) using rapid object categorization and speeded response times and (2) top-down mechanisms using manipulations of category level, animacy and background. The brief presentation time of stimuli (30 msec) was chosen to keep high-level factors known to affect object categorization, such as expectations, intentions and expertise <xref rid="pone.0068051-Harel1" ref-type="bibr">[15]</xref>, <xref rid="pone.0068051-Johnson1" ref-type="bibr">[44]</xref>&#x02013;<xref rid="pone.0068051-Viggiano2" ref-type="bibr">[46]</xref>, to a minimum. An image, belonging to one of two given categories, was presented briefly and responses were given as quickly as possible via a button press in a two-alternative forced choice. Objects could either belong to a living (animal) or a non-living (vehicle) category and each object was presented on three different backgrounds (gray, upright and inverted). Upright and inverted backgrounds consisted of complex natural scenes and were semantically congruent with the given object. The task was to categorize objects at the superordinate, basic and subordinate level of abstraction in separate runs (e.g. animal, dog and St. Bernard, respectively). Importantly, no verbal response was necessary, in order to exclude lexical processes.</p></sec></sec><sec sec-type="materials|methods" id="s2"><title>Materials and Methods</title><sec id="s2a"><title>Ethics Statement</title><p>The study was approved by the local ethics committee of the University of Bremen (IACUC permit numbers and IRB name: n/a) and is in accordance with the Declaration of Helsinki. All subjects gave informed written consent.</p></sec><sec id="s2b"><title>Participants</title><p>Sixteen healthy participants (8 male, 8 female) volunteered for the study. The age ranged between 18&#x02013;31 years (mean: 25&#x000b1;4 years). Subjects had normal or corrected-to-normal visual acuity and no ophthalmological or neurological disorders (self-report). They were informed about the study and received course credits for participation. Subjects were non-experts but familiar with the given categories.</p></sec><sec id="s2c"><title>Experimental Design and Procedure</title><p>Three different category-levels where examined, namely the superordinate, basic and subordinate levels (<xref ref-type="fig" rid="pone-0068051-g001">Figure 1</xref>). The superordinate level contained the categories animal vs. vehicle. The basic level included the categories dog vs. cat and car vs. bus, while the subordinate level included the categories German Shepherd vs. St. Bernard, Siamese cat vs. Persian cat, estate car vs. Jeep and overland bus vs. city bus. Each pair of categories was presented in a single run, resulting in seven independent runs. In each run 180 trials were presented, including two categories with 30 objects; each object was presented on three different backgrounds (gray, upright natural scene and inverted natural scene, <xref ref-type="fig" rid="pone-0068051-g001">Figure 1a</xref>). The sequence of stimulus presentations was counterbalanced throughout each run.</p><fig id="pone-0068051-g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.g001</object-id><label>Figure 1</label><caption><title>Illustration of stimuli and experimental design.</title><p>(A) Two example objects on different context conditions. (B) Trial sequence. (C) Schematic of tested categories on each level of categorization. The arrow indicates pairs of categories that were tested against each other.</p></caption><graphic xlink:href="pone.0068051.g001"/></fig><p>A trial started with a fixation time of 1000 msec (&#x000b1;500 msec jitter), followed by stimulus presentation for 30 msec and an unlimited response time (<xref ref-type="fig" rid="pone-0068051-g001">Figure 1b</xref>). The stimuli were presented centrally with a size of 9.4&#x000b0; of visual angle. The trial ended with a 500 msec break after the subject&#x000b4;s response and before a new trial started. The screen was dark and a red fixation point (12 arcmin) was on the screen at all times. Participants had to indicate to which of the two categories the presented stimulus belonged through button press, irrespective of the background the object was presented on (two-alternative forced choice task). Response buttons were held in the left and right hand. Participants were instructed to respond as quickly and as accurately as possible and an auditory feedback was applied after false responses.</p></sec><sec id="s2d"><title>Stimuli and Apparatus</title><p>Stimuli consisted of black and white photographs of natural scenes, which were taken from an image hosting website (flickr.com) and which had no copyright restrictions. The target objects were cropped out and pasted on a gray, an upright and an inverted natural scene background (on the same position). Edges of objects were smoothed to reduce pasting effects. All target objects had roughly the same size (120 pixels) and covered one third of the total image (350&#x000d7;350 pixels). For the subordinate level, each category consisted of 30 different objects and each object was pasted on three different backgrounds (gray, upright natural scene and inverted natural scene), resulting in 90 images for each category. In total, 720 images were created for the eight categories of the subordinate level. Both, superordinate and basic level image sets were composed of the subordinate image set. For the basic level category &#x0201c;dog&#x0201d;, half of the images derived from the German Shepherd set and the other half from the St. Bernard set. The other three basic level categories derived from their corresponding subordinate categories likewise. The image sets of the superordinate level consisted of equal portions of the appropriate subordinate image sets. The categories German Shepherd, St. Bernard, Siamese cat and Persian cat composed the superordinate category &#x0201c;animal&#x0201d; and the categories estate car, Jeep, overland bus and city bus composed the category &#x0201c;vehicle&#x0201d;. With respect to a balanced composition of images for basic and superordinate level sets, different images were taken from the original subordinate set. This was applied to avoid memory effects due to repetitive presentations of identical images. Participants were divided into two groups, which were presented with different image sets of the basic and superordinate level. Each participant was presented with all subordinate level images.</p><p>Stimuli were presented in a dark room on a 20&#x0201d; CRT monitor (SAMSUNG Syncmaster 1100 MB; refresh rate 100 Hz and 1280&#x000d7;1024-pixel resolution) at a viewing distance of 60 cm, which was sustained by a chinrest. The stimulus presentation was conducted with an in-house software on a standard PC.</p></sec><sec id="s2e"><title>Analysis</title><p>Reaction times (RT) and the proportion of correct responses (%-correct) were calculated. Statistical analysis was performed by using PASW statistics 18 (version 18.0.0). To analyze the obtained results repeated measures Analysis of Variance (ANOVA), post hoc pairwise comparisons and paired t-tests were conducted. In case sphericity was violated in the repeated measures ANOVA, a Greenhouse-Geisser correction was applied. Error bars of graphs represent normalized confidence intervals according to an approach by Cousineau (2005) and Morey (2008) <xref rid="pone.0068051-Cousineau1" ref-type="bibr">[47]</xref>, <xref rid="pone.0068051-Morey1" ref-type="bibr">[48]</xref>.</p></sec></sec><sec id="s3"><title>Results</title><p>This study investigated the influence of different a) levels of abstraction, b) animacy and c) background on ultra rapid object categorization. The percentages of correct responses as well as the reaction times were analyzed with a 3 (level: superordinate, basic, subordinate)&#x000d7;2 (animacy: animal, vehicle)&#x000d7;3 (background: gray, upright, inverted) repeated-measures Analysis of Variance (ANOVA). Results are shown in <xref ref-type="table" rid="pone-0068051-t001">Table 1</xref>.</p><table-wrap id="pone-0068051-t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.t001</object-id><label>Table 1</label><caption><title>Mean reaction times (ms) and %-correct for categorization of different levels, animacy and context.</title></caption><alternatives><graphic id="pone-0068051-t001-1" xlink:href="pone.0068051.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Level</td><td align="left" rowspan="1" colspan="1">Animacy</td><td align="left" rowspan="1" colspan="1">Context</td><td colspan="2" align="left" rowspan="1">Reaction time</td><td colspan="2" align="left" rowspan="1">%-correct</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Mean</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">Mean</td><td align="left" rowspan="1" colspan="1">SE</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">superordinate</td><td align="left" rowspan="1" colspan="1">animal</td><td align="left" rowspan="1" colspan="1">gray</td><td align="left" rowspan="1" colspan="1">459</td><td align="left" rowspan="1" colspan="1">49</td><td align="left" rowspan="1" colspan="1">94.2</td><td align="left" rowspan="1" colspan="1">6.3</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">upright</td><td align="left" rowspan="1" colspan="1">455</td><td align="left" rowspan="1" colspan="1">58</td><td align="left" rowspan="1" colspan="1">91.5</td><td align="left" rowspan="1" colspan="1">5.8</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">inverted</td><td align="left" rowspan="1" colspan="1">487</td><td align="left" rowspan="1" colspan="1">41</td><td align="left" rowspan="1" colspan="1">94.8</td><td align="left" rowspan="1" colspan="1">3.2</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">vehicle</td><td align="left" rowspan="1" colspan="1">gray</td><td align="left" rowspan="1" colspan="1">469</td><td align="left" rowspan="1" colspan="1">61</td><td align="left" rowspan="1" colspan="1">92.5</td><td align="left" rowspan="1" colspan="1">7.3</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">upright</td><td align="left" rowspan="1" colspan="1">488</td><td align="left" rowspan="1" colspan="1">57</td><td align="left" rowspan="1" colspan="1">95.8</td><td align="left" rowspan="1" colspan="1">4.5</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">inverted</td><td align="left" rowspan="1" colspan="1">473</td><td align="left" rowspan="1" colspan="1">59</td><td align="left" rowspan="1" colspan="1">94.2</td><td align="left" rowspan="1" colspan="1">4.3</td></tr><tr><td align="left" rowspan="1" colspan="1">basic</td><td align="left" rowspan="1" colspan="1">animal</td><td align="left" rowspan="1" colspan="1">gray</td><td align="left" rowspan="1" colspan="1">519</td><td align="left" rowspan="1" colspan="1">40</td><td align="left" rowspan="1" colspan="1">90.8</td><td align="left" rowspan="1" colspan="1">7.3</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">upright</td><td align="left" rowspan="1" colspan="1">465</td><td align="left" rowspan="1" colspan="1">39</td><td align="left" rowspan="1" colspan="1">89.6</td><td align="left" rowspan="1" colspan="1">3.9</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">inverted</td><td align="left" rowspan="1" colspan="1">546</td><td align="left" rowspan="1" colspan="1">47</td><td align="left" rowspan="1" colspan="1">87.8</td><td align="left" rowspan="1" colspan="1">6.1</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">vehicle</td><td align="left" rowspan="1" colspan="1">gray</td><td align="left" rowspan="1" colspan="1">479</td><td align="left" rowspan="1" colspan="1">35</td><td align="left" rowspan="1" colspan="1">94.3</td><td align="left" rowspan="1" colspan="1">4.6</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">upright</td><td align="left" rowspan="1" colspan="1">544</td><td align="left" rowspan="1" colspan="1">45</td><td align="left" rowspan="1" colspan="1">93.5</td><td align="left" rowspan="1" colspan="1">4.5</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">inverted</td><td align="left" rowspan="1" colspan="1">481</td><td align="left" rowspan="1" colspan="1">40</td><td align="left" rowspan="1" colspan="1">94.3</td><td align="left" rowspan="1" colspan="1">4.2</td></tr><tr><td align="left" rowspan="1" colspan="1">subordinate</td><td align="left" rowspan="1" colspan="1">animal</td><td align="left" rowspan="1" colspan="1">gray</td><td align="left" rowspan="1" colspan="1">539</td><td align="left" rowspan="1" colspan="1">52</td><td align="left" rowspan="1" colspan="1">88.8</td><td align="left" rowspan="1" colspan="1">6.2</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">upright</td><td align="left" rowspan="1" colspan="1">559</td><td align="left" rowspan="1" colspan="1">41</td><td align="left" rowspan="1" colspan="1">86.6</td><td align="left" rowspan="1" colspan="1">5.9</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">inverted</td><td align="left" rowspan="1" colspan="1">564</td><td align="left" rowspan="1" colspan="1">43</td><td align="left" rowspan="1" colspan="1">85.8</td><td align="left" rowspan="1" colspan="1">5.2</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">vehicle</td><td align="left" rowspan="1" colspan="1">gray</td><td align="left" rowspan="1" colspan="1">571</td><td align="left" rowspan="1" colspan="1">47</td><td align="left" rowspan="1" colspan="1">85.8</td><td align="left" rowspan="1" colspan="1">3.8</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">upright</td><td align="left" rowspan="1" colspan="1">561</td><td align="left" rowspan="1" colspan="1">53</td><td align="left" rowspan="1" colspan="1">80.9</td><td align="left" rowspan="1" colspan="1">5.2</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">inverted</td><td align="left" rowspan="1" colspan="1">568</td><td align="left" rowspan="1" colspan="1">42</td><td align="left" rowspan="1" colspan="1">80.2</td><td align="left" rowspan="1" colspan="1">4.7</td></tr></tbody></table></alternatives></table-wrap><sec id="s3a"><title>Main Effects of Top-down Manipulations</title><sec id="s3a1"><title>Accuracy</title><p>Subjects showed different accuracies at the superordinate level, the basic level and the subordinate level (F<sub>(2,30)</sub>&#x0200a;=&#x0200a;98.8, p&#x0003c;0.001; <xref ref-type="fig" rid="pone-0068051-g002">Figure 2a</xref>). Performance was best at the superordinate level and was decreased at the basic (p&#x02264;0.05) and subordinate levels (p&#x0003c;0.001). Accuracy was higher at the basic level than at the subordinate level (p&#x0003c;0.001). No significant difference was found for animal and vehicle categorization (F<sub>(1, 15)</sub>&#x0200a;=&#x0200a;.1, p&#x0003e;.05; <xref ref-type="fig" rid="pone-0068051-g003">Figure 3a</xref>). Subjects were much better at categorizing objects on a gray background than objects on complex natural scenes (p&#x02264;.05; <xref ref-type="fig" rid="pone-0068051-g004">Figure 4a</xref>). No significant difference was obtained between the &#x0201c;upright background&#x0201d; and &#x0201c;inverted background&#x0201d; conditions (p&#x0003e;.05). Thus, a gray background facilitated object categorization but manipulating the orientation of a complex background had no influence on the categorization performance (F<sub>(2, 30)</sub>&#x0200a;=&#x0200a;6.6; p&#x0200a;=&#x0200a;0.004).</p><fig id="pone-0068051-g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.g002</object-id><label>Figure 2</label><caption><title>Influence of &#x0201c;level&#x0201d; on ultra-rapid object categorization.</title><p>For each level (superordinate, basic and subordinate) the mean performance (A) and reaction times (B) are shown. Error bars represent the normalized 95% confidence intervals of the mean (Cousineau-Morey approach <xref rid="pone.0068051-Cousineau1" ref-type="bibr">[47]</xref>, <xref rid="pone.0068051-Morey1" ref-type="bibr">[48]</xref>). *p&#x0003c;.05 **p&#x02264;.01 ***p&#x02264;.001.</p></caption><graphic xlink:href="pone.0068051.g002"/></fig><fig id="pone-0068051-g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.g003</object-id><label>Figure 3</label><caption><title>Influence of &#x0201c;animacy&#x0201d; on ultra-rapid object categorization.</title><p>Performance (A) and reaction times (B) for animal and vehicle category are shown. Error bars represent the normalized 95% confidence intervals of the mean (Cousineau-Morey approach <xref rid="pone.0068051-Cousineau1" ref-type="bibr">[47]</xref>, <xref rid="pone.0068051-Morey1" ref-type="bibr">[48]</xref>). *p&#x0003c;.05, **p&#x02264;.01, ***p&#x02264;.001.</p></caption><graphic xlink:href="pone.0068051.g003"/></fig><fig id="pone-0068051-g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.g004</object-id><label>Figure 4</label><caption><title>Effect of context on object categorization.</title><p>The responses (percentage correct (A) and reaction times (B)) are shown for different context conditions. Objects were embedded in either a gray, upright or inverted context. Error bars represent the normalized 95% confidence intervals of the mean (Cousineau-Morey approach <xref rid="pone.0068051-Cousineau1" ref-type="bibr">[47]</xref>, <xref rid="pone.0068051-Morey1" ref-type="bibr">[48]</xref>). *p&#x0003c;.05, **p&#x02264;.01 ***, p&#x02264;.001.</p></caption><graphic xlink:href="pone.0068051.g004"/></fig></sec><sec id="s3a2"><title>Reaction Times</title><p>Reaction times were different between superordinate, basic and subordinate level (F<sub>(1,21)</sub>&#x0200a;=&#x0200a;68.8, p&#x0003c;0.001; due to violation of sphericity Greenhouse-Geisser corrected; <xref ref-type="fig" rid="pone-0068051-g002">Figure 2b</xref>). Object categorization at the superordinate level was significantly faster than at the basic (p&#x02264;0.05) and subordinate (p&#x0003c;0.001) levels. Reaction times were faster at the basic level than at the subordinate level (p&#x0003c;0.001). Thus, no basic level advantage was observed. Animate categories were processed slower than inanimate categories (<xref ref-type="fig" rid="pone-0068051-g003">Figure 3b</xref>). Reaction times for animals (523&#x000b1;41ms) were significantly slower than the reaction times for vehicles (502&#x000b1;41ms; F<sub>(1,15)</sub>&#x0200a;=&#x0200a;14.2, p&#x0003c;.01). These results suggest that processing speed, as measured by reaction time, is influenced by animacy, but performance is not. The background had an effect on categorization (F<sub>(2,30)</sub>&#x0200a;=&#x0200a;39.3; p&#x0003c;0.001): Objects on a gray background were categorized faster than objects embedded in scenes (p&#x02264;.05). No difference was found between upright and inverted scenes (p&#x0003e;.05). This demonstrates that a lack of a complex background facilitates object recognition (<xref ref-type="fig" rid="pone-0068051-g004">Figure 4b</xref>).</p></sec></sec><sec id="s3b"><title>Interaction of Level and Animacy</title><sec id="s3b1"><title>Accuracy</title><p>There was a significant interaction between level and animacy (F<sub>(2, 30)</sub>&#x0200a;=&#x0200a;20.6; p&#x0003c;0.001). Animals and vehicles were thus compared at each category level, showing that at the basic level subjects showed higher accuracy for vehicles than for animals (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;3.9, p&#x02264;0.001). At the subordinate level higher accuracy was observed for the animal category (t<sub>(15)</sub>&#x0200a;=&#x0200a;4.4, p&#x0003c;0.001) and at the superordinate level, no significant difference was obtained for animal and vehicle categories (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;.9, p&#x02265;.05). These results indicate that the categories used in this study reveal different performance patterns at different category levels (significant results shown in <xref ref-type="fig" rid="pone-0068051-g005">Figure 5</xref>, vertical comparison).</p><fig id="pone-0068051-g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.g005</object-id><label>Figure 5</label><caption><title>Interaction of level of categorization and different categories (animal and vehicle).</title><p>Performance (A) and reaction times (B) for animal and vehicle category on each level of categorization are shown. Error bars represent the normalized 95% confidence intervals of the mean (Cousineau-Morey approach <xref rid="pone.0068051-Cousineau1" ref-type="bibr">[47]</xref>, <xref rid="pone.0068051-Morey1" ref-type="bibr">[48]</xref>). *p&#x0003c;.05, **p&#x02264;.01, ***p&#x02264;.001.</p></caption><graphic xlink:href="pone.0068051.g005"/></fig><p>We compared the category levels within each category (horizontal comparisons in <xref ref-type="fig" rid="pone-0068051-g005">Figure 5</xref>). Subjects showed highest accuracy for animals at the superordinate level but performance was significantly lower at the basic (t<sub>(15)</sub>&#x0200a;=&#x0200a;3.1, p&#x0003c;.01) and subordinate (t<sub>(15)</sub>&#x0200a;=&#x0200a;6.1, p&#x0003c;.001) level, both to a similar degree. For the vehicle category, we observed a different response pattern. Here, subjects obtained the same accuracy at the superordinate and basic level (t<sub>(15)</sub>&#x0200a;=&#x0200a;.3, &#x0003e;.05) and accuracy was decreased at the subordinate level compared to the superordinate (t<sub>(15)</sub>&#x0200a;=&#x0200a;15.6, p&#x0003c;001) and basic levels (t<sub>(15)</sub>&#x0200a;=&#x0200a;15.8, p&#x0003c;001).</p></sec><sec id="s3b2"><title>Reaction times</title><p>Subjects responded with the same speed to animals and vehicles at the superordinate (t<sub>(15)</sub>&#x0200a;=&#x0200a;1.9, p&#x0200a;=&#x0200a;&#x02265;.05) and subordinate (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;1.3, p&#x02265;.05) level. However, at the basic level they were significantly faster for vehicles than animals (t<sub>(15)</sub>&#x0200a;=&#x0200a;8.6, p&#x0003c;0.001; F<sub>(2, 30)</sub>&#x0200a;=&#x0200a;20.6; p&#x0003c;0.001). This interaction reveals that the benefit for vehicles is completely driven by the basic level.</p><p>Within the category &#x0201c;animal&#x0201d;, subjects showed the fastest reaction times at the superordinate level, as compared to the basic (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;7.6, p&#x0003c;.001) and subordinate (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;7.2, p&#x0003c;.001) level. Subjects responded significantly slower for animals on the subordinate level than on the basic level (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;2.7, p&#x0200a;=&#x0200a;.015). Within the category &#x0201c;vehicle&#x0201d; subjects showed no difference in reaction times on the superordinate and basic level (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;.9, p&#x0003e;.05), but reaction times were significantly decreased at the subordinate level compared to the superordinate (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;8.7, p&#x0003c;001) and basic level (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;18.3, p&#x0003c;001).</p><p>These results suggest that the living and non-living categories tested in this study, yield different performance patterns across the levels. Especially at the basic level, subjects showed a behavioral advantage for vehicles over animals.</p></sec></sec><sec id="s3c"><title>Interaction of Level and Background</title><sec id="s3c1"><title>Accuracy</title><p>Subjects were better at categorizing objects on a gray background than objects on a natural upright (t<sub>(15)</sub>&#x0200a;=&#x0200a;5.1, p&#x0003c;.001) or inverted (t<sub>(15)</sub>&#x0200a;=&#x0200a;7.3, p&#x0003c;.001) background only at the subordinate level, as revealed by a significant interaction between level and context for percentage correct (F<sub>(2, 33)</sub>&#x0200a;=&#x0200a;5.2, p&#x0200a;=&#x0200a;.01; Greenhouse-Geisser corrected; <xref ref-type="fig" rid="pone-0068051-g006">Figure 6</xref>). This is at odds with the reaction time data, where responses were faster for objects on gray backgrounds at each level (see section &#x0201c;Main effects of top-down manipulations&#x0201d;).</p><fig id="pone-0068051-g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.g006</object-id><label>Figure 6</label><caption><title>Interaction of level and context.</title><p>Higher accuracy was obtained for the gray context condition at the subordinate level. Error bars represent the normalized 95% confidence intervals of the mean (Cousineau-Morey approach <xref rid="pone.0068051-Cousineau1" ref-type="bibr">[47]</xref>, <xref rid="pone.0068051-Morey1" ref-type="bibr">[48]</xref>). *p&#x0003c;.05, **p&#x02264;.01, ***p&#x02264;.001.</p></caption><graphic xlink:href="pone.0068051.g006"/></fig></sec></sec><sec id="s3d"><title>Interaction of Animacy and Background</title><sec id="s3d1"><title>Reaction times</title><p>Reaction times were significantly slower for animate than vehicle categories in all background conditions (gray: t<sub>(15)</sub>&#x0200a;=&#x0200a;2.2, p&#x0200a;=&#x0200a;.047; upright: t<sub>(15)</sub>&#x0200a;=&#x0200a;4.3, p&#x0200a;=&#x0200a;.001; inverted: t<sub>(15)</sub>&#x0200a;=&#x0200a;3.9, p&#x0200a;=&#x0200a;.001), but subjects showed even slower response times when animals were on complex backgrounds (F<sub>(2, 30)</sub>&#x0200a;=&#x0200a;6.4, p&#x0200a;=&#x0200a;.005). This was evaluated by calculating the difference of reaction times between animate and vehicle categories for each background condition. The difference between animate and vehicle conditions on gray backgrounds was smaller than for upright backgrounds (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;3.5, p&#x0200a;=&#x0200a;.003) and inverted backgrounds (t<sub>(15)</sub>&#x0200a;=&#x0200a;&#x02212;2.9, p&#x0200a;=&#x0200a;.01). A higher difference indicated slower reaction times for the animate category (<xref ref-type="fig" rid="pone-0068051-g007">Figure 7</xref>).</p><fig id="pone-0068051-g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068051.g007</object-id><label>Figure 7</label><caption><title>Interaction of animacy and context.</title><p>Reaction times for animal and vehicle categories in all three context conditions are shown. The bar graphs represent the calculated difference between animal and vehicle condition. Error bars represent the normalized 95% confidence intervals of the mean (Cousineau-Morey approach <xref rid="pone.0068051-Cousineau1" ref-type="bibr">[47]</xref>, <xref rid="pone.0068051-Morey1" ref-type="bibr">[48]</xref>). *p&#x0003c;.05, **p&#x02264;.01, ***p&#x02264;.001.</p></caption><graphic xlink:href="pone.0068051.g007"/></fig></sec></sec></sec><sec id="s4"><title>Discussion</title><p>In this study we investigated aspects of three different top-down parameters on a perceptual object categorization task. We manipulated the level (superordinate, basic and subordinate), animacy (animal, vehicle) and background (gray, upright, inverted) of objects and found that all of these manipulations affected rapid categorization processes. Importantly, the top-down processes interacted with each other specifically, such that their effect on the categorization task depended on the combination of parameters. This has an impact on the interpretation of categorization results.</p><sec id="s4a"><title>Levels of Categorization</title><p>Regarding the influence of level on object categorization, we found a clear advantage for the superordinate level over the basic and subordinate levels and hence, no basic level advantage. Because many studies have found an advantage for the basic level, this level was often thought to be the entry level of categorization <xref rid="pone.0068051-Rosch1" ref-type="bibr">[16]</xref>, <xref rid="pone.0068051-Jolicoeur1" ref-type="bibr">[49]</xref>. Subjects appeared to have a behavioral benefit for the category &#x0201c;dog&#x0201d; over the category &#x0201c;animal&#x0201d; or the category &#x0201c;St. Bernard&#x0201d;. In contrast, we did not find a basic level advantage in a two-alternative-forced choice rapid object categorization task, where categories were controlled for similarity. Recent evidence suggests that the basic level advantage does not occur for limited response times <xref rid="pone.0068051-Rogers1" ref-type="bibr">[18]</xref> and for rapid object and scene categorization <xref rid="pone.0068051-Mac1" ref-type="bibr">[19]</xref>&#x02013;<xref rid="pone.0068051-Loschky1" ref-type="bibr">[21]</xref>, <xref rid="pone.0068051-Rousselet1" ref-type="bibr">[50]</xref>, <xref rid="pone.0068051-Joubert2" ref-type="bibr">[51]</xref>. Compared to previous rapid categorization studies, categories in the current study were less heterogeneous because only a subset of existing categories that were as similar as possible perceptually were used. This was done deliberately to minimize any potential confounds caused by large differences in the visual features of the objects, both within and between categories. Nevertheless, visual features at the superordinate level (e.g. dog vs. car) differed to a greater extent than visual features at the basic level (e.g. dog vs. cat). This may explain the advantage of the superordinate level simply because it was more heterogeneous than the basic level. Mac&#x000e9; and colleagues (2009) obtained similar effects as presented here: they found an advantage for the detection of an animal (superordinate level) over the detection of a bird or a dog in a natural scene (basic level). Thus, the effect of better superordinate categorization performance seems to occur irrespective of chosen categories. Furthermore, patient studies reveal a selective impairment of categorization on the basic level, while leaving the superordinate level unaffected <xref rid="pone.0068051-Rogers1" ref-type="bibr">[18]</xref>. This challenges the notion of a perceptual mechanism, where, at a first step, the basic level is processed as a general entry level. Rather, the basic level advantage might occur in certain test situations, e.g. for tasks with main emphasis on cognitive categorization with long presentation times (e.g. word-picture combination, <xref rid="pone.0068051-Rogers1" ref-type="bibr">[18]</xref>) or if verbal responses are required. Objects are usually named on their basic level more frequently than at their superordinate or subordinate level, which yields to faster retrieval of basic level categories <xref rid="pone.0068051-Rosch1" ref-type="bibr">[16]</xref>, <xref rid="pone.0068051-Tanaka1" ref-type="bibr">[17]</xref>, <xref rid="pone.0068051-Brown1" ref-type="bibr">[52]</xref>. In the current study processing time was limited due to very short image presentation times, and responses were given via a button press. It might be possible that, depending on task requirements (naming or button press), the category levels are processed differently or that the processing mechanisms are tapped at different processing stages, thus, leading to different outcomes. Our results reveal no advantage for the basic level and this strengthens the assumption of a coarse to fine grained analysis of perceptual object information <xref rid="pone.0068051-Mac1" ref-type="bibr">[19]</xref>, rather than a beneficial processing for basic level object features. The advantage for the basic level seems to depend on higher cognitive processes, which may be strongly influenced by task demands <xref rid="pone.0068051-FabreThorpe1" ref-type="bibr">[53]</xref>.</p></sec><sec id="s4b"><title>Animacy in Object Recognition</title><p>One topic which is still under debate is the question whether a behavioral benefit exists for inanimate over animate object categories. According to the <italic>sensory/functional account</italic>, an advantage for animate objects over non-manipulable inanimate objects was expected in the current study. However, the reversed effect was observed. Both categories revealed similar error rates, but reaction times were significantly faster for the inanimate category (vehicles) than for the animate category (animals). If the levels of categorization are taken into account, it becomes clear that the advantage of inanimate categories is only observed at the basic level, with the vehicles category displaying higher percentage correct and faster reaction times. Interestingly, at the basic level there was a significantly higher accuracy for vehicles than animals, which was not observed in the main effect. On the subordinate level, the effect was even reversed, showing higher accuracy (but not reaction times) for the animate category (which explains the missing main effect of animacy for accuracy). This shows that the animacy effect (better performance for vehicles) was mainly driven by basic level categorization. Previous studies investigating animacy effects in rapid categorization partly contradict our findings, as they found a small but consistent advantage for animal categories over vehicle categories <xref rid="pone.0068051-Crouzet1" ref-type="bibr">[31]</xref>, <xref rid="pone.0068051-Crouzet2" ref-type="bibr">[32]</xref>. Another study <xref rid="pone.0068051-VanRullen2" ref-type="bibr">[54]</xref>, however, did not find a difference between animal and vehicle categories for superordinate rapid object detection. This agrees with our results, as performance was the same for both categories on the superordinate level. Rapid object categorization studies most often use the superordinate level of categorization and very heterogeneous stimulus sets, including many kinds of animals (mammals, fish, birds, and insects) and vehicles (cars, planes, bicycles, and ships). Although the stimulus set of the current study had less variable visual features across categories, the results were similar to previous studies.</p><p>These findings support the existence of different processing mechanisms for animate and inanimate categories. The question arises why the advantage of vehicles occurs only at the basic level. The effect might be a result of inappropriate selection of categories at the basic level. If this is the case, different processing mechanisms are engaged just because of a difference in level. However, categories were carefully chosen and perceptual similarity between object categories was ensured to make them as comparable as possible. Thus, all objects had roughly the same shape and retinal size. It was especially important for the basic level to provide similar difficulty levels between the two categories that were tested against each other (i.e. dog/cat and car/bus). Therefore, we can rule out that another category level than basic level was tested. One explanation for a vehicle advantage may be that animate objects engage different neural circuits for processing than inanimate objects <xref rid="pone.0068051-Chao1" ref-type="bibr">[13]</xref>, which might influence the categorization predominantly on the basic level. It has been previously shown that for superordinate categorization coarse information about an object is sufficient to differentiate between two categories <xref rid="pone.0068051-Fenske1" ref-type="bibr">[5]</xref>. For basic level categorization, more detailed visual information is necessary, which leads to a recruitment of further object-selective brain regions. Fine grained analysis takes longer and because animate and inanimate categories might be processed in distinct neural circuits, an additional time consuming processing step might be necessary for the animal category.</p><p>The <italic>sensory/functional account</italic> potentially gives an explanation of how the brain might solve the animal/non-animal distinction. This account is based on the assumption that objects are represented regarding their information content and functionality <xref rid="pone.0068051-Warrington1" ref-type="bibr">[35]</xref>, <xref rid="pone.0068051-Humphreys1" ref-type="bibr">[36]</xref>. It assumes that recognition performance for manipulable objects (e.g. tools) is enhanced due to the fast retrieval of action related circuits, which results in a &#x0201c;non-living identification advantage&#x0201d; <xref rid="pone.0068051-McMullen1" ref-type="bibr">[24]</xref>. Based on this account, <italic>non-manipulable</italic> artifacts (e.g. vehicles) depend more on their visual information than on their function, which might result in a loss of the non-living advantage. McMullen and colleagues <xref rid="pone.0068051-McMullen1" ref-type="bibr">[24]</xref> confirmed this assumption and found a disadvantage for non-manipulable artifacts compared to living things. Other studies challenged this idea and found neither an advantage nor a disadvantage for vehicles over animals at the superordinate level <xref rid="pone.0068051-VanRullen2" ref-type="bibr">[54]</xref>. The results of the current experiment speak against the <italic>sensory/functional account</italic>. Reaction times to vehicles were faster than to animals, contra to our expectations. One possibility might be that we interact with vehicles more regularly in daily life and are therefore more trained to react to the category &#x0201c;vehicle&#x0201d; than to the category &#x0201c;animal&#x0201d;. Another reason might be based on common features of the object categories, corresponding to the <italic>correlated feature theory</italic>
<xref rid="pone.0068051-Bussey1" ref-type="bibr">[55]</xref>, <xref rid="pone.0068051-Tyler1" ref-type="bibr">[56]</xref>. This account addresses <italic>correlated features</italic> that often co-occur within an object category (e.g. has eyes, can see). According to this theory, living objects contain more similar features than non-living objects, and it should thus be more difficult to distinguish between them <xref rid="pone.0068051-Moss1" ref-type="bibr">[57]</xref>. This would offer an explanation for the reduced processing speed for the animal category presented in this study. Features of objects that are necessary for categorization at a certain level, may engage different cognitive processing steps. If these features 1) differ between animate and inanimate categories and 2) differ between levels, a different behavioral outcome between animate and inanimate categories at the different levels of abstraction is expected. Features that need to be evaluated to differentiate a dog from a cat and a car from a bus may need longer processing time for animate categories due to higher similarity between dog/cat than car/bus <xref rid="pone.0068051-Moss1" ref-type="bibr">[57]</xref>. Dogs and cats may share more common features (e.g. has eyes, can see; has legs, can walk) than cars and busses (e.g. has wheels, can drive &#x02013; but car: drive myself; bus: has driver). Thus, cars and busses might be more easily discriminated due to fewer shared features than cats and dogs. From the current experiment it is not possible to draw further conclusions about the influence of shared visual features. An additional experiment that manipulates and controls for visual features would be necessary to prove this assumption. Another explanation may be a figure-ground segmentation advantage for vehicles. Vehicles are composed of geometric shapes with clear edges, which are easier segmented than animal objects with organic shapes with a flowing and curving contour. Therefore, perceptual processing of vehicles might be faster, leading to faster reaction times.</p><p>On the subordinate level, reaction times and error rates increase because more information about the single object is required and even finer grained information processing needs to be conducted. This may equalize the processing time of subordinate level animals (e.g. Siamese cat vs. Persian cat) and vehicles (e.g. Jeep vs. estate car) and make it even more difficult to categorize subordinate vehicles.</p></sec><sec id="s4c"><title>Background of Objects</title><p>Contextual information is believed to facilitate object processing <xref rid="pone.0068051-Henderson1" ref-type="bibr">[58]</xref>. Any mismatch between object and background would, thus, lead to reduced facilitation effects. We hypothesized that an inverted background would inhibit rapid object categorization. The experiments showed that background inversion failed to affect performance. Subjects responded as accurately and as quickly to objects on an inverted background as to objects presented on an upright background. However, we also observed better performance for categorization of objects presented on a gray background than for objects presented on a complex background.</p><p>It is puzzling that we failed to show an effect of an inverted background on object categorization. Studies have shown that object/context incongruencies impact performance and produce higher error rates and slower reaction times in categorization tasks <xref rid="pone.0068051-Biederman1" ref-type="bibr">[41]</xref>, <xref rid="pone.0068051-Joubert1" ref-type="bibr">[40]</xref>. There is evidence that semantic incongruence of object and context impair performance <xref rid="pone.0068051-Davenport1" ref-type="bibr">[38]</xref>, <xref rid="pone.0068051-Joubert1" ref-type="bibr">[40]</xref>. This is true even for short exposure durations and speeded response times, indicating parallel processing of object and context information. Other experiments have demonstrated that manipulation of color <xref rid="pone.0068051-Oliva1" ref-type="bibr">[59]</xref>, proportions <xref rid="pone.0068051-Biederman1" ref-type="bibr">[41]</xref> and orientation <xref rid="pone.0068051-Rieger1" ref-type="bibr">[43]</xref> in scenes hamper the recognition of a target object. We failed to find impaired object categorization with an inverted background. According to Rieger et al. <xref rid="pone.0068051-Rieger1" ref-type="bibr">[43]</xref>, orientation effects are strongest if the context is rotated by 90&#x000b0;. They showed that rotation by 180&#x000b0; has a weaker effect in natural scene discrimination and suggest some &#x0201c;orientation compensation&#x0201d; mechanisms that allow faster processing of inverted scenes than for intermediate rotation angles. Only inclined orientation conflicts between object and background may affect behavior and inversion by 180&#x000b0; can be compensated for. Nevertheless, if the background information affects object processing, one would expect at least minor effects of inversion. One possibility is that in the current task the background information is redundant and not necessary to solve the task. Studies showing an influence of context on object processing found only small effects <xref rid="pone.0068051-Joubert1" ref-type="bibr">[40]</xref>. In these studies, objects were isolated and pasted on backgrounds, similar to our stimuli. However, the manipulation of background orientation is different from context manipulations used in other rapid categorization studies <xref rid="pone.0068051-Crouzet2" ref-type="bibr">[32]</xref>, <xref rid="pone.0068051-Joubert1" ref-type="bibr">[40]</xref>. In scene inversion, high-level visual information is modified, while low-level cues are kept constant <xref rid="pone.0068051-Crouzet3" ref-type="bibr">[60]</xref>. This may lead to the conclusion that context effects derive from simple natural scene statistics, which were not changed in this study. In other words, if the nature of context effects depends on low- and mid-level information, then scene inversion should have no effect on categorization processes. On the other hand, local information cues may vary between objects on the same upright and inverted background. Thus, global low- and mid-level information is constant for upright and inverted images but local stimulus features may differ. This may affect object segmentation processes in both background conditions. As no effect between upright and inverted scenes was found, it is unlikely that these different local features have a strong impact on the categorization process. We cannot rule out that the missing context effect in our study results from inappropriate stimulus material. The objects were cropped out and pasted on an extra background. Although pasting effects between object and context were controlled for, the object was always in the foreground. It is possible that this &#x0201c;pop out&#x0201d; of the object minimized the effect of the scene inversion <xref rid="pone.0068051-Naber1" ref-type="bibr">[61]</xref>. This may lead to the question of the strength of context effects.</p><p>We found an advantage for categorizing objects on a gray background relative to objects within scenes. Previous experiments which compared isolated objects to objects embedded in natural scenes also found that complex backgrounds affect categorization negatively <xref rid="pone.0068051-Davenport1" ref-type="bibr">[38]</xref> and challenge the idea of a general facilitation effect for congruent context <xref rid="pone.0068051-Sun1" ref-type="bibr">[62]</xref>. One reason for better performance for isolated objects over objects with context might be that the figure-ground segmentation of the object is easier and faster. Due to less visual information and a lack of distractor elements in the background, the object categorization is improved. Thus, objects on a homogenous gray background are generally recognized more accurately and quickly than objects embedded in a congruent background <xref rid="pone.0068051-Naber1" ref-type="bibr">[61]</xref>.</p><p>The accuracy for categorizing objects with gray vs. complex background information was dependent on the level of categorization (in contradiction to reaction times, where facilitation for the gray background was observed at each level). On the subordinate level, subjects responded less accurately to objects with a complex background than to objects on gray background. Thus, the subordinate level was affected more strongly by additional background information than the other levels. This is probably caused by the higher processing demands required to categorize objects at the subordinate level and the resulting greater vulnerability to surrounding information. Most studies used the superordinate or basic level to investigate contextual effects <xref rid="pone.0068051-Joubert1" ref-type="bibr">[40]</xref>, <xref rid="pone.0068051-Sun1" ref-type="bibr">[62]</xref>. Joubert et al. <xref rid="pone.0068051-Joubert1" ref-type="bibr">[40]</xref>, for example, found an advantage of objects on a gray background for reaction times but not for accuracy. As they tested the superordinate level only, their results are in accordance with the results presented here. As we also tested for background effects on the basic and subordinate levels, here we provide additional information about the strength of the influence of complex background information on object categorization. The subordinate level particularly is more affected by complex backgrounds, likely because higher processing demands are required to extract fine grained object information.</p><p>Animals were more influenced by additional background information than vehicles. Reaction times were significantly longer for the complex background condition than for the gray background condition for animals. As mentioned before, animal categories may share more common features than inanimate categories and, thus, require higher processing demands <xref rid="pone.0068051-Moss1" ref-type="bibr">[57]</xref>. We consider this finding to be an explanation for the greater influence of background for animate categories if additional background information is presented. A greater effect of background for animal categories may arise from the scene information itself. Each object was pasted on a semantically congruent background, resulting in different scenes for each category (e.g. cars on streets, dogs in gardens). Thus, the animal related scene may require, in combination with animals, more time consuming processing. However, the level of context was not manipulated, which makes a systematic background-driven effect difficult to prove.</p><p>In summary, this study shows that the contextual information surrounding an object has an influence on categorization processes, which hamper object segmentation and, thus slow down object categorization. Effects of object-background incongruencies strongly depend on the factor that is manipulated (low- or high-level information) and likely also on the quality of the available information.</p></sec><sec id="s4d"><title>Caveats</title><p>We termed the manipulations of this study &#x0201c;top-down&#x0201d; and each of them clearly has top-down processing components. However, we are aware that not only top-down mechanisms can account for the effects that were observed. Image statistics, such as spatial frequencies, influence categorization performance in a bottom-up fashion. Visual features and spatial frequencies differ between categories, which makes them inherently dissociable <xref rid="pone.0068051-Viggiano1" ref-type="bibr">[33]</xref>, <xref rid="pone.0068051-Harel2" ref-type="bibr">[34]</xref>, <xref rid="pone.0068051-Crouzet3" ref-type="bibr">[60]</xref>, <xref rid="pone.0068051-Torralba1" ref-type="bibr">[63]</xref>, <xref rid="pone.0068051-Mack1" ref-type="bibr">[64]</xref>. In the design of the current study, these confounds could not be completely controlled for, and such a limitation should be kept in mind when interpreting the results. Furthermore, physical dimensions and higher cognitive functions likely interact, such that low-level changes influence top-down processes <xref rid="pone.0068051-Bell1" ref-type="bibr">[65]</xref>.</p><p>Our interpretation of the results assumes that appropriate categories, levels of abstraction and context conditions were chosen. The concept of different category levels is not standardized and varies between studies. This hampers the interpretation of results and comparisons with other experiments. Studies investigating the influence of the cognitive components of object processing generally employ tasks that include imagination or naming of objects, using word or picture stimuli. Studies investigating the question of perceptual object processing rather use rapid categorization tasks (with limited object presentation time and button press responses). However, the overlap between studies investigating perceptual vs. cognitive object processing stages remains insufficient. Given that task requirements (go/no-go, match/mismatch or two-alternative forced choice task), stimuli (complex scenes, isolated objects, line drawings) and response criteria (button press or verbal response) tend to differ between studies, the different outcomes of these studies are not surprising. One interpretation of ambiguous findings regarding the animate/inanimate advantages might be the fact that inconsistent object categories at distinct levels of categorization were used. This could lead to different results. Nevertheless, further research needs to be conducted in order to amplify the understanding about the behavioral correlates of object categorization.</p><p>One further point to consider is the sometimes different outcome of accuracy and reaction time data. We interpreted accuracy as reflecting the difficulty of the task. This might arise from, for example situations where more information is needed to categorize an object on the subordinate than at the superordinate level. The additional amount of information needed increases the difficulty of categorization. Alternatively, due to higher similarity between categories at the subordinate level, misinterpretation of one category for the other can lead to higher error rates. Our interpretation of the reaction time data is different than our interpretation of the accuracy data. In our opinion, reaction times reflect the processing speed in the brain. If for two task conditions distinct networks with different processing time are engaged, different reaction times with similar accuracies may be observed. The dissociation between the response qualities may shed light on the underlying brain processes, but it has to be treated with caution. To our knowledge there is no proof for a neural correlate of accuracy and reaction times, respectively. Most often both processes work together and are difficult to disentangle.</p></sec><sec id="s4e"><title>Conclusions</title><p>The current study investigated the influence of different top-down manipulations on rapid object categorization. We found that 1) objects are processed according to coarse-to-fine grained information (no basic level advantage present), 2) a benefit for vehicles at the basic level and for animals at the subordinate level, and 3) deteriorated categorization of objects presented on a complex background in comparison to a gray background, with the strongest effect at the subordinate level. We conclude that object categorization effects depend highly on the level and the type of category.</p></sec></sec></body><back><ref-list><title>References</title><ref id="pone.0068051-Thorpe1"><label>1</label><mixed-citation publication-type="other">Thorpe S, Fize D, Marlot C (1996) Speed of processing in the human visual system. Nature 381 (6582): 520&#x02013;522. Available: <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381520a0">10.1038/381520a0</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Delorme1"><label>2</label><mixed-citation publication-type="other">Delorme A, Rousselet GA, Mac&#x000e9; MJ, Fabre-Thorpe M (2004) Interaction of top-down and bottom-up processing in the fast visual analysis of natural scenes. Brain Res Cogn Brain Res 19 (2): 103&#x02013;113. Available: <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cogbrainres.2003.11.010">10.1016/j.cogbrainres.2003.11.010</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-VanRullen1"><label>3</label><mixed-citation publication-type="journal">
<name><surname>VanRullen</surname><given-names>R</given-names></name>, <name><surname>Thorpe</surname><given-names>SJ</given-names></name> (<year>2001</year>) <article-title>The time course of visual processing: from early perception to decision-making</article-title>. <source>J Cogn Neurosci 13</source>
<volume>(4)</volume>: <fpage>454</fpage>&#x02013;<lpage>461</lpage>.</mixed-citation></ref><ref id="pone.0068051-Serre1"><label>4</label><mixed-citation publication-type="other">Serre T, Oliva A, Poggio T (2007) A feedforward architecture accounts for rapid categorization. Proc Natl Acad Sci U S A 104 (15): 6424&#x02013;6429. Available: <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0700622104">10.1073/pnas.0700622104</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Fenske1"><label>5</label><mixed-citation publication-type="other">Fenske MJ, Aminoff E, Gronau N, Bar M (2006) Top-down facilitation of visual object recognition: object-based and context-based contributions. Prog Brain Res 155: 3&#x02013;21. Available: <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0079-6123(06)55001-0">10.1016/S0079-6123(06)55001-0</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Gazzaley1"><label>6</label><mixed-citation publication-type="other">Gazzaley A, Cooney JW, McEvoy K, Knight RT, D&#x02019;Esposito M (2005) Top-down enhancement and suppression of the magnitude and speed of neural activity. J Cogn Neurosci 17 (3): 507&#x02013;517. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0898929053279522">10.1162/0898929053279522</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Delorme2"><label>7</label><mixed-citation publication-type="other">Delorme A, Richard G, Fabre-Thorpe M (2010) Key visual features for rapid categorization of animals in natural scenes. Front Psychol 1: 21. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2010.00021">10.3389/fpsyg.2010.00021</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Li1"><label>8</label><mixed-citation publication-type="other">Li F, VanRullen R, Koch C, Perona P (2002) Rapid natural scene categorization in the near absence of attention. Proc Natl Acad Sci U S A 99 (14): 9596&#x02013;9601. <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/doi:10.1073/pnas.092277599">doi:10.1073/pnas.092277599</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Evans1"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Evans</surname><given-names>KK</given-names></name>, <name><surname>Treisman</surname><given-names>A</given-names></name> (<year>2005</year>) <article-title>Perception of objects in natural scenes: Is it really attention free</article-title>. <source>J Exp Psychol Hum Percept Perform 31</source>
<volume>(6)</volume>: <fpage>1476</fpage>&#x02013;<lpage>1492</lpage>.</mixed-citation></ref><ref id="pone.0068051-Peelen1"><label>10</label><mixed-citation publication-type="other">Peelen MV, Fei-Fei L, Kastner S (2009) Neural mechanisms of rapid natural scene categorization in human visual cortex. Nature 460 (7251): 94&#x02013;97. Available: doi:10.1038/nature08103.</mixed-citation></ref><ref id="pone.0068051-Cohen1"><label>11</label><mixed-citation publication-type="other">Cohen MA, Alvarez GA, Nakayama K (2011) Natural-scene perception requires attention. Psychol Sci 22 (9): 1165&#x02013;1172. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0956797611419168">10.1177/0956797611419168</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Kiefer1"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Kiefer</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>Perceptual and semantic sources of category-specific effects: event-related potentials during picture and word categorization</article-title>. <source>Mem Cognit 29</source>
<volume>(1)</volume>: <fpage>100</fpage>&#x02013;<lpage>116</lpage>.</mixed-citation></ref><ref id="pone.0068051-Chao1"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Chao</surname><given-names>LL</given-names></name>, <name><surname>Weisberg</surname><given-names>J</given-names></name>, <name><surname>Martin</surname><given-names>A</given-names></name> (<year>2002</year>) <article-title>Experience-dependent modulation of category-related cortical activity</article-title>. <source>Cereb Cortex 12</source>
<volume>(5)</volume>: <fpage>545</fpage>&#x02013;<lpage>551</lpage>.</mixed-citation></ref><ref id="pone.0068051-Mahon1"><label>14</label><mixed-citation publication-type="other">Mahon BZ, Caramazza A (2009) Concepts and categories: a cognitive neuropsychological perspective. Annu Rev Psychol 60: 27&#x02013;51. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.psych.60.110707.163532">10.1146/annurev.psych.60.110707.163532</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Harel1"><label>15</label><mixed-citation publication-type="other">Harel A, Ullman S, Harari D, Bentin S (2011) Basic-level categorization of intermediate complexity fragments reveals top-down effects of expertise in visual perception. J Vis 11 (8): 18. Available: <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/11.8.18">10.1167/11.8.18</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Rosch1"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Rosch</surname><given-names>E</given-names></name>, <name><surname>Mervis</surname><given-names>CB</given-names></name>, <name><surname>Gray</surname><given-names>WD</given-names></name>, <name><surname>Johnson</surname><given-names>DM</given-names></name>, <name><surname>Boyesbraem</surname><given-names>P</given-names></name> (<year>1976</year>) <article-title>Basic Objects in Natural Categories</article-title>. <source>Cogn Psychol 8</source>
<volume>(3)</volume>: <fpage>382</fpage>&#x02013;<lpage>439</lpage>.</mixed-citation></ref><ref id="pone.0068051-Tanaka1"><label>17</label><mixed-citation publication-type="journal">
<name><surname>Tanaka</surname><given-names>JW</given-names></name>, <name><surname>Taylor</surname><given-names>M</given-names></name> (<year>1991</year>) <article-title>Object categories and expertise: Is the basic level in the eye of the beholder</article-title>. <source>Cogn Psychol 23</source>
<volume>(3)</volume>: <fpage>457</fpage>&#x02013;<lpage>482</lpage>.</mixed-citation></ref><ref id="pone.0068051-Rogers1"><label>18</label><mixed-citation publication-type="other">Rogers TT, Patterson K (2007) Object categorization: reversals and explanations of the basic-level advantage. J Exp Psychol Gen 136 (3): 451&#x02013;469. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-3445.136.3.451">10.1037/0096-3445.136.3.451</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Mac1"><label>19</label><mixed-citation publication-type="other">Mac&#x000e9; MJ, Joubert OR, Nespoulous J, Fabre-Thorpe M (2009) The time-course of visual categorizations: you spot the animal faster than the bird. PLoS ONE 4 (6): e5927. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0005927">10.1371/journal.pone.0005927</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Greene1"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Greene</surname><given-names>MR</given-names></name>, <name><surname>Oliva</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>The briefest of glances: the time course of natural scene understanding</article-title>. <source>Psychol Sci 20</source>
<volume>(4)</volume>: <fpage>464</fpage>&#x02013;<lpage>472</lpage>.</mixed-citation></ref><ref id="pone.0068051-Loschky1"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Loschky</surname><given-names>LC</given-names></name>, <name><surname>Larson</surname><given-names>AM</given-names></name> (<year>2010</year>) <article-title>The natural/man-made distinction is made before basic-level distinctions in scene gist processing</article-title>. <source>Vis Cogn 18</source>
<volume>(4)</volume>: <fpage>513</fpage>&#x02013;<lpage>536</lpage>.</mixed-citation></ref><ref id="pone.0068051-Collin1"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Collin</surname><given-names>CA</given-names></name>, <name><surname>McMullen</surname><given-names>PA</given-names></name> (<year>2005</year>) <article-title>Subordinate-level categorization relies on high spatial frequencies to a greater degree than basic-level categorization</article-title>. <source>Percept Psychophys 67</source>
<volume>(2)</volume>: <fpage>354</fpage>&#x02013;<lpage>364</lpage>.</mixed-citation></ref><ref id="pone.0068051-Goffaux1"><label>23</label><mixed-citation publication-type="other">Goffaux V, Peters J, Haubrechts J, Schiltz C, Jansma B, <etal>et al</etal>. (2011) From coarse to fine? Spatial and temporal dynamics of cortical face processing. Cereb Cortex 21 (2): 467&#x02013;476. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhq112">10.1093/cercor/bhq112</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-McMullen1"><label>24</label><mixed-citation publication-type="journal">
<name><surname>McMullen</surname><given-names>PA</given-names></name>, <name><surname>Purdy</surname><given-names>KS</given-names></name> (<year>2006</year>) <article-title>Category-specific effects on the identification of non-manipulable objects</article-title>. <source>Brain Cogn 62</source>
<volume>(3)</volume>: <fpage>228</fpage>&#x02013;<lpage>240</lpage>.</mixed-citation></ref><ref id="pone.0068051-Riddoch1"><label>25</label><mixed-citation publication-type="other">Riddoch MJ, Humphreys GW, Akhtar N, Allen H, Bracewell RM, <etal>et al</etal>. (2008) A tale of two agnosias: distinctions between form and integrative agnosia. Cogn Neuropsychol 25 (1): 56&#x02013;92. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02643290701848901">10.1080/02643290701848901</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Mahon2"><label>26</label><mixed-citation publication-type="other">Mahon BZ, Anzellotti S, Schwarzbach J, Zampini M, Caramazza A (2009) Category-specific organization in the human brain does not require visual experience. Neuron 63 (3): 397&#x02013;405. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.07.012">10.1016/j.neuron.2009.07.012</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Gerlach1"><label>27</label><mixed-citation publication-type="other">Gerlach C (2007) A review of functional imaging studies on category specificity. J Cogn Neurosci 19 (2): 296&#x02013;314. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/jocn.2007.19.2.296">10.1162/jocn.2007.19.2.296</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Martin1"><label>28</label><mixed-citation publication-type="other">Martin A (2007) The representation of object concepts in the brain. Annu Rev Psychol 58: 25&#x02013;45. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.psych.57.102904.190143">10.1146/annurev.psych.57.102904.190143</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-New1"><label>29</label><mixed-citation publication-type="other">New J, Cosmides L, Tooby J (2007) Category-specific attention for animals reflects ancestral priorities, not expertise. Proc. Natl. Acad. Sci. U.S.A. 104 (42): 16598&#x02013;16603. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0703913104">10.1073/pnas.0703913104</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Proverbio1"><label>30</label><mixed-citation publication-type="other">Proverbio AM, Del ZM, Zani A (2007) The emergence of semantic categorization in early visual processing: ERP indices of animal vs. artifact recognition. BMC Neurosci 8: 24. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2202-8-24">10.1186/1471-2202-8-24</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Crouzet1"><label>31</label><mixed-citation publication-type="other">Crouzet SM, Kirchner H, Thorpe SJ (2010) Fast saccades toward faces: face detection in just 100 ms. J Vis 10 (4): 16.1&#x02013;17. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/10.4.16">10.1167/10.4.16</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Crouzet2"><label>32</label><mixed-citation publication-type="other">Crouzet SM, Joubert OR, Thorpe SJ, Fabre-Thorpe M (2012) Animal detection precedes access to scene category. PLoS ONE 7 (12): e51471. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0051471">10.1371/journal.pone.0051471</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Viggiano1"><label>33</label><mixed-citation publication-type="other">Viggiano MP, Costantini A, Vannucci M, Righi S (2004) Hemispheric asymmetry for spatially filtered stimuli belonging to different semantic categories. Brain Res Cogn Brain Res 20 (3): 519&#x02013;524. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cogbrainres.2004.03.010">10.1016/j.cogbrainres.2004.03.010</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Harel2"><label>34</label><mixed-citation publication-type="other">Harel A, Bentin S (2009) Stimulus type, level of categorization, and spatial-frequencies utilization: implications for perceptual categorization hierarchies. J Exp Psychol Hum Percept Perform 35 (4): 1264&#x02013;1273. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0013621">10.1037/a0013621</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Warrington1"><label>35</label><mixed-citation publication-type="other">Warrington EK, Shallice T (1984) Category specific semantic impairments. Brain 107 (SEP): 829&#x02013;854.</mixed-citation></ref><ref id="pone.0068051-Humphreys1"><label>36</label><mixed-citation publication-type="other">Humphreys GW, Forde EM (2001) Hierarchies, similarity, and interactivity in object recognition: &#x0201c;Category-specific&#x0201d; neuropsychological deficits. Behav Brain Sci 24 (3): 453&#x02013;+.</mixed-citation></ref><ref id="pone.0068051-Bar1"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Bar</surname><given-names>M</given-names></name>, <name><surname>Ullman</surname><given-names>S</given-names></name> (<year>1996</year>) <article-title>Spatial context in recognition</article-title>. <source>Perception 25</source>
<volume>(3)</volume>: <fpage>343</fpage>&#x02013;<lpage>352</lpage>.</mixed-citation></ref><ref id="pone.0068051-Davenport1"><label>38</label><mixed-citation publication-type="journal">
<name><surname>Davenport</surname><given-names>JL</given-names></name>, <name><surname>Potter</surname><given-names>MC</given-names></name> (<year>2004</year>) <article-title>Scene consistency in object and background perception</article-title>. <source>Psychol Sci 15</source>
<volume>(8)</volume>: <fpage>559</fpage>&#x02013;<lpage>564</lpage>.</mixed-citation></ref><ref id="pone.0068051-Kveraga1"><label>39</label><mixed-citation publication-type="other">Kveraga K, Ghuman AS, Kassam KS, Aminoff EA, H&#x000e4;m&#x000e4;l&#x000e4;inen MS, <etal>et al</etal>. (2011) Early onset of neural synchronization in the contextual associations network. Proc Natl Acad Sci U S A 108 (8): 3389&#x02013;3394. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1013760108">10.1073/pnas.1013760108</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Joubert1"><label>40</label><mixed-citation publication-type="other">Joubert OR, Fize D, Rousselet GA, Fabre-Thorpe M (2008) Early interference of context congruence on object processing in rapid visual categorization of natural scenes. J Vis 8 (13): 11.1&#x02013;18. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/8.13.11">10.1167/8.13.11</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Biederman1"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Biederman</surname><given-names>I</given-names></name>, <name><surname>Mezzanotte</surname><given-names>RJ</given-names></name>, <name><surname>Rabinowitz</surname><given-names>JC</given-names></name> (<year>1982</year>) <article-title>Scene perception: detecting and judging objects undergoing relational violations</article-title>. <source>Cogn Psychol 14</source>
<volume>(2)</volume>: <fpage>143</fpage>&#x02013;<lpage>177</lpage>.</mixed-citation></ref><ref id="pone.0068051-Hollingworth1"><label>42</label><mixed-citation publication-type="journal">
<name><surname>Hollingworth</surname><given-names>A</given-names></name>, <name><surname>Henderson</surname><given-names>JM</given-names></name> (<year>1998</year>) <article-title>Does consistent scene context facilitate object perception</article-title>. <source>J Exp Psychol Gen 127</source>
<volume>(4)</volume>: <fpage>398</fpage>&#x02013;<lpage>415</lpage>.</mixed-citation></ref><ref id="pone.0068051-Rieger1"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Rieger</surname><given-names>JW</given-names></name>, <name><surname>Koechy</surname><given-names>N</given-names></name>, <name><surname>Schalk</surname><given-names>F</given-names></name>, <name><surname>Grueschow</surname><given-names>M</given-names></name>, <name><surname>Heinze</surname><given-names>H</given-names></name> (<year>2008</year>) <article-title>Speed limits: Orientation and semantic context interactions constrain natural scene discrimination dynamics</article-title>. <source>J Exp Psychol Hum Percept Perform 34</source>
<volume>(1)</volume>: <fpage>56</fpage>&#x02013;<lpage>76</lpage>.</mixed-citation></ref><ref id="pone.0068051-Johnson1"><label>44</label><mixed-citation publication-type="journal">
<name><surname>Johnson</surname><given-names>KE</given-names></name>, <name><surname>Mervis</surname><given-names>CB</given-names></name> (<year>1997</year>) <article-title>Effects of varying levels of expertise on the basic level of categorization</article-title>. <source>J Exp Psychol Gen 126</source>
<volume>(3)</volume>: <fpage>248</fpage>&#x02013;<lpage>277</lpage>.</mixed-citation></ref><ref id="pone.0068051-Schyns1"><label>45</label><mixed-citation publication-type="journal">
<name><surname>Schyns</surname><given-names>PG</given-names></name>, <name><surname>Oliva</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>Dr. Angry and Mr. Smile: when categorization flexibly modifies the perception of faces in rapid visual presentations</article-title>. <source>Cognition 69</source>
<volume>(3)</volume>: <fpage>243</fpage>&#x02013;<lpage>265</lpage>.</mixed-citation></ref><ref id="pone.0068051-Viggiano2"><label>46</label><mixed-citation publication-type="other">Viggiano MP, Righi S, Galli G (2006) Category-specific visual recognition as affected by aging and expertise. Arch Gerontol Geriatr 42 (3): 329&#x02013;338. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.archger.2005.08.003">10.1016/j.archger.2005.08.003</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Cousineau1"><label>47</label><mixed-citation publication-type="journal">
<name><surname>Cousineau</surname><given-names>D</given-names></name> (<year>2005</year>) <article-title>The rise of quantitative methods in Psychology</article-title>. <source>Tutorials in Quantitative Methods for Psychology 1</source>
<volume>(1)</volume>: <fpage>1</fpage>&#x02013;<lpage>3</lpage>.</mixed-citation></ref><ref id="pone.0068051-Morey1"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Morey</surname><given-names>RD</given-names></name> (<year>2008</year>) <article-title>Confidence Intervals from Normalized Data: A correction to Cousineau (2005)</article-title>. <source>Tutorials in Quantitative Methods for Psychology 4</source>
<volume>(2)</volume>: <fpage>61</fpage>&#x02013;<lpage>64</lpage>.</mixed-citation></ref><ref id="pone.0068051-Jolicoeur1"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Jolicoeur</surname><given-names>P</given-names></name>, <name><surname>Gluck</surname><given-names>MA</given-names></name>, <name><surname>Kosslyn</surname><given-names>SM</given-names></name> (<year>1984</year>) <article-title>Pictures and names: making the connection</article-title>. <source>Cogn Psychol 16</source>
<volume>(2)</volume>: <fpage>243</fpage>&#x02013;<lpage>275</lpage>.</mixed-citation></ref><ref id="pone.0068051-Rousselet1"><label>50</label><mixed-citation publication-type="journal">
<name><surname>Rousselet</surname><given-names>GA</given-names></name>, <name><surname>Joubert</surname><given-names>OR</given-names></name>, <name><surname>Fabre-Thorpe</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>How long to get to the &#x0201c;gist&#x0201d; of real-world natural scenes</article-title>. <source>Vis Cogn 12</source>
<volume>(6)</volume>: <fpage>852</fpage>&#x02013;<lpage>877</lpage>.</mixed-citation></ref><ref id="pone.0068051-Joubert2"><label>51</label><mixed-citation publication-type="other">Joubert OR, Rousselet GA, Fize D, Fabre-Thorpe M (2007) Processing scene context: fast categorization and object interference. Vision Res 47 (26): 3286&#x02013;3297. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2007.09.013">10.1016/j.visres.2007.09.013</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Brown1"><label>52</label><mixed-citation publication-type="journal">
<name><surname>Brown</surname><given-names>R</given-names></name> (<year>1958</year>) <article-title>How shall a thing be called</article-title>. <source>Psychological Review 65</source>
<volume>(1)</volume>: <fpage>14</fpage>&#x02013;<lpage>21</lpage>.</mixed-citation></ref><ref id="pone.0068051-FabreThorpe1"><label>53</label><mixed-citation publication-type="other">Fabre-Thorpe M (2011) The characteristics and limits of rapid visual categorization. Front Psychol 2: 243. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2011.00243">10.3389/fpsyg.2011.00243</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-VanRullen2"><label>54</label><mixed-citation publication-type="journal">
<name><surname>VanRullen</surname><given-names>R</given-names></name>, <name><surname>Thorpe</surname><given-names>SJ</given-names></name> (<year>2001</year>) <article-title>Is it a bird? Is it a plane? Ultra-rapid visual categorisation of natural and artifactual objects</article-title>. <source>Perception 30</source>
<volume>(6)</volume>: <fpage>655</fpage>&#x02013;<lpage>668</lpage>.</mixed-citation></ref><ref id="pone.0068051-Bussey1"><label>55</label><mixed-citation publication-type="journal">
<name><surname>Bussey</surname><given-names>TJ</given-names></name>, <name><surname>Saksida</surname><given-names>LM</given-names></name> (<year>2002</year>) <article-title>The organization of visual object representations: a connectionist model of effects of lesions in perirhinal cortex</article-title>. <source>Eur J Neurosci 15</source>
<volume>(2)</volume>: <fpage>355</fpage>&#x02013;<lpage>364</lpage>.</mixed-citation></ref><ref id="pone.0068051-Tyler1"><label>56</label><mixed-citation publication-type="other">Tyler LK, Stamatakis EA, Bright P, Acres K, Abdallah S, <etal>et al</etal>. (2004) Processing objects at different levels of specificity. J Cogn Neurosci 16 (3): 351&#x02013;362. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089892904322926692">10.1162/089892904322926692</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Moss1"><label>57</label><mixed-citation publication-type="journal">
<name><surname>Moss</surname><given-names>HE</given-names></name>, <name><surname>Tyler</surname><given-names>LK</given-names></name> (<year>2000</year>) <article-title>A progressive category-specific semantic deficit for non-living things</article-title>. <source>Neuropsychologia 38</source>
<volume>(1)</volume>: <fpage>60</fpage>&#x02013;<lpage>82</lpage>.</mixed-citation></ref><ref id="pone.0068051-Henderson1"><label>58</label><mixed-citation publication-type="other">Henderson JM, Hollingworth A (1999) High-level scene perception. Annu Rev Psychol 50: 243&#x02013;271. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.psych.50.1.243">10.1146/annurev.psych.50.1.243</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Oliva1"><label>59</label><mixed-citation publication-type="journal">
<name><surname>Oliva</surname><given-names>A</given-names></name>, <name><surname>Schyns</surname><given-names>PG</given-names></name> (<year>2000</year>) <article-title>Diagnostic colors mediate scene recognition</article-title>. <source>Cogn Psychol 41</source>
<volume>(2)</volume>: <fpage>176</fpage>&#x02013;<lpage>210</lpage>.</mixed-citation></ref><ref id="pone.0068051-Crouzet3"><label>60</label><mixed-citation publication-type="other">Crouzet SM, Serre T (2011) What are the Visual Features Underlying Rapid Object Recognition. Front Psychol 2: 326. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2011.00326">10.3389/fpsyg.2011.00326</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Naber1"><label>61</label><mixed-citation publication-type="other">Naber M, Hilger M, Einhaeuser W (2012) Animal detection and identification in natural scenes: Image statistics and emotional valence. J Vis 12 (1).</mixed-citation></ref><ref id="pone.0068051-Sun1"><label>62</label><mixed-citation publication-type="other">Sun H, Simon-Dack SL, Gordon RD, Teder WA (2011) Contextual influences on rapid object categorization in natural scenes. Brain Res 1398: 40&#x02013;54. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.brainres.2011.04.029">10.1016/j.brainres.2011.04.029</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Torralba1"><label>63</label><mixed-citation publication-type="journal">
<name><surname>Torralba</surname><given-names>A</given-names></name>, <name><surname>Oliva</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Statistics of natural image categories</article-title>. <source>Network 14</source>
<volume>(3)</volume>: <fpage>391</fpage>&#x02013;<lpage>412</lpage>.</mixed-citation></ref><ref id="pone.0068051-Mack1"><label>64</label><mixed-citation publication-type="other">Mack ML, Palmeri TJ (2010) Modeling categorization of scenes containing consistent versus inconsistent objects. J Vis 10 (3): 11.1&#x02013;11. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/10.3.11">10.1167/10.3.11</ext-link></comment>
</mixed-citation></ref><ref id="pone.0068051-Bell1"><label>65</label><mixed-citation publication-type="other">Bell AH, Hadj-Bouziane F, Frihauf JB, Tootell RB, Ungerleider LG (2009) Object representations in the temporal cortex of monkeys and humans as revealed by functional magnetic resonance imaging. J Neurophysiol 101 (2): 688&#x02013;700. <comment>Available: doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90657.2008">10.1152/jn.90657.2008</ext-link></comment>
</mixed-citation></ref></ref-list></back></article>