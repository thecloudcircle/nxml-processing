
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Biomed Res Int</journal-id><journal-id journal-id-type="iso-abbrev">Biomed Res Int</journal-id><journal-id journal-id-type="publisher-id">BMRI</journal-id><journal-title-group><journal-title>BioMed Research International</journal-title></journal-title-group><issn pub-type="ppub">2314-6133</issn><issn pub-type="epub">2314-6141</issn><publisher><publisher-name>Hindawi Publishing Corporation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27376088</article-id><article-id pub-id-type="pmc">4916327</article-id><article-id pub-id-type="doi">10.1155/2016/8797438</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A Comparative Study of Land Cover Classification by Using Multispectral and Texture Data</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Qadri</surname><given-names>Salman</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Khan</surname><given-names>Dost Muhammad</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Ahmad</surname><given-names>Farooq</given-names></name><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Qadri</surname><given-names>Syed Furqan</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Babar</surname><given-names>Masroor Ellahi</given-names></name><xref ref-type="aff" rid="I4">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Shahid</surname><given-names>Muhammad</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Ul-Rehman</surname><given-names>Muzammil</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Razzaq</surname><given-names>Abdul</given-names></name><xref ref-type="aff" rid="I5">
<sup>5</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Shah Muhammad</surname><given-names>Syed</given-names></name><xref ref-type="aff" rid="I6">
<sup>6</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Fahad</surname><given-names>Muhammad</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Ahmad</surname><given-names>Sarfraz</given-names></name><xref ref-type="aff" rid="I6">
<sup>6</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Pervez</surname><given-names>Muhammad Tariq</given-names></name><xref ref-type="aff" rid="I4">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Naveed</surname><given-names>Nasir</given-names></name><xref ref-type="aff" rid="I6">
<sup>6</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Aslam</surname><given-names>Naeem</given-names></name><xref ref-type="aff" rid="I5">
<sup>5</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Jamil</surname><given-names>Mutiullah</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Rehmani</surname><given-names>Ejaz Ahmad</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Ahmad</surname><given-names>Nazir</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Akhtar Khan</surname><given-names>Naeem</given-names></name><xref ref-type="aff" rid="I7">
<sup>7</sup>
</xref></contrib></contrib-group><aff id="I1"><sup>1</sup>Department of CS &#x00026; IT, The Islamia University of Bahawalpur, Punjab 63100, Pakistan</aff><aff id="I2"><sup>2</sup>Department of Computer Sciences, CIIT Lahore, Punjab 54000, Pakistan</aff><aff id="I3"><sup>3</sup>Key Laboratory of Photo-Electronic Imaging Technology and System, School of Computer Science, Beijing Institute of Technology (BIT), Beijing 100081, China</aff><aff id="I4"><sup>4</sup>Department of Bioinformatics and Computational Biology, Virtual University of Pakistan, Lahore, Punjab 54000, Pakistan</aff><aff id="I5"><sup>5</sup>Department of CS, NFC IET, Multan, Punjab 60000, Pakistan</aff><aff id="I6"><sup>6</sup>Department of CS, Virtual University of Pakistan, Lahore, Punjab 54000, Pakistan</aff><aff id="I7"><sup>7</sup>Faculty of Information Technology, University of Central Punjab (UCP), Lahore 54000, Pakistan</aff><author-notes><corresp id="cor1">*Salman Qadri: <email>salman.qadri@iub.edu.pk</email></corresp><fn fn-type="other"><p>Academic Editor: John P. Geisler</p></fn></author-notes><pub-date pub-type="ppub"><year>2016</year></pub-date><pub-date pub-type="epub"><day>8</day><month>6</month><year>2016</year></pub-date><volume>2016</volume><elocation-id>8797438</elocation-id><history><date date-type="received"><day>26</day><month>11</month><year>2015</year></date><date date-type="accepted"><day>28</day><month>4</month><year>2016</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016 Salman Qadri et al.</copyright-statement><copyright-year>2016</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>The main objective of this study is to find out the importance of machine vision approach for the classification of five types of land cover data such as bare land, desert rangeland, green pasture, fertile cultivated land, and Sutlej river land. A novel spectra-statistical framework is designed to classify the subjective land cover data types accurately. Multispectral data of these land covers were acquired by using a handheld device named multispectral radiometer in the form of five spectral bands (blue, green, red, near infrared, and shortwave infrared) while texture data were acquired with a digital camera by the transformation of acquired images into 229 texture features for each image. The most discriminant 30 features of each image were obtained by integrating the three statistical features selection techniques such as Fisher, Probability of Error plus Average Correlation, and Mutual Information (<italic>F</italic> + PA + MI). Selected texture data clustering was verified by nonlinear discriminant analysis while linear discriminant analysis approach was applied for multispectral data. For classification, the texture and multispectral data were deployed to artificial neural network (ANN:<italic> n</italic>-class). By implementing a cross validation method (80-20), we received an accuracy of 91.332% for texture data and 96.40% for multispectral data, respectively.</p></abstract></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>Image processing and remote sensing are playing a vital role for the betterment of the agriculture field [<xref rid="B1" ref-type="bibr">1</xref>]. By using this technology, we can classify vast land cover area into different categories [<xref rid="B2" ref-type="bibr">2</xref>]. Not only would this be helpful for the socioeconomic sector but also it fulfills the needs of the future for sustainable development. In the twenty-first century, the world is facing the challenge of hunger, food, and poverty [<xref rid="B3" ref-type="bibr">3</xref>]. This issue can be resolved by increase in crop production and better utilization of cultivated land. Land cover information is necessary for different policy making, planning, and management purposes including land record of a forest, desert, farmland, and wetland as well as other biophysical resources, which are required for land cover information. Researchers are trying to get the benefits of technology by involving it in the agriculture field [<xref rid="B4" ref-type="bibr">4</xref>]. It is being tried to enhance the cultivated land area and monitor the land through field survey [<xref rid="B5" ref-type="bibr">5</xref>]. For the success of such surveys, more time with expensive labor is required. In developing countries like Pakistan, it seems to be very difficult to spend a lot of resources on such projects. Whether directly or indirectly, almost 50% of the population of these countries is associated with the agriculture profession [<xref rid="B6" ref-type="bibr">6</xref>]. All the preceding issues highlight the importance of the proper land management and better crop growth and production. According to geographical distribution of the country, it is categorized into different land cover types like barren, fertile, rocky and sandy, and so forth. In Pakistan, the conventional field based survey system could not be properly managed due to financial and technical limitations. For this reason, remote sensing technology could not be used for natural resource management up till now, as was proposed by the relevant professionals [<xref rid="B7" ref-type="bibr">7</xref>]. Many researchers used this technology for better resource management; for example, a two-layer conditional random field (CRF) model was proposed for land cover and land use classification [<xref rid="B8" ref-type="bibr">8</xref>]. Similarly, a multilayer conditional random field (MCRF) land classification model was suggested. It was used for multitemporal with multiscale remote sensing data [<xref rid="B9" ref-type="bibr">9</xref>]. A gray level cooccurrence matrix with different window size images was used to find the four land types of aerial data. Different statistical features, that is, dissimilarity, homogeneity, angular second moment, and entropy, were calculated to classify the data [<xref rid="B10" ref-type="bibr">10</xref>]. A supervised pixel-based classification algorithm was used by implementing Markov Random Field (MRF) technique to distinguish the agriculture land cover area (cropland and grassland). It gave the satisfactory results for updating in GIS database for the cropland and grassland region [<xref rid="B11" ref-type="bibr">11</xref>]. A new idea of image spectroscopy (IS) and near-infrared spectroscopy (NIRS) was presented by [<xref rid="B12" ref-type="bibr">12</xref>] and it predicted that in the future it will be potentially used in many disciplines like geology, environmental sciences, precision agriculture, urban development, water and soil sciences, and so forth. The objective of this study is to design a simple, concise, and robust framework to classify the five types of land cover data in an absolute natural environment by using spectral and texture features. To accomplish this study, the procedural steps of data collection, image preprocessing, feature extraction, feature selection, feature reduction, and classification are employed for this classification framework.</p></sec><sec id="sec2"><title>2. Study Area</title><p>In this study, involving the technologies, that is, image processing and remote sensing, in land cover classification instead of conventional field surveys is tried. This study is conducted at division Bahawalpur of Punjab province (Pakistan) and covered area is 45,588 square kilometers, which is the areawise largest division of this province, located at 29&#x000b0;23&#x02032;44&#x02032;&#x02032;N and 71&#x000b0;41&#x02032;1&#x02032;&#x02032;E and shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. This study focuses on the land cover assessment, management, and classification through photographic and multispectral radiometric data of this area, which is mostly barren and desert rangeland. It will also help to monitor the land cover changes and estimate the biomass of land vegetation, which is used for forecasting different crops yield assessment.</p></sec><sec id="sec3"><title>3. Material and Methods</title><p>In this study, two types of data are being acquired: (1) photographic data for texture features and (2) radiometric data for remote sensing. Remote sensing data are acquired by using a device named multispectral radiometer (MSR5), CROPSCAN. It is a handheld device, which provides data equivalent to satellite Landsat 5 TM (Thematic Mapper). MSR5 provides an alternative way of acquiring data for remote sensing where satellite or radar datasets are not easily available. Its output data comprises five spectral bands which include visible (blue, green, and red) and infrared and shortwave infrared ranges from 450&#x02009;nm to 1750&#x02009;nm, whereas photographic data are acquired by a digital camera. This study will be based on the analysis of five types of land cover datasets, bare land, desert rangeland, fertile cultivated land, green pasture, and Sutlej river land.</p><sec id="sec3.1"><title>3.1. Photographic Data and Image Acquisition</title><p>The abovementioned five different types of land cover plots have a 43560-square-foot area (1 acre) for each type. Digital photographs of bare land, desert rangeland, fertile cultivated land, green pasture, and Sutlej river land are taken by a digital Nikon camera, model COOLPIX having a 10.1-megapixel resolution, which are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The 15 colored images of each type of land cover with the dimensions of 4288 &#x000d7; 3216 pixels and 24-bit depth of jpg format are acquired. To increase the dataset, four nonoverlapping regions of interests (ROIs) of size (512 &#x000d7; 512) on each image are developed; in this way total (75 &#x000d7; 4 = 300) subimages data are arranged for the analysis. The photographic data are acquired at the altitude of 5 feet from the ground surface of the same specific location where radiometric data are acquired.</p><p>To keep away from the sun shadow effect, the data are acquired at noontime (1.00 pm to 3.00 pm) under a clear sky. At the time of data acquisition, the light intensity is measured by digital Luxmeter MS 6610, MATECH, and described in <xref ref-type="table" rid="tab1">Table 1</xref>.</p></sec><sec id="sec3.2"><title>3.2. Remote Sensing Data Acquisition</title><p>Remote sensing can be defined as the collection of data in the form of radiations about an object taken from a particular distance [<xref rid="B14" ref-type="bibr">16</xref>]. Remote sensing is now playing an important role in many disciplines, that is, environmental sciences, geography, agriculture, forestry, botany, meteorology, oceanography, and earth sciences [<xref rid="B15" ref-type="bibr">17</xref>].</p></sec><sec id="sec3.3"><title>3.3. Multispectral Radiometer (MSR5)</title><p>Multispectral radiometer (MSR5) is made up by CROPSCAN Inc. (USA) for data collection. MSR5 has the quality to provide data similar to satellite Landsat 5 TM. CROPSCAN MSR5 has been already used for the assessment and measurement of crops weeds effect [<xref rid="B16" ref-type="bibr">18</xref>] and vegetation cover estimation and diseases estimation [<xref rid="B17" ref-type="bibr">19</xref>, <xref rid="B18" ref-type="bibr">20</xref>]. For remote sensing, data are acquired 50 MSR scans of each plot at 5 feet's height of land cover surface. Each MSR5 scan contains five wave bands, three visible (blue, green, and red) and two invisible (near infrared and shortwave infrared). Five different types of land cover contain total 250 spectral data instances.</p></sec><sec id="sec3.4"><title>3.4. Spectral Features</title><p>Multispectral radiometer (MSR5) has five different sections of spectrum, including visible, which include the blue, green, red, near infrared (NIR), and shortwave infrared (SWIR) [<xref rid="B19" ref-type="bibr">15</xref>]. MSR5 spectrum consists of different wavelengths, which are measured in nanometer (nm) and described in <xref ref-type="table" rid="tab2">Table 2</xref>.</p><p>This device will be used to collect data at a specific height normal to the land surface. The device that is used in this study is MSR5 with serial number 566. It contains five spectral bands, which are shown in <xref ref-type="table" rid="tab2">Table 2</xref>. In this study, the data acquired by this device in each scan is at the height of 5 feet and it covers land area for each scan that is almost half of the under height which is almost 2.5 square feet's diameter of land cover. The multispectral data acquiring process is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></sec><sec id="sec3.5"><title>3.5. Proposed Methodology</title><p>For this study any special laboratory setup for morphological and color features has not been established, just acquired texture features for photographic data and spectral features for remote sensing MSR5 data. A novel spectra-statistical design framework is proposed for subjective land cover classification. The proposed framework is described in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p>The proposed spectra-statistical design framework describes the functionality of this study that is given below in detail. The proposed methodology has been implemented by using MaZda software version 4.6 on Intel&#x000ae; Core i3 processor 2.4 gigahertz (GHz) with a 64-bit operating system.</p></sec><sec id="sec3.6"><title>3.6. Preprocessing</title><p>Each image has a vast irrelevant area, so prior to further processing the relevant portion of the image was extracted. The extracted relevant portions of the images were converted to grayscale images (8&#x02009;bits) and were stored in bitmap (bmp) format because the software MaZda better works for this format to calculate the statistical texture parameters [<xref rid="B21" ref-type="bibr">21</xref>]. By using image converter software, the contrast of grayscale images was enhanced.</p></sec><sec id="sec3.7"><title>3.7. Feature Extraction</title><p>Transition of an image into its statistical attributes is called feature extraction, which are used for the classification of an image. There are different methods for feature extraction, that is, texture, Gabor, wavelet transform and boundary features, and so forth.</p></sec><sec id="sec3.8"><title>3.8. Texture Features</title><p>Statistical texture features are categorized into the first order, which relates to the intensity of the individual pixels, while the second order relates to the occurrence of neighboring pixels. First-order statistical parameters are directly based on histogram features of an image while second-order parameters are based on the gray level cooccurrence matrix (GLCM). For this study, total 229 statistical texture features are calculated for each region of interest (ROI) by using MaZda software version 4.6. The calculated parameters are grouped as 9 first-order statistical parameters and 11 second-order (Haralick) statistical parameters derived from GLCM in all four directions (0&#x000b0;, 45&#x000b0;, 90&#x000b0;, and 135&#x000b0;) up to 5-pixel distance 220 (11 &#x000d7; 4 &#x000d7; 5) [<xref rid="B22" ref-type="bibr">22</xref>]. It means that each region of interest (ROI) has presented by 229 statistical textural features. Statistically total 300 subimages' data are presented by a 300 &#x000d7; 229 = 68700 dimensional features' vector space.</p></sec><sec id="sec3.9"><title>3.9. Feature Selection</title><p>Feature selection is an important study area where hundred to thousand features space datasets are available. Its objective is to select the most significant features in the employed procedures. Furthermore, reliable classification results are based on a large number of features; usually big data have been required, which is not easily available. It is necessary to reduce the dimensionality of statistical features vector space, which has the capability to discriminate and classify the different types of these land cover classes. These approaches have been used for the selection of the most discriminant set of features. Finally, we can achieve fast and cost-effective classification accuracy based on these selected features. In this study, three features selection approaches, that is, Fisher Coefficient (<italic>F</italic>), Probability of Error (POE) plus Average Correlation Coefficient (ACC), and Mutual Information (MI) Coefficient, have been used to reduce the features vector space. In this study, features selection has been performed through the combined set of the three already mentioned approaches (<italic>F</italic> + PA + MI) for the entire features vector space by using MaZda software. Fisher Coefficient (<italic>F</italic>) [<xref rid="B23" ref-type="bibr">23</xref>] mathematically is described as <disp-formula id="EEq1"><label>(1)</label><mml:math id="M1"><mml:mtable style="T6"><mml:mtr><mml:mtd><mml:maligngroup/><mml:mi>F</mml:mi><mml:malignmark/><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:msubsup><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>K</italic> is between-class variance, <italic>M</italic> is within-class variance, <italic>P</italic>
<sub><italic>a</italic></sub> is probability of feature <italic>a</italic>, and <italic>M</italic>
<sub><italic>a</italic></sub> and <italic>L</italic>
<sub><italic>a</italic></sub> are variance and mean value of feature <italic>a</italic> in the given class.</p><p>Probability of Error plus Average Correlation Coefficient (POE + ACC) [<xref rid="B24" ref-type="bibr">24</xref>] is defined as <disp-formula id="EEq2"><label>(2)</label><mml:math id="M2"><mml:mtable style="T&#x0221e;5"><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mtext>POE</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>number of misclassified samples</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total number of samples</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mtext>minimum</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="9.52998pt" depth="4.43001pt"/><mml:mtext>POE</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mspace width="10pt"/><mml:mo>+</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mspace height="9.52998pt" depth="4.43001pt"/><mml:mtext>correlation</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.52998pt" depth="4.43001pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.52998pt" depth="4.43001pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mtext>minimum</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="17.37094pt" depth="11.504pt"/><mml:mtext>POE</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mspace width="10pt"/><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mspace height="9.52998pt" depth="4.43001pt"/><mml:mtext>correlation</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mspace height="9.52998pt" depth="4.43001pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mspace height="17.37094pt" depth="11.504pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Mutual Information (MI) Coefficient [<xref rid="B25" ref-type="bibr">25</xref>] is explained by the given mathematical relation:<disp-formula id="EEq5"><label>(3)</label><mml:math id="M3"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.57999pt" depth="1.16998pt"/><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mspace height="6.57999pt" depth="1.16998pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.57999pt" depth="0.12pt"/><mml:mi>F</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>C</mml:mi><mml:mspace height="6.57999pt" depth="0.12pt"/></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>It is important to show here that, for this study, as all the 229 calculated features of each image have not been equally significant for land cover classification, MaZda software selects 10 most discriminant features for each method in descending order according to their significance. For analysis, it is observed that combined set of feature selection approaches provide better classification results; in this way total 30 features (10 features by each approach) have been selected [<xref rid="B26" ref-type="bibr">26</xref>]. A set of 30 features has been acquired for further processing. These selected features have been shown with respect to their corresponding three feature selection techniques including<italic> F</italic>, PA, and MI in <xref ref-type="table" rid="tab3">Table 3</xref>.</p><p>No doubt, in <xref ref-type="table" rid="tab3">Table 3</xref>, the MI based selected features are highly correlated such as &#x0201c;inverse difference moment&#x0201d; but they have different interpixel distance and direction and due to this difference, their calculated values are also different. For each pixel distance (<italic>d</italic>) and angular direction value (<italic>&#x003b8;</italic>), the intensive nature of computation is involved and acquired different texture feature values for the same parameter, that is, &#x0201c;inverse difference moment.&#x0201d; For this study, we have taken <italic>d</italic> = 1-, 2-, 3-, 4-, and 5-pixel distance with angle <italic>&#x003b8;</italic> = 0&#x000b0;, 45&#x000b0;, 90&#x000b0;, and 135&#x000b0;. Therefore, for this reason, we cannot ignore any value of the given texture features. Each value (MI base texture features) actually describes the land cover dataset into its own dimension or direction and as a whole these features reveal the entire texture patterns. It is reported by different researchers [<xref rid="B10" ref-type="bibr">10</xref>, <xref rid="B11" ref-type="bibr">11</xref>] that five different control features such as window size, texture derivative(s), input channel (i.e., spectral channel to measure the texture), quantization level of output channel (8&#x02009;bits, 16&#x02009;bits, and 32&#x02009;bits), and the spatial components (i.e., interpixel distance and angle during cooccurrence matrix computation) play a vital role during the analysis of GLCM texture features.</p></sec><sec id="sec3.10"><title>3.10. Feature Reduction</title><p>Feature reduction techniques are also called feature projection. In feature reduction, the original feature space of selected features is transformed to a new space having lower dimensionality. It is also called projection space in which data are clustered in respective classes. These feature projection techniques include the linear discriminant analysis (LDA), principal component analysis (PCA), and nonlinear discriminant analysis (NDA). For such purpose, usually PCA, LDA, and NDA approaches are employed. Features reduction techniques maintain the actual structure of the data as much as possible while reducing the number of dimensions. Thus in the reduced feature space, the execution time with cost is also reduced and we get smaller dimension space. It is observed that the obtained results are approximately reliable to the original data space. Before starting the classification, the data are standardized to reduce the impact of undesirable variation within the data due to exceptions and other factors by applying the following statistical equation:<disp-formula id="EEq6"><label>(4)</label><mml:math id="M4"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mo>&#x02212;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>K</italic>
<sub><italic>i</italic></sub>&#x02032; is the consistent value of the <italic>i</italic>th feature and <italic>i</italic> = 1, 2, 3,&#x02026;, <italic>n</italic>. <italic>K</italic>
<sub><italic>i</italic></sub> is original feature value, <inline-formula><mml:math id="M5"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is mean feature value, and <italic>&#x003c3;</italic> is standard deviation.</p><p>The above discussed feature selection techniques (<italic>F</italic> + PA + MI) only select the significant features but do not quantify how much these can be classified. To get the feature data projection, the selected 30 features' data are deployed to nonlinear discriminant analysis (NDA) available in B11 software integrated with MaZda [<xref rid="B27" ref-type="bibr">27</xref>]. In this technique there are 3 layers (input layer and the first and second hidden layer and output layer) of processing elements (neurons) that are presented. NDA can be described by logistic function. Its value is equal to 0.5 for <italic>&#x003b1;</italic> = 0, and it changes smoothly from 0 to 1 for <italic>&#x003b1;</italic> varying from large negative to large positive values:<disp-formula id="EEq7"><label>(5)</label><mml:math id="M6"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.53pt" depth="0.12pt"/><mml:mi>&#x003b1;</mml:mi><mml:mspace height="4.53pt" depth="0.12pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x02061;</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>If <italic>X</italic> is the feature vector and it is the input to the artificial neural network (ANN), the input terminals are equal to <italic>N</italic>
<sub><italic>x</italic></sub>. Vector <italic>Y</italic> is the output of ANN, whose dimension <italic>N</italic>
<sub><italic>y</italic></sub> is equal to the number of types in the dataset. Thus, the ANN had <italic>N</italic>
<sub><italic>y</italic></sub> output terminals:<disp-formula id="EEq8"><label>(6)</label><mml:math id="M7"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="18.95493pt" depth="12.015pt"/><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mspace height="18.95493pt" depth="12.015pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Now, here <italic>n</italic> = 1, 2, 3,&#x02026;, <italic>N</italic>
<sub><italic>y</italic></sub>. Consider<disp-formula id="EEq9"><label>(7)</label><mml:math id="M8"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="17.75493pt" depth="13.359pt"/><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mspace height="17.75493pt" depth="13.359pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Here <italic>k</italic> = 1, 2, 3,&#x02026;, <italic>N</italic>
<sub><italic>g</italic></sub>. Consider<disp-formula id="EEq10"><label>(8)</label><mml:math id="M9"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="17.75493pt" depth="11.644pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mspace height="17.75493pt" depth="11.644pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Here <italic>j</italic> = 1, 2, 3,&#x02026;, <italic>N</italic>
<sub><italic>h</italic></sub>.</p><p>Supervised learning methods are based on input patterns and correct classes where they belong to {<italic>x</italic>
<sub><italic>i</italic></sub>, <italic>d</italic>
<sub><italic>i</italic></sub>}, where <italic>i</italic> = 1, 2, 3,&#x02026;, <italic>M</italic>. For this purpose, the following errors function is calculated:<disp-formula id="EEq11"><label>(9)</label><mml:math id="M10"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="8.07999pt" depth="2.98001pt"/><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.57999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mspace height="6.57999pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="8.07999pt" depth="2.98001pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>While for MSR5 datasets, linear discriminant analysis (LDA) gives the better results for features data clustered and projection. Let <italic>x</italic>
<sub><italic>i</italic></sub>
<sup>(<italic>k</italic>)</sup> denote the <italic>i</italic>th pattern in class <italic>i</italic>, where <italic>i</italic> = 1, 2, 3,&#x02026;, <italic>M</italic>
<sub><italic>k</italic></sub>, and <italic>k</italic> = 1, 2, 3,&#x02026;, <italic>N</italic>
<sub><italic>c</italic></sub>. Define the within-class scatter matrix <italic>C</italic>
<sub><italic>W</italic></sub> as<disp-formula id="EEq12"><label>(10)</label><mml:math id="M11"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mspace height="9.75598pt" depth="2.984pt"/></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mspace height="9.75598pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>M</italic>
<sub><italic>k</italic></sub> is the mean vector of class <italic>k</italic>. Similarly, define the between-class scatter matrix <italic>C</italic>
<sub><italic>B</italic></sub> as<disp-formula id="EEq13"><label>(11)</label><mml:math id="M12"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="0.23pt"/><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mi>U</mml:mi><mml:mspace height="9.75598pt" depth="0.23pt"/></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="0.23pt"/><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mi>U</mml:mi><mml:mspace height="9.75598pt" depth="0.23pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Here <italic>M</italic> is the mean vector of the shared data. The total scatter matrix is the objective of LDA and through this we can get a linear transform matrix:<disp-formula id="EEq14"><label>(12)</label><mml:math id="M13"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mi>U</mml:mi><mml:mspace height="9.75598pt" depth="2.984pt"/></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.75598pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mi>U</mml:mi><mml:mspace height="9.75598pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The proposed NDA architecture is given for both types of dataset in Tables <xref ref-type="table" rid="tab4">4</xref> and <xref ref-type="table" rid="tab5">5</xref>.</p></sec><sec id="sec3.11"><title>3.11. Classification</title><p>For this work, we have applied supervised classification artificial neural network (ANN). This classifier is employed due to two reasons; first of all we have supervised data (due to five land covers) and it is discussed by [<xref rid="B28" ref-type="bibr">28</xref>] that ANN is a strong and efficient technique for noisy data and also for those datasets which are acquired in natural open environment. The implemented classifier based on feed forward approach with a single hidden layer of sigmoidal neurons is shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. If <italic>x</italic> is the number of deployed input feature vectors to ANN classifier then input terminals are equal to <italic>N</italic>
<sub><italic>x</italic></sub>. The output feature vector is <italic>y</italic>, whose dimensions <italic>N</italic>
<sub><italic>y</italic></sub> are determined by the number of classes to be classified. Thus, the ANN has <italic>N</italic>
<sub><italic>y</italic></sub> output terminals:<disp-formula id="EEq15"><label>(13)</label><mml:math id="M14"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="17.75493pt" depth="13.359pt"/><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mspace height="17.75493pt" depth="13.359pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>k</italic> = 1, 2, 3,&#x02026;, <italic>N</italic>
<sub><italic>y</italic></sub> and the outputs of the hidden layers are given as<disp-formula id="EEq16"><label>(14)</label><mml:math id="M15"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mspace height="17.75493pt" depth="11.644pt"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mspace height="17.75493pt" depth="11.644pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>We see here that <italic>j</italic> = 1, 2, 3,&#x02026;, <italic>N</italic>
<sub><italic>h</italic></sub>.</p><p>For training and testing purpose, the weight coefficients are adjusted and how much actual output value <italic>y</italic> is close to the desired output <italic>d</italic> is observed.</p><p>Supervised training techniques are based on input patterns and correct categories where they belong to {<italic>x</italic>
<sub><italic>i</italic></sub>, <italic>d</italic>
<sub><italic>i</italic></sub>}, where <italic>i</italic> = 1, 2, 3,&#x02026;, <italic>M</italic>; then following is the error function which is reduced by changing of weights <italic>v</italic> and <italic>w</italic>:<disp-formula id="EEq17"><label>(15)</label><mml:math id="M16"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="8.07999pt" depth="2.98001pt"/><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.57999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mspace height="6.57999pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mspace height="8.07999pt" depth="2.98001pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p></sec></sec><sec id="sec4"><title>4. Results and Discussion</title><sec id="sec4.1"><title>4.1. Photographic Data</title><p>For photographic dataset, the first attempt for features data selection and reduction is performed by individual feature selection techniques like Fisher (<italic>F</italic>), Probability of Error plus Average Correlation Coefficient (POE + ACC), and Mutual Information (MI) techniques on the basis of ROIs (64 &#x000d7; 64), (128 &#x000d7; 128), (256 &#x000d7; 256), and (512 &#x000d7; 512). Now the selected features are deployed for raw data analysis (RDA), principal component analysis (PCA), linear discriminant analysis (LDA), and nonlinear discriminant analysis (NDA) projection spaces to verify the capability of data clustering. Here, better data clustering based on the NDA approach has been received as compared to the other three approaches. It is observed that the discussed above first three ROIs do not give satisfactory results. They have received less than 70% feature projection accuracy based on these three ROIs, which are not acceptable, whereas, for ROI (512 &#x000d7; 512), we received 80%, 84%, and 88.324% accuracy by using<italic> F</italic>, PA, and MI, respectively, in projection space of NDA. Because it has been reported by a number of researchers that usually the classification is proportional to the number of features deployed [<xref rid="B28" ref-type="bibr">28</xref>], the same strategy was implemented to have better results. For this purpose, the authors merged the selected features by the already above discussed three approaches (<italic>F</italic> + PA + MI). In this way, a set of 30 features (10 features of each selection method) is received by combining these three approaches on ROI (512 &#x000d7; 512). Then these 30 features were deployed to RDA, PCA, LDA, and NDA by using the<italic> K</italic>-fold (80-20) cross validation method. It has been observed that NDA has given better data clustering and projection accuracy 99.64% as compared to the other three features reduction techniques. These results are summarized in <xref ref-type="table" rid="tab6">Table 6</xref>.</p><p>The statistical texture data analyses of RDA, PCA, LDA, and NDA are shown in <xref ref-type="table" rid="tab6">Table 6</xref>. From this table, it is clear that NDA leads the best data projection accuracy of 99.64% as compared to the remaining three approaches including RDA, PCA, and LDA. <xref ref-type="fig" rid="fig6"> Figure 6</xref> represents the photographic features data clustering of five input land cover classes in NDA projection space.</p><p>It is observed by different researchers [<xref rid="B27" ref-type="bibr">27</xref>, <xref rid="B28" ref-type="bibr">28</xref>] that feature reduction techniques including raw data analysis (RDA), linear discriminant analysis (LDA), and principal component analysis (PCA) performed well on linearly separable data because PCA and LDA use linear transformation of the input data. These techniques have the ability for feature compression. The most expressive features (MEF) are obtained by PCA and the most discriminating features (MDF) are found from LDA technique. These features vectors have not as many features as the original feature vectors space does. Due to this reason, they cannot help in classification of linearly nonseparable data. Such data need hypersurfaces instead of hyperplanes for data clusters separation. That is why nonlinear discriminant analysis (NDA) is used for nonlinear transformation of the feature vectors, such that the input data are projected on a space (probably of lower dimensionality as compared to PCA and LDA) in which they become linearly separated. In this study, this technique is implemented by using a feed forward artificial neural network (ANN) with two hidden layers of sigmoid-type neurons. To verify the capability of data clustering based on selected features of complex nonlinear datasets, nonlinear discriminant analysis (NDA) is the best approach. Therefore, we have employed the same approach; moreover, B11 software has also a number of options by which NDA may be configured to have the best result. Nonlinear discriminant analysis (NDA) graph shows the properly clustered data into its five appropriate classes. Data clustered graph is shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p><p>By the implementation of (ANN:<italic> n</italic>-class) training and testing, available in B11 integrated with MaZda software, is performed to verify the validity of classifier. For this purpose, a cross validation<italic> K</italic>-fold (80-20) method is used. For training purpose, 48 data instances of each ROIs size (512 &#x000d7; 512) from land cover type are used. Total 240 data instances out of 300 were used for training with each iteration. Testing is performed on 60 data instances (12 data instances from each land cover type). Here an accuracy of 100% is acquired when the classifier is trained over the architecture setting already discussed above in <xref ref-type="sec" rid="sec3.11">Section 3.11</xref> and an average classification accuracy of 91.334% is obtained when the classifier is tested for photographic data. So, five types of land cover data are classified properly by using (ANN:<italic> n</italic>-class) method. Statistical texture data are shown in <xref ref-type="table" rid="tab7">Table 7</xref>.</p><p>The performances of the classifier in testing phase for different classes are summarized in confusion matrix, <xref ref-type="table" rid="tab8">Table 8</xref>. Total 300 data instances of photographic data (60 data instances of each land cover) are shown in the appropriate five different classes.</p><p>Here confusion matrix, <xref ref-type="table" rid="tab8">Table 8</xref>, for photographic data is presented of five different land cover types by graphical way in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p></sec><sec id="sec4.2"><title>4.2. Spectral Data</title><p>As we have already mentioned, a scene is completely explored based on five spectral bands, blue, green, and red, near infrared, and shortwave infrared acquired by MSR5. The whole data (250 scans) are acquired by MSR5 and deployed to RDA, PCA, LDA, and NDA to verify the validity of data projection. Now, data projection accuracy of 98.7% for RDA, 98.4% for PCA, 99.5% for LDA, and 99.4% for NDA is received. It is clear that the best feature projection accuracy is received by LDA approach as shown in <xref ref-type="table" rid="tab9">Table 9</xref>. The results of data projection are presented in <xref ref-type="table" rid="tab9">Table 9</xref> in detail.</p><p>Multispectral features data analyses of RDA, PCA, LDA, and NDA are shown in <xref ref-type="table" rid="tab9">Table 9</xref>; this shows that LDA outperforms the others and gives 99.5% feature data projection accuracy. For feature reduction techniques, feature data projection graph of MSR5 is shown in <xref ref-type="fig" rid="fig9">Figure 9</xref>.</p><p>Linear discriminant analysis (LDA) graph shows the properly clustered data into its five appropriate classes as compared to other employed reduction techniques. Data cluster graph is shown in <xref ref-type="fig" rid="fig10">Figure 10</xref>.</p><p>For the purpose of training and testing, artificial neural network (ANN:<italic> n</italic>-class) classifier has been employed; the same<italic> K</italic>-fold (80-20) cross validation method is also used for multispectral data classification. A dataset of two hundred scans of five multispectral parameters, blue, green, and red, near infrared, and shortwave infrared, is deployed for (ANN:<italic> n</italic>-class) training purpose with the same architecture settings as mentioned above in <xref ref-type="sec" rid="sec3.11">Section 3.11</xref>. The output training results for multispectral data are summarized in <xref ref-type="table" rid="tab10">Table 10</xref> and are represented graphically in <xref ref-type="fig" rid="fig11">Figure 11</xref>. Similarly, under the same architecture setting as discussed earlier in classification <xref ref-type="sec" rid="sec3.11">Section 3.11</xref>, ANN classifier is tested by deploying 50 disjoints data instances (10 data instances of each land cover type) of the selected five multispectral features of land cover types. MSR5 data are shown in <xref ref-type="table" rid="tab10">Table 10</xref>.</p><p>ANN classifier revealed very promising results during this training and testing phase. An average classification accuracy of 100% has been achieved when the classifier has been trained over this data. Similarly, an average classification accuracy of 96.40% has been achieved when classifier is tested. So, five land cover types' data are classified properly by using (ANN:<italic> n</italic>-class) methods. Confusion matrix table of multispectral data classification by using (ANN:<italic> n</italic>-class) method of five different land cover types is shown in <xref ref-type="table" rid="tab11">Table 11</xref>.</p><p>Now, confusion matrix graph for MSR5 data is presented by using the (ANN:<italic> n</italic>-class) method of five different land cover types which are shown in <xref ref-type="fig" rid="fig11">Figure 11</xref>.</p><p>When comparing both multispectral and statistical texture data classification accuracies, it is observed that multispectral accuracy result is better 96.40% as compared to statistical texture data result which is 91.334%. A comparison graph between multispectral and statistical texture data is shown in <xref ref-type="fig" rid="fig12">Figure 12</xref>.</p><p>The reason for this classification accuracy difference is that statistical analysis outperforms other methods on fine texture as compared to coarse texture. This is the reason texture data classification accuracy is lower than multispectral data [<xref rid="B29" ref-type="bibr">29</xref>, <xref rid="B30" ref-type="bibr">30</xref>]. In this study, the photographic data are taken at 5 feet's height so the areas under these photographs are not equally covered and distributed; besides these ROIs also play an important role for classification [<xref rid="B31" ref-type="bibr">31</xref>]; as ROIs size increased then accuracy is also observed better. It is the fact that if photographs are taken on more height and area under the region is covered maximum then classification accuracy can be improved. Secondly it is observed that almost (5% to 6%) better classification results are obtained by the remote sensing MSR5 data as compared to photographic data (400&#x02009;nm to 700&#x02009;nm) because MSR5 data comprises visible (400&#x02009;nm to 700&#x02009;nm) and invisible near-infrared (NIR) and shortwave infrared (SWIR) (790&#x02009;nm to 1750&#x02009;nm) wavelength. Data acquisition techniques with normalization and standardization of data with classifier may also impact on results for better classification. By implementing these sophisticated quantitative parameters rather than conventional qualitative parameters, they can accurately classify the different types of land cover data. Generally, the proposed methodology provides a novel technique for mapping and classifying land cover data by using multispectral and digital photographic data.</p></sec></sec><sec id="sec5"><title>5. Conclusions</title><p>In this study, five types of land cover data are classified by using quantitative parameters instead of conventional qualitative parameters and an accuracy of 96.40% for spectral dataset and 91.334% for statistical texture dataset is achieved. Up to what extent these classes may be classified into their appropriate patterns classes is a difficult task and it is also a verification of intra- and interclassification pattern features of these five land cover data types. Five spectral and nine first-order with eleven second-order cooccurrence matrix features are used to test the land cover datasets which made this framework novel and more reliable and robust than other land classification frameworks in which morphological, size, color, and other geometry features have been used. Artificial neural network is used very effectively for the classification of these five different land cover types such as fertile cultivated land, green pasture, desert rangeland, bare land, and Sutlej river land. In the future, we may enhance this study for hyperspectral data of crop growth and yield assessment. We can also take results with new technique of data fusion by combining MSR5 data with digital photographic data for considering different environmental factors like rain, usage of fertilizers, dry weather, and soil and air moisture effects.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors are highly thankful to the Department of Computer Science &#x00026; IT, The Islamia University of Bahawalpur, Pakistan, and especially thankful to Mr. Dell Nantt, CROPSCAN Corporation, Minnesota, USA, for his technical support. They are also very thankful to their research colleagues at the Virtual University of Pakistan, Lahore, and especially thankful to Dr. Muhammad Shahid, Associate Professor, SE College Bahawalpur, and Dr. Farooq Ahmad, Associate Professor (CIIT Lahore), for their support in the statistical analysis and write-up of the paper.</p></ack><sec><title>Competing Interests</title><p>The authors declare no competing interests.</p></sec><sec><title>Authors' Contributions</title><p>The main concept and experimental work was performed by Salman Qadri and Dr. Dost Muhammad Khan made critical revisions and approved the final version. All authors reviewed and approved the final paper.</p></sec><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walter-Shea</surname><given-names>E. A.</given-names></name><name><surname>Blad</surname><given-names>B. L.</given-names></name><name><surname>Hays</surname><given-names>C. J.</given-names></name><name><surname>Mesarch</surname><given-names>M. A.</given-names></name><name><surname>Deering</surname><given-names>D. W.</given-names></name><name><surname>Middleton</surname><given-names>E. M.</given-names></name></person-group><article-title>Biophysical properties affecting vegetative canopy reflectance and absorbed photosynthetically active radiation at the FIFE site</article-title><source><italic>Journal of Geophysical Research</italic></source><year>1992</year><volume>97</volume><issue>D17</issue><fpage>18925</fpage><lpage>18934</lpage><pub-id pub-id-type="doi">10.1029/92JD00656</pub-id><pub-id pub-id-type="other">2-s2.0-0027075790</pub-id></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rundquist</surname><given-names>B. C.</given-names></name></person-group><source><italic>Fine-scale spatial and temporal variation in the relationship between spectral reflectance and a prairie vegetation canopy [Ph.D. thesis]</italic></source><year>2000</year><publisher-loc>Manhattan, Kan, USA</publisher-loc><publisher-name>Kansas State University</publisher-name></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foody</surname><given-names>G. M.</given-names></name></person-group><article-title>Status of land cover classification accuracy assessment</article-title><source><italic>Remote Sensing of Environment</italic></source><year>2002</year><volume>80</volume><issue>1</issue><fpage>185</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/S0034-4257(01)00295-4</pub-id><pub-id pub-id-type="other">2-s2.0-0036213079</pub-id></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Parihar</surname><given-names>J. S.</given-names></name><name><surname>Oza</surname><given-names>M. P.</given-names></name></person-group><article-title>Fasal: an integrated approach for crop assessment and production forecasting</article-title><conf-name>Proceedings of the Asia-Pacific Remote Sensing Symposium</conf-name><conf-date>2006</conf-date><publisher-name>International Society for Optics and Photonics</publisher-name><fpage>641101</fpage><lpage>641113</lpage></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="book"><collab>Statistics</collab><source><italic>F. B.O. Province Census Report of Sindh</italic></source><year>2000</year><publisher-loc>Islamabad, Pakistan</publisher-loc><publisher-name>Government of Pakistan</publisher-name></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kureshy</surname><given-names>K. U.</given-names></name></person-group><source><italic>In Geography of Pakistan</italic></source><year>1995</year><publisher-loc>Lahore, Pakistan</publisher-loc><publisher-name>National Book Services</publisher-name></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blaschke</surname><given-names>T.</given-names></name><name><surname>Lang</surname><given-names>S.</given-names></name><name><surname>Lorup</surname><given-names>E.</given-names></name><name><surname>Strobl</surname><given-names>J.</given-names></name><name><surname>Zeil</surname><given-names>P.</given-names></name></person-group><article-title>Object-oriented image processing in an integrated gis/remote sensing environment and perspectives for environmental applications</article-title><source><italic>Environmental Information for Planning, Politics and the Public</italic></source><year>2000</year><volume>2</volume><fpage>555</fpage><lpage>570</lpage></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname><given-names>L.</given-names></name><name><surname>Rottensteiner</surname><given-names>F.</given-names></name><name><surname>Heipke</surname><given-names>C.</given-names></name></person-group><article-title>A two-layer conditional random field model for simultaneous classification of land cover and land use</article-title><source><italic>ISPRS-International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</italic></source><year>2014</year><volume>1</volume><fpage>17</fpage><lpage>24</lpage></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoberg</surname><given-names>T.</given-names></name><name><surname>Rottensteiner</surname><given-names>F.</given-names></name><name><surname>Heipke</surname><given-names>C.</given-names></name></person-group><article-title>Context models for crf-based classification of multitemporal remote sensing data</article-title><source><italic>ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences</italic></source><year>2012</year><volume>7</volume><fpage>128</fpage><lpage>134</lpage></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caridade</surname><given-names>C. M. R.</given-names></name><name><surname>Mar&#x000e7;al</surname><given-names>A. R. S.</given-names></name><name><surname>Mendon&#x000e7;a</surname><given-names>T.</given-names></name></person-group><article-title>The use of texture for image classification of black &#x00026; white air photographs</article-title><source><italic>International Journal of Remote Sensing</italic></source><year>2008</year><volume>29</volume><issue>2</issue><fpage>593</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1080/01431160701281015</pub-id><pub-id pub-id-type="other">2-s2.0-37249010054</pub-id></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helmholz</surname><given-names>P.</given-names></name><name><surname>Rottensteiner</surname><given-names>F.</given-names></name><name><surname>Heipke</surname><given-names>C.</given-names></name></person-group><article-title>Semi-automatic verification of cropland and grassland using very high resolution mono-temporal satellite images</article-title><source><italic>ISPRS Journal of Photogrammetry and Remote Sensing</italic></source><year>2014</year><volume>97</volume><fpage>204</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2014.09.008</pub-id><pub-id pub-id-type="other">2-s2.0-84907756097</pub-id></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dor</surname><given-names>E. B.</given-names></name></person-group><article-title>Soil spectral imaging: moving from proximal sensing to spatial quantitative domain</article-title><source><italic>ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</italic></source><year>2012</year><volume>1</volume><fpage>67</fpage><lpage>70</lpage></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="other"><comment>Map of world, August 2014, <ext-link ext-link-type="uri" xlink:href="http://www.mapsofworld.com/pakistan/">http://www.mapsofworld.com/pakistan/</ext-link></comment></element-citation></ref><ref id="B20"><label>14</label><element-citation publication-type="other"><comment>April 2015, <ext-link ext-link-type="uri" xlink:href="http://www.cropscan.com//msr.html">http://www.cropscan.com//msr.html</ext-link></comment></element-citation></ref><ref id="B19"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szczypi&#x00144;ski</surname><given-names>P. M.</given-names></name><name><surname>Strzelecki</surname><given-names>M.</given-names></name><name><surname>Materka</surname><given-names>A.</given-names></name><name><surname>Klepaczko</surname><given-names>A.</given-names></name></person-group><article-title>MaZda&#x02014;a software package for image texture analysis</article-title><source><italic>Computer Methods and Programs in Biomedicine</italic></source><year>2009</year><volume>94</volume><issue>1</issue><fpage>66</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2008.08.005</pub-id><pub-id pub-id-type="other">2-s2.0-60049100264</pub-id><pub-id pub-id-type="pmid">18922598</pub-id></element-citation></ref><ref id="B14"><label>16</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>E. R.</given-names></name></person-group><source><italic>Machine Vision: Theory, Algorithms, Practicalities</italic></source><year>2004</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="B15"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panigrahy</surname><given-names>S.</given-names></name><name><surname>Upadhyay</surname><given-names>G.</given-names></name><name><surname>Ray</surname><given-names>S. S.</given-names></name><name><surname>Parihar</surname><given-names>J. S.</given-names></name></person-group><article-title>Mapping of cropping system for the indo-gangetic plain using multi-date SPOT NDVI-VGT data</article-title><source><italic>Journal of the Indian Society of Remote Sensing</italic></source><year>2010</year><volume>38</volume><issue>4</issue><fpage>627</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1007/s12524-011-0059-5</pub-id><pub-id pub-id-type="other">2-s2.0-79952901354</pub-id></element-citation></ref><ref id="B16"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsirogiannis</surname><given-names>I. L.</given-names></name><name><surname>Katsoulas</surname><given-names>N.</given-names></name><name><surname>Savvas</surname><given-names>D.</given-names></name><name><surname>Karras</surname><given-names>G.</given-names></name><name><surname>Kittas</surname><given-names>C.</given-names></name></person-group><article-title>Relationships between reflectance and water status in a greenhouse rocket (<italic>Eruca sativa</italic> Mill.) cultivation</article-title><source><italic>European Journal of Horticultural Science</italic></source><year>2013</year><volume>78</volume><issue>6</issue><fpage>275</fpage><lpage>282</lpage><pub-id pub-id-type="other">2-s2.0-84890072606</pub-id></element-citation></ref><ref id="B17"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>J.</given-names></name><name><surname>Clay</surname><given-names>S. A.</given-names></name><name><surname>Clay</surname><given-names>D. E.</given-names></name><name><surname>Dalsted</surname><given-names>K.</given-names></name></person-group><article-title>Detecting weed-free and weed-infested areas of a soybean field using near-infrared spectral data</article-title><source><italic>Weed Science</italic></source><year>2009</year><volume>52</volume><issue>4</issue><fpage>642</fpage><lpage>648</lpage></element-citation></ref><ref id="B18"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vrindts</surname><given-names>E.</given-names></name><name><surname>Reyniers</surname><given-names>M.</given-names></name><name><surname>Darius</surname><given-names>P.</given-names></name><etal/></person-group><article-title>Analysis of soil and crop properties for precision agriculture for winter wheat</article-title><source><italic>Biosystems Engineering</italic></source><year>2003</year><volume>85</volume><issue>2</issue><fpage>141</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/S1537-5110(03)00040-0</pub-id><pub-id pub-id-type="other">2-s2.0-0038039219</pub-id></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>R. C.</given-names></name><name><surname>Woods</surname><given-names>R. E.</given-names></name><name><surname>Eddins</surname><given-names>S. L.</given-names></name></person-group><source><italic>Digital Image Processing Using Matlab</italic></source><year>2004</year><publisher-loc>New Delhi, India</publisher-loc><publisher-name>Pearson Education</publisher-name></element-citation></ref><ref id="B22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haralick</surname><given-names>R. M.</given-names></name><name><surname>Shanmugam</surname><given-names>K.</given-names></name><name><surname>Dinstein</surname><given-names>I.</given-names></name></person-group><article-title>Textural features for image classification</article-title><source><italic>IEEE Transactions on Systems, Man and Cybernetics</italic></source><year>1973</year><volume>3</volume><issue>6</issue><fpage>610</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1109/tsmc.1973.4309314</pub-id><pub-id pub-id-type="other">2-s2.0-0015680481</pub-id></element-citation></ref><ref id="B23"><label>23</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sch&#x000fc;rmann</surname><given-names>J.</given-names></name></person-group><source><italic>Pattern Classification: A Unified View of Statistical and Neural Approaches</italic></source><year>1996</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Wiley Online Library</publisher-name><pub-id pub-id-type="other">MR1376369</pub-id></element-citation></ref><ref id="B24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dash</surname><given-names>M.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name></person-group><article-title>Feature selection for classification</article-title><source><italic>Intelligent Data Analysis</italic></source><year>1997</year><volume>1</volume><issue>3</issue><fpage>131</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.3233/IDA-1997-1302</pub-id><pub-id pub-id-type="other">2-s2.0-0013326060</pub-id></element-citation></ref><ref id="B25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tourassi</surname><given-names>G. D.</given-names></name><name><surname>Frederick</surname><given-names>E. D.</given-names></name><name><surname>Markey</surname><given-names>M. K.</given-names></name><name><surname>Floyd</surname><given-names>C. E.</given-names><suffix>Jr.</suffix></name></person-group><article-title>Application of the mutual information criterion for feature selection in computer-aided diagnosis</article-title><source><italic>Medical Physics</italic></source><year>2001</year><volume>28</volume><issue>12</issue><fpage>2394</fpage><lpage>2402</lpage><pub-id pub-id-type="doi">10.1118/1.1418724</pub-id><pub-id pub-id-type="other">2-s2.0-0035661275</pub-id><pub-id pub-id-type="pmid">11797941</pub-id></element-citation></ref><ref id="B26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ojala</surname><given-names>T.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name><name><surname>Harwood</surname><given-names>D.</given-names></name></person-group><article-title>A comparative study of texture measures with classification based on feature distributions</article-title><source><italic>Pattern Recognition</italic></source><year>1996</year><volume>29</volume><issue>1</issue><fpage>51</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/0031-3203(95)00067-4</pub-id><pub-id pub-id-type="other">2-s2.0-0029669420</pub-id></element-citation></ref><ref id="B27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerski</surname><given-names>R. A.</given-names></name><name><surname>Schad</surname><given-names>L. R.</given-names></name></person-group><article-title>The use of reticulated foam in texture test objects for magnetic resonance imaging</article-title><source><italic>Magnetic Resonance Imaging</italic></source><year>1998</year><volume>16</volume><issue>9</issue><fpage>1139</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1016/S0730-725X(98)00096-4</pub-id><pub-id pub-id-type="other">2-s2.0-0032411906</pub-id><pub-id pub-id-type="pmid">9839997</pub-id></element-citation></ref><ref id="B28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>S. C.</given-names></name><name><surname>Pu</surname><given-names>J.</given-names></name><name><surname>Zheng</surname><given-names>B.</given-names></name></person-group><article-title>Improving performance of computer-aided detection scheme by combining results from two machine learning classifiers</article-title><source><italic>Academic Radiology</italic></source><year>2009</year><volume>16</volume><issue>3</issue><fpage>266</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1016/j.acra.2008.08.012</pub-id><pub-id pub-id-type="other">2-s2.0-59449110510</pub-id><pub-id pub-id-type="pmid">19201355</pub-id></element-citation></ref><ref id="B29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zapotoczny</surname><given-names>P.</given-names></name></person-group><article-title>Discrimination of wheat grain varieties using image analysis and neural networks. Part I. Single kernel texture</article-title><source><italic>Journal of Cereal Science</italic></source><year>2011</year><volume>54</volume><issue>1</issue><fpage>60</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.jcs.2011.02.012</pub-id><pub-id pub-id-type="other">2-s2.0-79960745980</pub-id></element-citation></ref><ref id="B30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alajlan</surname><given-names>N.</given-names></name><name><surname>Bazi</surname><given-names>Y.</given-names></name><name><surname>Melgani</surname><given-names>F.</given-names></name><name><surname>Yager</surname><given-names>R. R.</given-names></name></person-group><article-title>Fusion of supervised and unsupervised learning for improved classification of hyperspectral images</article-title><source><italic>Information Sciences</italic></source><year>2012</year><volume>217</volume><fpage>39</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2012.06.031</pub-id><pub-id pub-id-type="other">2-s2.0-84865577207</pub-id></element-citation></ref><ref id="B31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahid</surname><given-names>M.</given-names></name><name><surname>Naweed</surname><given-names>M. S.</given-names></name><name><surname>Qadri</surname><given-names>S.</given-names></name><name><surname>Mutiullah</surname></name><name><surname>Rehmani</surname><given-names>E. A.</given-names></name></person-group><article-title>Varietal discrimination of wheat seeds by machine vision approach</article-title><source><italic>Life Science Journal</italic></source><year>2014</year><volume>11</volume><issue>6</issue><fpage>245</fpage><lpage>252</lpage></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Geographic location of the study area. The red highlighted left side on the map represents the study area [<xref rid="B13" ref-type="bibr">13</xref>].</p></caption><graphic xlink:href="BMRI2016-8797438.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Five land cover images and Luxmeter.</p></caption><graphic xlink:href="BMRI2016-8797438.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>MSR5 data acquiring process for each scan [<xref rid="B20" ref-type="bibr">14</xref>].</p></caption><graphic xlink:href="BMRI2016-8797438.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Proposed spectra-statistical design framework for land cover classification.</p></caption><graphic xlink:href="BMRI2016-8797438.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Implemented ANN classifier model [<xref rid="B19" ref-type="bibr">15</xref>].</p></caption><graphic xlink:href="BMRI2016-8797438.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Digital photographic features data projection graph.</p></caption><graphic xlink:href="BMRI2016-8797438.006"/></fig><fig id="fig7" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Statistical texture features data clustered results for NDA.</p></caption><graphic xlink:href="BMRI2016-8797438.007"/></fig><fig id="fig8" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Confusion graph for statistical texture test data classification.</p></caption><graphic xlink:href="BMRI2016-8797438.008"/></fig><fig id="fig9" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Multispectral feature data projection graph.</p></caption><graphic xlink:href="BMRI2016-8797438.009"/></fig><fig id="fig10" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Multispectral features data clustered result for LDA.</p></caption><graphic xlink:href="BMRI2016-8797438.010"/></fig><fig id="fig11" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Confusion graph for multispectral test data classification.</p></caption><graphic xlink:href="BMRI2016-8797438.011"/></fig><fig id="fig12" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Multispectral and statistical texture data graph.</p></caption><graphic xlink:href="BMRI2016-8797438.012"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Time and sunlight intensity information.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Sr. number</th><th align="center" rowspan="1" colspan="1">Land cover type</th><th align="center" rowspan="1" colspan="1">Time</th><th align="center" rowspan="1" colspan="1">Sunshine intensity</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">(1)</td><td align="center" rowspan="1" colspan="1">Bare land</td><td align="center" rowspan="1" colspan="1">1.00&#x02009;pm</td><td align="center" rowspan="1" colspan="1">34300 Lux</td></tr><tr><td align="left" rowspan="1" colspan="1">(2)</td><td align="center" rowspan="1" colspan="1">Desert rangeland</td><td align="center" rowspan="1" colspan="1">2.00&#x02009;pm</td><td align="center" rowspan="1" colspan="1">34000 Lux</td></tr><tr><td align="left" rowspan="1" colspan="1">(3)</td><td align="center" rowspan="1" colspan="1">Fertile cultivated land</td><td align="center" rowspan="1" colspan="1">1.30&#x02009;pm</td><td align="center" rowspan="1" colspan="1">34500 Lux</td></tr><tr><td align="left" rowspan="1" colspan="1">(4)</td><td align="center" rowspan="1" colspan="1">Green pasture</td><td align="center" rowspan="1" colspan="1">1.30&#x02009;pm</td><td align="center" rowspan="1" colspan="1">35000 Lux</td></tr><tr><td align="left" rowspan="1" colspan="1">(5)</td><td align="center" rowspan="1" colspan="1">Sutlej river land</td><td align="center" rowspan="1" colspan="1">1.00&#x02009;pm</td><td align="center" rowspan="1" colspan="1">34300 Lux</td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>MSR5 (S. number 566).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">MSR types</th><th align="center" rowspan="1" colspan="1">Blue</th><th align="center" rowspan="1" colspan="1">Green</th><th align="center" rowspan="1" colspan="1">Red</th><th align="center" rowspan="1" colspan="1">Near infrared</th><th align="center" rowspan="1" colspan="1">Shortwave infrared</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">MSR5 (generic)</td><td align="center" rowspan="1" colspan="1">450&#x02013;520&#x02009;nm</td><td align="center" rowspan="1" colspan="1">520&#x02013;600&#x02009;nm</td><td align="center" rowspan="1" colspan="1">630&#x02013;690&#x02009;nm</td><td align="center" rowspan="1" colspan="1">760&#x02013;930&#x02009;nm</td><td align="center" rowspan="1" colspan="1">1550&#x02013;1750&#x02009;nm</td></tr><tr><td align="left" rowspan="1" colspan="1">MSR5 (S. number 566)</td><td align="center" rowspan="1" colspan="1">485&#x02009;nm</td><td align="center" rowspan="1" colspan="1">560&#x02009;nm</td><td align="center" rowspan="1" colspan="1">660&#x02009;nm</td><td align="center" rowspan="1" colspan="1">830&#x02009;nm</td><td align="center" rowspan="1" colspan="1">1650&#x02009;nm</td></tr></tbody></table></table-wrap><table-wrap id="tab3" orientation="portrait" position="float"><label>Table 3</label><caption><p>Feature selection table (<italic>F</italic> + PA + MI) for ROIs (512 &#x000d7; 512).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Features</th><th align="center" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1">&#x02009;</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="center" rowspan="10" colspan="1">
<italic>F</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,3) correlation</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,4) correlation</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,3) contrast</td></tr><tr><td align="left" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,4) contrast</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,5) correlation</td></tr><tr><td align="left" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,5) contrast</td></tr><tr><td align="left" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(2,2) correlation</td></tr><tr><td align="left" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,3) sum variance</td></tr><tr><td align="left" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,1) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,4) sum variance</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">11</td><td align="center" rowspan="10" colspan="1">PA</td><td align="center" rowspan="1" colspan="1"> Percent .01%</td></tr><tr><td align="left" rowspan="1" colspan="1">12</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(1,1) sum variance</td></tr><tr><td align="left" rowspan="1" colspan="1">13</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,1) ang. sec. mom</td></tr><tr><td align="left" rowspan="1" colspan="1">14</td><td align="center" rowspan="1" colspan="1"> Skewness</td></tr><tr><td align="left" rowspan="1" colspan="1">15</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,2) sum variance</td></tr><tr><td align="left" rowspan="1" colspan="1">16</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(5,5) entropy</td></tr><tr><td align="left" rowspan="1" colspan="1">17</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(5,&#x02212;5) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">18</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(1,0) sum. average</td></tr><tr><td align="left" rowspan="1" colspan="1">19</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(1,0) correlation</td></tr><tr><td align="left" rowspan="1" colspan="1">20</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(3,3) entropy</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">21</td><td align="center" rowspan="10" colspan="1">MI</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,5) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">22</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(5,&#x02212;5) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,4) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">24</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(4,&#x02212;4) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">25</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,3) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">26</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(3,&#x02212;3) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">27</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,2) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">28</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(2,2) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">29</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(2,&#x02212;2) inv. diff. mom.</td></tr><tr><td align="left" rowspan="1" colspan="1">30</td><td align="center" rowspan="1" colspan="1">
<italic>S</italic>(0,1) inv. diff. mom.</td></tr></tbody></table></table-wrap><table-wrap id="tab4" orientation="portrait" position="float"><label>Table 4</label><caption><p>NDA architecture for statistical texture dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Input layers = 5</th><th align="center" rowspan="1" colspan="1">1st hidden layer = 5</th><th align="center" rowspan="1" colspan="1">2nd hidden layer = 2</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Learning rate eta = 0.25</td><td align="center" rowspan="1" colspan="1">Back propagation iteration = 200000</td><td align="center" rowspan="1" colspan="1">Optimized iteration limit = 70</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="center" colspan="3" rowspan="1">
<italic>Output layers = 5</italic>
</td></tr></tbody></table></table-wrap><table-wrap id="tab5" orientation="portrait" position="float"><label>Table 5</label><caption><p>NDA architecture for multispectral dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Input layers = 5</th><th align="center" rowspan="1" colspan="1">1st hidden layer = 5</th><th align="center" rowspan="1" colspan="1">2nd hidden layer = 2</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Learning rate eta = 0.20</td><td align="center" rowspan="1" colspan="1">Back propagation iteration = 200000</td><td align="center" rowspan="1" colspan="1">Optimized iteration limit = 70</td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="center" colspan="3" rowspan="1">
<italic>Output layers = 5</italic>
</td></tr></tbody></table></table-wrap><table-wrap id="tab6" orientation="portrait" position="float"><label>Table 6</label><caption><p>Statistical texture features data projection table.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Statistical data analysis <break/>
<italic>K</italic>-fold (80-20)</th><th align="center" rowspan="1" colspan="1">RDA</th><th align="center" rowspan="1" colspan="1">PCA</th><th align="center" rowspan="1" colspan="1">LDA</th><th align="center" rowspan="1" colspan="1">NDA</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1-fold</td><td align="center" rowspan="1" colspan="1">92.5%</td><td align="center" rowspan="1" colspan="1">92.50%</td><td align="center" rowspan="1" colspan="1">97.50%</td><td align="center" rowspan="1" colspan="1">99.5%</td></tr><tr><td align="left" rowspan="1" colspan="1">2-fold</td><td align="center" rowspan="1" colspan="1">88.75%</td><td align="center" rowspan="1" colspan="1">87.92%</td><td align="center" rowspan="1" colspan="1">96.25%</td><td align="center" rowspan="1" colspan="1">100%</td></tr><tr><td align="left" rowspan="1" colspan="1">3-fold</td><td align="center" rowspan="1" colspan="1">90%</td><td align="center" rowspan="1" colspan="1">89.17%</td><td align="center" rowspan="1" colspan="1">98.75%</td><td align="center" rowspan="1" colspan="1">99%</td></tr><tr><td align="left" rowspan="1" colspan="1">4-fold</td><td align="center" rowspan="1" colspan="1">88.75%</td><td align="center" rowspan="1" colspan="1">87.50%</td><td align="center" rowspan="1" colspan="1">96.67%</td><td align="center" rowspan="1" colspan="1">100%</td></tr><tr><td align="left" rowspan="1" colspan="1">5-fold</td><td align="center" rowspan="1" colspan="1">90.42%</td><td align="center" rowspan="1" colspan="1">90.42%</td><td align="center" rowspan="1" colspan="1">99.17%</td><td align="center" rowspan="1" colspan="1">99.69%</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic>Average accuracy</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>90.08%</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>89.502%</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>97.668%</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>99.64%</italic>
</td></tr></tbody></table></table-wrap><table-wrap id="tab7" orientation="portrait" position="float"><label>Table 7</label><caption><p>Classification table of statistical texture data using artificial neural network (ANN: <italic>n</italic>-class).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Statistical data iteration (80-20)</th><th align="center" rowspan="1" colspan="1">Training dataset</th><th align="center" rowspan="1" colspan="1">Training data classification accuracy %</th><th align="center" rowspan="1" colspan="1">Test dataset</th><th align="center" rowspan="1" colspan="1">Misclassified data</th><th align="center" rowspan="1" colspan="1">Test data classification accuracy %</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1-fold</td><td align="center" rowspan="1" colspan="1">240</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">60</td><td align="center" rowspan="1" colspan="1">5/60</td><td align="center" rowspan="1" colspan="1">91.67%</td></tr><tr><td align="left" rowspan="1" colspan="1">2-fold</td><td align="center" rowspan="1" colspan="1">240</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">60</td><td align="center" rowspan="1" colspan="1">6/60</td><td align="center" rowspan="1" colspan="1">90%</td></tr><tr><td align="left" rowspan="1" colspan="1">3-fold</td><td align="center" rowspan="1" colspan="1">240</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">60</td><td align="center" rowspan="1" colspan="1">6/60</td><td align="center" rowspan="1" colspan="1">90%</td></tr><tr><td align="left" rowspan="1" colspan="1">4-fold</td><td align="center" rowspan="1" colspan="1">240</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">60</td><td align="center" rowspan="1" colspan="1">3/60</td><td align="center" rowspan="1" colspan="1">95%</td></tr><tr><td align="left" rowspan="1" colspan="1">5-fold</td><td align="center" rowspan="1" colspan="1">240</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">60</td><td align="center" rowspan="1" colspan="1">6/60</td><td align="center" rowspan="1" colspan="1">90%</td></tr><tr><td align="center" colspan="3" rowspan="1">
<italic>Average training data classification accuracy: 100%</italic>
</td><td align="center" colspan="3" rowspan="1">
<italic>Average test data classification accuracy: 91.334%</italic>
</td></tr></tbody></table></table-wrap><table-wrap id="tab8" orientation="portrait" position="float"><label>Table 8</label><caption><p>Confusion matrix for statistical texture data classification using (ANN: <italic>n</italic>-class).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Type</th><th align="center" rowspan="1" colspan="1">Fertile land</th><th align="center" rowspan="1" colspan="1">Green pasture</th><th align="center" rowspan="1" colspan="1">Desert rangeland</th><th align="center" rowspan="1" colspan="1">Bare land</th><th align="center" rowspan="1" colspan="1">Sutlej river land</th><th align="center" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Fertile land</td><td align="center" rowspan="1" colspan="1">51</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">60</td></tr><tr><td align="left" rowspan="1" colspan="1">Green pasture</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">59</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">60</td></tr><tr><td align="left" rowspan="1" colspan="1">Desert rangeland</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">48</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">60</td></tr><tr><td align="left" rowspan="1" colspan="1">Bare land</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">57</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">60</td></tr><tr><td align="left" rowspan="1" colspan="1">Sutlej river land</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">55</td><td align="center" rowspan="1" colspan="1">60</td></tr></tbody></table></table-wrap><table-wrap id="tab9" orientation="portrait" position="float"><label>Table 9</label><caption><p>Multispectral features data projection table.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Spectral data analysis (80-20)</th><th align="center" rowspan="1" colspan="1">RDA</th><th align="center" rowspan="1" colspan="1">PCA</th><th align="center" rowspan="1" colspan="1">LDA</th><th align="center" rowspan="1" colspan="1">NDA</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1-fold</td><td align="center" rowspan="1" colspan="1">99%</td><td align="center" rowspan="1" colspan="1">97.5%</td><td align="center" rowspan="1" colspan="1">99.5%</td><td align="center" rowspan="1" colspan="1">100%</td></tr><tr><td align="left" rowspan="1" colspan="1">2-fold</td><td align="center" rowspan="1" colspan="1">99%</td><td align="center" rowspan="1" colspan="1">99%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">99%</td></tr><tr><td align="left" rowspan="1" colspan="1">3-fold</td><td align="center" rowspan="1" colspan="1">98.5%</td><td align="center" rowspan="1" colspan="1">98.5%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">100%</td></tr><tr><td align="left" rowspan="1" colspan="1">4-fold</td><td align="center" rowspan="1" colspan="1">98.5%</td><td align="center" rowspan="1" colspan="1">98.5%</td><td align="center" rowspan="1" colspan="1">99%</td><td align="center" rowspan="1" colspan="1">99%</td></tr><tr><td align="left" rowspan="1" colspan="1">5-fold</td><td align="center" rowspan="1" colspan="1">98.5%</td><td align="center" rowspan="1" colspan="1">98.5%</td><td align="center" rowspan="1" colspan="1">99%</td><td align="center" rowspan="1" colspan="1">99%</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic>Average projection data accuracy</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>98.7%</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>98.4%</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>99.5%</italic>
</td><td align="center" rowspan="1" colspan="1">
<italic>99.4%</italic>
</td></tr></tbody></table></table-wrap><table-wrap id="tab10" orientation="portrait" position="float"><label>Table 10</label><caption><p>Classification table for multispectral data using artificial neural network (ANN: <italic>n</italic>-class).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Multispectral data iteration (80-20)</th><th align="center" rowspan="1" colspan="1">Training dataset</th><th align="center" rowspan="1" colspan="1">Training data classification accuracy %</th><th align="center" rowspan="1" colspan="1">Test dataset</th><th align="center" rowspan="1" colspan="1">Misclassified data</th><th align="center" rowspan="1" colspan="1">Test data classification accuracy %</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1-fold</td><td align="center" rowspan="1" colspan="1">200</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">6/50</td><td align="center" rowspan="1" colspan="1">88%</td></tr><tr><td align="left" rowspan="1" colspan="1">2-fold</td><td align="center" rowspan="1" colspan="1">200</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">2/50</td><td align="center" rowspan="1" colspan="1">96%</td></tr><tr><td align="left" rowspan="1" colspan="1">3-fold</td><td align="center" rowspan="1" colspan="1">200</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">0/50</td><td align="center" rowspan="1" colspan="1">100%</td></tr><tr><td align="left" rowspan="1" colspan="1">4-fold</td><td align="center" rowspan="1" colspan="1">200</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">1/50</td><td align="center" rowspan="1" colspan="1">98%</td></tr><tr><td align="left" rowspan="1" colspan="1">5-fold</td><td align="center" rowspan="1" colspan="1">200</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">0/50</td><td align="center" rowspan="1" colspan="1">100%</td></tr><tr><td align="center" colspan="6" rowspan="1">
<hr/>
</td></tr><tr><td align="center" colspan="6" rowspan="1">
<italic>Average multispectral training data classification accuracy: (100 + 100 + 100 + 100 + 100)/5 = 100%</italic>
</td></tr><tr><td align="center" colspan="6" rowspan="1">
<italic>Average multispectral test data classification accuracy: (88 + 96 + 100 + 98 + 100)/5 = 96.40%</italic>
</td></tr></tbody></table></table-wrap><table-wrap id="tab11" orientation="portrait" position="float"><label>Table 11</label><caption><p>Confusion matrix for multispectral data classification using (ANN: <italic>n</italic>-class).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Type</th><th align="center" rowspan="1" colspan="1">Fertile land</th><th align="center" rowspan="1" colspan="1">Green pasture</th><th align="center" rowspan="1" colspan="1">Desert rangeland</th><th align="center" rowspan="1" colspan="1">Bare land</th><th align="center" rowspan="1" colspan="1">Sutlej river land</th><th align="center" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Fertile land</td><td align="center" rowspan="1" colspan="1">47</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">50</td></tr><tr><td align="left" rowspan="1" colspan="1">Green pasture</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">50</td></tr><tr><td align="left" rowspan="1" colspan="1">Desert rangeland</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">48</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">50</td></tr><tr><td align="left" rowspan="1" colspan="1">Bare land</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">48</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">50</td></tr><tr><td align="left" rowspan="1" colspan="1">Sutlej river land</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">48</td><td align="center" rowspan="1" colspan="1">50</td></tr></tbody></table></table-wrap></floats-group></article>