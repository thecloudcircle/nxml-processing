
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">29944698</article-id><article-id pub-id-type="pmc">6019673</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0199443</article-id><article-id pub-id-type="publisher-id">PONE-D-18-01943</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Signal Filtering</subject><subj-group><subject>Bandpass Filters</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Social Cognition</subject><subj-group><subject>Sense of Agency</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Social Cognition</subject><subj-group><subject>Sense of Agency</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Social Cognition</subject><subj-group><subject>Sense of Agency</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Social Psychology</subject><subj-group><subject>Social Cognition</subject><subj-group><subject>Sense of Agency</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Social Psychology</subject><subj-group><subject>Social Cognition</subject><subj-group><subject>Sense of Agency</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Classical Mechanics</subject><subj-group><subject>Vibration</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Mechanical Engineering</subject><subj-group><subject>Robotics</subject><subj-group><subject>Robots</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Signal Filtering</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Auditory traits of "own voice"</article-title><alt-title alt-title-type="running-head">Auditory traits of "own voice"</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kimura</surname><given-names>Marino</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7408-0339</contrib-id><name><surname>Yotsumoto</surname><given-names>Yuko</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Project administration</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="corresp" rid="cor001">*</xref><xref ref-type="aff" rid="aff001"/></contrib></contrib-group><aff id="aff001"><addr-line>Department of Life Sciences, The University of Tokyo, Tokyo, Japan</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Sakakibara</surname><given-names>Manabu</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>Tokai University, JAPAN</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>cyuko@mail.ecc.u-tokyo.ac.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>6</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>13</volume><issue>6</issue><elocation-id>e0199443</elocation-id><history><date date-type="received"><day>19</day><month>1</month><year>2018</year></date><date date-type="accepted"><day>7</day><month>6</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 Kimura, Yotsumoto</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Kimura, Yotsumoto</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0199443.pdf"/><abstract><p>People perceive their recorded voice differently from their actively spoken voice. The uncanny valley theory proposes that as an object approaches humanlike characteristics, there is an increase in the sense of familiarity; however, eventually a point is reached where the object becomes strangely similar and makes us feel uneasy. The feeling of discomfort experienced when people hear their recorded voice may correspond to the floor of the proposed uncanny valley. To overcome the feeling of eeriness of own-voice recordings, previous studies have suggested equalization of the recorded voice with various types of filters, such as step, bandpass, and low-pass, yet the effectiveness of these filters has not been evaluated. To address this, the aim of experiment 1 was to identify what type of voice recording was the most representative of one&#x02019;s own voice. The voice recordings were presented in five different conditions: unadjusted recorded voice, step filtered voice, bandpass filtered voice, low-pass filtered voice, and a voice for which the participants freely adjusted the parameters. We found large individual differences in the most representative own-voice filter. In order to consider roles of sense of agency, experiment 2 investigated if lip-synching would influence the rating of own voice. The result suggested lip-synching did not affect own voice ratings. In experiment 3, based on the assumption that the voices used in previous experiments corresponded to continuous representations of non-own voice to own voice, the existence of an uncanny valley was examined. Familiarity, eeriness, and the sense of own voice were rated. The result did not support the existence of an uncanny valley. Taken together, the experiments led us to the following conclusions: there is no general filter that can represent own voice for everyone, sense of agency has no effect on own voice rating, and the uncanny valley does not exist for own voice, specifically.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>17K18693</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7408-0339</contrib-id><name><surname>Yotsumoto</surname><given-names>Yuko</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>16H03749</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7408-0339</contrib-id><name><surname>Yotsumoto</surname><given-names>Yuko</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>25119003</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7408-0339</contrib-id><name><surname>Yotsumoto</surname><given-names>Yuko</given-names></name></principal-award-recipient></award-group><funding-statement>This work was supported by the Japan Society for the Promotion of Science (<ext-link ext-link-type="uri" xlink:href="http://www.jsps.go.jp/english/">http://www.jsps.go.jp/english/</ext-link>), #17K18693, #16H03749, #25119003 for YY. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="8"/><table-count count="0"/><page-count count="16"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All behavioral data are available from the Dryad database (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.mp7v3">https://doi.org/10.5061/dryad.mp7v3</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All behavioral data are available from the Dryad database (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.mp7v3">https://doi.org/10.5061/dryad.mp7v3</ext-link>).</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>&#x0201c;Who am I?&#x0201d; This question, which is at the heart of the sense of self, has been asked and challenged for a long time by artists, philosophers, and scientists [<xref rid="pone.0199443.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0199443.ref004" ref-type="bibr">4</xref>]. To measure the conceptual &#x0201c;self&#x0201d; scientifically, the sense of &#x0201c;self&#x0201d; has been represented using several modalities as stimuli. The self-face is the most frequently used experimental stimuli due to its representativeness and convenience. Although most self-focused psychological experiments have used self-face, one&#x02019;s voice is also an important component of &#x0201c;self.&#x0201d; Indeed, one does not witness one&#x02019;s own face except on horizontally flipped images on mirrors. However, humans are frequently exposed to their own voice suggesting it may be a better, more representative example of real world self-representation.</p><p>Speech sounds are produced in the vocal fold and delivered to the vocal cavity. They then travel to the ear and auditory nerve via an air-conducted pathway from the mouth and a bone-conducted pathway via the cranial bones [<xref rid="pone.0199443.ref005" ref-type="bibr">5</xref>]. The bone conduction pathway also includes soft tissues. These different forms of sound conduction result in the different sounds and manifestations of hearing. Even though one can recognize if the presented voice is theirs, the recorded voice is found to be very unlike the voice that one hears when they are speaking. This is because the voice that one hears (own-voice) includes both bone conduction and air conduction while the recorded voice only includes air conduction [<xref rid="pone.0199443.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0199443.ref007" ref-type="bibr">7</xref>]. In addition, air conduction may also be distorted in the recorded voice, because the recorded voice is recorded close to the mouth, while own voice is &#x0201c;played&#x0201d; in the mouth. Further, depending on the audio set up, recorded voice may originate closer or farther from the ear than spoken voice. This difference may also contribute to the difference between own voice and recorded voice.</p><p>Over decades, researchers of the transfer function in own voice have employed various experimental methods. For example, the resonance frequencies of the human skull of patients with skin penetrating titanium implants were measured [<xref rid="pone.0199443.ref008" ref-type="bibr">8</xref>]. Bone transfer functions have been estimated using distortion product otoacoustic emissions [<xref rid="pone.0199443.ref009" ref-type="bibr">9</xref>]. Finally, the frequency characteristics of four different bone conduction actuators have been investigated [<xref rid="pone.0199443.ref010" ref-type="bibr">10</xref>]. Based on bone conduction characteristics described in previous research, the equalization filter is considered a suitable method to reproduce own-voice from recorded voice. Although filtered voice was rated as own-voice rather than recorded voice, the filter types varied across studies [<xref rid="pone.0199443.ref011" ref-type="bibr">11</xref>&#x02013;<xref rid="pone.0199443.ref013" ref-type="bibr">13</xref>]. Moreover, differences in the experimental settings, e.g. the words used as stimuli, impede the direct comparison of experimental results.</p><p>As previous studies were only concerned with frequency cut-off filters, the possible contributions of other sound characteristics, such as vibrato and pitch, as a component of own voice have not yet been examined. As some people tremble when they speak, instability of the voice may affect own-voice perception. Voice instability corresponds to vibrato, as they share characteristics [<xref rid="pone.0199443.ref014" ref-type="bibr">14</xref>]. Pitch may be another specific trait of own voice. Poor-pitch (i.e. tone deaf) singers have difficulty in mapping pitch onto action, but perceptual, motor, or memory problems have not been found in these individuals [<xref rid="pone.0199443.ref015" ref-type="bibr">15</xref>]. When the speaker tries to reproduce required pitch sounds, the speaker may have recognized bone conducted own voice as the correct pitch resulting in &#x0201c;poor-pitch&#x0201d;.</p><p>Other than sound characteristics, sense of agency is said to be an important component of self-ness. The online sense of action performance (&#x0201c;I am the one who is causing action&#x0201d;) is referred to as sense of agency, in which the performance done by someone else is being distinguished [<xref rid="pone.0199443.ref016" ref-type="bibr">16</xref>]. Sense of agency does not only concern body movement but also speech monitoring of auditory perception. It is known that mouth movement during sound presentation induces a higher sense of agency than images or hearing alone [<xref rid="pone.0199443.ref017" ref-type="bibr">17</xref>]. The effect of sense of agency presence on own voice, whether it encourages or changes the own voice representation within one&#x02019;s self, has not been investigated.</p><p>There are strong links between speech acoustics and emotions [<xref rid="pone.0199443.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0199443.ref019" ref-type="bibr">19</xref>]. Listeners are able to perceive the intended emotions from spoken voices, indicating that listeners associate particular patterns of acoustic cues with various discrete emotional states, and that the ability to infer emotion from speech is a fundamental component of human vocal communication [<xref rid="pone.0199443.ref020" ref-type="bibr">20</xref>]. Besides the profound relationship between emotion and voice [<xref rid="pone.0199443.ref021" ref-type="bibr">21</xref>], perception of voices is also critical in various situations. For example, newborn infants clearly prefer their mother&#x02019;s voice [<xref rid="pone.0199443.ref022" ref-type="bibr">22</xref>,<xref rid="pone.0199443.ref023" ref-type="bibr">23</xref>], and voice-only communication elicits greater empathy [<xref rid="pone.0199443.ref024" ref-type="bibr">24</xref>]. Furthermore, recent technology developments have increased the demand to use human-like voice in vocal assistance robots. A number of studies have examined how synthesized robotic voices are perceived by humans [<xref rid="pone.0199443.ref025" ref-type="bibr">25</xref>,<xref rid="pone.0199443.ref026" ref-type="bibr">26</xref>], and explored the best form of user-friendly acoustic interfaces. Despite the importance of voice perception in the human interactions, as well as human-machine interfaces, we are yet to fully understand how we perceive our own voices. Hence, it is critical to precisely evaluate the perception and representation of own-voice.</p><p>In addition to own voice reproduction, we also focused on differences in discomfort between own-voice and recorded voice. Even though most people may judge the presented voice as own-voice, non-modified recorded voice is found to be unpleasant. This phenomenon may be due to the recorded voice creating a so-called the uncanny valley (<xref ref-type="fig" rid="pone.0199443.g001">Fig 1</xref>). The uncanny valley is a widely used concept first proposed in the field of robotics [<xref rid="pone.0199443.ref027" ref-type="bibr">27</xref>]. The idea claims the familiarity and empathy to humanlike robots increases as the appearance of the robot becomes similar to human beings. However, in robots very closely approximating but failing to attain human appearance, the response by humans turns into revulsion. As an explanation, the original theory stated, &#x0201c;eeriness can be represented by negative familiarity.&#x0201d; Previous studies investigating the existence of the uncanny valleys have used eeriness, familiarity, and humanlike-ness as measurements [<xref rid="pone.0199443.ref028" ref-type="bibr">28</xref>,<xref rid="pone.0199443.ref029" ref-type="bibr">29</xref>].</p><fig id="pone.0199443.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g001</object-id><label>Fig 1</label><caption><title>Conceptual diagram of the uncanny valley in the voice field.</title><p><bold>Adapted from &#x0201c;The Uncanny Valley,&#x0201d; by M. Mori, 1970.</bold> Conceptual diagram of the theoretical graph presented in the original uncanny valley theory. X-axis corresponds to similarity between robots and humans and y-axis corresponds to familiarity of the robots. Recorded voice may represent the valley part and own voice the highest point after the valley. Sense of one&#x02019;s self instead of similarity was used in the present study.</p></caption><graphic xlink:href="pone.0199443.g001"/></fig><p>Our first experiment investigated the consistency of own-voice rating and queried which equalization filter among those employed in previous studies best represents one&#x02019;s own voice in a controlled experimental setting. The filters compared were: one that attenuated and amplified a certain range of frequency, one that cut off frequency at a strict threshold, and one that omitted a certain range of frequency. In addition to the filter comparison, the possibility of contributions from other sound characteristics, such as pitch and vibrato, to one&#x02019;s own voice representation was examined. In experiment 2, we examined the effect of sense of agency on own-voice representation by activating the motor system. Finally, in experiment 3, we measured familiarity, eeriness, and sense of one&#x02019;s self to investigate the existence of the uncanny valley in the acoustic field, focusing on each individual's voice features.</p></sec><sec id="sec002"><title>Experiment 1</title><sec id="sec003" sec-type="intro"><title>Introduction</title><p>In experiment 1, the sound profile that best represents own voice was examined. We used filters described in previous studies, as follows: +3 dB for a signal higher than 1 kHz and -3 dB for a signal lower than 1 kHz as a step filter [<xref rid="pone.0199443.ref011" ref-type="bibr">11</xref>]; a trapezoid like filter as a lowpass filter [<xref rid="pone.0199443.ref012" ref-type="bibr">12</xref>]; filter passing from 300 to 1200 Hz as a bandpass filter [<xref rid="pone.0199443.ref013" ref-type="bibr">13</xref>]. In addition to these three types of filters, an adjusted voice protocol, in which the participants adjusted all or part of pitch, vibrato, and frequency cut off filters of recorded voice to reproduce own-voice, was added for comparison. The participants chose the stimulus that best represented own-voice by comparing recorded voice, step filtered voice, lowpass filtered voice, bandpass filtered voice, and adjusted voice. To examine the consistency of the own-voice rating, the participants rated own-voiceness twice on two different days.</p></sec><sec id="sec004" sec-type="materials|methods"><title>Methods</title><sec id="sec005"><title>Participants</title><p>Ten Japanese students (four females and six males, 18&#x02013;22 years old) who reported no hearing disorders were paid to participate in the experiment. All participants gave written informed consent in accordance with the Declaration of Helsinki for their participation in the experimental protocol, which was approved by the institutional review board at The University of Tokyo.</p></sec><sec id="sec006"><title>Apparatus</title><p>Each participant&#x02019;s voice was recorded in a soundproof room using Sennheiser Microphone ME62 (Sennheiser electronic GmbH &#x00026; Co.KG, Germany) and Focusrite audio interface (Scarlett 2i4, First Generation model; Focusrite, UK). Audacity, downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.audacityteam.org/">www.audacityteam.org</ext-link>, was used to save a digital recording of the voice. All recorded voice was digitized at a 16 bit/44.1 kHz sampling rate. The auditory stimuli were presented through a USB digital-to-analog converter Focusrite audio interface Scarlett 2i4 1<sup>st</sup> Generation and MDR-XB500 headphones at 60 dB (SONY, Japan). The visual stimuli were presented on a LCD monitor (BenQ, China) using MATLAB R2015b (The MathWorks, Inc., USA) and the Psychtoolbox (<ext-link ext-link-type="uri" xlink:href="http://www.psychtoolbox.org/">www.psychtoolbox.org</ext-link>). The open-source patch DAVID (Da Amazing Voice Inflection Device)[<xref rid="pone.0199443.ref021" ref-type="bibr">21</xref>] for the close-source audio processing platform Max (Cycling &#x02018;74, USA) was used to allow participant control of auditory features of voice in real-time.</p></sec><sec id="sec007"><title>Stimuli and procedure</title><p>The experiment consisted of three sessions with the protocol for each filter setting conducted on 3 individual days. In session 1, the voice was recorded and the parameters of the voice were modified. Twenty-six three-syllable Japanese words categorized as neutral were selected [<xref rid="pone.0199443.ref030" ref-type="bibr">30</xref>] and recorded as the stimuli. The participants pronounced the stimuli in their usual manner. The participants were instructed not to correct their dialects. After the recording of all 26 words, the participants freely modified filters for pitch, vibrato, and frequency features of the original voice (recorded voice) such that the recording sounded like the voice that they hear when speaking (own voice). The participants were given the instruction of how to use graphical user interface for modification. The experimenter sat aside of each participant, and instructed the usage of GUI step by step until the participant fully understood the procedure. After this training period, the participants underwent the actual experimental trials. They were allowed to take time as long as they needed until they were convinced that the adjusted voice was their own. Vocalization was neither restricted nor encouraged while the participant modified the parameters of the voice. To control familiarity to the stimulus, six words of the recorded voices were used in this voice adjustment phase, and the remaining 20 recorded voices were used later in the rating phase; i.e., words used in the voice adjustment phase were not used in the rating phase in order to control for familiarity of the rated words.</p><p>In sessions 2 and 3, the participants were asked to participate in the voice rating task, and the exact same procedures were repeated. The participants performed two alternative forced choice tasks that involved listening to two different voice conditions and answering which voice sounded more like their own voice (<xref ref-type="fig" rid="pone.0199443.g002">Fig 2</xref>). The voice conditions that were judged included: recorded voice, step filtered voice, bandpass filtered voice, lowpass filtered voice, and adjusted-by-will voice (adjusted voice). In order to control the individual difference of own voice perception and to prevent individual variability in the rating procedure, stimuli were presented as a pair to force participants to decide which of the presented stimuli sounded more like own voice. Each of the five voice conditions was paired with another condition in each trial. Combinations of five filters with counterbalanced presentation orders resulted in 20 pairs of the filters. Each pair of the filters was tested with the 20 words prepared for the rating phase. As a result, 400 trials were conducted in the rating phase. All 400 trials were randomized within the session. Inter-stimulus interval was fixed as 400 ms and each stimulus was 800 ms. Within a trial, each stimulus was presented only once without repetition. There were 10 blocks in one session, each block containing 40 trials. Participants were able to take a break between the blocks.</p><fig id="pone.0199443.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g002</object-id><label>Fig 2</label><caption><title>Experiment 1.</title><p><bold>Schematic of the task.</bold> After the presentation of stimuli, participants chose which of the stimuli sounded more like own-voice by button press.</p></caption><graphic xlink:href="pone.0199443.g002"/></fig></sec><sec id="sec008"><title>Analysis</title><p>The own voice ratings were analyzed by a pairwise comparison method [<xref rid="pone.0199443.ref031" ref-type="bibr">31</xref>], which enables plotting of the scores of each condition on the same scale, so that each participant&#x02019;s relative preference could be evaluated. Thurstone&#x02019;s pairwise comparison method ranks the responses based on the z values calculated from the percentage of the choice of each item. For all pairs of recorded, step-filtered, lowpass-filtered, bandpass-filtered, and adjusted voices, the proportion of the stimuli chosen as a more own-voice like sound was calculated. The inverse function of the standard normal distribution was calculated and averaged for each stimulus. Then, each participant&#x02019;s own-voice rating was schematized into a scale bar. To evaluate the consistency of own-voice rating for a participant, Spearman&#x02019;s rank correlation coefficient was also calculated across two sessions carried out on two independent days.</p></sec></sec><sec id="sec009" sec-type="conclusions"><title>Results and discussions</title><p>We verified that voice transformation with DAVID worked as the participants intended, by analyzing the pitch of modified and non-modified speech samples using the SWIPE algorithm [<xref rid="pone.0199443.ref032" ref-type="bibr">32</xref>], and confirmed that actual pitch differences matched the parameter settings saved by the participants (see <xref ref-type="supplementary-material" rid="pone.0199443.s001">S1 Fig</xref>, <xref ref-type="supplementary-material" rid="pone.0199443.s003">S2</xref> and <xref ref-type="supplementary-material" rid="pone.0199443.s004">S3</xref> Tables).</p><p>Individual results of pairwise comparisons are shown in <xref ref-type="fig" rid="pone.0199443.g003">Fig 3</xref> and <xref ref-type="supplementary-material" rid="pone.0199443.s002">S1 Table</xref>. The voice rated as most similar to own voice differed across participants. Two participants chose the recorded voice most representative of own voice, and eight participants rated modified voice as most like own voice. Individual differences were found in the own voice rating, indicating there was no general filter that represented own voice. Even though each participant adjusted part or all of the pitch, vibrato, and frequency cut off filter to sound like own-voice (see <xref ref-type="supplementary-material" rid="pone.0199443.s003">S2 Table</xref> for details), only Sub 01 and 09 rated the adjusted voice as the own-voice. The various availabilities of modifiable parameter choices may have confused participants, resulting in prolonged adjustment times that made participants tired. There is also a possibility of participants unknowingly vocalizing the own voice closer to the recorded voice as part of their review of own voice.</p><fig id="pone.0199443.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g003</object-id><label>Fig 3</label><caption><title>Experiment 1.</title><p><bold>Individual results of pairwise comparison.</bold> The bar represents the similarity to own voice, rightmost represents the most own-voice like and leftmost represents the least own-voice like rating. The numbers on the top-half of the bar represents the result of the second session and the ones on bottom-half of the bar are the results of the third session. The numbers are for types of conditions: 1) Recorded voice, 2) Step filtered voice, 3) Bandpass filtered voice, 4) Lowpass filtered voice, 5) Adjusted voice.</p></caption><graphic xlink:href="pone.0199443.g003"/></fig><p><xref ref-type="fig" rid="pone.0199443.g004">Fig 4</xref> represents the consistency of similarity to own-voice rating across days. Six participants rated the voices the least and the most similar to own voice consistently, two participants rated the least own voice representative condition consistently, and one participant rated the most own voice representative condition consistently, while one participant showed no congruence. Spearman&#x02019;s rank correlation coefficient calculation across the participants revealed high rank correlation of most (<italic>&#x003c1;</italic> = .899) and least (<italic>&#x003c1;</italic> = .900) own-voice ratings between the two different sessions done on two different days. The result suggests that the perception of own-voice was steady to a certain extent across experimental days.</p><fig id="pone.0199443.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g004</object-id><label>Fig 4</label><caption><title>Experiment 1.</title><p><bold>Consistency of own-voice rating across trials.</bold> The consistency of the most and the least own-voice like rating is presented. Blue represents the number of participants who rated both the most and least own voice-like voice consistently, orange represents the number of participants who rated only the least own voice-like sound consistently, yellow represents the number of participants who rated only the most voice-like sound consistently, and gray represents the number of participants who rated both the most and the least own voice-like sound inconsistently.</p></caption><graphic xlink:href="pone.0199443.g004"/></fig></sec></sec><sec id="sec010"><title>Experiment 2</title><sec id="sec011" sec-type="intro"><title>Introduction</title><p>Whenever one listens to their own voice, the mouth is moving due to speaking. In experiment 1, the participants did not move their mouth during voice rating, creating a difference from the actual vocalizing environment. Lack of mouth movement may have resulted in decreased sense of agency. The aim of experiment 2 was to investigate if mouth movements during hearing one&#x02019;s voice affects own-voice rating. Thus, an additional day was added to the experiment 1 paradigm in which participants were asked to move their mouth at the same time as they heard their voice.</p></sec><sec id="sec012" sec-type="materials|methods"><title>Methods</title><sec id="sec013"><title>Participants</title><p>Seven subjects from experiment 1 returned to participate in experiment 2, and nine additional participants were recruited. In total, 16 Japanese students (11 males and five females, 18&#x02013;22 years old) who reported no hearing disorders were paid to participate in the experiment. Payments were transferred to the subjects&#x02019; bank accounts after participation. All participants gave written informed consent in accordance with the Declaration of Helsinki for their participation in the experimental protocol, which was approved by the institutional review board at The University of Tokyo. The data of one participant was excluded due to data corruption caused by Wi-Fi disconnection during the experiment.</p></sec><sec id="sec014"><title>Apparatus</title><p>The experimental apparatus was same as that in experiment 1.</p></sec><sec id="sec015"><title>Stimuli and procedure</title><p>Experiment 2 consisted of four sessions, each of which was conducted on 4 different days. Session 1 was assigned for voice recording and parameters were adjusted as in experiment 1. All or part of pitch, vibrato, and frequency cut off filters were adjusted. Three remaining sessions were assigned to rate which of two presented stimuli sounded more like own voice by two alternative forced choice tasks. The five conditions (recorded voice, step-filtered voice, bandpass-filtered voice, lowpass-filtered voice, and adjusted voice) were presented and judged in the task. Each of the five voice conditions was paired with another condition in each trial. Permutations of five filters were taken two at a time resulting in 20 total trials. Twenty words were rated in this experiment, resulting in 400 trials in total. All the 400 trials were randomized within the session. Inter-stimulus interval was fixed as 400 ms and each stimulus was 800 ms. Within a trial, each stimulus was presented only once without repetition. There were 10 blocks in one session, with each block containing 40 trials. Participants were able to take a break between blocks.</p><p>One of the three sessions was assigned as a lip synchronization session while the other two sessions were non-lip synchronization sessions. In the lip synchronization session, the participants were asked to synchronize their lips without vocalization as the stimulus was presented with the corresponding letter on the screen. On each trial, two stimuli were presented as a pair and the participants synchronized their lips for both stimuli. A training session with twenty trials was conducted prior to the actual experimental trials. In the training phase, the voices used in the parameter adjustment phase were used, so that practice did not affect familiarity of the rated word. The order of the lip synchronization session and non-lip synchronization sessions were counter-balanced across participants.</p></sec><sec id="sec016"><title>Analysis</title><p>Experimental analysis was same as in experiment 1.</p></sec></sec><sec id="sec017" sec-type="conclusions"><title>Results and discussions</title><p>The results of pairwise comparison on individual data are shown in <xref ref-type="fig" rid="pone.0199443.g005">Fig 5</xref> and <xref ref-type="supplementary-material" rid="pone.0199443.s002">S1 Table</xref>. Similar to that of experiment 1, insomuch as individual ratings largely differed. Each of the voice manipulations was chosen as the most own-voice like by at least one of the participants, including the non-manipulated voice which was preferred by 4 of the participants. Four others rated the recorded voice the highest for own-voice similarity. The results of the lip synchronization session resemble those of the non-lip synchronization sessions, few participants chose the adjusted voice as the most own voice representative. It should be noted that adjustments of various parameters that participants were unfamiliar to might have caused confusion and tiredness.</p><fig id="pone.0199443.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g005</object-id><label>Fig 5</label><caption><title>Experiment 2.</title><p><bold>Individual results of pairwise comparison.</bold> The bar represents the similarity to own voice, rightmost as the most own-voice like and leftmost as the least own-voice like rating. There were two non-lip synchronization sessions conducted and the results are presented as the numbers on the top bar. The numbers on the bottom bar represent the results of the lip synchronization session. The numbers are the types of conditions: 1) Recorded voice, 2) Step-filtered voice, 3) Bandpass filtered voice, 4) Lowpass filtered voice, 5) Adjusted voice.</p></caption><graphic xlink:href="pone.0199443.g005"/></fig><p>The consistency of own voice similarity rating across sessions is shown in <xref ref-type="fig" rid="pone.0199443.g006">Fig 6</xref>. Six participants rated both the least and most own-voice representative consistently, eight participants rated either the least or the most consistently, and one participant rated inconsistently. High rank correlations of the most (<italic>&#x003c1;</italic> = .899) and the least (<italic>&#x003c1;</italic> = .900) own voice across three different sessions were shown by Spearman&#x02019;s rank correlation coefficient calculation across the participants. The results showed the voice chosen for best own-voice representative was consistent within participant but not across participants, meaning own voice depiction remains consistent within individuals.</p><fig id="pone.0199443.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g006</object-id><label>Fig 6</label><caption><title>Experiment 2.</title><p><bold>Participant own voice rating consistency across days.</bold> The consistency of own voice-like rating across participants is charted. Blue represents the number of participants who rated both the most and least own voice-like voice consistently, orange represents least choice consistency only, yellow represent most choice consistency only, and gray represents inconsistency for both the most and least own voice-like voice.</p></caption><graphic xlink:href="pone.0199443.g006"/></fig><p>Thus, lip synchronization, which aims to increase sense of agency by replicating motor control during vocalization, may not affect own voice rating. There were only two participants who changed the rating of the least own-voice-like condition on the lip synchronization session and two who changed the most own-voice-like rating. Lip synchronization may not affect own-voice perception, at least in our experimental paradigm.</p></sec></sec><sec id="sec018"><title>Experiment 3</title><sec id="sec019" sec-type="intro"><title>Introduction</title><p>Experiments 1 and 2 showed the consistency of own-voice rating for individuals and the difference among individuals for own-voice perception. In experiment 3, we examined how own-voice relates to the uncanny valley. The participants were asked to rate how much the voice sounded like own-voice (sense of one&#x02019;s self), familiarity, and eeriness of each stimuli.</p></sec><sec id="sec020" sec-type="materials|methods"><title>Methods</title><sec id="sec021"><title>Participants</title><p>Twelve Japanese students (seven males and five females, 18&#x02013;24 years old) who reported no hearing disorders were paid to participate in the experiment. All participants gave written informed consent in accordance with the Declaration of Helsinki for their participation in the experimental protocol, which was approved by the institutional review board at The University of Tokyo.</p></sec><sec id="sec022"><title>Apparatus</title><p>The experimental apparatus was same as the one in experiments 1 and 2.</p></sec><sec id="sec023"><title>Stimuli and procedure</title><p>Experiment 3 consisted of two sessions and was conducted on 2 different days. In session 1, the voice of each participant was recorded, and participants adjusted parameters to make the recorded voice sound like their own voice as in Experiments 1 and 2. The words used as stimuli in experiment 3 were the same as those in experiment 1 and 2. The voice conditions were also the same; recorded, step-filtered, lowpass-filtered, bandpass-filtered, and adjusted voice.</p><p>The rating of the presented voice was conducted in session 2. The session consisted of three blocks, each block containing a different type of rating; familiarity, eeriness, and sense of oneself. The order of the blocks was counter-balanced across participants. In each trial, an 800 ms fixation cross was presented followed by 800 ms of stimulus presented through the headphones. The task was to rate either the sense of oneself, familiarity, or eeriness of the presented voice. The participants made responses by moving a cursor over a 9-poinit Likert scale (<xref ref-type="fig" rid="pone.0199443.g007">Fig 7</xref>). The black cursor, corresponding to chosen score, turned red after a click. In each trial, the stimulus was played only once and there was no option of replay. The participants were instructed to rate familiarity, eeriness, or sense of oneself, using the whole scale to be as precise as possible. After 30 practice trials, the participants completed all 300 trials. The participants took a break between each block.</p><fig id="pone.0199443.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g007</object-id><label>Fig 7</label><caption><title>Experiment 3.</title><p><bold>Schematic of the experimental task.</bold> After the presentation of stimulus, the participant rated the stimulus in terms of the presented feature from one to nine by moving a cursor.</p></caption><graphic xlink:href="pone.0199443.g007"/></fig></sec><sec id="sec024"><title>Analysis</title><p>After standardization of the corrected data, a cubic equation was used to examine if sense of oneself relates to the uncanny valley. Our research is the first study focusing on the voice representation in the uncanny valley theory, as humanoid face context had been the most researched topic, the analysis herein was exploratory. We therefore investigated if the presented stimuli covered the whole range from 0 to 100% similarity or part of the range, such as from 80 to 100%. In addition, the mode of the posterior distribution of the correlation coefficient was calculated with Bayesian statistics to assess if sense of oneself or familiarity showed a correlative relationship with the eeriness of the voice. In addition to the coefficient between two scores, average and variance of each score were treated as unknown parameters to estimate posterior distribution. The distribution of correlation was sampled by Markov-chain-Monte-Carlo calculation, and the mode of the posterior distribution was treated as the correlation coefficient.</p></sec></sec><sec id="sec025" sec-type="conclusions"><title>Results and discussions</title><p><xref ref-type="fig" rid="pone.0199443.g008">Fig 8</xref> shows scores of familiarity, eeriness, and sense of oneself of all five conditions in scatter plots. The eeriness score was plotted in reverse, higher scores indicating lower eeriness, as shown in <xref ref-type="fig" rid="pone.0199443.g008">Fig 8B</xref>. Bayesian statistics showed a strong positive correlation for both familiarity and sense of oneself (<italic>r</italic> = .807), and eeriness and sense of oneself (<italic>r</italic> = .803). In addition, the cubic equation calculated from the data drew a gradual undulating curve for both familiarity and sense of oneself, and eeriness and sense of oneself. While there were strong positive correlations, the slope exhibited a constant increase without indicating the existence of an uncanny valley. Thus, the expected extreme valley proposed by Mori (1970) was not observed.</p><fig id="pone.0199443.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0199443.g008</object-id><label>Fig 8</label><caption><title>Experiment 3.</title><p><bold>Results of voice features scoring.</bold> The X-axis represents sense of oneself, y-axis represents familiarity for A and eeriness for B. Each individual score is plotted as green dots. The dotted line shows the Pearson&#x02019;s correlation and the solid represents the cubic equation.</p></caption><graphic xlink:href="pone.0199443.g008"/></fig></sec></sec><sec id="sec026"><title>General discussions</title><p>In the present study, we explored the best modification protocol for reproducing own voice from recorded voice, and investigated the effect of the motor system on own voice representation. The voice manipulation perceived as most similar to the own voice varied substantially between individuals, but was consistent across separate days. In addition, we found no effects of lip-synching on own voice perception. The correspondence of recorded voice familiarity to the uncanny valley was also examined in this study. Although proportional relationships were found between familiarity and sense of oneself, and between eeriness and sense of oneself, evidence to support the existence of the uncanny valley was insufficient.</p><p>Beside the attempts to find one generalized filter to reproduce own voice, our study indicated that there was no such universal filter. The sound chosen as the best own voice representative differed greatly across the participants. No specific filter or any specific modification of acoustic traits could be applied to everyone to make recorded voice sound like own-voice. A previous work suggested a band-pass filter to be universal [<xref rid="pone.0199443.ref013" ref-type="bibr">13</xref>]. There are two major differences between their study and the present study. First, the previous work examined voice perception of singers, who are more likely to be exposed to their own-voices, while the participants of the present study had no particular training prior to this experiment. Second, the previous study used 'Ah' in the sung voice as the parameter for manipulation, whereas we used six words selected from the vocabulary used in our daily life. Therefore, we believe our results are more generalizable. Differences in body structures or experience of exposure to recorded voice may be considered as reasons for individual differences. As no human being is identical to someone else, everyone&#x02019;s voice is distinct. Some people, such as actors or singers, listen to their recorded voices in daily life, but some rarely listen to their recorded voices as much as their own voices. This difference of own-voice exposure frequency may result in individual differences in own voice perception.</p><p>Along with the individual differences found in own voice perception, the stability of own voice perception within individuals was explored. Although people now observe their faces not only in the mirror but via easily taken photographs on mobile devices, own voice is still the most frequently perceived self-representing feature. As people listen to their own voice countless times in their daily lives, the perceptions of own voice may become solid and robust.</p><p>In the context of own voice reproduction, bone conduction was thought to be the most important component in addition to air conduction. A recent study proposed that the bone is not the only substrate in own voice conduction, as cartilage is now discussed as a third sound transmission pathway [<xref rid="pone.0199443.ref033" ref-type="bibr">33</xref>]. The aural cartilage is part of the outer ear and covers half of the exterior auditory canal. The differences between transmission mechanisms result in differences in hearing, cartilage conduction produces a broader sound range and stereophony [<xref rid="pone.0199443.ref034" ref-type="bibr">34</xref>]. It should be noted that our experimental procedure evaluated the sound transmission pathway including bone conduction as well as cartilage conduction. Therefore, we were unable to dissociate the effect of bone conduction and cartilage conduction from our result.</p><p>It is said that the human body conducts low and rich tones, and people often claim they perceive their own voice to be lower and richer in tone than their recorded voice. Despite these phenomena, some people have reported the recorded voice to be higher than modified voices. Our study allowed detailed and independent modulation accounting for such individual differences, but the voice adjusted by each participant was not necessarily the most representative of own-voice. This might be because the modification of voice characteristics induced changes in emotional impressions of the voice, as previous studies showed that slight modifications of voice parameters cause changes of emotion rating in a congruent direction [<xref rid="pone.0199443.ref035" ref-type="bibr">35</xref>]. There is a possibility of some emotional characteristics of adjusting the voice recording distracting from own-voice perception, such that absolute rating of adjusted voice differs from relative rating.</p><p>Our study used lip-synching during voice presentation in order to consider the effect of sense of agency on own-voice rating. There was no effect of lip-synching on sense of agency in own voice perception. This may owe to issues with the experimental procedure. Instructions to move the mouth at the same time as voice presentation with completely random and various vocabularies may have produced time lag between auditory perception and sense of agency. Moreover, the possibility of unpredictability and mismatch between the motor system and perception having an effect on eeriness are quite possible.</p><p>Although we tried to make experimental setting as similar as possible to the real-world situations, there still is a technical limitation such that, we only used isolated words to evaluate the perception of own voice. Spoken language consists of a series of words with various acoustic characteristics. Thus, our result may not be directly applicable to the spoken language in our daily life situations.</p><p>Despite the phenomenon that people feel creepiness when confronted with objects having a human-like appearance such as mannequins, the existence of the uncanny valley has been questioned in several experimental studies. A proportional relationship between eeriness and the human similarity was found only for digitally created faces [<xref rid="pone.0199443.ref036" ref-type="bibr">36</xref>]. A nonlinear curve showing a gradual valley-like shape was observed in the morphing of robot, android, and human &#x0201c;faces&#x0201d; but it was not as clear as that in the original uncanny valley theory [<xref rid="pone.0199443.ref029" ref-type="bibr">29</xref>,<xref rid="pone.0199443.ref037" ref-type="bibr">37</xref>]. Our study supported an absence of the uncanny valley [<xref rid="pone.0199443.ref036" ref-type="bibr">36</xref>] in own-voice perception, especially when focusing on oneself as a measure of human likeness. Future studies using a completely unfamiliar voice will examine the existence of the valley in other aspects of audition.</p><p>Our results indicate the importance of individual consideration in own voice reproduction experiments and cast doubt on the existence of the uncanny valley in terms of own voice perception. Methods for complete and genuine reproduction of own-voice may be of use to various fields. For example, listening to non-stuttering own voice may be used to treat stutter. In terms of clinical research, presentation of own voice may facilitate research on hallucination in schizophrenia. Thus, our study may act as stepping-stone for more detailed research on own-voice and perception of self across many fields.</p></sec><sec sec-type="supplementary-material" id="sec027"><title>Supporting information</title><supplementary-material content-type="local-data" id="pone.0199443.s001"><label>S1 Fig</label><caption><title>Results of a-posterior pitch analysis.</title><p>Blue line indicates recorded voice and red line indicates adjusted voice. Results from the first six participants from experiment 1 are shown.</p><p>(EPS)</p></caption><media xlink:href="pone.0199443.s001.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0199443.s002"><label>S1 Table</label><caption><title>Individual results of pairwise comparisons.</title><p>The values represent numbers that the filter in the row was judged more like own voice compared to the filter in the column.</p><p>(XLSX)</p></caption><media xlink:href="pone.0199443.s002.xlsx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0199443.s003"><label>S2 Table</label><caption><title>Parameters adjusted by each participant.</title><p>Parameters adjusted by each subject to generate the adjusted voice.</p><p>(XLSX)</p></caption><media xlink:href="pone.0199443.s003.xlsx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0199443.s004"><label>S3 Table</label><caption><title>Results of SWIPE analyses.</title><p>Voice transformations with DAVID were evaluated by the SWIPE algorithm.</p><p>(XLSX)</p></caption><media xlink:href="pone.0199443.s004.xlsx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0199443.ref001"><label>1</label><mixed-citation publication-type="book"><name><surname>Decartese</surname><given-names>R</given-names></name>. <chapter-title>Discourse on the Method (I. Maclean, Trans.)</chapter-title>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford World&#x02019;s Classics</publisher-name>; <year>2008</year>.</mixed-citation></ref><ref id="pone.0199443.ref002"><label>2</label><mixed-citation publication-type="book"><name><surname>Young</surname><given-names>JZ</given-names></name>. <chapter-title>Philosophy and the brain</chapter-title>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>1987</year>.</mixed-citation></ref><ref id="pone.0199443.ref003"><label>3</label><mixed-citation publication-type="book"><name><surname>van Rijn</surname><given-names>RH</given-names></name>. <chapter-title>Self-portrait</chapter-title>
<publisher-loc>Numerberg, Bavaria</publisher-loc>: <publisher-name>Germanisches Nationalmuseum</publisher-name>; <year>1629</year>.</mixed-citation></ref><ref id="pone.0199443.ref004"><label>4</label><mixed-citation publication-type="book"><name><surname>van Rijn</surname><given-names>RH</given-names></name>. <source>Self-portrait with Beret and Turned-Up Collar</source>. <publisher-loc>Washington, the United States of America</publisher-loc>: <publisher-name>National Gallery of Art</publisher-name>; <year>1659</year>.</mixed-citation></ref><ref id="pone.0199443.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Tonndorf</surname><given-names>J</given-names></name>. <article-title>A New Concept of Bone Conduction</article-title>. <source>Arch Otolaryngol</source>. <year>1968</year>;<volume>87</volume>: <fpage>595</fpage>&#x02013;<lpage>600</lpage>. <pub-id pub-id-type="pmid">5649538</pub-id></mixed-citation></ref><ref id="pone.0199443.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Maurer</surname><given-names>D</given-names></name>, <name><surname>Landis</surname><given-names>T</given-names></name>. <article-title>Role of bone conduction in the self-perception of speech</article-title>. <source>Folia Phoniatr Logop</source>. <year>1990</year>;<volume>42</volume>: <fpage>226</fpage>&#x02013;<lpage>229</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1159/000266070">10.1159/000266070</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>v. B&#x000e9;k&#x000e9;sy</surname><given-names>G</given-names></name>. <article-title>Note on the Definition of the Term: Hearing by Bone Conduction</article-title>. <source>J Acoust Soc Am</source>. <year>1954</year>;<volume>26</volume>: <fpage>106</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1907278">10.1121/1.1907278</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>H&#x000e5;kansson</surname><given-names>B</given-names></name>, <name><surname>Brandt</surname><given-names>A</given-names></name>, <name><surname>Carlsson</surname><given-names>P</given-names></name>, <name><surname>Tjellstr&#x000f6;m</surname><given-names>A</given-names></name>, <name><surname>Tjellstrom</surname><given-names>A</given-names></name>. <article-title>Resonance frequencies of the human skull in vivo</article-title>. <source>J Acoust Soc Am</source>. <year>1994</year>;<volume>95</volume>: <fpage>1474</fpage>&#x02013;<lpage>1481</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.408535">10.1121/1.408535</ext-link></comment>
<pub-id pub-id-type="pmid">8176050</pub-id></mixed-citation></ref><ref id="pone.0199443.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Purcell</surname><given-names>DW</given-names></name>, <name><surname>Kunov</surname><given-names>H</given-names></name>, <name><surname>Cleghorn</surname><given-names>W</given-names></name>. <article-title>Estimating bone conduction transfer functions using otoacoustic emissions</article-title>. <source>J Acoust Soc Am</source>. <year>2003</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1582436">10.1121/1.1582436</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Qin</surname><given-names>X</given-names></name>, <name><surname>Jinnai</surname><given-names>S</given-names></name>, <name><surname>Usagawa</surname><given-names>T</given-names></name>. <article-title>Frequency characteristics of bone conduction actuators&#x02014;the discussion on loudness and OFL</article-title>. <source>Appl Acoust</source>. Elsevier Ltd; <year>2017</year>;<volume>126</volume>: <fpage>603</fpage>&#x02013;<lpage>606</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.apacoust.2017.05.007">10.1016/j.apacoust.2017.05.007</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Shuster</surname><given-names>LI</given-names></name>, <name><surname>Durrant</surname><given-names>JD</given-names></name>. <article-title>Toward a better understanding of the perception of self-produced speech</article-title>. <source>J Commun Disord</source>. <year>2003</year>;<volume>36</volume>: <fpage>1</fpage>&#x02013;<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0021-9924(02)00132-6">10.1016/S0021-9924(02)00132-6</ext-link></comment>
<pub-id pub-id-type="pmid">12493635</pub-id></mixed-citation></ref><ref id="pone.0199443.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Vurma</surname><given-names>A</given-names></name>. <article-title>The timbre of the voice as perceived by the singer him-/herself</article-title>. <source>Logop Phoniatr Vocology</source>. <year>2014</year>;<volume>39</volume>: <fpage>1</fpage>&#x02013;<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3109/14015439.2013.775334">10.3109/14015439.2013.775334</ext-link></comment>
<pub-id pub-id-type="pmid">23510260</pub-id></mixed-citation></ref><ref id="pone.0199443.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Won</surname><given-names>SY</given-names></name>, <name><surname>Berger</surname><given-names>J</given-names></name>, <name><surname>Slaney</surname><given-names>M</given-names></name>. <article-title>Simulation of One &#x02018; s Own Voice in a Two-parameter Model</article-title>. <source>Proc Int Conf Music Percept Cogn</source>. <year>2014</year>;</mixed-citation></ref><ref id="pone.0199443.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Sundberg</surname><given-names>J</given-names></name>. <article-title>Quarterly Progress and Status Report: Acoustic and psychoacoustic aspects of vocal vibrato</article-title>. <source>StL-QPSR</source>. <year>1994</year>;<volume>35</volume>: <fpage>45</fpage>&#x02013;<lpage>68</lpage>.</mixed-citation></ref><ref id="pone.0199443.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Pfordresher</surname><given-names>PQ</given-names></name>, <name><surname>Brown</surname><given-names>S</given-names></name>. <article-title>Poor-Pitch Singing in the Absence of &#x0201c;Tone Deafness.&#x0201d;</article-title>
<source>Music Percept</source>. <year>2007</year>;<volume>25</volume>: <fpage>95</fpage>&#x02013;<lpage>115</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1525/mp.2007.25.2.95">10.1525/mp.2007.25.2.95</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Gallagher</surname><given-names>S</given-names></name>. <article-title>Philosophical conceptions of the self: Implications for cognitive science</article-title>. <source>Trends Cogn Sci</source>. <year>2000</year>;<volume>4</volume>: <fpage>14</fpage>&#x02013;<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1364-6613(99)01417-5">10.1016/S1364-6613(99)01417-5</ext-link></comment>
<pub-id pub-id-type="pmid">10637618</pub-id></mixed-citation></ref><ref id="pone.0199443.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Sugimori</surname><given-names>E</given-names></name>, <name><surname>Asai</surname><given-names>T</given-names></name>, <name><surname>Tanno</surname><given-names>Y</given-names></name>. <article-title>The potential link between sense of agency and output monitoring over speech</article-title>. <source>Conscious Cogn</source>. <year>2013</year>;<volume>22</volume>: <fpage>360</fpage>&#x02013;<lpage>374</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.concog.2012.07.010">10.1016/j.concog.2012.07.010</ext-link></comment>
<pub-id pub-id-type="pmid">22910578</pub-id></mixed-citation></ref><ref id="pone.0199443.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Bachorowski</surname><given-names>AJ</given-names></name>, <name><surname>Bachorowski</surname><given-names>J</given-names></name>. <source>Vocal Expression and Perception of Emotion Vocal Expression of Emotion</source>. <year>2010</year>;<volume>8</volume>: <fpage>53</fpage>&#x02013;<lpage>57</lpage>.</mixed-citation></ref><ref id="pone.0199443.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Latinus</surname><given-names>M</given-names></name>, <name><surname>Belin</surname><given-names>P</given-names></name>. <article-title>Human voice perception</article-title>. <source>Current Biolology</source> Elsevier; <year>2011</year>;<volume>21</volume>: <fpage>R143</fpage>&#x02013;<lpage>R145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2010.12.033">10.1016/j.cub.2010.12.033</ext-link></comment>
<pub-id pub-id-type="pmid">21334289</pub-id></mixed-citation></ref><ref id="pone.0199443.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Scherer</surname><given-names>KR</given-names></name>, <name><surname>Banse</surname><given-names>R</given-names></name>, <name><surname>Wallbott</surname><given-names>HG</given-names></name>. <article-title>Emotion inferences from vocal expression correlate across languages and cultures</article-title>. <source>J Cross Cult Psychol</source>. <year>2001</year>;<volume>32</volume>: <fpage>76</fpage>&#x02013;<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0022022101032001009">10.1177/0022022101032001009</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Rachman</surname><given-names>L</given-names></name>, <name><surname>Liuni</surname><given-names>M</given-names></name>, <name><surname>Arias</surname><given-names>P</given-names></name>, <name><surname>Lind</surname><given-names>A</given-names></name>, <name><surname>Johansson</surname><given-names>P</given-names></name>, <name><surname>Hall</surname><given-names>L</given-names></name>, <etal>et al</etal>
<article-title>DAVID: An open-source platform for real-time transformation of infra-segmental emotional cues in running speech</article-title>. <source>Behav Res Methods</source>. <year>2017</year>;<volume>50</volume>: <fpage>323</fpage>&#x02013;<lpage>343</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13428-017-0873-y">10.3758/s13428-017-0873-y</ext-link></comment>
<pub-id pub-id-type="pmid">28374144</pub-id></mixed-citation></ref><ref id="pone.0199443.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Hooper</surname><given-names>PG</given-names></name>, <name><surname>Scott</surname><given-names>D</given-names></name>, <name><surname>Shahidullah</surname><given-names>S</given-names></name>. <article-title>Newborn and fetal response to maternal voice</article-title>. <source>J Reprod Infant Psychol</source>. <year>1993</year>;<volume>11</volume>.</mixed-citation></ref><ref id="pone.0199443.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Ockleford</surname><given-names>EM</given-names></name>, <name><surname>Vince</surname><given-names>MA</given-names></name>, <name><surname>Layton</surname><given-names>C</given-names></name>, <name><surname>Reader</surname><given-names>MR</given-names></name>. <article-title>Responses of neonates to parents&#x02019; and others&#x02019; voices</article-title>. <source>Early Hum Dev</source>. <year>1988</year>;<volume>18</volume>: <fpage>27</fpage>&#x02013;<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0378-3782(88)90040-0">10.1016/0378-3782(88)90040-0</ext-link></comment>
<pub-id pub-id-type="pmid">3234282</pub-id></mixed-citation></ref><ref id="pone.0199443.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Kraus</surname><given-names>MW</given-names></name>. <source>Voice-Only Communication Enhances Empathic Accuracy</source>. <year>2017</year>;<volume>72</volume>: <fpage>644</fpage>&#x02013;<lpage>654</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/amp0000147.supp">http://dx.doi.org/10.1037/amp0000147.supp</ext-link></mixed-citation></ref><ref id="pone.0199443.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Eyssel</surname><given-names>F</given-names></name>, <name><surname>Kuchenbrandt</surname><given-names>D</given-names></name>, <name><surname>Bobinger</surname><given-names>S</given-names></name>, <name><surname>de Ruiter</surname><given-names>L</given-names></name>, <name><surname>Hegel</surname><given-names>F</given-names></name>. &#x0201c;<article-title>If you sound like me, you must be more human</article-title>.&#x0201d; <source>Proc seventh Annu ACM/IEEE Int Conf Human-Robot Interact&#x02014;HRI &#x02018;12</source>. <year>2012</year>; <volume>125</volume>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/2157689.2157717">10.1145/2157689.2157717</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Tamagawa</surname><given-names>R</given-names></name>, <name><surname>Watson</surname><given-names>CI</given-names></name>, <name><surname>Kuo</surname><given-names>IH</given-names></name>, <name><surname>Macdonald</surname><given-names>BA</given-names></name>, <name><surname>Broadbent</surname><given-names>E</given-names></name>. <article-title>The effects of synthesized voice accents on user perceptions of robots</article-title>. <source>Int J Soc Robot</source>. <year>2011</year>;<volume>3</volume>: <fpage>253</fpage>&#x02013;<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s12369-011-0100-4">10.1007/s12369-011-0100-4</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Mori</surname><given-names>M</given-names></name>. <article-title>The Uncanny Valley</article-title>. <source>Energy</source>. <year>1970</year>;<volume>7</volume>: <fpage>1</fpage>&#x02013;<lpage>2</lpage>.</mixed-citation></ref><ref id="pone.0199443.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>MacDorman</surname><given-names>K</given-names></name>. <article-title>Subjective ratings of robot video clips for human likeness, familiarity, and eeriness: An exploration of the uncanny valley</article-title>. <source>ICCS/CogSci-2006 long Symp Towar</source> &#x02026;. <year>2006</year>; <fpage>26</fpage>&#x02013;<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nsr025">10.1093/scan/nsr025</ext-link></comment>
<pub-id pub-id-type="pmid">21515639</pub-id></mixed-citation></ref><ref id="pone.0199443.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>MacDorman</surname><given-names>KF</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>. <article-title>Opening Pandora&#x02019;s uncanny Box: Reply to commentaries on &#x0201c;The uncanny advantage of using androids in social and cognitive science research.&#x0201d;</article-title>
<source>Interact Stud</source>. <year>2006</year>;<volume>7</volume>: <fpage>361</fpage>&#x02013;<lpage>368</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1075/is.7.3.10">10.1075/is.7.3.10</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Gotoh</surname><given-names>F</given-names></name>, <name><surname>Ohta</surname><given-names>N</given-names></name>. <article-title>Affective valence of two-compound kanji words</article-title>. <source>Journal of Phonetic Society of Japan</source>. Tsukuba Psychological Research; <year>2001</year> pp. <fpage>37</fpage>&#x02013;<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0022-3514.80.2.294">10.1037/0022-3514.80.2.294</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Thurstone</surname><given-names>LL</given-names></name>. <article-title>A law of comparative judgment</article-title>. <source>Psychol Rev</source>. <year>1927</year>;<volume>34</volume>: <fpage>273</fpage>&#x02013;<lpage>286</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0070288">10.1037/h0070288</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Camacho</surname><given-names>A</given-names></name>., <article-title>Harris JG. A sawtooth waveform inspired pitch estimator for speech and music</article-title>. <source>J Acoust Soc Am</source>. <year>2008</year>;<volume>124</volume>(<issue>3</issue>):<fpage>1638</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.2951592">10.1121/1.2951592</ext-link></comment>
<pub-id pub-id-type="pmid">19045655</pub-id></mixed-citation></ref><ref id="pone.0199443.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Shimokura</surname><given-names>R</given-names></name>, <name><surname>Hosoi</surname><given-names>H</given-names></name>, <name><surname>Nishimura</surname><given-names>T</given-names></name>, <name><surname>Yamanaka</surname><given-names>T</given-names></name>. <article-title>Aural cartilage vibration and sound measured in the external auditory canal for several transducer positions</article-title>. <year>2004</year>; <fpage>137</fpage>&#x02013;<lpage>143</lpage>.</mixed-citation></ref><ref id="pone.0199443.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Shimokura</surname><given-names>R</given-names></name>, <name><surname>Hosoi</surname><given-names>H</given-names></name>, <name><surname>Nishimura</surname><given-names>T</given-names></name>, <name><surname>Yamanaka</surname><given-names>T</given-names></name>, <name><surname>Levitt</surname><given-names>H</given-names></name>. <article-title>Cartilage conduction hearing</article-title>. <source>J Acoust Soc Am</source>. <year>2014</year>;<volume>135</volume>: <fpage>1959</fpage>&#x02013;<lpage>1966</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.4868372">10.1121/1.4868372</ext-link></comment>
<pub-id pub-id-type="pmid">25234994</pub-id></mixed-citation></ref><ref id="pone.0199443.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Aucouturier</surname><given-names>J-J</given-names></name>, <name><surname>Johansson</surname><given-names>P</given-names></name>, <name><surname>Hall</surname><given-names>L</given-names></name>, <name><surname>Segnini</surname><given-names>R</given-names></name>, <name><surname>Mercadi&#x000e9;</surname><given-names>L</given-names></name>, <name><surname>Watanabe</surname><given-names>K</given-names></name>. <article-title>Covert digital manipulation of vocal emotion alter speakers&#x02019; emotional states in a congruent direction</article-title>. <source>Proc Natl Acad Sci</source>. <year>2016</year>;<volume>113</volume>: <fpage>948</fpage>&#x02013;<lpage>953</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1506552113">10.1073/pnas.1506552113</ext-link></comment>
<pub-id pub-id-type="pmid">26755584</pub-id></mixed-citation></ref><ref id="pone.0199443.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Burleigh</surname><given-names>TJ</given-names></name>, <name><surname>Schoenherr</surname><given-names>JR</given-names></name>, <name><surname>Lacroix</surname><given-names>GL</given-names></name>. <article-title>Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces</article-title>. <source>Comput Human Behav</source>. <year>2013</year>;<volume>29</volume>: <fpage>759</fpage>&#x02013;<lpage>771</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.chb.2012.11.021">10.1016/j.chb.2012.11.021</ext-link></comment></mixed-citation></ref><ref id="pone.0199443.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Hanson</surname><given-names>D</given-names></name>. <article-title>Expanding the Aesthetic Possibilities for Humanoid Robots</article-title>. <source>IEEE-RAS Int Conf humanoid Robot</source>. <year>2005</year>; <fpage>24</fpage>&#x02013;<lpage>31</lpage>.</mixed-citation></ref></ref-list></back></article>